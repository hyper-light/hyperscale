---
ad_number: 50
name: Manager Health Aggregation and Alerting
description: Enable managers to aggregate peer health states and fire threshold-based alerts
---

# AD-50: Manager Health Aggregation and Alerting

**Decision**: Extend `ManagerHealthMonitor` to aggregate peer manager health states and fire threshold-based alerts when datacenter control plane health degrades.

**Related**: AD-18 (Hybrid Overload Detection), AD-33 (Federated Health Monitoring), AD-17 (Smart Dispatch)

**Rationale**:
- Managers already track peer health states via `_peer_manager_health_states`
- Gate-level aggregation exists in `DatacenterHealthManager._aggregate_manager_health_states()`
- Manager-level aggregation is missing, preventing early warning of control plane saturation
- Operators need alerts before DC health degrades to DEGRADED/UNHEALTHY

---

## Part 1: Current State

### What Exists

| Component | Location | Function |
|-----------|----------|----------|
| `_peer_manager_health_states` | `ManagerState` | Stores `dict[str, str]` of peer_id → health_state |
| `_handle_manager_peer_heartbeat()` | `server.py` | Updates peer state from SWIM gossip |
| `_log_peer_manager_health_transition()` | `server.py` | Logs individual peer transitions |
| `_check_aggregate_health_alerts()` | `health.py` | Aggregates worker health (pattern to follow) |
| `_aggregate_manager_health_states()` | `datacenter_health_manager.py` | Gate-level aggregation |

### What's Missing

1. Manager-side aggregation method: `get_peer_manager_health_counts()`
2. Threshold-based alerting: `check_peer_manager_health_alerts()`
3. DC leader tracking: Know when the leader is overloaded
4. Integration into heartbeat processing

---

## Part 2: Architecture

```
                    MANAGER HEALTH AGGREGATION FLOW

  ┌─────────────────────────────────────────────────────────────────┐
  │                     DATACENTER (3 Managers)                      │
  │                                                                  │
  │   Manager A          Manager B           Manager C               │
  │   (Leader)           (Peer)              (Peer)                  │
  │   CPU: 99%           CPU: 45%            CPU: 30%                │
  │   State: OVERLOADED  State: HEALTHY      State: HEALTHY          │
  │                                                                  │
  │         │                 │                   │                  │
  │         └────── SWIM Gossip ──────────────────┘                  │
  │                      │                                           │
  │                      ▼                                           │
  │   ┌──────────────────────────────────────────────────────────┐  │
  │   │              Manager B receives heartbeat                 │  │
  │   │                                                           │  │
  │   │  _peer_manager_health_states = {                         │  │
  │   │      "manager-A": "overloaded",                          │  │
  │   │      "manager-C": "healthy",                             │  │
  │   │  }                                                        │  │
  │   │                                                           │  │
  │   │  get_peer_manager_health_counts() → {                    │  │
  │   │      "healthy": 1, "overloaded": 1                       │  │
  │   │  }                                                        │  │
  │   │                                                           │  │
  │   │  check_peer_manager_health_alerts() →                    │  │
  │   │      ALERT: "DC leader manager-A overloaded"             │  │
  │   └──────────────────────────────────────────────────────────┘  │
  │                                                                  │
  └─────────────────────────────────────────────────────────────────┘
```

---

## Part 3: Alert Thresholds

| Condition | Threshold | Severity | Message |
|-----------|-----------|----------|---------|
| DC leader overloaded | leader_state == "overloaded" | ALERT | "DC leader {id} overloaded - control plane saturated" |
| Majority managers overloaded | overloaded_ratio >= 0.5 | ALERT | "Majority DC managers overloaded ({count}/{total})" |
| High manager stress | non_healthy_ratio >= 0.8 | WARNING | "DC control plane stressed ({ratio}% non-healthy)" |
| All managers non-healthy | healthy_count == 0 | CRITICAL | "All DC managers in non-healthy state" |
| Single peer overloaded | peer transitions to overloaded | WARNING | "Peer manager {id} overloaded" |
| Peer recovered | peer transitions from overloaded | INFO | "Peer manager {id} recovered" |

---

## Part 4: Data Model

### ManagerState Addition

```python
# Track DC leader identity for overload detection
self._dc_leader_manager_id: str | None = None
```

### Alert Configuration (Optional Future Extension)

```python
@dataclass(slots=True)
class ManagerHealthAlertConfig:
    majority_overloaded_threshold: float = 0.5
    high_stress_threshold: float = 0.8
    enable_leader_alerts: bool = True
    enable_peer_alerts: bool = True
```

---

## Part 5: Implementation

### 5.1 ManagerHealthMonitor Methods

```python
def get_peer_manager_health_counts(self) -> dict[str, int]:
    """Aggregate peer manager health states into counts."""
    counts = {"healthy": 0, "busy": 0, "stressed": 0, "overloaded": 0}
    
    for health_state in self._state._peer_manager_health_states.values():
        counts[health_state] = counts.get(health_state, 0) + 1
    
    return counts


def check_peer_manager_health_alerts(
    self,
    dc_leader_id: str | None = None,
) -> None:
    """Check aggregate peer manager health and fire alerts."""
    counts = self.get_peer_manager_health_counts()
    total_peers = sum(counts.values())
    
    if total_peers == 0:
        return
    
    # Check leader overload first (highest priority)
    if dc_leader_id and dc_leader_id in self._state._peer_manager_health_states:
        leader_state = self._state._peer_manager_health_states[dc_leader_id]
        if leader_state == "overloaded":
            self._fire_leader_overload_alert(dc_leader_id)
            return  # Don't spam with multiple alerts
    
    # Check aggregate thresholds
    overloaded_count = counts.get("overloaded", 0)
    healthy_count = counts.get("healthy", 0)
    non_healthy = total_peers - healthy_count
    
    overloaded_ratio = overloaded_count / total_peers
    non_healthy_ratio = non_healthy / total_peers
    
    if healthy_count == 0:
        self._fire_all_managers_unhealthy_alert(counts, total_peers)
    elif overloaded_ratio >= 0.5:
        self._fire_majority_overloaded_alert(overloaded_count, total_peers)
    elif non_healthy_ratio >= 0.8:
        self._fire_high_stress_alert(counts, total_peers, non_healthy_ratio)
```

### 5.2 Integration Point

In `_handle_manager_peer_heartbeat()`:

```python
# After updating peer health state
if previous_peer_state != peer_health_state:
    self._log_peer_manager_health_transition(...)
    
    # Fire aggregate alerts
    self._health_monitor.check_peer_manager_health_alerts(
        dc_leader_id=self._manager_state._dc_leader_manager_id,
    )
```

### 5.3 Leader Tracking

In `_handle_manager_peer_heartbeat()`:

```python
# Track DC leader identity
if heartbeat.is_leader:
    self._manager_state._dc_leader_manager_id = peer_id
```

---

## Part 6: Files Modified

| File | Change |
|------|--------|
| `nodes/manager/state.py` | Add `_dc_leader_manager_id` field |
| `nodes/manager/health.py` | Add `get_peer_manager_health_counts()`, `check_peer_manager_health_alerts()`, alert firing methods |
| `nodes/manager/server.py` | Update `_handle_manager_peer_heartbeat()` to track leader and call alerts |

---

## Part 7: Design Principles

1. **Reuse existing patterns**: Mirror `_check_aggregate_health_alerts()` for workers
2. **Single responsibility**: Each alert method fires one type of alert
3. **Low cyclomatic complexity**: Use early returns, avoid nested conditions
4. **Asyncio compatible**: Alert methods are sync but use task_runner for async logging
5. **No alert spam**: Return after firing highest-priority alert

---

## Part 8: Alert Suppression (Future)

To prevent alert storms:
- Track `_last_peer_alert_time` and enforce cooldown
- Use exponential backoff for repeated alerts
- Aggregate multiple peer failures into single alert

Not implemented in initial version for simplicity.
