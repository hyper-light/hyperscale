---
ad_number: 47
name: Worker Event Log for Crash Forensics and Observability
description: Append-only event log for workers using existing Logger infrastructure for audit trail and debugging
---

# AD-47: Worker Event Log for Crash Forensics and Observability

**Decision**: Implement an append-only event log for workers using the existing `hyperscale/logging` Logger infrastructure. This provides crash forensics and observability without adding durability overhead to the hot execution path.

**Related**: AD-38 (Global Job Ledger), AD-33 (Federated Health Monitoring)

**Rationale**:
- Workers are stateless executors under heavy CPU/memory load during tests
- Per AD-38, workers have NO durability responsibility - recovery is handled by Manager reassignment
- However, crash forensics ("What was the worker doing when it died?") is valuable for debugging
- Existing Logger provides async writes, file rotation, retention policies - no need to build new infrastructure
- Fire-and-forget semantics (no fsync, drop on overflow) keeps worker execution path fast

---

## Part 1: Architecture Overview

```
┌─────────────────────────────────────────────────────────────────────────┐
│                           WORKER NODE                                    │
│                                                                          │
│  ┌─────────────────────────────────────────────────────────────────┐    │
│  │                     WorkerServer                                 │    │
│  │                                                                  │    │
│  │   ┌─────────────┐    ┌─────────────┐    ┌─────────────┐        │    │
│  │   │ Job Handler │    │Action Runner│    │Health Check │        │    │
│  │   └──────┬──────┘    └──────┬──────┘    └──────┬──────┘        │    │
│  │          │                  │                  │                │    │
│  │          │  emit event      │  emit event      │  emit event   │    │
│  │          ▼                  ▼                  ▼                │    │
│  │   ┌─────────────────────────────────────────────────────┐      │    │
│  │   │              _event_logger: Logger                   │      │    │
│  │   │         (fire-and-forget, async writes)             │      │    │
│  │   └──────────────────────┬──────────────────────────────┘      │    │
│  │                          │                                      │    │
│  └──────────────────────────┼──────────────────────────────────────┘    │
│                             │                                            │
│                             ▼                                            │
│  ┌─────────────────────────────────────────────────────────────────┐    │
│  │                    Event Log Files                               │    │
│  │  ┌──────────────────────────────────────────────────────────┐   │    │
│  │  │ events.jsonl (current)                                    │   │    │
│  │  │ {"ts":"...","entry":{"type":"WorkerJobReceived",...}}    │   │    │
│  │  │ {"ts":"...","entry":{"type":"WorkerActionStarted",...}}  │   │    │
│  │  │ {"ts":"...","entry":{"type":"WorkerActionCompleted",...}}│   │    │
│  │  └──────────────────────────────────────────────────────────┘   │    │
│  │  ┌──────────────────────────────────────────────────────────┐   │    │
│  │  │ events_1736697600_archived.zst (rotated, compressed)     │   │    │
│  │  └──────────────────────────────────────────────────────────┘   │    │
│  └─────────────────────────────────────────────────────────────────┘    │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘
```

---

## Part 2: Comparison with WAL (AD-38)

| Aspect | WAL (Gate/Manager) | Event Log (Worker) |
|--------|--------------------|--------------------|
| **Purpose** | Crash recovery, state reconstruction | Crash forensics, observability |
| **Durability** | fsync on every write | Buffered, best-effort (FLUSH mode) |
| **Blocking** | Caller may wait for disk | Fire-and-forget |
| **Recovery** | Replay on restart | No replay - just audit trail |
| **Checkpointing** | Yes (compaction) | No (rotation only) |
| **Backpressure** | Yes (propagates to caller) | Drop on overflow |
| **Format** | Binary with CRC | JSON (human-readable, tooling-friendly) |
| **Infrastructure** | Custom NodeWAL | Existing Logger |

**Key Insight**: Workers don't need durability guarantees because:
1. Manager tracks workflow state and handles recovery via reassignment
2. If worker crashes, Manager detects via health check and reschedules
3. In-flight execution progress isn't recoverable anyway (can't resume half-executed HTTP request)

---

## Part 3: Event Model Design

### Design Principles

1. **Type-safe**: Separate Entry class per event type (not generic `event_type: str` field)
2. **Consistent fields**: All events share `node_id`, `node_host`, `node_port` for correlation
3. **Level-appropriate**: TRACE for high-volume (action start/complete), INFO for lifecycle events
4. **Follows existing patterns**: Uses `Entry` with `kw_only=True` like other models in `hyperscale_logging_models.py`

### Event Categories

```
┌─────────────────────────────────────────────────────────────────────────┐
│                         WORKER EVENTS                                    │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│  LIFECYCLE EVENTS (INFO level)                                          │
│  ├── WorkerStarted      - Worker process initialized                    │
│  └── WorkerStopping     - Worker shutting down (graceful or forced)     │
│                                                                          │
│  JOB EVENTS (INFO/ERROR level)                                          │
│  ├── WorkerJobReceived  - Job dispatch received from Manager            │
│  ├── WorkerJobStarted   - Job execution beginning                       │
│  ├── WorkerJobCompleted - Job finished successfully                     │
│  └── WorkerJobFailed    - Job failed with error                         │
│                                                                          │
│  ACTION EVENTS (TRACE/WARN level)                                       │
│  ├── WorkerActionStarted   - Individual action beginning                │
│  ├── WorkerActionCompleted - Action finished (with duration)            │
│  └── WorkerActionFailed    - Action failed (with error type)            │
│                                                                          │
│  HEALTH EVENTS (TRACE/DEBUG level)                                      │
│  ├── WorkerHealthcheckReceived - Health probe from Manager              │
│  └── WorkerExtensionRequested  - Deadline extension requested (AD-26)   │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘
```

### Event Model Definitions

```python
# hyperscale/logging/hyperscale_logging_models.py

# --- Worker Lifecycle Events ---

class WorkerStarted(Entry, kw_only=True):
    node_id: str
    node_host: str
    node_port: int
    manager_host: str | None = None
    manager_port: int | None = None
    level: LogLevel = LogLevel.INFO


class WorkerStopping(Entry, kw_only=True):
    node_id: str
    node_host: str
    node_port: int
    reason: str | None = None
    level: LogLevel = LogLevel.INFO


# --- Worker Job Events ---

class WorkerJobReceived(Entry, kw_only=True):
    node_id: str
    node_host: str
    node_port: int
    job_id: str
    workflow_id: str
    source_manager_host: str
    source_manager_port: int
    level: LogLevel = LogLevel.INFO


class WorkerJobStarted(Entry, kw_only=True):
    node_id: str
    node_host: str
    node_port: int
    job_id: str
    workflow_id: str
    level: LogLevel = LogLevel.INFO


class WorkerJobCompleted(Entry, kw_only=True):
    node_id: str
    node_host: str
    node_port: int
    job_id: str
    workflow_id: str
    duration_ms: float
    level: LogLevel = LogLevel.INFO


class WorkerJobFailed(Entry, kw_only=True):
    node_id: str
    node_host: str
    node_port: int
    job_id: str
    workflow_id: str
    error_type: str
    duration_ms: float
    level: LogLevel = LogLevel.ERROR


# --- Worker Action Events ---

class WorkerActionStarted(Entry, kw_only=True):
    node_id: str
    node_host: str
    node_port: int
    job_id: str
    action_name: str
    level: LogLevel = LogLevel.TRACE


class WorkerActionCompleted(Entry, kw_only=True):
    node_id: str
    node_host: str
    node_port: int
    job_id: str
    action_name: str
    duration_ms: float
    level: LogLevel = LogLevel.TRACE


class WorkerActionFailed(Entry, kw_only=True):
    node_id: str
    node_host: str
    node_port: int
    job_id: str
    action_name: str
    error_type: str
    duration_ms: float
    level: LogLevel = LogLevel.WARN


# --- Worker Health Events ---

class WorkerHealthcheckReceived(Entry, kw_only=True):
    node_id: str
    node_host: str
    node_port: int
    source_host: str
    source_port: int
    level: LogLevel = LogLevel.TRACE


class WorkerExtensionRequested(Entry, kw_only=True):
    node_id: str
    node_host: str
    node_port: int
    job_id: str
    requested_seconds: float
    level: LogLevel = LogLevel.DEBUG
```

---

## Part 4: Logger Configuration

### Configuration Parameters

| Parameter | Value | Rationale |
|-----------|-------|-----------|
| `durability` | `DurabilityMode.FLUSH` | Best-effort writes, no fsync overhead |
| `log_format` | `"json"` | Human-readable, tooling-friendly |
| `max_size` | `"50MB"` | Reasonable rotation size |
| `max_age` | `"24h"` | Keep recent history for debugging |

### WorkerConfig Addition

```python
# hyperscale/distributed/nodes/worker/config.py

from pathlib import Path

@dataclass(slots=True)
class WorkerConfig:
    # ... existing fields ...
    
    # Event log configuration (AD-47)
    event_log_dir: Path | None = None
```

### Logger Initialization

```python
# hyperscale/distributed/nodes/worker/server.py

from hyperscale.logging import Logger
from hyperscale.logging.config import DurabilityMode

class WorkerServer:
    def __init__(self, ...):
        # ... existing init ...
        self._event_logger: Logger | None = None
    
    async def start(self) -> None:
        # ... existing start logic ...
        
        # Initialize event logger if configured (AD-47)
        if self._config.event_log_dir is not None:
            self._event_logger = Logger()
            self._event_logger.configure(
                name="worker_events",
                path=str(self._config.event_log_dir / "events.jsonl"),
                durability=DurabilityMode.FLUSH,
                log_format="json",
                retention_policy={
                    "max_size": "50MB",
                    "max_age": "24h",
                },
            )
            
            # Log startup event
            await self._event_logger.log(
                WorkerStarted(
                    message="Worker started",
                    node_id=self._node_id.full,
                    node_host=self._host,
                    node_port=self._tcp_port,
                    manager_host=self._manager_addr[0] if self._manager_addr else None,
                    manager_port=self._manager_addr[1] if self._manager_addr else None,
                ),
                name="worker_events",
            )
    
    async def stop(self) -> None:
        # Log shutdown event
        if self._event_logger is not None:
            await self._event_logger.log(
                WorkerStopping(
                    message="Worker stopping",
                    node_id=self._node_id.full,
                    node_host=self._host,
                    node_port=self._tcp_port,
                    reason="graceful_shutdown",
                ),
                name="worker_events",
            )
            await self._event_logger.close()
        
        # ... existing stop logic ...
```

---

## Part 5: Event Emission Points

### Job Lifecycle Events

```python
# In job dispatch handler
async def _handle_workflow_dispatch(self, dispatch: WorkflowDispatch, addr: tuple[str, int]) -> None:
    if self._event_logger:
        await self._event_logger.log(
            WorkerJobReceived(
                message=f"Received job {dispatch.job_id}",
                node_id=self._node_id.full,
                node_host=self._host,
                node_port=self._tcp_port,
                job_id=dispatch.job_id,
                workflow_id=dispatch.workflow_id,
                source_manager_host=addr[0],
                source_manager_port=addr[1],
            ),
            name="worker_events",
        )
    
    # ... existing dispatch handling ...
```

### Action Execution Events

```python
# In action execution loop
async def _execute_action(self, action: Action, job_id: str) -> ActionResult:
    start_time = time.monotonic()
    
    if self._event_logger:
        await self._event_logger.log(
            WorkerActionStarted(
                message=f"Starting action {action.name}",
                node_id=self._node_id.full,
                node_host=self._host,
                node_port=self._tcp_port,
                job_id=job_id,
                action_name=action.name,
            ),
            name="worker_events",
        )
    
    try:
        result = await action.execute()
        duration_ms = (time.monotonic() - start_time) * 1000
        
        if self._event_logger:
            await self._event_logger.log(
                WorkerActionCompleted(
                    message=f"Completed action {action.name}",
                    node_id=self._node_id.full,
                    node_host=self._host,
                    node_port=self._tcp_port,
                    job_id=job_id,
                    action_name=action.name,
                    duration_ms=duration_ms,
                ),
                name="worker_events",
            )
        
        return result
        
    except Exception as e:
        duration_ms = (time.monotonic() - start_time) * 1000
        
        if self._event_logger:
            await self._event_logger.log(
                WorkerActionFailed(
                    message=f"Action {action.name} failed: {type(e).__name__}",
                    node_id=self._node_id.full,
                    node_host=self._host,
                    node_port=self._tcp_port,
                    job_id=job_id,
                    action_name=action.name,
                    error_type=type(e).__name__,
                    duration_ms=duration_ms,
                ),
                name="worker_events",
            )
        
        raise
```

---

## Part 6: Output Format

### JSON Lines Format (NDJSON)

Each line is a complete JSON object, enabling easy `tail -f`, `grep`, and streaming:

```json
{"timestamp":"2026-01-12T19:30:00.123Z","entry":{"type":"WorkerStarted","node_id":"worker-abc123","node_host":"10.0.1.5","node_port":8080,"manager_host":"10.0.1.1","manager_port":9000,"level":"INFO","message":"Worker started"}}
{"timestamp":"2026-01-12T19:30:01.456Z","entry":{"type":"WorkerJobReceived","node_id":"worker-abc123","node_host":"10.0.1.5","node_port":8080,"job_id":"j-xyz789","workflow_id":"wf-001","source_manager_host":"10.0.1.1","source_manager_port":9000,"level":"INFO","message":"Received job j-xyz789"}}
{"timestamp":"2026-01-12T19:30:01.460Z","entry":{"type":"WorkerActionStarted","node_id":"worker-abc123","node_host":"10.0.1.5","node_port":8080,"job_id":"j-xyz789","action_name":"login","level":"TRACE","message":"Starting action login"}}
{"timestamp":"2026-01-12T19:30:02.789Z","entry":{"type":"WorkerActionCompleted","node_id":"worker-abc123","node_host":"10.0.1.5","node_port":8080,"job_id":"j-xyz789","action_name":"login","duration_ms":1329.0,"level":"TRACE","message":"Completed action login"}}
```

### File Rotation

Logger handles rotation automatically via retention policy:

```
event_log_dir/
├── events.jsonl                          # Current log file
├── events_1736697600_archived.zst       # Rotated + compressed
├── events_1736611200_archived.zst       # Older
└── events_1736524800_archived.zst       # Oldest (will be cleaned up by max_age)
```

---

## Part 7: Performance Characteristics

### Hot Path Impact

| Operation | Overhead | Notes |
|-----------|----------|-------|
| Event creation | ~1μs | Dataclass instantiation |
| Logger.log() call | ~5μs | Queue put, no I/O in caller |
| Background write | Async | Doesn't block caller |
| Disk I/O | Batched | Multiple events per write() |

### Memory Bounds

| Component | Bound | Rationale |
|-----------|-------|-----------|
| In-memory buffer | ~1000 entries | Logger internal queue |
| Per-event size | ~500 bytes JSON | Reasonable event size |
| Max buffer memory | ~500KB | Bounded, won't OOM |

### Overflow Behavior

If background writer falls behind:
1. Logger buffer fills
2. New events dropped (not blocking caller)
3. Worker execution continues unimpeded

This is **intentional** - worker execution must never be blocked by logging.

---

## Part 8: Debugging Workflows

### Scenario 1: Worker Crash Investigation

```bash
# Find what worker was doing when it died
tail -100 /var/log/hyperscale/worker/events.jsonl | jq 'select(.entry.type | startswith("Worker"))'

# Find last action before crash
grep "WorkerAction" /var/log/hyperscale/worker/events.jsonl | tail -5
```

### Scenario 2: Slow Action Detection

```bash
# Find actions taking > 5 seconds
cat events.jsonl | jq 'select(.entry.duration_ms > 5000)'
```

### Scenario 3: Job Timeline Reconstruction

```bash
# Reconstruct timeline for specific job
grep "j-xyz789" events.jsonl | jq -s 'sort_by(.timestamp)'
```

### Scenario 4: Real-time Monitoring

```bash
# Stream events as they happen
tail -f events.jsonl | jq --unbuffered '.entry | "\(.type): \(.message)"'
```

---

## Part 9: Integration with External Systems

### Shipping to Central Logging

Event log files can be shipped to central logging systems:

```yaml
# Example: Filebeat configuration
filebeat.inputs:
  - type: log
    paths:
      - /var/log/hyperscale/worker/events.jsonl
    json.keys_under_root: true
    json.add_error_key: true
    
output.elasticsearch:
  hosts: ["elasticsearch:9200"]
  index: "hyperscale-worker-events-%{+yyyy.MM.dd}"
```

### Metrics Extraction

Events can be parsed for Prometheus metrics:

```python
# Example: Event-based metrics
worker_actions_total = Counter('worker_actions_total', 'Total actions', ['action_name', 'status'])
worker_action_duration = Histogram('worker_action_duration_ms', 'Action duration', ['action_name'])

# Parse events and emit metrics
for event in parse_events(event_file):
    if event.type == "WorkerActionCompleted":
        worker_actions_total.labels(action_name=event.action_name, status="success").inc()
        worker_action_duration.labels(action_name=event.action_name).observe(event.duration_ms)
```

---

## Part 10: Files Modified

| File | Change |
|------|--------|
| `hyperscale/logging/hyperscale_logging_models.py` | Add 11 worker event Entry classes |
| `hyperscale/distributed/nodes/worker/config.py` | Add `event_log_dir: Path \| None` field |
| `hyperscale/distributed/nodes/worker/server.py` | Initialize Logger, emit events at key points |

---

## Part 11: Anti-Patterns to Avoid

**DO NOT**:

```python
# Block on event logging
await self._event_logger.log(...).wait()  # WRONG - blocks caller

# Use fsync mode
durability=DurabilityMode.FSYNC  # WRONG - adds latency to hot path

# Create new Entry types per log message
class WorkerActionLoginStarted(Entry): ...  # WRONG - use generic WorkerActionStarted
class WorkerActionLogoutStarted(Entry): ... # WRONG - action_name field handles this

# Log at high frequency without throttling
for item in million_items:
    await self._event_logger.log(...)  # WRONG - will overwhelm logger
```

**DO**:

```python
# Fire-and-forget event logging
if self._event_logger:
    await self._event_logger.log(event, name="worker_events")

# Use FLUSH mode (default)
durability=DurabilityMode.FLUSH

# Use generic event types with discriminating fields
WorkerActionStarted(action_name="login", ...)
WorkerActionStarted(action_name="logout", ...)

# Log meaningful boundaries, not every iteration
await self._event_logger.log(WorkerJobReceived(...))  # Once per job
# ... execute many actions ...
await self._event_logger.log(WorkerJobCompleted(...))  # Once per job
```

---

## Part 12: Testing Strategy

1. **Unit tests**: Verify event models serialize correctly to JSON
2. **Integration tests**: Verify Logger writes events to file with rotation
3. **Load tests**: Verify event logging doesn't impact worker execution latency
4. **Failure tests**: Verify worker continues executing if logger fails/overflows
