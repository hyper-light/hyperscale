---
ad_number: 39
name: Logger Extension for AD-38 WAL Compliance
description: Extends Logger with optional WAL features including fsync, binary format, and sequence numbers.
---

# AD-39: Logger Extension for AD-38 WAL Compliance

**Decision**: Extend the existing `hyperscale/logging` Logger with optional WAL-compliant features (durability modes, binary format, sequence numbers, read-back) while maintaining full backward compatibility with existing usage patterns.

**Related**: AD-38 (Global Job Ledger), AD-20 (Cancellation)

**Rationale**:
- AD-38 identified that Logger is unsuitable for Control Plane WAL due to missing fsync, sequence numbers, and read-back capability.
- However, creating a completely separate NodeWAL class duplicates async I/O patterns already proven in Logger.
- By extending Logger with **optional** WAL features, we achieve code reuse, consistent API patterns, and progressive enhancement.
- All existing Logger usage (Data Plane stats) continues unchanged with default parameters.
- New WAL use cases opt-in to durability features via new parameters.

---

## Part 1: Current Logger Architecture Analysis

### 1.1 Current Usage Patterns

All Logger file usage follows a consistent pattern across the codebase:

```python
# Pattern 1: Configure then use context
self._logger.configure(
    name="context_name",
    path="hyperscale.leader.log.json",
    template="{timestamp} - {level} - {...} - {message}",
    models={...},
)

async with self._logger.context(name="context_name") as ctx:
    await ctx.log(Entry(message="...", level=LogLevel.INFO))
```

### 1.2 Critical Gap: `_write_to_file` Implementation

```python
# CURRENT IMPLEMENTATION (INSUFFICIENT for WAL):
logfile.write(msgspec.json.encode(log) + b"\n")  # JSON only
logfile.flush()  # NO fsync - data can be lost!
```

**Problems for WAL**:
1. **No fsync** - `flush()` only pushes to OS buffer, not disk
2. **JSON only** - No binary format with CRC checksums
3. **No LSN** - No sequence number generation
4. **Write-only** - No read-back for recovery
5. **Errors swallowed** - Silent failures unacceptable for WAL

---

## Part 2: Extension Design

### 2.1 Design Principles

1. **Additive Only** - New optional parameters with backward-compatible defaults
2. **Zero Breaking Changes** - All existing code works unchanged
3. **Progressive Enhancement** - Enable WAL features per-context as needed
4. **Single Responsibility** - Each new feature independently toggleable
5. **Consistent Patterns** - Same `context()` API already familiar to codebase

### 2.2 New Configuration Enum

```python
class DurabilityMode(IntEnum):
    """
    Durability levels for log writes.
    """
    NONE = 0         # No sync (testing only)
    FLUSH = 1        # Current behavior - flush() to OS buffer
    FSYNC = 2        # fsync per write (safest, ~1-10ms latency)
    FSYNC_BATCH = 3  # Batched fsync every N writes or T ms
```

### 2.3 API Extension

```
Logger.context() - EXTENDED

EXISTING PARAMETERS (unchanged):
- name: str | None = None
- template: str | None = None
- path: str | None = None
- retention_policy: RetentionPolicyConfig | None = None
- nested: bool = False
- models: dict[...] | None = None

NEW PARAMETERS (all optional, defaults = current behavior):
- durability: DurabilityMode = DurabilityMode.FLUSH    # NEW
- format: Literal['json', 'binary'] = 'json'           # NEW
- enable_lsn: bool = False                             # NEW
- instance_id: int = 0                                 # NEW
```

### 2.4 Usage Comparison

```python
# =====================================================================
# EXISTING CODE - COMPLETELY UNCHANGED (Data Plane - stats)
# =====================================================================

async with self._logger.context(
    name="remote_graph_manager",
    path="hyperscale.leader.log.json",
    template="{timestamp} - {level} - {...} - {message}",
) as ctx:
    await ctx.log(Entry(message="Stats update", level=LogLevel.INFO))
    # Uses: JSON format, flush() only, no LSN
    # Behavior: IDENTICAL to current implementation


# =====================================================================
# NEW CODE - WAL MODE (Control Plane - job/workflow commands)
# =====================================================================

async with self._logger.context(
    name="node_wal",
    path="hyperscale.wal.log",               # Can use .wal extension
    durability=DurabilityMode.FSYNC_BATCH,   # NEW: Batched fsync
    format='binary',                          # NEW: Binary with CRC
    enable_lsn=True,                          # NEW: Sequence numbers
    instance_id=self._node_id,                # NEW: For snowflake LSN
) as ctx:
    lsn = await ctx.log(WALEntry(...))
    # Uses: Binary format, CRC32 checksum, fsync, LSN tracking
    # Returns: LSN for replication tracking
```

---

## Part 3: LoggerStream Modifications

### 3.1 Binary Encoding with CRC

```python
def _encode_binary(self, log: Log, lsn: LSN | None) -> bytes:
    """
    Encode log entry in binary format with CRC32 checksum.

    Binary Format (128-bit LSN):
    +----------+----------+----------+---------------------+
    | CRC32    | Length   | LSN      | Payload (msgpack)   |
    | (4 bytes)| (4 bytes)| (16 bytes)| (variable)         |
    +----------+----------+----------+---------------------+

    Total header: 24 bytes
    CRC32 covers: length + LSN + payload
    
    LSN is 128-bit Hybrid Lamport Timestamp (see Part 11).
    """
```

### 3.2 Read-Back for Recovery

```python
async def read_entries(
    self,
    logfile_path: str,
    from_offset: int = 0,
) -> AsyncIterator[tuple[int, Log, int | None]]:
    """
    Read entries from file for WAL recovery.

    Yields tuples of (file_offset, log_entry, lsn).
    Handles both JSON and binary formats based on self._format.
    """
```

### 3.3 Batched Fsync

```python
async def _schedule_batch_fsync(self, logfile_path: str) -> None:
    """
    Schedule entry for batch fsync.

    Batches are flushed when:
    - batch_max_size entries accumulated, OR
    - batch_timeout_ms elapsed since first entry

    This provides ~10x throughput improvement over per-write fsync
    while maintaining bounded latency.
    """
```

---

## Part 4: Log Model Extension

### 4.1 Add Optional LSN Field

```python
@dataclass
class Log(Generic[T]):
    """
    Wrapper around log entries with metadata.
    Extended with optional LSN for WAL use cases.
    """
    entry: T
    filename: str | None = None
    function_name: str | None = None
    line_number: int | None = None
    thread_id: int | None = None
    timestamp: str | None = None

    # NEW: Optional 128-bit Hybrid Lamport LSN for WAL entries
    lsn: LSN | None = field(default=None)
```

---

## Part 5: Provider WAL Architecture

### 5.1 Problem Statement

WAL systems face competing requirements:

| Requirement | Constraint |
|-------------|------------|
| **Durability** | Every entry MUST be persisted - no drops |
| **Memory Safety** | Bounded memory usage - unbounded queues cause OOM in K8s |
| **No Silent Failures** | Errors must propagate to callers |
| **Performance** | High throughput via batching |
| **Atomic Fan-out** | Multiple consumers must see same entries consistently |
| **Failure Isolation** | One slow/crashed consumer must not affect others |

The original push-based architecture (provider pushes to consumer queues) has fundamental problems:

1. **Partial delivery**: If provider crashes mid-fanout, some consumers have the entry, others don't
2. **No replay**: Crashed consumer loses its queue contents
3. **Coupled failure**: Slow consumer blocks provider, affecting all consumers

### 5.2 Solution: Provider WAL with Pull-Based Consumers

The solution is a **pull-based architecture** where:

1. **Provider owns a bounded ring buffer (WAL)** - single source of truth
2. **Consumers pull from WAL at their own pace** - independent progress
3. **Consumers track and acknowledge their position** - enables replay on failure
4. **WAL advances when ALL consumers acknowledge** - no premature discard

This is the same pattern used by Kafka, Pulsar, etcd, and every serious message broker.

### 5.3 Architecture Diagram

```
                                    ┌─────────────────┐
                                    │    Producer     │
                                    │  (application)  │
                                    └────────┬────────┘
                                             │ append()
                                             ▼
┌────────────────────────────────────────────────────────────────────────────┐
│                              LogProvider                                    │
│  ┌───────────────────────────────────────────────────────────────────────┐ │
│  │                          Provider WAL (Ring Buffer)                    │ │
│  │                                                                        │ │
│  │  ┌─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┐       │ │
│  │  │ E0  │ E1  │ E2  │ E3  │ E4  │ E5  │     │     │     │     │       │ │
│  │  └─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┘       │ │
│  │    ▲                                   ▲                               │ │
│  │    │                                   │                               │ │
│  │  head=0                             tail=6                             │ │
│  │  (oldest unacked)                   (next write)                       │ │
│  │                                                                        │ │
│  │  Consumer Positions:                                                   │ │
│  │    file_writer: 4  ─────────────────────┐                             │ │
│  │    subscriber_a: 2  ────────────┐       │                             │ │
│  │    subscriber_b: 6  ◄── caught up       │                             │ │
│  │                                 │       │                             │ │
│  │  min_position = 2 (subscriber_a is slowest)                           │ │
│  │  head cannot advance past 2                                            │ │
│  └───────────────────────────────────────────────────────────────────────┘ │
│                    │              │              │                          │
│            pull    │       pull   │       pull   │                          │
│                    ▼              ▼              ▼                          │
│  ┌──────────────────┐  ┌──────────────────┐  ┌──────────────────┐         │
│  │   File Writer    │  │  Subscriber A    │  │  Subscriber B    │         │
│  │   (batched I/O)  │  │  (external sub)  │  │  (external sub)  │         │
│  │                  │  │                  │  │                  │         │
│  │  local_buf: 100  │  │  local_buf: 100  │  │  local_buf: 100  │         │
│  └────────┬─────────┘  └──────────────────┘  └──────────────────┘         │
│           │                                                                │
└───────────┼────────────────────────────────────────────────────────────────┘
            │
            ▼
        [Disk]
```

### 5.4 Why Pull-Based is Correct

| Property | Push Model (Original) | Pull Model (Provider WAL) |
|----------|----------------------|---------------------------|
| **Atomicity** | ❌ Partial delivery possible | ✅ Entry in WAL or not |
| **Consistency** | ❌ Consumers may diverge | ✅ All read from same WAL |
| **Backpressure source** | Slowest consumer blocks push | Slowest consumer blocks WAL head advancement |
| **Failure isolation** | Consumer crash mid-push = inconsistent | Consumer crash = restart from last ack |
| **Recovery** | None | Replay from last acknowledged position |
| **Memory bound** | N × consumer_queue_size | WAL_size + N × local_buffer_size |
| **Ordering guarantee** | Per-consumer only | Global (WAL sequence) |

### 5.5 State Diagram: Producer Append

```
                                    ┌─────────────────────────────────────────┐
                                    │                                         │
                                    ▼                                         │
┌──────────────┐ append() ┌─────────────────┐   WAL has space   ┌────────────┴───────┐
│   Producer   │ ────────►│  Check WAL      │ ─────────────────►│  Write to WAL      │
│   (caller)   │          │  Capacity       │                   │  Return seq number │
└──────────────┘          └─────────────────┘                   └────────────────────┘
                                    │
                                    │ WAL full (tail - head >= max_size)
                                    ▼
                          ┌─────────────────┐
                          │ Advance Head    │ ◄─── discard entries all consumers acked
                          │ (if possible)   │
                          └─────────────────┘
                                    │
                    ┌───────────────┴───────────────┐
                    │                               │
                    ▼                               ▼
          ┌─────────────────┐             ┌─────────────────┐
          │  Space Freed    │             │  Still Full     │
          │  (write entry)  │             │  (block + wait) │
          └─────────────────┘             └─────────────────┘
                                                    │
                                    ┌───────────────┴───────────────┐
                                    │                               │
                                    ▼                               ▼
                          ┌─────────────────┐             ┌─────────────────┐
                          │  Consumer Acks  │             │ Timeout Expired │
                          │  (space freed)  │             │ (raise error)   │
                          └─────────────────┘             └─────────────────┘
                                    │                               │
                                    ▼                               ▼
                          ┌─────────────────┐             ┌─────────────────┐
                          │  Write Entry    │             │ WALBackpressure │
                          │  (success)      │             │     Error       │
                          └─────────────────┘             └─────────────────┘
```

### 5.6 State Diagram: Consumer Pull

```
┌────────────────────────────────────────────────────────────────────────────┐
│                           Consumer Pull Loop                                │
└────────────────────────────────────────────────────────────────────────────┘
                                    │
                                    ▼
                          ┌─────────────────┐
                          │  Read from WAL  │ ◄─── at current position
                          │  (my_position)  │
                          └─────────────────┘
                                    │
                    ┌───────────────┴───────────────┐
                    │                               │
                    ▼                               ▼
          ┌─────────────────┐             ┌─────────────────┐
          │ Entry Available │             │  Caught Up      │
          │ (seq < tail)    │             │  (seq >= tail)  │
          └─────────────────┘             └─────────────────┘
                    │                               │
                    ▼                               ▼
          ┌─────────────────┐             ┌─────────────────┐
          │ Add to Local    │             │  Wait for       │
          │ Buffer          │             │  New Entry      │
          └─────────────────┘             └─────────────────┘
                    │                               │
                    ▼                               │
          ┌─────────────────┐                       │
          │ Buffer Full or  │                       │
          │ Batch Timeout?  │                       │
          └─────────────────┘                       │
                    │ yes                           │
                    ▼                               │
          ┌─────────────────┐                       │
          │ Process Batch   │                       │
          │ (write/forward) │                       │
          └─────────────────┘                       │
                    │                               │
                    ▼                               │
          ┌─────────────────┐                       │
          │ Acknowledge     │                       │
          │ (update pos)    │                       │
          └─────────────────┘                       │
                    │                               │
                    └───────────────┬───────────────┘
                                    │
                                    └──────────► (loop back to read)
```

### 5.7 Sequence Diagram: Normal Operation

```
Producer              Provider WAL              File Writer            Subscriber A
   │                       │                        │                       │
   │─── append(E1) ───────►│                        │                       │
   │◄── seq=0 ─────────────│                        │                       │
   │                       │                        │                       │
   │─── append(E2) ───────►│                        │                       │
   │◄── seq=1 ─────────────│                        │                       │
   │                       │                        │                       │
   │                       │◄─── read_from(0) ──────│                       │
   │                       │──── (0, E1) ──────────►│                       │
   │                       │──── (1, E2) ──────────►│                       │
   │                       │                        │                       │
   │                       │                        │── write + fsync ─────►│
   │                       │                        │                       │
   │                       │◄─── ack(1) ────────────│                       │
   │                       │                        │                       │
   │                       │◄─── read_from(0) ──────┼───────────────────────│
   │                       │────────────────────────┼──── (0, E1) ─────────►│
   │                       │────────────────────────┼──── (1, E2) ─────────►│
   │                       │                        │                       │
   │                       │◄─── ack(1) ────────────┼───────────────────────│
   │                       │                        │                       │
   │                       │ (all consumers at 2,   │                       │
   │                       │  head advances to 2)   │                       │
   │                       │                        │                       │
```

### 5.8 Sequence Diagram: Slow Consumer Backpressure

```
Producer              Provider WAL              Fast Consumer          Slow Consumer
   │                       │                        │                       │
   │  (WAL filling up,     │                        │                       │
   │   slow consumer at 0, │                        │                       │
   │   fast consumer at    │                        │                       │
   │   9999)               │                        │                       │
   │                       │                        │                       │
   │─── append(E10000) ───►│                        │                       │
   │                       │ (WAL FULL)             │                       │
   │                       │ (cannot advance head,  │                       │
   │                       │  slow consumer at 0)   │                       │
   │         ┊             │                        │                       │
   │  (BLOCKED waiting     │                        │                       │
   │   for slow consumer)  │                        │                       │
   │         ┊             │                        │◄── read_from(0) ──────│
   │         ┊             │                        │                       │
   │         ┊             │                        │    (0, E0) ──────────►│
   │         ┊             │                        │    ...                │
   │         ┊             │                        │    (999, E999) ──────►│
   │         ┊             │                        │                       │
   │         ┊             │◄─── ack(999) ──────────┼───────────────────────│
   │         ┊             │                        │                       │
   │         ┊             │ (head advances to 1000)│                       │
   │         ┊             │ (space available)      │                       │
   │◄── seq=10000 ─────────│                        │                       │
   │                       │                        │                       │
```

### 5.9 Sequence Diagram: Consumer Crash Recovery

```
Producer              Provider WAL              Consumer (crashes)         Consumer (restarts)
   │                       │                        │                              │
   │─── append(E0-E99) ───►│                        │                              │
   │                       │                        │                              │
   │                       │◄─── read_from(0) ──────│                              │
   │                       │──── (0-49) ───────────►│                              │
   │                       │◄─── ack(49) ───────────│                              │
   │                       │                        │                              │
   │                       │◄─── read_from(50) ─────│                              │
   │                       │──── (50-74) ──────────►│                              │
   │                       │                        │                              │
   │                       │                        X (CRASH - no ack sent)        │
   │                       │                        │                              │
   │                       │ (consumer position     │                              │
   │                       │  still at 50)          │                              │
   │                       │                        │                              │
   │                       │                        │    (restart, reconnect)      │
   │                       │                        │                              │
   │                       │◄─── register() ────────┼──────────────────────────────│
   │                       │──── pos=50 ────────────┼─────────────────────────────►│
   │                       │                        │                              │
   │                       │◄─── read_from(50) ─────┼──────────────────────────────│
   │                       │────────────────────────┼───── (50-99) ───────────────►│
   │                       │                        │                              │
   │                       │ (entries 50-74 replayed│                              │
   │                       │  - exactly once with   │                              │
   │                       │  idempotent processing)│                              │
```

---

## Part 6: Implementation Guide

### 6.1 Provider WAL (Ring Buffer)

```python
class WALBackpressureError(Exception):
    """Raised when WAL is full and slowest consumer doesn't catch up in time."""
    pass


class WALConsumerTooSlowError(Exception):
    """Raised when consumer falls so far behind that entries were discarded."""
    pass


class ProviderWAL:
    def __init__(
        self,
        max_size: int = 10000,
        put_timeout: float = 30.0,
    ) -> None:
        self._buffer: list[Log | None] = [None] * max_size
        self._max_size = max_size
        self._put_timeout = put_timeout
        
        # Sequence tracking
        self._head: int = 0  # Oldest unacknowledged entry
        self._tail: int = 0  # Next write position
        
        # Synchronization
        self._lock = asyncio.Lock()
        self._not_full = asyncio.Condition(self._lock)
        self._not_empty = asyncio.Condition(self._lock)
        
        # Consumer position tracking
        self._consumer_positions: dict[str, int] = {}

    @property
    def _size(self) -> int:
        """Current number of entries in WAL."""
        return self._tail - self._head

    @property
    def _is_full(self) -> bool:
        """Check if WAL is at capacity."""
        return self._size >= self._max_size

    @property
    def _min_consumer_position(self) -> int:
        """Position of slowest consumer (blocks head advancement)."""
        if not self._consumer_positions:
            return self._tail  # No consumers, can discard all
        return min(self._consumer_positions.values())

    async def append(self, log: Log) -> int:
        """
        Append entry to WAL.
        
        Returns:
            Sequence number of appended entry.
            
        Raises:
            WALBackpressureError: WAL full and timeout expired waiting for consumers.
        """
        async with self._lock:
            # Try to advance head (discard fully-acknowledged entries)
            self._advance_head()
            
            if self._is_full:
                try:
                    await asyncio.wait_for(
                        self._wait_for_space(),
                        timeout=self._put_timeout,
                    )
                except asyncio.TimeoutError:
                    raise WALBackpressureError(
                        f"Provider WAL full ({self._max_size} entries) for {self._put_timeout}s. "
                        f"Slowest consumer at position {self._min_consumer_position}, "
                        f"head={self._head}, tail={self._tail}."
                    ) from None
            
            # Write entry
            seq = self._tail
            self._buffer[seq % self._max_size] = log
            self._tail += 1
            
            # Notify waiting consumers
            self._not_empty.notify_all()
            
            return seq

    async def _wait_for_space(self) -> None:
        """Wait until WAL has space for new entries."""
        while self._is_full:
            await self._not_full.wait()
            self._advance_head()

    def _advance_head(self) -> None:
        """Advance head to discard entries all consumers have acknowledged."""
        min_pos = self._min_consumer_position
        entries_discarded = 0
        
        while self._head < min_pos:
            self._buffer[self._head % self._max_size] = None
            self._head += 1
            entries_discarded += 1
        
        return entries_discarded

    async def read_from(
        self,
        consumer_id: str,
        start_seq: int | None = None,
    ) -> AsyncIterator[tuple[int, Log]]:
        """
        Read entries starting from sequence number.
        
        Yields:
            Tuples of (sequence_number, log_entry).
            
        Raises:
            WALConsumerTooSlowError: Consumer position is behind head (missed entries).
        """
        if start_seq is None:
            start_seq = self._consumer_positions.get(consumer_id, self._head)
        
        current = start_seq
        
        while True:
            async with self._lock:
                # Wait if caught up
                while current >= self._tail:
                    await self._not_empty.wait()
                
                # Validate position still valid
                if current < self._head:
                    raise WALConsumerTooSlowError(
                        f"Consumer '{consumer_id}' at seq {current} but head advanced to {self._head}. "
                        f"Consumer fell too far behind and missed {self._head - current} entries."
                    )
                
                # Read entry
                log = self._buffer[current % self._max_size]
                if log is None:
                    raise RuntimeError(f"WAL corruption: null entry at seq {current}")
            
            yield current, log
            current += 1

    async def acknowledge(self, consumer_id: str, seq: int) -> None:
        """
        Acknowledge processing of entries up to seq (inclusive).
        
        This allows the WAL to discard old entries once all consumers acknowledge.
        """
        async with self._lock:
            current_pos = self._consumer_positions.get(consumer_id, self._head)
            
            if seq < current_pos:
                return  # Already acknowledged (idempotent)
            
            if seq >= self._tail:
                raise ValueError(
                    f"Cannot acknowledge seq {seq}, tail is {self._tail}"
                )
            
            self._consumer_positions[consumer_id] = seq + 1
            
            # Try to advance head and free space
            old_head = self._head
            self._advance_head()
            
            # Notify blocked producers if we freed space
            if self._head > old_head:
                self._not_full.notify_all()

    def register_consumer(
        self,
        consumer_id: str,
        start_from: Literal["earliest", "latest"] = "earliest",
    ) -> int:
        """
        Register a new consumer.
        
        Args:
            consumer_id: Unique identifier for consumer.
            start_from: "earliest" = from head (replay all), "latest" = from tail (new only)
            
        Returns:
            Starting sequence number for consumer.
        """
        if start_from == "earliest":
            pos = self._head
        elif start_from == "latest":
            pos = self._tail
        else:
            raise ValueError(f"Invalid start_from: {start_from}")
        
        self._consumer_positions[consumer_id] = pos
        return pos

    def unregister_consumer(self, consumer_id: str) -> None:
        """
        Unregister consumer, removing its position tracking.
        
        This may allow head to advance if this was the slowest consumer.
        """
        self._consumer_positions.pop(consumer_id, None)
```

### 6.2 Pull-Based Consumer

```python
class LogConsumer:
    def __init__(
        self,
        consumer_id: str,
        provider_wal: ProviderWAL,
        local_buffer_size: int = 1000,
        batch_size: int = 100,
        ack_interval: int = 100,
    ) -> None:
        self._consumer_id = consumer_id
        self._provider_wal = provider_wal
        self._local_buffer: asyncio.Queue[tuple[int, Log]] = asyncio.Queue(
            maxsize=local_buffer_size
        )
        self._batch_size = batch_size
        self._ack_interval = ack_interval
        
        self._last_acked_seq: int | None = None
        self._running = False
        self._pull_task: asyncio.Task | None = None
        self.status = ConsumerStatus.READY

    async def start(self) -> None:
        """Start the consumer pull loop."""
        self._running = True
        self.status = ConsumerStatus.RUNNING
        
        start_pos = self._provider_wal.register_consumer(
            self._consumer_id,
            start_from="earliest",
        )
        
        self._pull_task = asyncio.create_task(
            self._pull_loop(start_pos)
        )

    async def _pull_loop(self, start_seq: int) -> None:
        """Continuously pull entries from provider WAL into local buffer."""
        try:
            async for seq, log in self._provider_wal.read_from(
                self._consumer_id,
                start_seq,
            ):
                if not self._running:
                    break
                
                # Blocks if local buffer is full (backpressure to WAL)
                await self._local_buffer.put((seq, log))
                
        except WALConsumerTooSlowError as err:
            self.status = ConsumerStatus.FAILED
            raise
        except asyncio.CancelledError:
            pass
        finally:
            self.status = ConsumerStatus.CLOSED

    async def iter_logs(
        self,
        filter_fn: Callable[[Log], bool] | None = None,
    ) -> AsyncIterator[Log]:
        """
        Iterate over logs, yielding entries and batching acknowledgments.
        """
        pending_seqs: list[int] = []
        
        while self._running or not self._local_buffer.empty():
            try:
                seq, log = await asyncio.wait_for(
                    self._local_buffer.get(),
                    timeout=0.1,
                )
            except asyncio.TimeoutError:
                continue
            
            if filter_fn is None or filter_fn(log):
                yield log
            
            pending_seqs.append(seq)
            
            # Batch acknowledge periodically
            if len(pending_seqs) >= self._ack_interval:
                await self._acknowledge_batch(pending_seqs)
                pending_seqs.clear()
        
        # Final acknowledgment
        if pending_seqs:
            await self._acknowledge_batch(pending_seqs)

    async def _acknowledge_batch(self, seqs: list[int]) -> None:
        """Acknowledge the highest sequence number in batch."""
        if not seqs:
            return
        
        max_seq = max(seqs)
        await self._provider_wal.acknowledge(self._consumer_id, max_seq)
        self._last_acked_seq = max_seq

    async def stop(self) -> None:
        """Stop consumer gracefully."""
        self._running = False
        self.status = ConsumerStatus.CLOSING
        
        if self._pull_task:
            self._pull_task.cancel()
            try:
                await self._pull_task
            except asyncio.CancelledError:
                pass
        
        self._provider_wal.unregister_consumer(self._consumer_id)
        self.status = ConsumerStatus.CLOSED

    @property
    def pending(self) -> bool:
        """Check if there are unprocessed entries in local buffer."""
        return not self._local_buffer.empty()

    @property
    def queue_depth(self) -> int:
        """Number of entries in local buffer."""
        return self._local_buffer.qsize()
```

### 6.3 Updated LogProvider

```python
class LogProvider:
    def __init__(
        self,
        wal_size: int = 10000,
        put_timeout: float = 30.0,
    ) -> None:
        self._wal = ProviderWAL(max_size=wal_size, put_timeout=put_timeout)
        self._consumers: dict[str, LogConsumer] = {}
        self.status = ProviderStatus.READY

    async def put(self, log: Log) -> int:
        """
        Append log to provider WAL.
        
        Returns:
            Sequence number.
            
        Note:
            Consumers pull independently - this does NOT push to consumers.
        """
        if self.status != ProviderStatus.RUNNING:
            if self.status == ProviderStatus.READY:
                self.status = ProviderStatus.RUNNING
            else:
                raise RuntimeError(f"Provider not running: {self.status}")
        
        return await self._wal.append(log)

    def subscribe(self, consumer: LogConsumer) -> None:
        """Register a consumer to pull from this provider's WAL."""
        self._consumers[consumer._consumer_id] = consumer

    async def unsubscribe(self, consumer_id: str) -> None:
        """Unregister a consumer."""
        consumer = self._consumers.pop(consumer_id, None)
        if consumer:
            await consumer.stop()

    @property
    def subscriptions_count(self) -> int:
        """Number of registered consumers."""
        return len(self._consumers)

    async def signal_shutdown(self) -> None:
        """Signal all consumers to stop and wait for completion."""
        self.status = ProviderStatus.CLOSING
        
        for consumer in self._consumers.values():
            await consumer.stop()
        
        self.status = ProviderStatus.CLOSED
```

### 6.4 Error Propagation

```python
async def log(
    self,
    entry: T,
    ...
) -> int | None:
    """
    Log entry with durability guarantees.
    
    For WAL modes (FSYNC, FSYNC_BATCH):
    - Raises WALBackpressureError if WAL full and consumers don't catch up
    - Raises WALWriteError if disk write fails
    - Returns LSN on success
    
    For Data Plane modes (NONE, FLUSH):
    - Returns None on any failure (fire-and-forget)
    - Logs warning to stderr
    """
    try:
        seq = await self._provider.put(log)
        # ... write to file logic ...
        return seq
        
    except WALBackpressureError:
        if self._durability in (DurabilityMode.FSYNC, DurabilityMode.FSYNC_BATCH):
            raise  # Propagate to caller - they must handle
        else:
            self._log_backpressure_warning()
            return None
            
    except Exception as err:
        if self._durability in (DurabilityMode.FSYNC, DurabilityMode.FSYNC_BATCH):
            raise WALWriteError(f"Failed to write WAL entry: {err}") from err
        else:
            await self._log_error(entry, err)
            return None
```

---

## Part 7: Memory Safety Guarantees

### 7.1 Bounded Structures

| Structure | Bound | Cleanup |
|-----------|-------|---------|
| `ProviderWAL._buffer` | `max_size` (ring buffer) | Entries nulled on head advance |
| `LogConsumer._local_buffer` | `local_buffer_size` | Drained on close |
| `_consumer_positions` | One entry per consumer | Removed on unregister |
| `_files` | Explicit open/close | Removed on close |
| `_file_locks` | One per file path | Removed on close |
| `Logger._contexts` | Explicit management | Cleared on close |

### 7.2 Memory Lifecycle

```
Entry Lifecycle:
                                                                          
  append()          Consumer reads        Consumer acks       Head advances
     │                    │                    │                   │
     ▼                    ▼                    ▼                   ▼
┌─────────┐         ┌─────────┐         ┌─────────┐         ┌─────────┐
│ Written │ ──────► │  Read   │ ──────► │  Acked  │ ──────► │ Nulled  │
│ to WAL  │         │ by all  │         │ by all  │         │ (freed) │
└─────────┘         └─────────┘         └─────────┘         └─────────┘
     │                                                            │
     │                                                            │
     └──────────── Entry exists in memory ────────────────────────┘
                   (bounded by max_size)
```

### 7.3 Cleanup on Close

```python
async def close(self) -> None:
    """
    Close logger stream with full cleanup.
    
    Order:
    1. Stop accepting new entries
    2. Signal consumers to stop
    3. Wait for consumers to drain
    4. Close all files
    5. Clear all internal state
    """
    self._closing = True
    
    # Stop provider and consumers
    if self._provider:
        await self._provider.signal_shutdown()
    
    # Close files and clear dicts
    for logfile_path in list(self._files.keys()):
        await self._close_file(logfile_path)
        del self._files[logfile_path]
        if logfile_path in self._file_locks:
            del self._file_locks[logfile_path]
    
    # Clear read state
    self._read_files.clear()
    self._read_locks.clear()
    
    self._initialized = False
    self._closing = False
```

---

## Part 8: Summary

### 8.1 Architecture Comparison

| Aspect | Old (Push) | New (Provider WAL) |
|--------|-----------|-------------------|
| **Data flow** | Provider pushes to consumer queues | Consumers pull from shared WAL |
| **Source of truth** | Distributed across consumer queues | Single WAL ring buffer |
| **Backpressure** | Per-consumer queue bounds | Slowest consumer blocks WAL head |
| **Failure recovery** | None (queue lost on crash) | Replay from last ack position |
| **Consistency** | Consumers may diverge | All see same sequence |
| **Memory model** | N × queue_size | WAL_size + N × buffer_size |

### 8.2 Guarantees by Durability Mode

| Mode | WAL Bound | On Full | Error Handling | Recovery |
|------|-----------|---------|----------------|----------|
| NONE | Unbounded | N/A | Silent | None |
| FLUSH | 10,000 | Drop + warn | Log to stderr | None |
| FSYNC | 10,000 | Block + timeout | Raise error | Replay from ack |
| FSYNC_BATCH | 10,000 | Block + timeout | Raise error | Replay from ack |

### 8.3 Key Guarantees

1. **Bounded Memory**: WAL is fixed-size ring buffer, consumers have bounded local buffers
2. **Atomic Delivery**: Entry is in WAL or not - no partial fan-out states
3. **No Silent Drops**: WAL modes raise explicit `WALBackpressureError`
4. **Failure Isolation**: Consumer crash doesn't affect WAL or other consumers
5. **Replay Capability**: Consumers restart from last acknowledged position
6. **Global Ordering**: All consumers see entries in same WAL sequence order

### 8.4 Usage

**For Data Plane (Stats/Metrics)**:
- Use Logger as-is with default parameters
- Fire-and-forget semantics
- Loss acceptable under extreme load

**For Control Plane (WAL)**:
- Use `durability=FSYNC_BATCH`
- Pull-based consumers with acknowledgment
- Guaranteed durability via replay on failure

---

## Part 9: Additional Remediations

### 9.1 File Lock and Dict Cleanup (Memory Leak Fixes)

**Problem**: `_file_locks`, `_read_locks`, `_files`, and `_read_files` dicts grow without cleanup.

**Solution**: Clean up all related entries when a file is closed.

```python
# In LoggerStream

def __init__(self, ...):
    # Replace defaultdict with regular dict for explicit management
    self._file_locks: dict[str, asyncio.Lock] = {}
    self._read_locks: dict[str, asyncio.Lock] = {}
    self._files: dict[str, io.FileIO] = {}
    self._read_files: dict[str, io.FileIO] = {}

def _get_file_lock(self, logfile_path: str) -> asyncio.Lock:
    """Get or create lock for file path."""
    if logfile_path not in self._file_locks:
        self._file_locks[logfile_path] = asyncio.Lock()
    return self._file_locks[logfile_path]

def _get_read_lock(self, logfile_path: str) -> asyncio.Lock:
    """Get or create read lock for file path."""
    if logfile_path not in self._read_locks:
        self._read_locks[logfile_path] = asyncio.Lock()
    return self._read_locks[logfile_path]

async def _close_file(self, logfile_path: str) -> None:
    """
    Close file and clean up all associated resources.
    
    Removes entries from:
    - _files
    - _file_locks
    - _read_files
    - _read_locks
    """
    file_lock = self._file_locks.get(logfile_path)
    if not file_lock:
        return

    await file_lock.acquire()
    try:
        # Close write file
        logfile = self._files.get(logfile_path)
        if logfile and not logfile.closed:
            await self._loop.run_in_executor(None, logfile.close)
        
        # Close read file if open
        read_file = self._read_files.get(logfile_path)
        if read_file and not read_file.closed:
            await self._loop.run_in_executor(None, read_file.close)
    finally:
        file_lock.release()
    
    # Remove all dict entries for this path
    self._files.pop(logfile_path, None)
    self._file_locks.pop(logfile_path, None)
    self._read_files.pop(logfile_path, None)
    self._read_locks.pop(logfile_path, None)
```

### 9.2 Logger Context Cleanup

**Problem**: `Logger._contexts` grows without bounds, not cleared in `close()`.

**Solution**: Clear contexts after closing all streams.

```python
# In Logger

async def close(self) -> None:
    """
    Close logger and all contexts.
    
    Order:
    1. Stop all watch tasks
    2. Close all context streams
    3. Clear context dict
    4. Clear watch task dict
    """
    # Stop watch tasks first
    if self._watch_tasks:
        await asyncio.gather(*[
            self.stop_watch(name) 
            for name in list(self._watch_tasks.keys())
        ])
    
    # Close all context streams
    if self._contexts:
        await asyncio.gather(*[
            context.stream.close(shutdown_subscribed=True)
            for context in self._contexts.values()
        ])
    
    # Clear all tracking dicts
    self._contexts.clear()
    self._watch_tasks.clear()
```

### 9.3 Batch Overflow Error Propagation

**Problem**: `_pending_batch` silently completes without write when batch is full.

**Solution**: Raise error in WAL modes, drop with warning in data plane modes.

```python
class WALBatchOverflowError(Exception):
    """Raised when fsync batch is full and cannot accept more entries."""
    pass


async def _schedule_batch_fsync(self, logfile_path: str) -> asyncio.Future[None]:
    """
    Schedule entry for batched fsync.
    
    For WAL modes: Raises WALBatchOverflowError if batch full.
    For Data Plane: Drops with warning if batch full.
    """
    if self._closing:
        future = self._loop.create_future()
        future.set_result(None)
        return future

    if self._batch_lock is None:
        self._batch_lock = asyncio.Lock()

    future: asyncio.Future[None] = self._loop.create_future()

    async with self._batch_lock:
        # Check batch capacity
        if len(self._pending_batch) >= self._batch_max_size:
            if self._durability in (DurabilityMode.FSYNC, DurabilityMode.FSYNC_BATCH):
                raise WALBatchOverflowError(
                    f"Fsync batch full ({self._batch_max_size} entries). "
                    f"Disk I/O not keeping up with write rate."
                )
            
            # Data plane: drop with warning
            self._log_batch_overflow_warning()
            future.set_result(None)
            return future

        self._pending_batch.append((logfile_path, future))

        # Schedule flush on first entry
        if len(self._pending_batch) == 1:
            self._batch_timer_handle = self._loop.call_later(
                self._batch_timeout_ms / 1000.0,
                self._trigger_batch_flush,
                logfile_path,
            )

        # Trigger immediate flush if batch is full
        should_flush = len(self._pending_batch) >= self._batch_max_size

    if should_flush:
        if self._batch_timer_handle:
            self._batch_timer_handle.cancel()
            self._batch_timer_handle = None
        await self._flush_batch(logfile_path)

    return future

def _log_batch_overflow_warning(self) -> None:
    """Log warning when batch overflows in data plane mode."""
    stream_writer = self._stream_writers.get(StreamType.STDERR)
    if not stream_writer or stream_writer.is_closing():
        return

    timestamp = datetime.datetime.now(datetime.UTC).isoformat()
    warning = f"{timestamp} - WARN - Fsync batch full, dropping entry (data plane mode)\n"

    try:
        stream_writer.write(warning.encode())
    except Exception:
        pass
```

### 9.4 Schedule Method Restriction for WAL Modes

**Problem**: `schedule()` is fire-and-forget and cannot propagate errors, incompatible with WAL guarantees.

**Solution**: Disallow `schedule()` for WAL durability modes.

```python
def schedule(
    self,
    entry: T,
    template: str | None = None,
    path: str | None = None,
    retention_policy: RetentionPolicyConfig | None = None,
    filter: Callable[[T], bool] | None = None,
) -> None:
    """
    Schedule log entry for async processing (fire-and-forget).
    
    NOT available for WAL durability modes - use `await log()` instead.
    
    Raises:
        TypeError: If called with WAL durability mode.
    """
    if self._closing:
        return

    # WAL modes require synchronous error handling
    if self._durability in (DurabilityMode.FSYNC, DurabilityMode.FSYNC_BATCH):
        raise TypeError(
            "schedule() cannot be used with WAL durability modes (FSYNC, FSYNC_BATCH). "
            "Use 'await log()' to ensure errors propagate to caller."
        )

    # Data plane: fire-and-forget with bounded queue
    task = asyncio.create_task(
        self.log(
            entry,
            template=template,
            path=path,
            retention_policy=retention_policy,
            filter=filter,
        )
    )

    self._scheduled_tasks.add(task)
    task.add_done_callback(self._scheduled_tasks.discard)

    try:
        self._queue.put_nowait(task)
    except asyncio.QueueFull:
        self._log_backpressure_warning()
        task.cancel()
        self._scheduled_tasks.discard(task)
```

### 9.5 File Write Error Propagation

**Problem**: File write errors are caught and logged but not propagated in WAL modes.

**Solution**: Re-raise as `WALWriteError` in WAL modes.

```python
class WALWriteError(Exception):
    """Raised when WAL file write fails."""
    pass


async def _write_log_to_file(
    self,
    entry: Entry,
    log: Log[T],
    logfile_path: str,
) -> int | None:
    """
    Write log entry to file with durability guarantees.
    
    For WAL modes: Raises WALWriteError on failure.
    For Data Plane: Logs error and returns None.
    """
    file_lock = self._get_file_lock(logfile_path)

    await file_lock.acquire()
    try:
        lsn = await self._loop.run_in_executor(
            None,
            self._write_to_file,
            log,
            logfile_path,
            self._durability,
        )
    except Exception as err:
        if self._durability in (DurabilityMode.FSYNC, DurabilityMode.FSYNC_BATCH):
            raise WALWriteError(
                f"Failed to write to WAL file '{logfile_path}': {err}"
            ) from err
        
        # Data plane: log error, continue
        log_file, line_number, function_name = self._find_caller()
        await self._log_error(entry, log_file, line_number, function_name, err)
        return None
    finally:
        file_lock.release()

    # Schedule batched fsync if needed
    if self._durability == DurabilityMode.FSYNC_BATCH:
        await self._schedule_batch_fsync(logfile_path)

    return lsn
```

### 9.6 LSN Generation: Hybrid Lamport Clock

**Problem**: The original `SnowflakeGenerator` has fundamental limitations for globally distributed systems:
- 4096 LSNs/ms limit (12-bit sequence) - insufficient for high-throughput load testing
- Clock dependency - NTP drift, VM clock issues cause failures
- No global ordering - cannot compare LSNs across nodes
- Silent failures on sequence exhaustion or clock regression

**Solution**: Replace with Hybrid Lamport Timestamp (see Part 11 for full specification).

```python
# In LoggerStream.__init__

if enable_lsn:
    self._lamport_clock = HybridLamportClock(node_id=instance_id)
```

**Usage**:

```python
# Generate LSN
lsn = self._lamport_clock.generate()

# On receiving replicated entry from another node
self._lamport_clock.receive(remote_lsn)
```

### 9.7 Exception Hierarchy

All WAL-related exceptions for clear error handling:

```python
class WALError(Exception):
    """Base class for all WAL-related errors."""
    pass


class WALBackpressureError(WALError):
    """Raised when WAL is full and consumers don't catch up in time."""
    pass


class WALWriteError(WALError):
    """Raised when WAL file write fails."""
    pass


class WALBatchOverflowError(WALError):
    """Raised when fsync batch is full."""
    pass


class WALConsumerTooSlowError(WALError):
    """Raised when consumer falls behind and misses entries."""
    pass


class LSNGenerationError(WALError):
    """Raised when LSN generation fails (sequence exhausted or clock drift)."""
    pass


class WALClosingError(WALError):
    """Raised when attempting to write to a closing WAL."""
    pass
```

---

## Part 10: Remediation Summary

### 10.1 Issues Addressed

| Issue | Category | Fix | Section |
|-------|----------|-----|---------|
| 1.4 | Memory Leak | Replace defaultdict, cleanup on file close | 9.1 |
| 1.5 | Memory Leak | Remove dict entries in `_close_file()` | 9.1 |
| 1.6 | Memory Leak | Clear `_contexts` in `Logger.close()` | 9.2 |
| 2.3 | Silent Drop | Raise `WALBatchOverflowError` in WAL modes | 9.3 |
| 3.1 | Silent Error | Disallow `schedule()` for WAL modes | 9.4 |
| 3.2 | Silent Error | Same as 2.3 | 9.3 |
| 3.3 | Silent Error | Raise `WALWriteError` in WAL modes | 9.5 |
| 3.5 | Silent Error | Add strict mode to `SnowflakeGenerator` | 9.6 |

### 10.2 Backward Compatibility

All fixes maintain backward compatibility:

| Change | Data Plane Impact | WAL Mode Impact |
|--------|-------------------|-----------------|
| Dict cleanup | None (internal) | None (internal) |
| Context cleanup | None (internal) | None (internal) |
| Batch overflow | Warn + drop (unchanged) | New error (correct behavior) |
| Schedule restriction | Works (unchanged) | New error (correct behavior) |
| Write error propagation | Log + continue (unchanged) | New error (correct behavior) |
| LSN strict mode | Non-strict (unchanged) | Strict (correct behavior) |

### 10.3 Error Handling by Mode

| Scenario | NONE | FLUSH | FSYNC | FSYNC_BATCH |
|----------|------|-------|-------|-------------|
| WAL full | N/A | Drop + warn | Raise `WALBackpressureError` | Raise `WALBackpressureError` |
| Batch full | N/A | Drop + warn | Raise `WALBatchOverflowError` | Raise `WALBatchOverflowError` |
| Write fails | Silent | Log to stderr | Raise `WALWriteError` | Raise `WALWriteError` |
| LSN fails | Return None | Return None | Raise `LSNGenerationError` | Raise `LSNGenerationError` |
| `schedule()` | Allowed | Allowed | Raise `TypeError` | Raise `TypeError` |
