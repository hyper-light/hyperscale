---
ad_number: 39
name: Logger Extension for AD-38 WAL Compliance
description: Extends Logger with optional WAL features including fsync, binary format, and sequence numbers.
---

# AD-39: Logger Extension for AD-38 WAL Compliance

**Decision**: Extend the existing `hyperscale/logging` Logger with optional WAL-compliant features (durability modes, binary format, sequence numbers, read-back) while maintaining full backward compatibility with existing usage patterns.

**Related**: AD-38 (Global Job Ledger), AD-20 (Cancellation)

**Rationale**:
- AD-38 identified that Logger is unsuitable for Control Plane WAL due to missing fsync, sequence numbers, and read-back capability.
- However, creating a completely separate NodeWAL class duplicates async I/O patterns already proven in Logger.
- By extending Logger with **optional** WAL features, we achieve code reuse, consistent API patterns, and progressive enhancement.
- All existing Logger usage (Data Plane stats) continues unchanged with default parameters.
- New WAL use cases opt-in to durability features via new parameters.

---

## Part 1: Current Logger Architecture Analysis

### 1.1 Current Usage Patterns

All Logger file usage follows a consistent pattern across the codebase:

```python
# Pattern 1: Configure then use context
self._logger.configure(
    name="context_name",
    path="hyperscale.leader.log.json",
    template="{timestamp} - {level} - {...} - {message}",
    models={...},
)

async with self._logger.context(name="context_name") as ctx:
    await ctx.log(Entry(message="...", level=LogLevel.INFO))
```

### 1.2 Critical Gap: `_write_to_file` Implementation

```python
# CURRENT IMPLEMENTATION (INSUFFICIENT for WAL):
logfile.write(msgspec.json.encode(log) + b"\n")  # JSON only
logfile.flush()  # NO fsync - data can be lost!
```

**Problems for WAL**:
1. **No fsync** - `flush()` only pushes to OS buffer, not disk
2. **JSON only** - No binary format with CRC checksums
3. **No LSN** - No sequence number generation
4. **Write-only** - No read-back for recovery
5. **Errors swallowed** - Silent failures unacceptable for WAL

---

## Part 2: Extension Design

### 2.1 Design Principles

1. **Additive Only** - New optional parameters with backward-compatible defaults
2. **Zero Breaking Changes** - All existing code works unchanged
3. **Progressive Enhancement** - Enable WAL features per-context as needed
4. **Single Responsibility** - Each new feature independently toggleable
5. **Consistent Patterns** - Same `context()` API already familiar to codebase

### 2.2 New Configuration Enum

```python
class DurabilityMode(IntEnum):
    """
    Durability levels for log writes.
    """
    NONE = 0         # No sync (testing only)
    FLUSH = 1        # Current behavior - flush() to OS buffer
    FSYNC = 2        # fsync per write (safest, ~1-10ms latency)
    FSYNC_BATCH = 3  # Batched fsync every N writes or T ms
```

### 2.3 API Extension

```
Logger.context() - EXTENDED

EXISTING PARAMETERS (unchanged):
- name: str | None = None
- template: str | None = None
- path: str | None = None
- retention_policy: RetentionPolicyConfig | None = None
- nested: bool = False
- models: dict[...] | None = None

NEW PARAMETERS (all optional, defaults = current behavior):
- durability: DurabilityMode = DurabilityMode.FLUSH    # NEW
- format: Literal['json', 'binary'] = 'json'           # NEW
- enable_lsn: bool = False                             # NEW
- instance_id: int = 0                                 # NEW
```

### 2.4 Usage Comparison

```python
# =====================================================================
# EXISTING CODE - COMPLETELY UNCHANGED (Data Plane - stats)
# =====================================================================

async with self._logger.context(
    name="remote_graph_manager",
    path="hyperscale.leader.log.json",
    template="{timestamp} - {level} - {...} - {message}",
) as ctx:
    await ctx.log(Entry(message="Stats update", level=LogLevel.INFO))
    # Uses: JSON format, flush() only, no LSN
    # Behavior: IDENTICAL to current implementation


# =====================================================================
# NEW CODE - WAL MODE (Control Plane - job/workflow commands)
# =====================================================================

async with self._logger.context(
    name="node_wal",
    path="hyperscale.wal.log",               # Can use .wal extension
    durability=DurabilityMode.FSYNC_BATCH,   # NEW: Batched fsync
    format='binary',                          # NEW: Binary with CRC
    enable_lsn=True,                          # NEW: Sequence numbers
    instance_id=self._node_id,                # NEW: For snowflake LSN
) as ctx:
    lsn = await ctx.log(WALEntry(...))
    # Uses: Binary format, CRC32 checksum, fsync, LSN tracking
    # Returns: LSN for replication tracking
```

---

## Part 3: LoggerStream Modifications

### 3.1 Binary Encoding with CRC

```python
def _encode_binary(self, log: Log, lsn: int | None) -> bytes:
    """
    Encode log entry in binary format with CRC32 checksum.

    Binary Format:
    +----------+----------+----------+---------------------+
    | CRC32    | Length   | LSN      | Payload (JSON)      |
    | (4 bytes)| (4 bytes)| (8 bytes)| (variable)          |
    +----------+----------+----------+---------------------+

    Total header: 16 bytes
    CRC32 covers: length + LSN + payload
    """
```

### 3.2 Read-Back for Recovery

```python
async def read_entries(
    self,
    logfile_path: str,
    from_offset: int = 0,
) -> AsyncIterator[tuple[int, Log, int | None]]:
    """
    Read entries from file for WAL recovery.

    Yields tuples of (file_offset, log_entry, lsn).
    Handles both JSON and binary formats based on self._format.
    """
```

### 3.3 Batched Fsync

```python
async def _schedule_batch_fsync(self, logfile_path: str) -> None:
    """
    Schedule entry for batch fsync.

    Batches are flushed when:
    - batch_max_size entries accumulated, OR
    - batch_timeout_ms elapsed since first entry

    This provides ~10x throughput improvement over per-write fsync
    while maintaining bounded latency.
    """
```

---

## Part 4: Log Model Extension

### 4.1 Add Optional LSN Field

```python
@dataclass
class Log(Generic[T]):
    """
    Wrapper around log entries with metadata.
    Extended with optional LSN for WAL use cases.
    """
    entry: T
    filename: str | None = None
    function_name: str | None = None
    line_number: int | None = None
    thread_id: int | None = None
    timestamp: str | None = None

    # NEW: Optional LSN for WAL entries
    lsn: int | None = field(default=None)
```

---

## Part 5: Summary

**For Data Plane (Stats/Metrics)**:
- Use Logger as-is with default parameters
- JSON format, flush() only, no sequence numbers
- Fire-and-forget semantics, eventual consistency

**For Control Plane (WAL)**:
- Use Logger with new optional parameters
- Binary format with CRC32, fsync (batched), LSN tracking
- Crash recovery capability via read-back
- Guaranteed durability for job/workflow commands
