---
ad_number: 39
name: Logger Extension for AD-38 WAL Compliance
description: Extends Logger with optional WAL features including fsync, binary format, and sequence numbers.
---

# AD-39: Logger Extension for AD-38 WAL Compliance

**Decision**: Extend the existing `hyperscale/logging` Logger with optional WAL-compliant features (durability modes, binary format, sequence numbers, read-back) while maintaining full backward compatibility with existing usage patterns.

**Related**: AD-38 (Global Job Ledger), AD-20 (Cancellation)

**Rationale**:
- AD-38 identified that Logger is unsuitable for Control Plane WAL due to missing fsync, sequence numbers, and read-back capability.
- However, creating a completely separate NodeWAL class duplicates async I/O patterns already proven in Logger.
- By extending Logger with **optional** WAL features, we achieve code reuse, consistent API patterns, and progressive enhancement.
- All existing Logger usage (Data Plane stats) continues unchanged with default parameters.
- New WAL use cases opt-in to durability features via new parameters.

---

## Part 1: Current Logger Architecture Analysis

### 1.1 Current Usage Patterns

All Logger file usage follows a consistent pattern across the codebase:

```python
# Pattern 1: Configure then use context
self._logger.configure(
    name="context_name",
    path="hyperscale.leader.log.json",
    template="{timestamp} - {level} - {...} - {message}",
    models={...},
)

async with self._logger.context(name="context_name") as ctx:
    await ctx.log(Entry(message="...", level=LogLevel.INFO))
```

### 1.2 Critical Gap: `_write_to_file` Implementation

```python
# CURRENT IMPLEMENTATION (INSUFFICIENT for WAL):
logfile.write(msgspec.json.encode(log) + b"\n")  # JSON only
logfile.flush()  # NO fsync - data can be lost!
```

**Problems for WAL**:
1. **No fsync** - `flush()` only pushes to OS buffer, not disk
2. **JSON only** - No binary format with CRC checksums
3. **No LSN** - No sequence number generation
4. **Write-only** - No read-back for recovery
5. **Errors swallowed** - Silent failures unacceptable for WAL

---

## Part 2: Extension Design

### 2.1 Design Principles

1. **Additive Only** - New optional parameters with backward-compatible defaults
2. **Zero Breaking Changes** - All existing code works unchanged
3. **Progressive Enhancement** - Enable WAL features per-context as needed
4. **Single Responsibility** - Each new feature independently toggleable
5. **Consistent Patterns** - Same `context()` API already familiar to codebase

### 2.2 New Configuration Enum

```python
class DurabilityMode(IntEnum):
    """
    Durability levels for log writes.
    """
    NONE = 0         # No sync (testing only)
    FLUSH = 1        # Current behavior - flush() to OS buffer
    FSYNC = 2        # fsync per write (safest, ~1-10ms latency)
    FSYNC_BATCH = 3  # Batched fsync every N writes or T ms
```

### 2.3 API Extension

```
Logger.context() - EXTENDED

EXISTING PARAMETERS (unchanged):
- name: str | None = None
- template: str | None = None
- path: str | None = None
- retention_policy: RetentionPolicyConfig | None = None
- nested: bool = False
- models: dict[...] | None = None

NEW PARAMETERS (all optional, defaults = current behavior):
- durability: DurabilityMode = DurabilityMode.FLUSH    # NEW
- format: Literal['json', 'binary'] = 'json'           # NEW
- enable_lsn: bool = False                             # NEW
- instance_id: int = 0                                 # NEW
```

### 2.4 Usage Comparison

```python
# =====================================================================
# EXISTING CODE - COMPLETELY UNCHANGED (Data Plane - stats)
# =====================================================================

async with self._logger.context(
    name="remote_graph_manager",
    path="hyperscale.leader.log.json",
    template="{timestamp} - {level} - {...} - {message}",
) as ctx:
    await ctx.log(Entry(message="Stats update", level=LogLevel.INFO))
    # Uses: JSON format, flush() only, no LSN
    # Behavior: IDENTICAL to current implementation


# =====================================================================
# NEW CODE - WAL MODE (Control Plane - job/workflow commands)
# =====================================================================

async with self._logger.context(
    name="node_wal",
    path="hyperscale.wal.log",               # Can use .wal extension
    durability=DurabilityMode.FSYNC_BATCH,   # NEW: Batched fsync
    format='binary',                          # NEW: Binary with CRC
    enable_lsn=True,                          # NEW: Sequence numbers
    instance_id=self._node_id,                # NEW: For snowflake LSN
) as ctx:
    lsn = await ctx.log(WALEntry(...))
    # Uses: Binary format, CRC32 checksum, fsync, LSN tracking
    # Returns: LSN for replication tracking
```

---

## Part 3: LoggerStream Modifications

### 3.1 Binary Encoding with CRC

```python
def _encode_binary(self, log: Log, lsn: int | None) -> bytes:
    """
    Encode log entry in binary format with CRC32 checksum.

    Binary Format:
    +----------+----------+----------+---------------------+
    | CRC32    | Length   | LSN      | Payload (JSON)      |
    | (4 bytes)| (4 bytes)| (8 bytes)| (variable)          |
    +----------+----------+----------+---------------------+

    Total header: 16 bytes
    CRC32 covers: length + LSN + payload
    """
```

### 3.2 Read-Back for Recovery

```python
async def read_entries(
    self,
    logfile_path: str,
    from_offset: int = 0,
) -> AsyncIterator[tuple[int, Log, int | None]]:
    """
    Read entries from file for WAL recovery.

    Yields tuples of (file_offset, log_entry, lsn).
    Handles both JSON and binary formats based on self._format.
    """
```

### 3.3 Batched Fsync

```python
async def _schedule_batch_fsync(self, logfile_path: str) -> None:
    """
    Schedule entry for batch fsync.

    Batches are flushed when:
    - batch_max_size entries accumulated, OR
    - batch_timeout_ms elapsed since first entry

    This provides ~10x throughput improvement over per-write fsync
    while maintaining bounded latency.
    """
```

---

## Part 4: Log Model Extension

### 4.1 Add Optional LSN Field

```python
@dataclass
class Log(Generic[T]):
    """
    Wrapper around log entries with metadata.
    Extended with optional LSN for WAL use cases.
    """
    entry: T
    filename: str | None = None
    function_name: str | None = None
    line_number: int | None = None
    thread_id: int | None = None
    timestamp: str | None = None

    # NEW: Optional LSN for WAL entries
    lsn: int | None = field(default=None)
```

---

## Part 5: Backpressure and Memory Safety

### 5.1 Problem Statement

WAL systems face competing requirements:

| Requirement | Constraint |
|-------------|------------|
| **Durability** | Every entry MUST be persisted - no drops |
| **Memory Safety** | Bounded memory usage - unbounded queues cause OOM in K8s |
| **No Silent Failures** | Errors must propagate to callers |
| **Performance** | High throughput via batching |

An unbounded queue guarantees durability but causes memory leaks under sustained load.
A bounded queue with drops violates durability.
Naive blocking couples disk latency to application latency.

### 5.2 Solution: Block + Signal Checkpoint

When the queue reaches capacity, we:

1. **Signal checkpoint immediately** - notify consumer to flush everything NOW
2. **Block producer** - wait for space (bounded memory)
3. **Consumer drains queue** - emergency flush to disk
4. **Producer unblocks** - space available, continue
5. **Timeout protection** - raise explicit error if checkpoint stalls

This converts passive waiting into active recovery.

### 5.3 State Diagram

```
                                    ┌─────────────────────────────────────────┐
                                    │                                         │
                                    ▼                                         │
┌──────────────┐  put()   ┌─────────────────┐  queue has space   ┌───────────┴────────┐
│   Producer   │ ───────► │  Check Queue    │ ─────────────────► │   Enqueue Entry    │
│   (caller)   │          │    Capacity     │                    │   (non-blocking)   │
└──────────────┘          └─────────────────┘                    └────────────────────┘
                                    │
                                    │ queue full
                                    ▼
                          ┌─────────────────┐
                          │ Signal Checkpoint│ ◄─── async, non-blocking signal
                          │     Event        │
                          └─────────────────┘
                                    │
                                    ▼
                          ┌─────────────────┐
                          │  Block on put() │ ◄─── with timeout
                          │  (await space)  │
                          └─────────────────┘
                                    │
                    ┌───────────────┴───────────────┐
                    │                               │
                    ▼                               ▼
          ┌─────────────────┐             ┌─────────────────┐
          │  Space Freed    │             │ Timeout Expired │
          │  (unblock)      │             │ (raise error)   │
          └─────────────────┘             └─────────────────┘
                    │                               │
                    ▼                               ▼
          ┌─────────────────┐             ┌─────────────────┐
          │ Enqueue Entry   │             │ WALBackpressure │
          │   (success)     │             │     Error       │
          └─────────────────┘             └─────────────────┘
```

### 5.4 Consumer State Diagram

```
┌────────────────────────────────────────────────────────────────────────────┐
│                           Consumer Loop                                     │
└────────────────────────────────────────────────────────────────────────────┘
                                    │
                                    ▼
                          ┌─────────────────┐
                          │  Wait for Entry │ ◄─── await queue.get()
                          │  or Checkpoint  │
                          └─────────────────┘
                                    │
                    ┌───────────────┴───────────────┐
                    │                               │
                    ▼                               ▼
          ┌─────────────────┐             ┌─────────────────┐
          │  Entry Ready    │             │   Checkpoint    │
          │                 │             │    Signaled     │
          └─────────────────┘             └─────────────────┘
                    │                               │
                    ▼                               ▼
          ┌─────────────────┐             ┌─────────────────┐
          │  Batch Entry    │             │  Drain ENTIRE   │
          │  (up to limit)  │             │     Queue       │
          └─────────────────┘             └─────────────────┘
                    │                               │
                    ▼                               │
          ┌─────────────────┐                       │
          │ Batch Timeout   │                       │
          │  or Max Size?   │                       │
          └─────────────────┘                       │
                    │                               │
                    └───────────────┬───────────────┘
                                    │
                                    ▼
                          ┌─────────────────┐
                          │  Write Batch    │
                          │   to Disk       │
                          └─────────────────┘
                                    │
                                    ▼
                          ┌─────────────────┐
                          │     fsync()     │
                          └─────────────────┘
                                    │
                                    ▼
                          ┌─────────────────┐
                          │  Clear Checkpoint│
                          │     Event       │
                          └─────────────────┘
                                    │
                                    └──────────► (loop back to wait)
```

### 5.5 High Water Mark Optimization

To minimize blocking, signal checkpoint BEFORE queue is completely full:

```
Queue Capacity: 10,000 entries

├─────────────────────────────────────────────────────────────────┤
0                    7,000              9,000              10,000
                       │                  │                   │
                       │                  │                   └── FULL: block producer
                       │                  │
                       │                  └── HIGH_WATER (90%): signal checkpoint
                       │
                       └── LOW_WATER (70%): clear backpressure flag
```

This gives the consumer a head start on flushing before producers actually block.

### 5.6 Sequence Diagram: Normal Operation

```
Producer                    Queue                     Consumer                  Disk
   │                          │                          │                        │
   │─── put(entry) ──────────►│                          │                        │
   │                          │ (queue has space)        │                        │
   │◄── return (immediate) ───│                          │                        │
   │                          │                          │                        │
   │─── put(entry) ──────────►│                          │                        │
   │◄── return (immediate) ───│                          │                        │
   │                          │                          │                        │
   │                          │                          │── batch timeout ──────►│
   │                          │                          │                        │
   │                          │◄── get_batch() ─────────│                        │
   │                          │                          │                        │
   │                          │                          │─── write_batch() ─────►│
   │                          │                          │                        │
   │                          │                          │─── fsync() ───────────►│
   │                          │                          │◄── ok ─────────────────│
   │                          │                          │                        │
```

### 5.7 Sequence Diagram: Backpressure with Checkpoint

```
Producer                    Queue                     Consumer                  Disk
   │                          │                          │                        │
   │─── put(entry) ──────────►│                          │                        │
   │                          │ (queue FULL)             │                        │
   │                          │                          │                        │
   │─── signal_checkpoint() ─►│─── checkpoint_event ────►│                        │
   │                          │                          │                        │
   │─── await put() ─────────►│ (BLOCKED)                │                        │
   │         ┊                │                          │── drain_queue() ──────►│
   │         ┊                │◄── get_all() ────────────│                        │
   │         ┊                │                          │                        │
   │         ┊                │                          │─── write_batch() ─────►│
   │         ┊                │                          │                        │
   │         ┊                │                          │─── fsync() ───────────►│
   │         ┊                │                          │◄── ok ─────────────────│
   │         ┊                │                          │                        │
   │         ┊                │ (space available)        │                        │
   │◄── return ───────────────│                          │                        │
   │                          │                          │                        │
```

### 5.8 Sequence Diagram: Timeout Error

```
Producer                    Queue                     Consumer                  Disk
   │                          │                          │                        │
   │─── put(entry) ──────────►│                          │                        │
   │                          │ (queue FULL)             │                        │
   │                          │                          │                        │
   │─── signal_checkpoint() ─►│─── checkpoint_event ────►│                        │
   │                          │                          │                        │
   │─── await put() ─────────►│ (BLOCKED)                │                        │
   │         ┊                │                          │── (consumer stalled) ──│
   │         ┊                │                          │         ┊              │
   │         ┊                │                          │         ┊              │
   │         ┊ (30s timeout)  │                          │         ┊              │
   │         ┊                │                          │         ┊              │
   │◄── WALBackpressureError ─│                          │         ┊              │
   │                          │                          │         ┊              │
```

---

## Part 6: Implementation Guide

### 6.1 LogConsumer with Backpressure

```python
class WALBackpressureError(Exception):
    """Raised when WAL queue is full and checkpoint times out."""
    pass


class LogConsumer:
    def __init__(
        self,
        max_size: int = 10000,
        high_water_mark: int | None = None,
        low_water_mark: int | None = None,
        put_timeout: float = 30.0,
    ) -> None:
        self._queue: asyncio.Queue[Log] = asyncio.Queue(maxsize=max_size)
        self._max_size = max_size
        self._high_water_mark = high_water_mark or int(max_size * 0.9)
        self._low_water_mark = low_water_mark or int(max_size * 0.7)
        self._put_timeout = put_timeout
        
        # Checkpoint signaling
        self._checkpoint_event: asyncio.Event = asyncio.Event()
        self._backpressure_active: bool = False
        
        # Consumer state
        self._wait_task: asyncio.Task | None = None
        self._loop = asyncio.get_event_loop()
        self.status = ConsumerStatus.READY

    @property
    def under_pressure(self) -> bool:
        """Check if backpressure is currently active."""
        return self._backpressure_active

    @property 
    def queue_depth(self) -> int:
        """Current number of entries in queue."""
        return self._queue.qsize()

    async def put(self, log: Log) -> None:
        """
        Add log entry to queue with backpressure handling.
        
        For WAL mode:
        - Signals checkpoint when high water mark reached
        - Blocks when queue is full (bounded memory)
        - Raises WALBackpressureError on timeout
        
        Raises:
            WALBackpressureError: Queue full and checkpoint timed out
        """
        queue_size = self._queue.qsize()
        
        # Signal checkpoint at high water mark (early warning)
        if queue_size >= self._high_water_mark:
            self._backpressure_active = True
            self._checkpoint_event.set()
        
        # Fast path - queue has space
        if queue_size < self._max_size:
            await self._queue.put(log)
            return
        
        # Slow path - queue full, block with timeout
        try:
            await asyncio.wait_for(
                self._queue.put(log),
                timeout=self._put_timeout,
            )
        except asyncio.TimeoutError:
            raise WALBackpressureError(
                f"WAL queue full ({self._max_size} entries) for {self._put_timeout}s. "
                f"Consumer may be stalled or disk I/O blocked."
            ) from None

    def _update_backpressure_state(self) -> None:
        """Update backpressure flag based on queue depth."""
        queue_size = self._queue.qsize()
        
        if queue_size <= self._low_water_mark:
            self._backpressure_active = False

    def checkpoint_requested(self) -> bool:
        """Check if checkpoint has been requested."""
        return self._checkpoint_event.is_set()
    
    def clear_checkpoint(self) -> None:
        """Clear checkpoint event after flush completes."""
        self._checkpoint_event.clear()
        self._update_backpressure_state()
```

### 6.2 Consumer Loop with Checkpoint Handling

```python
async def _consumer_loop(self) -> None:
    """
    Main consumer loop with checkpoint support.
    
    Normal operation:
    - Batch entries up to batch_max_size or batch_timeout
    - Write batch to disk
    - fsync based on durability mode
    
    Checkpoint operation:
    - Drain entire queue immediately
    - Write all entries to disk
    - fsync
    - Clear checkpoint event
    """
    while self._running:
        batch: list[Log] = []
        
        try:
            # Wait for first entry with timeout
            async with asyncio.timeout(self._batch_timeout_ms / 1000):
                while len(batch) < self._batch_max_size:
                    # Check for checkpoint signal
                    if self._consumer.checkpoint_requested():
                        break
                    
                    try:
                        log = await asyncio.wait_for(
                            self._consumer._queue.get(),
                            timeout=0.001,  # 1ms poll
                        )
                        batch.append(log)
                    except asyncio.TimeoutError:
                        # No entry available, check checkpoint again
                        if self._consumer.checkpoint_requested():
                            break
                        continue
                        
        except asyncio.TimeoutError:
            pass  # Batch timeout, flush what we have
        
        # Handle checkpoint - drain entire queue
        if self._consumer.checkpoint_requested():
            batch = self._drain_queue_into(batch)
        
        # Write batch if non-empty
        if batch:
            await self._write_batch(batch)
            await self._fsync_if_needed()
        
        # Clear checkpoint after successful flush
        if self._consumer.checkpoint_requested():
            self._consumer.clear_checkpoint()


def _drain_queue_into(self, batch: list[Log]) -> list[Log]:
    """Drain all remaining entries from queue into batch."""
    while True:
        try:
            batch.append(self._consumer._queue.get_nowait())
        except asyncio.QueueEmpty:
            break
    return batch
```

### 6.3 Configuration by Durability Mode

```python
def _get_backpressure_config(
    durability: DurabilityMode,
) -> dict:
    """
    Get backpressure configuration based on durability mode.
    
    Data Plane (FLUSH): Lenient - warn and drop on overflow
    Control Plane (FSYNC/FSYNC_BATCH): Strict - block and checkpoint
    """
    match durability:
        case DurabilityMode.NONE:
            # Testing mode - no backpressure
            return {
                "max_size": 0,  # Unbounded (testing only!)
                "put_timeout": None,
                "on_full": "drop_with_warning",
            }
        
        case DurabilityMode.FLUSH:
            # Data plane - bounded, drop with warning
            return {
                "max_size": 10000,
                "put_timeout": None,
                "on_full": "drop_with_warning",
            }
        
        case DurabilityMode.FSYNC | DurabilityMode.FSYNC_BATCH:
            # WAL mode - bounded, block with checkpoint
            return {
                "max_size": 10000,
                "high_water_mark": 9000,
                "low_water_mark": 7000,
                "put_timeout": 30.0,
                "on_full": "block_and_checkpoint",
            }
```

### 6.4 Error Propagation

For WAL durability modes, errors MUST propagate to callers:

```python
async def log(
    self,
    entry: T,
    ...
) -> int | None:
    """
    Log entry with durability guarantees.
    
    For WAL modes (FSYNC, FSYNC_BATCH):
    - Raises WALBackpressureError if queue full and checkpoint times out
    - Raises WALWriteError if disk write fails
    - Returns LSN on success
    
    For Data Plane modes (NONE, FLUSH):
    - Returns None on any failure (fire-and-forget)
    - Logs warning to stderr
    """
    try:
        # ... write logic ...
        pass
    except WALBackpressureError:
        if self._durability in (DurabilityMode.FSYNC, DurabilityMode.FSYNC_BATCH):
            raise  # Propagate to caller
        else:
            self._log_backpressure_warning()
            return None
    except Exception as err:
        if self._durability in (DurabilityMode.FSYNC, DurabilityMode.FSYNC_BATCH):
            raise WALWriteError(f"Failed to write WAL entry: {err}") from err
        else:
            await self._log_error(entry, err)
            return None
```

---

## Part 7: Memory Safety Guarantees

### 7.1 Bounded Structures

| Structure | Bound | Cleanup |
|-----------|-------|---------|
| `LogConsumer._queue` | `maxsize=10000` | Drained on close |
| `_pending_batch` | `batch_max_size=100` | Cleared on flush |
| `_scheduled_tasks` | Bounded by queue | Done callback removes |
| `_files` | Explicit open/close | Removed on close |
| `_file_locks` | One per file path | Removed on close |
| `Logger._contexts` | Explicit management | Cleared on close |

### 7.2 Cleanup on Close

```python
async def close(self) -> None:
    """
    Close logger stream with full cleanup.
    
    Order of operations:
    1. Stop accepting new entries
    2. Signal final checkpoint
    3. Wait for consumer to drain queue
    4. Flush pending batch
    5. Close all files
    6. Clear all internal state
    """
    self._closing = True
    
    # Signal checkpoint to flush remaining entries
    if self._consumer:
        self._consumer._checkpoint_event.set()
        await self._consumer.wait_for_drain()
    
    # Cleanup batch state
    await self._cleanup_batch_fsync()
    
    # Close files and remove from dict
    for logfile_path in list(self._files.keys()):
        await self._close_file(logfile_path)
        del self._files[logfile_path]
        del self._file_locks[logfile_path]
    
    # Clear read state
    self._read_files.clear()
    self._read_locks.clear()
    
    # Reset state
    self._initialized = False
    self._closing = False
```

---

## Part 8: Summary

**For Data Plane (Stats/Metrics)**:
- Use Logger as-is with default parameters
- JSON format, flush() only, no sequence numbers
- Fire-and-forget semantics, eventual consistency
- Queue overflow: warn and drop (acceptable loss)

**For Control Plane (WAL)**:
- Use Logger with durability=FSYNC_BATCH
- Binary format with CRC32, batched fsync, LSN tracking
- Crash recovery capability via read-back
- Guaranteed durability for job/workflow commands
- Queue overflow: block + checkpoint + timeout + raise (no loss)

### 8.1 Backpressure Behavior by Mode

| Mode | Queue Bound | On Full | Error Handling |
|------|-------------|---------|----------------|
| NONE | Unbounded | N/A | Silent (testing) |
| FLUSH | 10,000 | Drop + warn | Log to stderr |
| FSYNC | 10,000 | Block + checkpoint | Raise on timeout |
| FSYNC_BATCH | 10,000 | Block + checkpoint | Raise on timeout |

### 8.2 Key Guarantees

1. **Bounded Memory**: All queues have maxsize, all dicts cleaned on close
2. **No Silent Drops**: WAL modes raise explicit errors
3. **Fast Recovery**: Checkpoint signal triggers immediate flush
4. **Timeout Protection**: Stalled consumers cause explicit errors, not hangs
