---
ad_number: 38
name: Global Job Ledger with Per-Node Write-Ahead Logging
description: Tiered durability with per-node WAL and globally replicated ledger for cross-DC job coordination.
---

# AD-38: Global Job Ledger with Per-Node Write-Ahead Logging

**Decision**: Implement a tiered durability architecture combining per-node Write-Ahead Logs (WAL) with a globally replicated Job Ledger for cross-datacenter job coordination, with operation-specific durability levels and separate control/data planes.

**Related**: AD-20 (Cancellation), AD-33 (Federated Health Monitoring), AD-35 (Vivaldi Coordinates), AD-36 (Cross-DC Routing), AD-37 (Backpressure)

**Rationale**:
- Gates assign jobs to datacenters worldwide; job state must survive node, rack, and region failures.
- Per-node WAL provides sub-millisecond local durability for immediate crash recovery.
- Global ledger provides cross-region consistency and authoritative job state.
- Event sourcing enables audit trail, conflict detection, and temporal queries.
- Hybrid Logical Clocks provide causal ordering without requiring synchronized clocks.
- **Workers are under heavy CPU/memory load during tests and MUST NOT participate in any consensus path.**
- **Different operations have different durability requirements; one-size-fits-all is inefficient.**
- **Stats/metrics streaming requires high throughput, not strong consistency (Data Plane).**

**Operational Model**:

Hyperscale operates with three distinct node types with different responsibilities:

| Node Type | Role | Consensus Participation | Durability Responsibility |
|-----------|------|------------------------|---------------------------|
| **Gates** | Job submission, monitoring, cross-DC coordination | GLOBAL (full participant) | Job lifecycle (create/cancel/complete) |
| **Managers** | Workflow dispatch, worker health, DC coordination | REGIONAL (within DC only) | Workflow lifecycle, aggregated stats |
| **Workers** | Execute load tests (high CPU/memory) | NONE (fire-and-forget) | None - reports upward to manager |

**Critical Design Constraint**: Workers running load tests may be slow to respond (100ms+ for acks). They MUST NOT be in any consensus or acknowledgment path. Managers are the "durability boundary" within each datacenter.

## Architecture Overview

```
+-------------------------------------------------------------------------+
|                    TIER 1: Global Job Ledger (Gates Only)               |
|                    ---------------------------------                    |
|   Participants: Gates (global consensus)                                |
|   Operations: Job create, cancel, complete, timeout                     |
|   Durability: Survives region failure                                   |
|   Latency: 50-300ms                                                     |
+-------------------------------------------------------------------------+
                                    ^
                                    | Async replication (Causal+ consistency)
                                    | Circuit breakers for cross-DC failures
                                    |
+-------------------------------------------------------------------------+
|                    TIER 2: Regional Consensus (Gates + Managers)        |
|                    ----------------------------------------             |
|   Participants: Gates and Managers within datacenter                    |
|   Operations: Workflow dispatch, workflow complete, job acceptance      |
|   Durability: Survives node failure within DC                          |
|   Latency: 2-10ms                                                       |
+-------------------------------------------------------------------------+
                                    ^
                                    | Sync replication within DC
                                    |
+-------------------------------------------------------------------------+
|                    TIER 3: Per-Node WAL (Gates + Managers Only)         |
|                    -------------------------------------------          |
|                                                                         |
|   +-----------+     +-----------+     +-----------+                    |
|   |  Gate WAL |     |Manager WAL|     |Manager WAL|                    |
|   |  (job ops)|     |(wf ops)   |     |(wf ops)   |                    |
|   +-----------+     +-----------+     +-----------+                    |
|                                                                         |
|   Durability: Survives process crash (<1ms)                            |
+-------------------------------------------------------------------------+
                                    ^
                                    | Fire-and-forget + Acknowledgment Windows
                                    | (NO consensus participation)
                                    |
+-------------------------------------------------------------------------+
|                    WORKERS (No Durability Responsibility)               |
|                    ----------------------------------                   |
|                                                                         |
|   +-----------+     +-----------+     +-----------+                    |
|   |  Worker-1 |     |  Worker-2 |     |  Worker-N |                    |
|   | (executing)|    | (executing)|    | (executing)|                   |
|   |High CPU/Mem|    |High CPU/Mem|    |High CPU/Mem|                   |
|   +-----------+     +-----------+     +-----------+                    |
|                                                                         |
|   Reports: Progress updates (fire-and-forget to Manager)               |
|   Health: Manager detects failures via health checks, NOT consensus    |
|   Recovery: Manager reschedules workflows without global coordination  |
+-------------------------------------------------------------------------+
```

## Separate Control Plane vs Data Plane

**Control Plane (Reliable, Lower Volume)**:
- Job commands (create, cancel) - GLOBAL durability
- Workflow commands (dispatch) - REGIONAL durability
- Leader election - REGIONAL durability
- Cancellation propagation - GLOBAL durability
- Protocol: TCP with acks, consensus, WAL
- Requires: NodeWAL with fsync, binary format, CRC checksums

**Data Plane (High Throughput, Eventual Consistency)**:
- Progress updates from workers - LOCAL or NONE
- Stats streaming to gates - Batched, sampled
- Metrics aggregation - Eventual consistency OK
- Protocol: Fire-and-forget TCP, UDP, batching, sampling
- Uses: hyperscale/logging Logger (JSON, no fsync required)

---

## Part 1: Event Sourcing Model

All job state changes are stored as immutable events rather than mutable state:

**Event Types**:

| Event | Fields | Semantics |
|-------|--------|-----------|
| `JobCreated` | job_id, spec, assigned_dcs, fence_token, hlc | New job submitted |
| `JobAccepted` | job_id, dc_id, worker_count, fence_token, hlc | DC accepted job |
| `JobProgressReported` | job_id, dc_id, completed, failed, hlc | Progress update |
| `JobCancellationRequested` | job_id, reason, requestor, fence_token, hlc | Cancel initiated |
| `JobCancellationAcked` | job_id, dc_id, workflows_cancelled, hlc | DC confirmed cancel |
| `JobCompleted` | job_id, final_status, aggregate_metrics, hlc | Job finished |
| `JobFailed` | job_id, error, failed_dc, hlc | Job failed |
| `JobTimedOut` | job_id, timeout_type, last_progress_hlc, hlc | Job exceeded timeout |

---

## Part 2: Hybrid Logical Clocks (HLC)

HLC combines physical time with logical counters for causal ordering without clock synchronization:

**HLC Invariants**:
1. If event A causally precedes B, then HLC(A) < HLC(B)
2. HLC is always within bounded drift of physical time
3. Total ordering achieved via (wall_time, logical_counter, node_id)

---

## Part 3: Per-Node Write-Ahead Log

Each node maintains a local WAL for immediate crash recovery:

**WAL Entry Binary Format**:

```
+----------+----------+----------+----------+----------+----------+
| CRC32    | Length   | LSN      | HLC      | State    | Type     |
| (4 bytes)| (4 bytes)| (8 bytes)|(16 bytes)| (1 byte) | (1 byte) |
+----------+----------+----------+----------+----------+----------+
|                        Payload (variable)                        |
+------------------------------------------------------------------+

Total header: 34 bytes
CRC32: Covers all fields except CRC32 itself
```

**WAL Entry State Machine**:

```
+---------+
| PENDING | --- Written to local WAL
+----+----+
     | Regional consensus achieved
     v
+----------+
| REGIONAL | --- Replicated within datacenter
+----+-----+
     | Global ledger confirmed
     v
+--------+
| GLOBAL | --- Committed to global ledger
+----+---+
     | Applied to state machine
     v
+---------+
| APPLIED | --- State machine updated
+----+----+
     | Checkpoint created
     v
+-----------+
| COMPACTED | --- Safe to garbage collect
+-----------+
```

---

## Part 3.1: WAL Group Commit Architecture

NodeWAL uses a dedicated writer thread with group commit for optimal throughput without sacrificing durability.

**Design Principles**:
- Single thread owns the file handle exclusively (no races, no leaks)
- Batches writes: collect for N microseconds OR until batch full
- Single write() + single fsync() commits entire batch
- Resolves all futures in batch after fsync completes
- File handle cleanup guaranteed by thread ownership

**Throughput Model**:

| fsync Latency | Batches/sec | Entries/Batch | Entries/sec |
|---------------|-------------|---------------|-------------|
| 500μs | 2,000 | 100 | 200,000 |
| 500μs | 2,000 | 1,000 | 2,000,000 |
| 100μs (NVMe) | 10,000 | 100 | 1,000,000 |

**Write Pipeline**:

```
Writers (concurrent)        WALWriter Thread            Disk
     │                           │                        │
     ├─► append(entry1) ────────►│                        │
     ├─► append(entry2) ────────►├─► write(batch)        │
     ├─► append(entry3) ────────►├─► fsync() ───────────►│
     │                           │                        │
     ◄── future1.resolve() ◄────┤                        │
     ◄── future2.resolve() ◄────┤                        │
     ◄── future3.resolve() ◄────┤                        │
```

**Batching Parameters**:
- `batch_timeout_microseconds`: Max time to wait for more entries (default: 500μs)
- `batch_max_entries`: Max entries per batch (default: 1,000)
- `batch_max_bytes`: Max bytes per batch (default: 1MB)

**Recovery Path**:
- Runs once at startup in executor thread
- Reads entire file into memory buffer with `with open()` (guaranteed cleanup)
- Parses entries from buffer after file is closed
- No file handle leak possible - parsing failures occur after close

**File Handle Safety**:
- Writer thread owns file handle exclusively
- Handle opened in `_run()`, closed in `finally` block
- If thread dies, handle closes with thread
- Recovery uses context manager - automatic cleanup on any failure

---

## Part 3.3: Logger Suitability Analysis

**Suitability Matrix**:

| Requirement | Logger Has? | WAL Needs? | Data Plane Needs? |
|-------------|-------------|------------|-------------------|
| Async file I/O | Yes | Yes | Yes |
| Per-file locking | Yes | Yes | Optional |
| fsync guarantee | No (flush only) | **Critical** | Not needed |
| Sequence numbers | No | **Critical** | Not needed |
| Binary format with CRC | No (JSON) | **Critical** | Not needed |
| Read-back capability | No (write-only) | **Critical** | Not needed |

**Verdict**:
- **Control Plane WAL**: Build dedicated NodeWAL class
- **Data Plane Stats**: Use Logger as-is

---

## Part 3.4: Operation-Specific Durability

| Operation | Durability | Latency | Rationale |
|-----------|------------|---------|-----------|
| **Job Create** | GLOBAL | 50-300ms | Must survive region loss; authoritative |
| **Job Cancel** | GLOBAL | 50-300ms | Safety-critical; must propagate everywhere |
| **Job Complete** | GLOBAL | 50-300ms | Final state; audit trail requirement |
| **Workflow Dispatch** | REGIONAL | 2-10ms | Manager is DC authority |
| **Workflow Complete** | REGIONAL | 2-10ms | Aggregated to gate async |
| **Progress Update** | LOCAL | <1ms | High volume; manager aggregates |
| **Stats Report** | NONE | ~0ms | Fire-and-forget; eventual consistency |

---

## Part 4: Commit Pipeline

Three-stage commit with progressive durability guarantees:

**Durability Levels**:

| Level | Latency | Survives | Use Case |
|-------|---------|----------|----------|
| LOCAL | <1ms | Process crash | High-throughput updates |
| REGIONAL | 2-10ms | Node failure | Normal job operations |
| GLOBAL | 50-300ms | Region failure | Critical operations (cancel) |

---

## Part 5: Global Job Ledger

Cross-region consensus for authoritative job state:

**Job ID Format** (encodes home region):

```
Format: {region_code}-{timestamp_ms}-{gate_id}-{sequence}
Example: use1-1704931200000-gate42-00001

Benefits:
- Lexicographically sortable by time
- Instant routing to authoritative region
- No coordination needed for ID generation
- Region encoded for fast authority lookup
```

**Conflict Resolution**:

Resolution priority (deterministic):
1. Cancellation always wins (fail-safe)
2. Higher fence token wins (later operation)
3. HLC ordering (causal precedence)
4. Lexicographic node_id (deterministic tie-breaker)

---

## Part 6: Anti-Entropy and Repair

Merkle tree-based consistency verification enables efficient repair of divergent state across regions.

---

## Part 7: Checkpoint and Compaction

Efficient recovery through periodic snapshots:
- Checkpoint captures local state machine snapshot
- Records LSN watermarks (local, regional, global)
- Enables WAL compaction (remove checkpointed entries)
- Supports state transfer to new nodes

---

## Part 8: Session Consistency Guarantees

| Level | Guarantee | Latency | Use Case |
|-------|-----------|---------|----------|
| EVENTUAL | May read stale | Fastest | Dashboards, monitoring |
| SESSION | Read-your-writes | Low | Normal operations |
| BOUNDED_STALENESS | Max lag = X ms | Medium | Cross-region queries |
| STRONG | Authoritative | Highest | Status verification |
