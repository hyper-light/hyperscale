---
ad_number: 30
name: Hierarchical Failure Detection for Multi-Job Distributed Systems
description: Two-layer failure detection separating machine liveness from job-specific responsiveness
---

# AD-30: Hierarchical Failure Detection for Multi-Job Distributed Systems

**Decision**: Implement a two-layer hierarchical failure detection system that separates machine-level liveness (global layer) from job-specific responsiveness (job layer), solving timer starvation issues and enabling accurate result routing in multi-job environments.

**Rationale**:
The original SWIM + Lifeguard implementation suffered from **timer starvation** where rapid gossip confirmations caused suspicion timers to be continuously rescheduled before they could expire. In a globally distributed system with multiple concurrent jobs, we also need to distinguish between "machine is dead" (affects all jobs) and "node is slow for job X" (affects only that job).

## Problem Statement - Timer Starvation

```
Original SuspicionManager flow with confirmation-based rescheduling:

T=0.00: Node A fails probe to Node B -> start_suspicion(B, timeout=5s)
T=0.05: Node C gossips "B is suspect" -> confirm_suspicion(B) -> RESCHEDULE timer
T=0.10: Node D gossips "B is suspect" -> confirm_suspicion(B) -> RESCHEDULE timer
T=0.15: Node E gossips "B is suspect" -> confirm_suspicion(B) -> RESCHEDULE timer
...
T=4.95: Node Z gossips "B is suspect" -> confirm_suspicion(B) -> RESCHEDULE timer
T=5.00: Timer should expire... but was just reset to 4.5s remaining!

Result: Timer NEVER expires. Node B is never declared dead even though
        it hasn't responded to probes for 5+ seconds.

Root cause: Each confirmation cancels the old timer and creates a new one.
            With gossip echo (O(log n) dissemination), confirmations arrive
            faster than the (now shorter) timeout can elapse.
```

## Problem Statement - Multi-Job Routing

```
Scenario: Manager M1 runs jobs A, B, C simultaneously

Job A: High CPU load (90%), responses slow
Job B: Normal load (30%), responses normal
Job C: Memory pressure (85%), responses slow

With single-layer detection:
- M1 is either "alive" or "dead" for ALL jobs
- Can't route Job A results away from slow M1
- Can't keep Job B results on healthy M1

Need: Per-job suspicion that tracks "is this node responsive for THIS job?"
```

## Solution: Two-Layer Hierarchical Detection

```
+---------------------------------------------------------------------------------+
|                   HIERARCHICAL FAILURE DETECTION                                |
+---------------------------------------------------------------------------------+
|                                                                                 |
|  +-----------------------------------------------------------------------------+|
|  |                      GLOBAL LAYER (TimingWheel)                             ||
|  |                                                                             ||
|  |   Question: "Is this MACHINE alive?"                                        ||
|  |                                                                             ||
|  |   Triggers: SWIM probe timeout (machine-level liveness)                     ||
|  |   Timeout: 5-30 seconds (configurable)                                      ||
|  |   Effect: Global death clears ALL job suspicions for that node              ||
|  |                                                                             ||
|  |   Implementation: Kafka-style hierarchical timing wheel                     ||
|  |   - O(1) timer insertion and removal                                        ||
|  |   - Single timer advancement (no per-suspicion timers)                      ||
|  |   - Confirmation updates state, NOT timer                                   ||
|  |                                                                             ||
|  |   Coarse Wheel (1s ticks) -> Fine Wheel (100ms ticks)                       ||
|  |   Entries cascade from coarse to fine as they approach expiration           ||
|  +-----------------------------------------------------------------------------+|
|                                    |                                            |
|                                    | Global death -> Clear job suspicions       |
|                                    v                                            |
|  +-----------------------------------------------------------------------------+|
|  |                       JOB LAYER (JobSuspicionManager)                       ||
|  |                                                                             ||
|  |   Question: "Is this node RESPONSIVE for THIS JOB?"                         ||
|  |                                                                             ||
|  |   Triggers: Job-specific communication timeout                              ||
|  |   Timeout: 1-10 seconds (faster than global)                                ||
|  |   Effect: Job-specific routing decisions                                    ||
|  |                                                                             ||
|  |   Implementation: Adaptive polling with LHM integration                     ||
|  |   - Per (job_id, node) suspicion state                                      ||
|  |   - Poll interval adapts: far (1s) -> medium (250ms) -> near (50ms)         ||
|  |   - Confirmation updates state only (no timer reschedule)                   ||
|  |   - LHM multiplier extends polling under load                               ||
|  |                                                                             ||
|  |   Job A          | Job B          | Job C                                   ||
|  |   Node1: OK      | Node1: OK      | Node1: SUSPECT                          ||
|  |   Node2: SUSPECT | Node2: OK      | Node2: OK                               ||
|  |   Node3: OK      | Node3: OK      | Node3: SUSPECT                          ||
|  |                                                                             ||
|  |   Independent suspicion per (job_id, node) pair                             ||
|  +-----------------------------------------------------------------------------+|
|                                                                                 |
+---------------------------------------------------------------------------------+
```

## Component Architecture

```
+---------------------------------------------------------------------------------+
|                      HierarchicalFailureDetector                                |
|                                                                                 |
|  +-----------------------------------------------------------------------------+|
|  |                              PUBLIC API                                     ||
|  +-----------------------------------------------------------------------------+|
|  | start() / stop()              - Lifecycle management                        ||
|  | suspect_global(node, inc)     - Start global suspicion                      ||
|  | suspect_job(job, node, inc)   - Start job-specific suspicion                ||
|  | confirm_global/job(...)       - Add confirmation (NO timer reschedule)      ||
|  | refute_global/job(...)        - Clear suspicion (higher incarnation)        ||
|  | is_alive_global(node)         - Query: machine up?                          ||
|  | is_alive_for_job(job, node)   - Query: node responsive for job?             ||
|  | clear_job(job_id)             - Cleanup when job completes                  ||
|  | get_node_status(node)         - Comprehensive status query                  ||
|  +-----------------------------------------------------------------------------+|
|                                    |                                            |
|          +-------------------------+---------------------------+                |
|          v                                                     v                |
|  +-------------------+                            +-------------------+         |
|  |   TimingWheel     |                            | JobSuspicionMgr   |         |
|  |                   |                            |                   |         |
|  | * Coarse buckets  |                            | * Per-job tracking|         |
|  | * Fine buckets    |                            | * Adaptive polling|         |
|  | * Single tick     |                            | * LHM integration |         |
|  | * O(1) ops        |                            | * Resource limits |         |
|  +-------------------+                            +-------------------+         |
|          |                                                  |                   |
|          | on_expired(node, state)                          | on_expired(job,   |
|          v                                                  v  node, inc)       |
|  +-----------------------------------------------------------------------+      |
|  |                         CALLBACK HANDLERS                              |      |
|  |                                                                        |      |
|  |  _handle_global_expiration:           _handle_job_expiration:          |      |
|  |  1. Mark node as globally dead        1. Record job-specific death     |      |
|  |  2. Clear ALL job suspicions          2. Invoke on_job_death callback  |      |
|  |  3. Invoke on_global_death callback   3. Update job routing state      |      |
|  |  4. Record failure event                                               |      |
|  +-----------------------------------------------------------------------+      |
|                                                                                 |
|  +-----------------------------------------------------------------------+      |
|  |                        RECONCILIATION LOOP                             |      |
|  |                                                                        |      |
|  |  Periodic (every 5s):                                                  |      |
|  |  - Clear job suspicions for globally-dead nodes                        |      |
|  |  - Detect inconsistencies between layers                               |      |
|  |  - Log/escalate anomalies                                              |      |
|  +-----------------------------------------------------------------------+      |
+---------------------------------------------------------------------------------+
```

## Timing Wheel Design (Global Layer)

```
+---------------------------------------------------------------------------------+
|                           TIMING WHEEL INTERNALS                                |
+---------------------------------------------------------------------------------+
|                                                                                 |
|  Configuration:                                                                 |
|  * coarse_tick_ms: 1000 (1 second per coarse bucket)                           |
|  * fine_tick_ms: 100 (100ms per fine bucket)                                   |
|  * coarse_buckets: 64 (64 seconds max timeout in coarse wheel)                 |
|  * fine_buckets: 10 (1 second of fine-grained resolution)                      |
|                                                                                 |
|  COARSE WHEEL (1s resolution)                                                  |
|  Bucket 0    Bucket 1    Bucket 2    ...    Bucket 63                          |
|  [Entry A]   [      ]    [Entry C]          [      ]                           |
|  [Entry B]                                                                     |
|                                                                                 |
|  When current bucket expires -> cascade entries to fine wheel                  |
|                                                                                 |
|  FINE WHEEL (100ms resolution)                                                 |
|  Bucket 0    Bucket 1    Bucket 2    ...    Bucket 9                           |
|  [Entry X]   [Entry Y]   [      ]           [      ]                           |
|                                                                                 |
|  When fine bucket expires -> fire expiration callbacks                         |
|                                                                                 |
|  TICK ADVANCEMENT (single task, runs every fine_tick_ms):                      |
|                                                                                 |
|  async def _tick():                                                             |
|      # Advance fine wheel                                                       |
|      fine_idx = (fine_idx + 1) % fine_buckets                                  |
|      if fine_idx == 0:                                                          |
|          # Wrapped around - advance coarse wheel                                |
|          coarse_idx = (coarse_idx + 1) % coarse_buckets                        |
|          # Cascade coarse bucket entries to fine wheel                         |
|          for entry in coarse_buckets[coarse_idx]:                              |
|              fine_target = calculate_fine_bucket(entry.expiration)             |
|              fine_buckets[fine_target].add(entry)                              |
|                                                                                 |
|      # Fire expired entries in current fine bucket                             |
|      for entry in fine_buckets[fine_idx]:                                      |
|          if entry.expiration <= now:                                           |
|              on_expired(entry.node, entry.state)                               |
|                                                                                 |
+---------------------------------------------------------------------------------+
```

## Adaptive Polling Design (Job Layer)

```
+---------------------------------------------------------------------------------+
|                       ADAPTIVE POLLING ALGORITHM                                |
+---------------------------------------------------------------------------------+
|                                                                                 |
|  Each JobSuspicion has a single polling task (NOT timer-per-suspicion):        |
|                                                                                 |
|  async def _poll_suspicion(suspicion):                                          |
|      while not suspicion.cancelled and running:                                 |
|          remaining = suspicion.time_remaining(n_members)                        |
|                                                                                 |
|          if remaining <= 0:                                                     |
|              # EXPIRED - declare dead                                           |
|              await _handle_expiration(suspicion)                                |
|              return                                                             |
|                                                                                 |
|          # Calculate adaptive poll interval                                     |
|          poll_interval = _calculate_poll_interval(remaining)                    |
|          sleep_time = min(poll_interval, remaining)                             |
|                                                                                 |
|          await asyncio.sleep(sleep_time)                                        |
|          # Loop continues - if confirmations arrived, time_remaining shorter   |
|                                                                                 |
|  Poll Interval Selection:                                                       |
|  +-----------------------------------------------------------------------+     |
|  |  Time Remaining      Base Interval    After LHM (x2)                  |     |
|  |  ----------------    -------------    --------------                  |     |
|  |  > 5 seconds         1000ms (far)     2000ms                          |     |
|  |  1-5 seconds         250ms (medium)   500ms                           |     |
|  |  < 1 second          50ms (near)      100ms                           |     |
|  +-----------------------------------------------------------------------+     |
|                                                                                 |
|  KEY INSIGHT: Confirmations update suspicion STATE (confirmation_count).        |
|               The poll loop naturally picks up the shorter timeout on next poll.|
|               NO timer cancellation/rescheduling needed!                        |
|                                                                                 |
|  Before (timer starvation):           After (adaptive polling):                 |
|  -------------------------           -----------------------                   |
|  T=0: start_suspicion                 T=0: start_suspicion                      |
|  T=0.1: confirm -> CANCEL + NEW timer T=0.1: confirm -> update count            |
|  T=0.2: confirm -> CANCEL + NEW timer T=0.2: confirm -> update count            |
|  ...timer never expires...            T=0.5: poll -> remaining=4.0s, sleep      |
|                                       T=1.0: poll -> remaining=3.0s, sleep      |
|                                       ...                                       |
|                                       T=5.0: poll -> remaining=0, EXPIRE        |
|                                                                                 |
+---------------------------------------------------------------------------------+
```

## Node Status State Machine

```
NodeStatus enum:
+---------------+  +---------------------+  +-----------------+
|    ALIVE      |  |  SUSPECTED_GLOBAL   |  |  SUSPECTED_JOB  |
|               |  |                     |  |                 |
| Not suspected |  | Suspected at global |  | Suspected for   |
| at any layer  |  | layer (machine may  |  | specific job(s) |
|               |  | be down)            |  | but not global  |
+-------+-------+  +----------+----------+  +--------+--------+
        |                     |                      |
        |                     v                      v
        |          +---------------------+  +-----------------+
        |          |    DEAD_GLOBAL      |  |    DEAD_JOB     |
        |          |                     |  |                 |
        |          | Declared dead at    |  | Declared dead   |
        |          | global level        |  | for specific    |
        |          | (machine is down)   |  | job only        |
        |          +---------------------+  +-----------------+
        |                     |
        +---------------------+
                              |
                              v
                    Global death clears all job suspicions

State Transitions:
+---------+    suspect_global()     +------------------+
|  ALIVE  | ----------------------> | SUSPECTED_GLOBAL |
+---------+                         +--------+---------+
     ^                                       |
     | refute_global() or                    | timeout without
     | clear_global_death()                  | refutation
     |                                       v
     |                              +------------------+
     +------------------------------+   DEAD_GLOBAL    |
       (node rejoins with           +------------------+
        higher incarnation)                  |
                                             | triggers
                                             v
                                    Clear all job suspicions
                                    for this node
```

## Integration with HealthAwareServer

```python
class HealthAwareServer(MercurySyncBaseServer):
    """Base SWIM server with optional hierarchical detection."""

    def __init__(self, ...):
        ...
        # Optional hierarchical detector (initialized by subclasses)
        self._hierarchical_detector: HierarchicalFailureDetector | None = None

    # Initialization (called by subclasses in their __init__)
    def init_hierarchical_detector(
        self,
        config: HierarchicalConfig | None = None,
        on_global_death: Callable[[tuple[str,int], int], None] | None = None,
        on_job_death: Callable[[str, tuple[str,int], int], None] | None = None,
        get_job_n_members: Callable[[str], int] | None = None,
    ) -> HierarchicalFailureDetector:
        """Initialize hierarchical detector with callbacks."""
        self._hierarchical_detector = HierarchicalFailureDetector(
            config=config,
            on_global_death=on_global_death,
            on_job_death=on_job_death,
            get_n_members=self._get_member_count,   # From SWIM membership
            get_job_n_members=get_job_n_members,
            get_lhm_multiplier=self._get_lhm_multiplier,  # From LHM
        )
        return self._hierarchical_detector

    # Lifecycle (called by subclasses in start()/stop())
    async def start_hierarchical_detector(self) -> None:
        if self._hierarchical_detector:
            await self._hierarchical_detector.start()

    async def stop_hierarchical_detector(self) -> None:
        if self._hierarchical_detector:
            await self._hierarchical_detector.stop()

    # Convenience methods (fail-open if detector not initialized)
    async def suspect_node_global(self, node, inc, from_node) -> bool
    async def suspect_node_for_job(self, job, node, inc, from_node) -> bool
    async def is_node_alive_global(self, node) -> bool
    def is_node_alive_for_job(self, job, node) -> bool
    async def clear_job_suspicions(self, job_id) -> int
    async def get_node_hierarchical_status(self, node) -> NodeStatus | None
```

## Resource Limits and Bounds

```
Global Layer (TimingWheel):
---------------------------
* max_entries: 10,000 (default)
* Memory per entry: ~200 bytes (SuspicionState + wheel bookkeeping)
* Max memory: ~2MB for 10K entries
* Single tick task: O(bucket_size) per tick

Job Layer (JobSuspicionManager):
--------------------------------
* max_suspicions_per_job: 1,000 (default)
* max_total_suspicions: 50,000 (default)
* Memory per suspicion: ~300 bytes (JobSuspicion + polling state)
* Max memory: ~15MB for 50K suspicions
* One poll task per active suspicion (lightweight, mostly sleeping)

Graceful Degradation:
---------------------
When limits are reached:
* New suspicions are REJECTED (start_suspicion returns None/False)
* Existing suspicions continue to be tracked
* Cleanup runs periodically to remove expired entries
* Metrics/logs indicate limit reached

if len(suspicions) >= max_total_suspicions:
    # Try cleanup first
    cleanup_orphaned()
    if len(suspicions) >= max_total_suspicions:
        return None  # Reject - at capacity
```

## Files Modified/Created

| File | Description |
|------|-------------|
| `hyperscale/distributed_rewrite/swim/detection/timing_wheel.py` | Kafka-style hierarchical timing wheel for O(1) timer operations |
| `hyperscale/distributed_rewrite/swim/detection/job_suspicion_manager.py` | Per-job adaptive polling suspicion manager |
| `hyperscale/distributed_rewrite/swim/detection/hierarchical_failure_detector.py` | Coordinator for global + job layers |
| `hyperscale/distributed_rewrite/swim/detection/__init__.py` | Updated exports |
| `hyperscale/distributed_rewrite/swim/health_aware_server.py` | Integration methods for subclasses |
| `tests/integration/test_timing_wheel.py` | Comprehensive timing wheel tests |
| `tests/integration/test_job_suspicion_manager.py` | Job suspicion manager tests |
| `tests/integration/test_hierarchical_failure_detector.py` | End-to-end hierarchical detection tests |

## Testing Strategy

**1. Unit Tests** (per component):
- TimingWheel: bucket operations, tick advancement, cascade, expiration
- JobSuspicionManager: adaptive polling, confirmation handling, cleanup
- HierarchicalFailureDetector: layer coordination, reconciliation

**2. Integration Tests**:
- Timer starvation scenario (rapid confirmations)
- Global death clears job suspicions
- Job-specific failure with global alive
- LHM adjustment propagation
- Concurrent operations (asyncio correctness)

**3. Edge Cases**:
- Max limits reached (graceful rejection)
- Node rejoins after global death
- Job completion during active suspicion
- Network partition (some layers detect, others don't)

**Alternatives Considered**:

1. **Single Timer with Dynamic Timeout**: Simpler but still has reschedule overhead
2. **Confirmation Debouncing**: Delays confirmation propagation, affects protocol correctness
3. **Timeout Floor**: Minimum timeout regardless of confirmations, but wastes time when node is clearly dead
4. **Batch Confirmation Processing**: Reduces reschedules but adds latency
5. **Hierarchical Without Job Layer**: Loses per-job routing capability

**Trade-offs**:

| Aspect | Before | After |
|--------|--------|-------|
| Timer management | Per-suspicion timers | Single tick + adaptive polling |
| Confirmation handling | Cancel + reschedule | State update only |
| Memory overhead | Lower | Higher (two layers) |
| Complexity | Simpler | More complex |
| Job awareness | None | Full per-job tracking |
| Timer starvation | Vulnerable | Immune |
| Routing accuracy | Global only | Per-job granularity |
