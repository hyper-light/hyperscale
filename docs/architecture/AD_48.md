---
ad_number: 48
name: Cross-Manager Worker Visibility via TCP Broadcast and Gossip Piggyback
description: Disseminate worker state across managers using TCP for critical events and UDP gossip for steady-state
---

# AD-48: Cross-Manager Worker Visibility via TCP Broadcast and Gossip Piggyback

**Decision**: Implement cross-manager worker visibility using TCP broadcast for critical events (registration, death) and UDP gossip piggyback for steady-state dissemination. Each worker has ONE owner manager that is authoritative; other managers track workers as "remote" with reduced trust.

**Related**: AD-33 (Federated Health Monitoring), AD-19 (Three-Signal Health Model), AD-21 (Jitter Strategies)

**Rationale**:
- Currently workers only register with a single manager, meaning each manager only sees workers that directly registered with it
- In a cluster with 3 managers and 6 workers, each manager only sees ~2 workers instead of all 6
- For proper workflow scheduling and load balancing, managers need visibility into ALL workers in the cluster
- Existing `WorkerDiscoveryBroadcast` message exists but is never instantiated/sent (stub implementation)

---

## Part 1: Architecture Overview

```
                           WORKER STATE DISSEMINATION
                           
  ┌─────────────────────────────────────────────────────────────────────────┐
  │                         MANAGER CLUSTER                                  │
  │                                                                          │
  │  ┌─────────────────┐     ┌─────────────────┐     ┌─────────────────┐   │
  │  │   Manager A     │     │   Manager B     │     │   Manager C     │   │
  │  │                 │     │                 │     │                 │   │
  │  │  Local Workers: │     │  Local Workers: │     │  Local Workers: │   │
  │  │  - Worker 1     │     │  - Worker 3     │     │  - Worker 5     │   │
  │  │  - Worker 2     │     │  - Worker 4     │     │  - Worker 6     │   │
  │  │                 │     │                 │     │                 │   │
  │  │  Remote Workers:│     │  Remote Workers:│     │  Remote Workers:│   │
  │  │  - Worker 3*    │◄────│                 │────►│  - Worker 1*    │   │
  │  │  - Worker 4*    │     │  - Worker 1*    │     │  - Worker 2*    │   │
  │  │  - Worker 5*    │     │  - Worker 2*    │     │  - Worker 3*    │   │
  │  │  - Worker 6*    │     │  - Worker 5*    │     │  - Worker 4*    │   │
  │  │                 │     │  - Worker 6*    │     │                 │   │
  │  └────────┬────────┘     └────────┬────────┘     └────────┬────────┘   │
  │           │                       │                       │             │
  │           │    TCP (critical)     │    TCP (critical)     │             │
  │           │◄─────────────────────►│◄─────────────────────►│             │
  │           │                       │                       │             │
  │           │    UDP gossip         │    UDP gossip         │             │
  │           │    (steady-state)     │    (steady-state)     │             │
  │           │◄ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ►│◄ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─►│             │
  │           │                       │                       │             │
  └───────────┴───────────────────────┴───────────────────────┴─────────────┘
  
  * Remote workers: tracked with is_remote=True, owner_manager_id set
```

---

## Part 2: Dissemination Strategy

### Two-Channel Approach

| Channel | Use Case | Latency | Reliability |
|---------|----------|---------|-------------|
| **TCP Broadcast** | Critical events (register, death, eviction) | Immediate (~ms) | Guaranteed delivery |
| **UDP Gossip** | Steady-state, missed updates | O(log n) rounds | Eventual consistency |

### Why Both Channels?

1. **TCP alone is insufficient**: If a manager misses a broadcast (network partition, restart), it never learns about the worker
2. **UDP alone is too slow**: Registration should be visible cluster-wide immediately for scheduling
3. **Combined**: TCP provides immediate visibility, gossip provides convergence guarantee

### Incarnation Numbers

Each worker state update carries an incarnation number:
- Incremented by owner manager on each state change
- Receivers reject updates with lower incarnation (stale)
- Prevents out-of-order updates from overwriting newer state

---

## Part 3: Message Model

### WorkerStateUpdate

```python
# hyperscale/distributed/models/worker_state.py

@dataclass(slots=True, kw_only=True)
class WorkerStateUpdate:
    """
    Worker state update for cross-manager dissemination.
    
    Sent via TCP on critical events and piggybacked on UDP gossip.
    """
    worker_id: str
    owner_manager_id: str
    host: str
    tcp_port: int
    udp_port: int
    
    # State info
    state: str  # "registered", "dead", "evicted", "left"
    incarnation: int  # Monotonic, reject lower incarnation
    
    # Capacity (for scheduling decisions)
    total_cores: int
    available_cores: int
    
    # Metadata
    timestamp: float  # time.monotonic() on owner manager
    datacenter: str = ""
    
    def to_bytes(self) -> bytes:
        """Serialize for piggyback transmission."""
        ...
    
    @classmethod
    def from_bytes(cls, data: bytes) -> "WorkerStateUpdate | None":
        """Deserialize from piggyback."""
        ...
```

---

## Part 4: Gossip Buffer for Worker State

### WorkerStateGossipBuffer

Follows the same pattern as `GossipBuffer` but specialized for worker state:

```python
# hyperscale/distributed/swim/gossip/worker_state_gossip_buffer.py

WORKER_STATE_SEPARATOR = b"#|w"  # New separator for worker state piggyback

@dataclass(slots=True)
class WorkerStateGossipBuffer:
    """
    Buffer for worker state updates to be piggybacked on SWIM messages.
    
    Same dissemination strategy as membership gossip:
    - Updates broadcast lambda * log(n) times
    - Higher incarnation replaces lower
    - Stale updates cleaned up periodically
    """
    updates: dict[str, WorkerStatePiggybackUpdate]  # worker_id -> update
    broadcast_multiplier: int = 3  # lambda in SWIM paper
    max_updates: int = 500
    stale_age_seconds: float = 60.0
    max_piggyback_size: int = 600  # Leave room for membership piggyback
```

### Piggyback Integration

Worker state piggyback is appended AFTER membership piggyback:

```
[base_message][#|m membership_updates][#|w worker_state_updates]
```

This maintains backward compatibility - nodes that don't understand `#|w` simply ignore it.

---

## Part 5: WorkerDisseminator Class

### Responsibilities

1. **Broadcast worker events** to peer managers via TCP
2. **Add updates to gossip buffer** for piggyback dissemination
3. **Track worker incarnations** for stale update rejection
4. **Handle incoming updates** from peers

```python
# hyperscale/distributed/nodes/manager/worker_dissemination.py

class WorkerDisseminator:
    """
    Handles cross-manager worker state dissemination.
    
    Broadcasts worker events (register, death) to peer managers via TCP
    and adds updates to gossip buffer for steady-state dissemination.
    """
    
    def __init__(
        self,
        state: "ManagerState",
        config: "ManagerConfig",
        logger: "Logger",
        node_id: str,
        task_runner,
        send_tcp,
        gossip_buffer: WorkerStateGossipBuffer,
    ) -> None:
        ...
    
    async def broadcast_worker_registered(self, registration: WorkerRegistration) -> None:
        """Broadcast worker registration to all peer managers."""
        ...
    
    async def broadcast_worker_dead(self, worker_id: str, reason: str) -> None:
        """Broadcast worker death/eviction to all peer managers."""
        ...
    
    async def handle_worker_state_update(
        self,
        update: WorkerStateUpdate,
        source_addr: tuple[str, int],
    ) -> bool:
        """Handle incoming worker state update from peer manager."""
        ...
    
    async def request_worker_list_from_peers(self) -> None:
        """Request full worker list from peer managers (on join)."""
        ...
```

---

## Part 6: WorkerPool Modifications

### Remote Worker Tracking

```python
# hyperscale/distributed/jobs/worker_pool.py

class WorkerPool:
    def __init__(self, ...):
        ...
        # Remote worker tracking (AD-48)
        self._remote_workers: dict[str, WorkerStatus] = {}
        self._worker_incarnations: dict[str, int] = {}
    
    async def register_remote_worker(
        self,
        update: WorkerStateUpdate,
    ) -> bool:
        """
        Register a worker owned by another manager.
        
        Remote workers are tracked separately and have reduced trust:
        - Not used for scheduling unless owner manager is unreachable
        - State updates only accepted from owner manager
        - Cleaned up if owner manager dies
        """
        ...
    
    async def deregister_remote_worker(self, worker_id: str) -> bool:
        """Remove a remote worker."""
        ...
    
    def get_all_workers(self) -> list[WorkerStatus]:
        """Get all workers (local + remote)."""
        return list(self._workers.values()) + list(self._remote_workers.values())
    
    def is_worker_local(self, worker_id: str) -> bool:
        """Check if worker is locally owned."""
        return worker_id in self._workers
```

---

## Part 7: TCP Handlers

### New Handlers in ManagerServer

```python
# hyperscale/distributed/nodes/manager/server.py

# Message type: "worker_state_update"
async def handle_worker_state_update(
    self,
    data: bytes,
    addr: tuple[str, int],
) -> bytes:
    """Handle worker state update from peer manager."""
    update = WorkerStateUpdate.from_bytes(data)
    if update:
        accepted = await self._worker_disseminator.handle_worker_state_update(update, addr)
        return b"accepted" if accepted else b"rejected"
    return b"invalid"

# Message type: "list_workers"
async def handle_list_workers(
    self,
    data: bytes,
    addr: tuple[str, int],
) -> bytes:
    """Return list of locally-owned workers to requesting peer."""
    workers = self._worker_pool.iter_workers()
    updates = [
        WorkerStateUpdate(
            worker_id=w.worker_id,
            owner_manager_id=self._node_id,
            host=w.registration.node.host,
            tcp_port=w.registration.node.tcp_port,
            udp_port=w.registration.node.udp_port,
            state="registered",
            incarnation=self._state.get_worker_incarnation(w.worker_id),
            total_cores=w.total_cores,
            available_cores=w.available_cores,
            timestamp=time.monotonic(),
            datacenter=self._config.datacenter,
        )
        for w in workers
        if w.registration
    ]
    return WorkerListResponse(workers=updates).to_bytes()
```

---

## Part 8: Event Trigger Points

### On Worker Registration (`manager/server.py`)

```python
async def handle_worker_register(self, data: bytes, addr: tuple[str, int]) -> bytes:
    # ... existing registration logic ...
    
    # AD-48: Broadcast to peer managers
    if self._worker_disseminator:
        await self._worker_disseminator.broadcast_worker_registered(registration)
    
    return response
```

### On Worker Death (`manager/server.py`)

```python
def _on_worker_globally_dead(self, worker_id: str) -> None:
    self._health_monitor.on_global_death(worker_id)
    
    # AD-48: Broadcast death to peer managers
    if self._worker_disseminator:
        self._task_runner.run(
            self._worker_disseminator.broadcast_worker_dead,
            worker_id,
            "dead",
        )
```

### On Worker Eviction (`manager/server.py`)

```python
async def _evict_worker_deadline_expired(self, worker_id: str) -> None:
    # ... existing eviction logic ...
    
    # AD-48: Broadcast eviction to peer managers
    if self._worker_disseminator:
        await self._worker_disseminator.broadcast_worker_dead(worker_id, "evicted")
```

### On Worker Leave (`manager/registry.py`)

```python
async def unregister_worker(self, worker_id: str) -> bool:
    # AD-48: Broadcast leave to peer managers (before cleanup)
    if self._worker_disseminator:
        await self._worker_disseminator.broadcast_worker_dead(worker_id, "left")
    
    # ... existing cleanup logic ...
```

---

## Part 9: Gossip Integration

### Health-Aware Server Modifications

```python
# hyperscale/distributed/swim/health_aware_server.py

class HealthAwareServer:
    def __init__(self, ...):
        ...
        # AD-48: Worker state gossip buffer
        self._worker_state_gossip = WorkerStateGossipBuffer()
    
    def _encode_piggyback_data(self, base_message: bytes) -> bytes:
        """Encode all piggyback data for transmission."""
        result = base_message
        
        # Existing piggybacks
        result += self._gossip_buffer.encode_piggyback_with_base(result)
        result += self._state_piggyback.encode_with_base(result)
        result += self._health_piggyback.encode_with_base(result)
        result += self._vivaldi_piggyback.encode_with_base(result)
        
        # AD-48: Worker state piggyback
        result += self._worker_state_gossip.encode_piggyback_with_base(result)
        
        return result
    
    def _decode_and_process_piggyback(self, data: bytes) -> None:
        """Decode and process all piggyback data."""
        # ... existing piggyback processing ...
        
        # AD-48: Process worker state piggyback
        if WORKER_STATE_SEPARATOR in data:
            worker_idx = data.index(WORKER_STATE_SEPARATOR)
            worker_data = data[worker_idx:]
            updates = WorkerStateGossipBuffer.decode_piggyback(worker_data)
            for update in updates:
                self._process_worker_state_update(update)
```

---

## Part 10: Manager Join Protocol

When a manager joins the cluster, it needs to learn about all existing workers:

```python
# hyperscale/distributed/nodes/manager/worker_dissemination.py

async def request_worker_list_from_peers(self) -> None:
    """
    Request full worker list from peer managers.
    
    Called when this manager joins the cluster to bootstrap
    knowledge of workers registered with other managers.
    """
    peers = list(self._state._active_manager_peers)
    if not peers:
        return
    
    self._task_runner.run(
        self._logger.log,
        ServerInfo(
            message=f"Requesting worker lists from {len(peers)} peer managers",
            node_host=self._config.host,
            node_port=self._config.tcp_port,
            node_id=self._node_id,
        )
    )
    
    for peer_addr in peers:
        try:
            response = await self._send_tcp(
                peer_addr,
                "list_workers",
                b"",
                timeout=5.0,
            )
            if response:
                worker_list = WorkerListResponse.from_bytes(response)
                for update in worker_list.workers:
                    await self._handle_worker_state_update(update, peer_addr)
        except Exception:
            # Peer may be unreachable - gossip will eventually converge
            pass
```

---

## Part 11: Protocol Flow Summary

| Event | Immediate Action | Background |
|-------|------------------|------------|
| Worker registers with Manager A | TCP broadcast `worker_state_update` to B, C | Add to gossip buffer |
| Worker dies (detected by owner) | TCP broadcast `worker_state_update` (state=dead) | Add to gossip buffer |
| Worker evicted (deadline) | TCP broadcast `worker_state_update` (state=evicted) | Add to gossip buffer |
| Worker leaves gracefully | TCP broadcast `worker_state_update` (state=left) | Add to gossip buffer |
| Manager D joins cluster | Request `list_workers` from A, B, C | N/A |
| Steady state | N/A | Gossip piggyback on SWIM messages |

---

## Part 12: Files to Create/Modify

### New Files

| File | Description |
|------|-------------|
| `hyperscale/distributed/models/worker_state.py` | `WorkerStateUpdate` and `WorkerListResponse` models |
| `hyperscale/distributed/swim/gossip/worker_state_gossip_buffer.py` | Gossip buffer for worker state |
| `hyperscale/distributed/nodes/manager/worker_dissemination.py` | `WorkerDisseminator` class |

### Modified Files

| File | Changes |
|------|---------|
| `hyperscale/distributed/nodes/manager/server.py` | Add handlers, integrate disseminator |
| `hyperscale/distributed/nodes/manager/state.py` | Add worker incarnation tracking |
| `hyperscale/distributed/nodes/manager/registry.py` | Trigger broadcasts on events |
| `hyperscale/distributed/jobs/worker_pool.py` | Add remote worker tracking |
| `hyperscale/distributed/swim/health_aware_server.py` | Add worker state piggyback |
| `hyperscale/distributed/models/__init__.py` | Export new models |

---

## Part 13: Incarnation Tracking

### In ManagerState

```python
# hyperscale/distributed/nodes/manager/state.py

class ManagerState:
    def __init__(self, ...):
        ...
        # AD-48: Worker incarnation numbers
        self._worker_incarnations: dict[str, int] = {}
    
    def get_worker_incarnation(self, worker_id: str) -> int:
        """Get current incarnation for a worker."""
        return self._worker_incarnations.get(worker_id, 0)
    
    def increment_worker_incarnation(self, worker_id: str) -> int:
        """Increment and return new incarnation for a worker."""
        current = self._worker_incarnations.get(worker_id, 0)
        new_incarnation = current + 1
        self._worker_incarnations[worker_id] = new_incarnation
        return new_incarnation
    
    def should_accept_worker_update(
        self,
        worker_id: str,
        incoming_incarnation: int,
    ) -> bool:
        """Check if incoming worker update should be accepted."""
        current = self._worker_incarnations.get(worker_id, 0)
        return incoming_incarnation > current
```

---

## Part 14: Anti-Patterns to Avoid

**DO NOT**:

```python
# Send to all peers synchronously
for peer in peers:
    await self._send_tcp(peer, ...)  # WRONG - sequential, slow

# Accept updates without incarnation check
self._worker_pool.register_remote_worker(update)  # WRONG - may be stale

# Treat remote workers same as local
if self._worker_pool.get_worker(id):  # WRONG - doesn't distinguish local/remote
    await self._dispatch_to_worker(id)

# Block on TCP broadcast failure
await self._send_tcp_or_raise(peer, ...)  # WRONG - one peer failure blocks all
```

**DO**:

```python
# Send to all peers concurrently
await asyncio.gather(*[
    self._send_tcp(peer, ...) for peer in peers
], return_exceptions=True)

# Always check incarnation before accepting
if self._state.should_accept_worker_update(update.worker_id, update.incarnation):
    await self._worker_pool.register_remote_worker(update)

# Distinguish local vs remote workers
if self._worker_pool.is_worker_local(id):
    await self._dispatch_to_worker(id)
else:
    # Route through owner manager or use as fallback
    ...

# Fire-and-forget with logging on failure
try:
    await asyncio.wait_for(self._send_tcp(peer, ...), timeout=5.0)
except Exception as e:
    self._task_runner.run(self._logger.log, ServerWarning(...))
```

---

## Part 15: Testing Strategy

1. **Unit tests**: Verify incarnation logic, gossip buffer encoding/decoding
2. **Integration tests**: Multi-manager cluster with worker registration visibility
3. **Partition tests**: Verify gossip convergence after network heal
4. **Ordering tests**: Verify stale updates rejected via incarnation numbers
