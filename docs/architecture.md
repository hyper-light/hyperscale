# Hyperscale Distributed Architecture

A high-performance, fault-tolerant distributed workflow execution system designed for multi-datacenter deployments with high CPU and memory utilization per node.

## Table of Contents

- [Overview](#overview)
- [Architecture](#architecture)
  - [Node Types](#node-types)
  - [Communication Protocols](#communication-protocols)
  - [Leadership Election](#leadership-election)
- [Component Diagrams](#component-diagrams)
- [State Machines](#state-machines)
  - [SWIM Node States](#swim-node-states)
  - [Worker States](#worker-states)
  - [Job Lifecycle](#job-lifecycle)
  - [Workflow Lifecycle](#workflow-lifecycle)
  - [Leadership States](#leadership-states)
- [Data Flow](#data-flow)
- [Timing Diagrams](#timing-diagrams)
  - [SWIM Probe Cycle](#swim-probe-cycle)
  - [Quorum Confirmation](#quorum-confirmation)
  - [Leader Election Sequence](#leader-election-sequence)
- [Failure Handling](#failure-handling)
  - [Failure Recovery Flows](#failure-recovery-flows)
  - [Network Partition Handling](#network-partition-handling)
  - [Cascading Failure Protection](#cascading-failure-protection)
- [Zombie Job Prevention & Detection](#zombie-job-prevention--detection)
  - [Zombie Job Lifecycle Diagram](#zombie-job-lifecycle-diagram)
  - [Detection Mechanisms](#detection-mechanisms)
  - [Prevention Mechanisms](#prevention-mechanisms)
  - [Cleanup Mechanisms](#cleanup-mechanisms)
  - [Cancellation Flow](#cancellation-flow-killing-zombie-jobs)
  - [Complete Zombie Prevention State Machine](#complete-zombie-prevention-state-machine)
  - [Known Gaps and Future Improvements](#known-gaps-and-future-improvements)
- [Backpressure & Degradation](#backpressure--degradation)
- [Scaling Operations](#scaling-operations)
- [State Management](#state-management)
- [Security](#security)
- [Message Protocol Reference](#message-protocol-reference)
- [Module Structure](#module-structure)
- [Bootstrap & Service Discovery](#bootstrap--service-discovery)
  - [Design Goals](#design-goals)
  - [Architecture Decision](#architecture-decision)
  - [Discovery Approaches Evaluated](#discovery-approaches-evaluated)
  - [Chosen Solution: DNS + Seeds with Parallel Probing](#chosen-solution-dns--seeds-with-parallel-probing)
  - [Bootstrap Protocol](#bootstrap-protocol)
  - [DNS Resolution](#dns-resolution)
  - [Peer Probing](#peer-probing)
  - [Health-Aware Peer Cache](#health-aware-peer-cache)
  - [Failure Scenarios](#failure-scenarios)
  - [Configuration](#configuration)
  - [Module Structure](#bootstrap-module-structure)
  - [Example Implementations](#example-implementations)

---

## Overview

The distributed system implements a three-tier architecture optimized for executing load testing workflows across multiple datacenters:

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                              CLIENT                                          │
│                         (Job Submission)                                     │
└─────────────────────────────────┬───────────────────────────────────────────┘
                                  │
                                  ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│                           GATE CLUSTER                                       │
│                    (Optional, Cross-DC Coordination)                         │
│  ┌─────────┐  ┌─────────┐  ┌─────────┐                                      │
│  │  Gate   │◄─┤  Gate   │◄─┤  Gate   │  ◄── Leader elected via SWIM        │
│  │(Leader) │  │(Follower│  │(Follower│                                      │
│  └────┬────┘  └─────────┘  └─────────┘                                      │
└───────┼─────────────────────────────────────────────────────────────────────┘
        │
        ├──────────────────┬──────────────────┐
        ▼                  ▼                  ▼
┌───────────────┐  ┌───────────────┐  ┌───────────────┐
│ DATACENTER A  │  │ DATACENTER B  │  │ DATACENTER C  │
│               │  │               │  │               │
│ ┌───────────┐ │  │ ┌───────────┐ │  │ ┌───────────┐ │
│ │  Manager  │ │  │ │  Manager  │ │  │ │  Manager  │ │
│ │  Cluster  │ │  │ │  Cluster  │ │  │ │  Cluster  │ │
│ └─────┬─────┘ │  │ └─────┬─────┘ │  │ └─────┬─────┘ │
│       │       │  │       │       │  │       │       │
│ ┌─────┴─────┐ │  │ ┌─────┴─────┐ │  │ ┌─────┴─────┐ │
│ │  Workers  │ │  │ │  Workers  │ │  │ │  Workers  │ │
│ │ (N cores) │ │  │ │ (N cores) │ │  │ │ (N cores) │ │
│ └───────────┘ │  │ └───────────┘ │  │ └───────────┘ │
└───────────────┘  └───────────────┘  └───────────────┘
```

### Detailed Single-Datacenter View

```
                          ┌─────────────────────────────────────────────┐
                          │             GATE CLUSTER                    │
                          │          (Gossip Protocol)                  │
                          │    ┌────┐   ┌────┐   ┌────┐                 │
                          │    │ G1 │◄─►│ G2 │◄─►│ G3 │  ← Job submit   │
                          │    └──┬─┘   └──┬─┘   └──┬─┘    from users   │
                          │       │        │        │                   │
                          └───────┼────────┼────────┼───────────────────┘
                                  │        │        │
                           TCP (job submission) + UDP (health checks)
                                  │        │        │
           ┌──────────────────────┼────────┼────────┼──────────────────────┐
           │                      ▼        ▼        ▼                      │
           │  ┌────────────────────────────────────────────────────────┐   │
           │  │               MANAGER CLUSTER (DC-A)                   │   │
           │  │            (Gossip + Leader Election)                  │   │
           │  │    ┌────┐       ┌────┐       ┌────┐                    │   │
           │  │    │ M1 │◄─────►│ M2 │◄─────►│ M3 │                    │   │
           │  │    │    │       │ ★  │       │    │   ★ = Leader       │   │
           │  │    └──┬─┘       └──┬─┘       └──┬─┘                    │   │
           │  │       │    TCP    │    TCP    │    (Full state sync)   │   │
           │  │       └───────────┼───────────┘                        │   │
           │  └───────────────────┼────────────────────────────────────┘   │
           │                      │                                        │
           │             UDP/TCP (workflow dispatch + status reports)      │
           │                      │                                        │
           │  ┌───────────────────┼────────────────────────────────────┐   │
           │  │            WORKER POOL (DC-A)                          │   │
           │  │                   │                                    │   │
           │  │    ┌──────────────┼──────────────┐                     │   │
           │  │    │              │              │                     │   │
           │  │    ▼              ▼              ▼                     │   │
           │  │  ┌──────────┐  ┌──────────┐  ┌──────────┐              │   │
           │  │  │ Worker1  │  │ Worker2  │  │ Worker3  │              │   │
           │  │  │ 8 cores  │  │ 8 cores  │  │ 8 cores  │              │   │
           │  │  │[■■■■□□□□]│  │[■■□□□□□□]│  │[□□□□□□□□]│              │   │
           │  │  │ 4 in use │  │ 2 in use │  │ 0 idle   │              │   │
           │  │  └──────────┘  └──────────┘  └──────────┘              │   │
           │  │                                                        │   │
           │  │  ■ = core running workflow    □ = core available       │   │
           │  └────────────────────────────────────────────────────────┘   │
           │                       DATACENTER A                            │
           └───────────────────────────────────────────────────────────────┘
```

### Key Design Principles

1. **Workers are the source of truth** - Workers maintain authoritative state for their own workflows
2. **Passive state discovery** - Serf-style heartbeat embedding in SWIM messages
3. **Quorum-based provisioning** - Manager decisions require quorum confirmation
4. **Lease-based execution** - Gates use leases for at-most-once DC semantics
5. **Graceful degradation** - Load shedding under pressure, LHM-aware timeouts
6. **Composition over inheritance** - All extensibility via callbacks, not method overriding
7. **TaskRunner for lifecycle management** - All background tasks managed via TaskRunner
8. **Quorum uses configured size** - Prevents split-brain in partitions (see below)

---

## Architectural Decisions

This section documents key architectural decisions made during development.

### AD-1: Composition Over Inheritance

**Decision**: All extensibility is via callbacks and composition, never method overriding.

**Rationale**: 
- Prevents fragile base class problems
- Makes dependencies explicit
- Easier to test individual components
- Allows runtime reconfiguration

**Implementation**:
- `StateEmbedder` protocol for heartbeat embedding
- Leadership callbacks: `register_on_become_leader()`, `register_on_lose_leadership()`
- Node status callbacks: `register_on_node_dead()`, `register_on_node_join()`
- All node types (Worker, Manager, Gate) use these instead of overriding UDPServer methods

### AD-2: TaskRunner for All Background Tasks

**Decision**: All background/async tasks must be managed through TaskRunner, not raw `asyncio.create_task()`.

**Rationale**:
- Prevents orphaned tasks on shutdown
- Provides cancellation via tokens
- Enables task lifecycle monitoring
- Centralizes cleanup logic

**Implementation**:
- `self._task_runner.run(coro, *args)` returns a token
- `self._task_runner.cancel(token)` for cancellation
- Cleanup loops, state sync, progress reporting all use TaskRunner

### AD-3: Quorum Uses Configured Cluster Size

**Decision**: Quorum calculation uses the **configured** cluster size, not the **active** member count.

**Rationale**:
- Prevents split-brain in network partitions
- A partition with 1 of 3 managers won't think it has quorum
- Standard Raft/Paxos behavior

**Implementation**:
```python
def _quorum_size(self) -> int:
    """Uses CONFIGURED peer count."""
    total_managers = len(self._manager_peers) + 1  # Include self
    return (total_managers // 2) + 1

def _has_quorum_available(self) -> bool:
    """Uses ACTIVE peer count for monitoring only."""
    active_count = len(self._active_manager_peers) + 1
    return active_count >= self._quorum_size()
```

### AD-4: Workers Are Source of Truth

**Decision**: Workers maintain authoritative state for their workflows. Managers rebuild state from workers on leader election.

**Rationale**:
- Workers have the actual running processes
- Eliminates single point of failure for state
- New leader can recover without distributed log

**Implementation**:
- `_on_manager_become_leader()` triggers `_sync_state_from_workers()`
- Workers respond with `WorkerStateSnapshot` containing `active_workflows`
- Manager rebuilds `_workflow_assignments` from worker responses

### AD-5: Pre-Voting for Split-Brain Prevention

**Decision**: Leader election uses a pre-vote phase before the actual election.

**Rationale**:
- Pre-vote doesn't increment term (prevents term explosion)
- Candidate checks if it would win before disrupting cluster
- Nodes only grant pre-vote if no healthy leader exists

**Implementation**:
- `_run_pre_vote()` gathers pre-votes without changing state
- Only proceeds to real election if pre-vote majority achieved
- If pre-vote fails, election is aborted

### AD-6: Manager Peer Failure Detection

**Decision**: Managers track peer liveness and quorum availability separately.

**Rationale**:
- Need to know if quorum operations will succeed
- Leadership re-election is automatic via lease expiry
- Logging quorum status aids debugging

**Implementation**:
- `_manager_udp_to_tcp`: Maps UDP addresses to TCP addresses
- `_active_manager_peers`: Set of currently live peers
- `_on_node_dead()` checks both workers AND manager peers
- `_handle_manager_peer_failure()` updates active set

### AD-7: Worker Manager Failover

**Decision**: Workers detect manager failure via SWIM and automatically failover to backup managers.

**Rationale**:
- Workers must continue operating during manager transitions
- Active workflows shouldn't be lost on manager failure
- New manager needs to know about in-flight work

**Implementation**:
- Worker registers `_handle_manager_failure` as `on_node_dead` callback
- On manager death: clear current manager, try alternatives
- On successful failover: call `_report_active_workflows_to_manager()`

### AD-8: Cores Completed for Faster Provisioning

**Decision**: Workers report `cores_completed` in progress updates; managers optimistically update available cores.

**Rationale**:
- Don't wait for entire workflow to complete before provisioning
- Enables pipelining of workflow execution
- Better utilization of worker capacity

**Implementation**:
- `WorkflowProgress.cores_completed` field
- Manager's `_update_worker_cores_from_progress()` calculates freed cores
- Optimistic update may be superseded by next heartbeat (acceptable)

### AD-9: Retry Data Preserved at Dispatch

**Decision**: Original `WorkflowDispatch` bytes are stored when workflow is first dispatched, not reconstructed on retry.

**Rationale**:
- Ensures retry has exact same parameters (VUs, timeout, context)
- Avoids serialization round-trip errors
- Simplifies retry logic

**Implementation**:
- `_workflow_retries[workflow_id] = (count, original_dispatch_bytes, failed_workers)`
- On retry: deserialize original, create new dispatch with updated fence_token
- `failed_workers` set prevents re-dispatching to same worker

### AD-10: Fencing Tokens from Terms

**Decision**: Fencing tokens are derived from election terms.

**Rationale**:
- Monotonically increasing
- Tied to leadership changes
- Workers can reject stale leader operations

**Implementation**:
- `get_fencing_token()` returns current term
- `is_fencing_token_valid(token)` checks `token >= current_term`
- Included in `WorkflowDispatch`, checked by workers

### AD-11: State Sync Retries with Exponential Backoff

**Decision**: State sync operations use retries with exponential backoff.

**Rationale**:
- Network partitions are often transient
- Single-attempt sync may miss temporarily unavailable workers
- Exponential backoff prevents thundering herd on recovery

**Implementation**:
- `_request_worker_state(max_retries=3, base_delay=0.5)` retries with backoff
- `_request_manager_peer_state(max_retries=3, base_delay=0.5)` similarly
- Delay formula: `base_delay * (2 ** attempt)`
- After exhausting retries, error is logged but sync continues with other peers

### AD-12: Manager Peer State Sync on Leadership

**Decision**: New leaders sync from both workers AND peer managers.

**Rationale**:
- Workers are source of truth for workflow execution state
- Peer managers have job-level metadata (retry counts, completion status)
- Both are needed for complete state recovery

**Implementation**:
- `_on_manager_become_leader()` calls both sync methods
- `_sync_state_from_workers()` - gets workflow execution state
- `_sync_state_from_manager_peers()` - gets job metadata
- Both use retry logic (AD-11)

### AD-13: Gate Split-Brain Prevention

**Decision**: Gates use the same split-brain prevention as managers.

**Rationale**:
- Gates coordinate across datacenters - split-brain would cause duplicate jobs
- Same SWIM-based detection works for gate clusters
- Consistent patterns reduce complexity

**Implementation**:
- `_gate_udp_to_tcp` maps UDP addresses to TCP for peer tracking
- `_active_gate_peers` tracks currently reachable peers
- `_on_node_dead` / `_on_node_join` handle peer failure/recovery
- Leadership re-election via `LocalLeaderElection` (same as managers)
- Pre-voting and term-based resolution prevent split-brain

### AD-14: CRDT-Based Cross-DC Statistics

**Decision**: Use Conflict-free Replicated Data Types (CRDTs) for cross-datacenter job statistics.

**Rationale**:
- Cross-DC coordination is expensive (10-100ms+ RTT)
- Stats like `completed_count` and `failed_count` are monotonic and perfect for G-Counters
- CRDTs allow coordination-free updates with guaranteed eventual consistency
- Merge is always safe - gates can combine stats from any subset of DCs

**Implementation**:
```python
class GCounter:
    """Grow-only counter - each DC has its own slot."""
    counts: dict[str, int]  # dc_id -> count
    
    def increment(self, dc_id: str, amount: int = 1) -> None
    def merge(self, other: "GCounter") -> "GCounter"  # commutative, associative, idempotent
    @property
    def value(self) -> int  # sum of all slots

class JobStatsCRDT:
    """CRDT-based job statistics."""
    completed: GCounter  # Monotonic - perfect for G-Counter
    failed: GCounter     # Monotonic - perfect for G-Counter
    rates: dict[str, tuple[float, int]]  # dc -> (rate, lamport_timestamp) - LWW register
```

### AD-15: Tiered Update Strategy for Cross-DC Stats

**Decision**: Use tiered update frequency based on stat criticality.

**Rationale**:
- Not all stats need real-time updates
- Critical events (completion, failure) need immediate notification
- Aggregate stats can be batched for efficiency
- Detailed stats should be pull-based to avoid overhead

**Tiers**:
| Tier | Stats | Frequency | Transport |
|------|-------|-----------|-----------|
| Immediate | Job completion, failure, critical alerts | Event-driven | TCP push |
| Periodic | Workflow progress, aggregate rates | Every 1-5s | TCP batch |
| On-Demand | Step-level stats, historical data | Client request | TCP pull |

**Implementation**:
- `_send_immediate_update()` for tier 1 events
- `_batch_stats_loop()` aggregates tier 2 stats periodically
- `receive_job_status_request()` fetches tier 3 on demand

### AD-16: Datacenter Health Classification

**Decision**: Classify datacenter health into four distinct states to enable intelligent routing.

**Rationale**:
- BUSY ≠ UNHEALTHY (critical distinction)
- BUSY = transient, will clear when workflows complete
- DEGRADED = structural problem, reduced capacity but operational
- UNHEALTHY = severe problem, requires intervention
- Routing should actively seek healthier DCs before accepting degraded states

**States** (evaluated in order):

| State | Definition | Condition |
|-------|------------|-----------|
| UNHEALTHY | No managers responding OR no workers registered | `alive_managers == 0` OR `worker_count == 0` |
| DEGRADED | Majority of workers unhealthy OR majority of managers unhealthy | `healthy_workers < worker_count // 2 + 1` OR `alive_managers < total_managers // 2 + 1` |
| BUSY | Not degraded AND no available capacity | NOT degraded AND `available_cores == 0` |
| HEALTHY | Not degraded AND capacity available | NOT degraded AND `available_cores > 0` |

**Key Metrics from ManagerHeartbeat**:
- `worker_count`: Total registered workers
- `healthy_worker_count`: Workers responding to SWIM probes
- `available_cores`: Available cores from healthy workers only
- `total_cores`: Total cores across all registered workers

**Implementation**:
```python
class DatacenterHealth(Enum):
    HEALTHY = "healthy"      # Capacity available, all systems operational
    BUSY = "busy"            # No capacity but structurally healthy (transient)
    DEGRADED = "degraded"    # Majority of workers/managers unhealthy
    UNHEALTHY = "unhealthy"  # No managers OR no workers

def _classify_datacenter_health(self, dc_id: str) -> DatacenterStatus:
    # 1. Check manager liveness via SWIM
    # 2. If alive_managers == 0 → UNHEALTHY
    # 3. If no workers registered → UNHEALTHY
    # 4. Check majority health:
    #    - healthy_workers < worker_quorum → DEGRADED
    #    - alive_managers < manager_quorum → DEGRADED
    # 5. If not degraded and available_cores == 0 → BUSY
    # 6. If not degraded and available_cores > 0 → HEALTHY
```

### AD-17: Smart Dispatch with Fallback Chain

**Decision**: Implement cascading fallback for job dispatch across datacenters.

**Rationale**:
- Single DC failure shouldn't fail entire job
- Automatic recovery without client involvement
- Actively seek healthier DCs before accepting degraded states
- Preserve user's datacenter preferences while enabling fallback

**Routing Rules** (in order of preference):

| Current DC State | Action |
|------------------|--------|
| HEALTHY | Enqueue job (preferred) |
| BUSY | Fallback to HEALTHY DC if available, else queue |
| DEGRADED | Fallback to HEALTHY or BUSY DC if available, else queue with warning |
| UNHEALTHY | Fallback to any non-UNHEALTHY DC, else **fail job with error** |

**Selection Priority**: HEALTHY > BUSY > DEGRADED (UNHEALTHY excluded)

**Flow**:
1. Classify all DCs by health
2. Bucket DCs: HEALTHY (sorted by capacity), BUSY, DEGRADED
3. Determine `worst_health` we must accept
4. Select primary DCs from best available bucket
5. Build fallback list from remaining usable DCs
6. Dispatch with appropriate logging:
   - If `worst_health == "unhealthy"` → **fail job immediately**
   - If `worst_health == "degraded"` → log warning, then queue
   - If `worst_health == "busy"` → log info, then queue
   - If `worst_health == "healthy"` → queue normally

**Implementation**:
```python
def _select_datacenters_with_fallback(
    self,
    count: int,
    preferred: list[str] | None = None,
) -> tuple[list[str], list[str], str]:  # (primary_dcs, fallback_dcs, worst_health)
    # worst_health: "healthy" | "busy" | "degraded" | "unhealthy"

async def _dispatch_job_to_datacenters(
    self,
    submission: JobSubmission,
    target_dcs: list[str],
) -> None:
    primary_dcs, fallback_dcs, worst_health = self._select_datacenters_with_fallback(...)
    
    if worst_health == "unhealthy":
        # Fail job - no usable DCs
        job.status = JobStatus.FAILED
        return
    
    if worst_health == "degraded":
        log_warning("Routing to DEGRADED DCs")
    elif worst_health == "busy":
        log_info("Routing to BUSY DCs")
    
    # Dispatch with fallback support
    await self._dispatch_job_with_fallback(submission, primary_dcs, fallback_dcs)
```

### AD-18: Hybrid Overload Detection (Delta + Absolute)

**Decision**: Use delta-based detection with absolute safety bounds for overload detection.

**Rationale**:
- Fixed thresholds cause flapping and require per-workload tuning
- Delta-based detection (rate of change) is self-calibrating
- Pure delta misses absolute capacity limits and suffers baseline drift
- Hybrid approach combines benefits of both

**Detection Model**:
```
┌─────────────────────────────────────────────────────────────────┐
│                    Hybrid Overload Detection                    │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  Primary: Delta-based (% above EMA baseline + trend slope)     │
│  ├─ Tracks latency/queue depth relative to baseline            │
│  ├─ Uses Exponential Moving Average for baseline               │
│  ├─ Calculates trend via linear regression on delta history    │
│  └─ Self-calibrates to workload characteristics                │
│                                                                 │
│  Secondary: Absolute safety bounds (hard limits)               │
│  ├─ Prevents baseline drift masking real problems              │
│  ├─ Catches "stable but maxed out" scenarios                   │
│  └─ Example: latency > 5000ms = overloaded regardless          │
│                                                                 │
│  Tertiary: Resource signals (CPU, memory, queue depth)         │
│  ├─ Provides capacity awareness                                │
│  └─ Catches "about to fail" before latency spikes              │
│                                                                 │
│  Final State = max(delta_state, absolute_state, resource_state)│
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

**State Levels**:
| State | Delta Threshold | Absolute Bound | Action |
|-------|-----------------|----------------|--------|
| healthy | < 20% above baseline | < 200ms | Normal operation |
| busy | 20-50% above baseline | 200-500ms | Reduce new work |
| stressed | 50-100% above baseline | 500-2000ms | Shed low-priority |
| overloaded | > 100% above baseline OR rising trend | > 2000ms | Emergency shed |

**Implementation**:
```python
@dataclass
class OverloadConfig:
    """Configuration for hybrid overload detection."""
    # Delta detection
    ema_alpha: float = 0.1  # Smoothing factor for baseline
    current_window: int = 10  # Samples for current average
    trend_window: int = 20  # Samples for trend calculation
    delta_thresholds: tuple[float, float, float] = (0.2, 0.5, 1.0)  # busy/stressed/overloaded

    # Absolute bounds (safety rails)
    absolute_bounds: tuple[float, float, float] = (200.0, 500.0, 2000.0)

    # Resource signals
    cpu_thresholds: tuple[float, float, float] = (0.7, 0.85, 0.95)
    memory_thresholds: tuple[float, float, float] = (0.7, 0.85, 0.95)

class HybridOverloadDetector:
    """Combines delta-based and absolute detection."""

    def __init__(self, config: OverloadConfig | None = None):
        self._config = config or OverloadConfig()
        self._baseline_ema: float = 0.0
        self._recent: deque[float] = deque(maxlen=self._config.current_window)
        self._delta_history: deque[float] = deque(maxlen=self._config.trend_window)

    def record_latency(self, latency_ms: float) -> None:
        """Record a latency sample and update state."""
        # Update baseline EMA
        if self._baseline_ema == 0.0:
            self._baseline_ema = latency_ms
        else:
            alpha = self._config.ema_alpha
            self._baseline_ema = alpha * latency_ms + (1 - alpha) * self._baseline_ema

        self._recent.append(latency_ms)

        # Calculate delta (% above baseline)
        if self._baseline_ema > 0:
            current_avg = sum(self._recent) / len(self._recent)
            delta = (current_avg - self._baseline_ema) / self._baseline_ema
            self._delta_history.append(delta)

    def get_state(self, cpu_percent: float = 0.0, memory_percent: float = 0.0) -> str:
        """Get current overload state using hybrid detection."""
        states = []

        # Delta-based state
        if len(self._recent) >= 3:
            current_avg = sum(self._recent) / len(self._recent)
            delta = (current_avg - self._baseline_ema) / max(self._baseline_ema, 1.0)
            trend = self._calculate_trend()

            if delta > self._config.delta_thresholds[2] or trend > 0.1:
                states.append("overloaded")
            elif delta > self._config.delta_thresholds[1]:
                states.append("stressed")
            elif delta > self._config.delta_thresholds[0]:
                states.append("busy")
            else:
                states.append("healthy")

        # Absolute bound state
        if self._recent:
            current_avg = sum(self._recent) / len(self._recent)
            if current_avg > self._config.absolute_bounds[2]:
                states.append("overloaded")
            elif current_avg > self._config.absolute_bounds[1]:
                states.append("stressed")
            elif current_avg > self._config.absolute_bounds[0]:
                states.append("busy")

        # Resource state
        cpu = cpu_percent / 100.0
        if cpu > self._config.cpu_thresholds[2]:
            states.append("overloaded")
        elif cpu > self._config.cpu_thresholds[1]:
            states.append("stressed")
        elif cpu > self._config.cpu_thresholds[0]:
            states.append("busy")

        # Return worst state
        state_order = {"healthy": 0, "busy": 1, "stressed": 2, "overloaded": 3}
        return max(states, key=lambda s: state_order.get(s, 0)) if states else "healthy"
```

**Advantages**:
- Self-calibrating: adapts to workload characteristics
- Less configuration: works across different deployments
- Catches both gradual degradation AND absolute limits
- Trend detection provides early warning

**Disadvantages**:
- Warm-up period required (mitigated by absolute bounds)
- More complex than simple thresholds
- Baseline drift possible over long periods (mitigated by absolute bounds)

### AD-19: Three-Signal Health Model (All Node Types)

**Decision**: Separate node health into three independent signals: Liveness, Readiness, and Progress. Apply this model uniformly to Workers, Managers, and Gates.

**Rationale**:
- All node types run demanding workloads in a distributed system
- Conflating "can't accept work" with "dead" causes premature eviction
- Resource metrics alone are meaningless for heavy workloads
- Progress (throughput) is ground truth for all node types
- Uniform model simplifies reasoning and implementation

**Health Model**:
```
┌─────────────────────────────────────────────────────────────────┐
│                 Three-Signal Worker Health Model                │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐             │
│  │  LIVENESS   │  │  READINESS  │  │  PROGRESS   │             │
│  │             │  │             │  │             │             │
│  │ Can respond │  │ Can accept  │  │ Completing  │             │
│  │ to probes?  │  │ new work?   │  │ workflows?  │             │
│  │             │  │             │  │             │             │
│  │ Binary:     │  │ Binary:     │  │ Rate-based: │             │
│  │ yes/no      │  │ yes/no      │  │ completions │             │
│  │             │  │             │  │ per interval│             │
│  └──────┬──────┘  └──────┬──────┘  └──────┬──────┘             │
│         │                │                │                     │
│         ▼                ▼                ▼                     │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │                   Decision Matrix                        │   │
│  ├─────────────────────────────────────────────────────────┤   │
│  │ Liveness  Readiness  Progress   →  Action               │   │
│  │ ────────  ─────────  ────────      ──────────────────── │   │
│  │ YES       YES        NORMAL     →  HEALTHY (route work) │   │
│  │ YES       NO         NORMAL     →  BUSY (drain only)    │   │
│  │ YES       YES        LOW        →  SLOW (investigate)   │   │
│  │ YES       NO         LOW        →  DEGRADED (drain)     │   │
│  │ YES       *          ZERO       →  STUCK (drain+timer)  │   │
│  │ NO        *          *          →  SUSPECT (begin evict)│   │
│  └─────────────────────────────────────────────────────────┘   │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

**Signal Definitions**:

| Signal | Question | Measurement | Failure Threshold |
|--------|----------|-------------|-------------------|
| Liveness | Is process alive? | Ping/pong response | 3 consecutive misses, 30s timeout |
| Readiness | Can accept work? | Self-reported + capacity | `accepting_work=false` OR `capacity=0` |
| Progress | Is work completing? | Completions per interval | `actual_rate < expected_rate * 0.3` |

**Implementation**:
```python
@dataclass
class WorkerHealthState:
    """Unified health state combining all three signals."""
    worker_id: str

    # Signal 1: Liveness
    last_liveness_response: float  # timestamp
    consecutive_liveness_failures: int

    # Signal 2: Readiness
    accepting_work: bool  # reported by worker
    available_capacity: int

    # Signal 3: Progress
    workflows_assigned: int
    completions_last_interval: int
    expected_completion_rate: float

    @property
    def liveness(self) -> bool:
        """Is the worker process alive and responsive?"""
        time_since_response = time.monotonic() - self.last_liveness_response
        return (
            time_since_response < 30.0
            and self.consecutive_liveness_failures < 3
        )

    @property
    def readiness(self) -> bool:
        """Can the worker accept new work?"""
        return self.accepting_work and self.available_capacity > 0

    @property
    def progress_state(self) -> str:
        """Is work completing at expected rate?"""
        if self.workflows_assigned == 0:
            return "idle"

        actual_rate = self.completions_last_interval / max(self.workflows_assigned, 1)

        if actual_rate >= self.expected_completion_rate * 0.8:
            return "normal"
        elif actual_rate >= self.expected_completion_rate * 0.3:
            return "slow"
        elif actual_rate > 0:
            return "degraded"
        else:
            return "stuck"

    def get_routing_decision(self) -> str:
        """Determine action: route, drain, investigate, or evict."""
        if not self.liveness:
            return "evict"

        progress = self.progress_state

        if progress == "stuck" and self.workflows_assigned > 0:
            return "evict"

        if progress in ("slow", "degraded"):
            return "investigate"

        if not self.readiness:
            return "drain"

        return "route"
```

**Why This Model Is Correct**:
| Alternative | Problem |
|-------------|---------|
| Single health score | Conflates independent failure modes |
| Resource thresholds | Doesn't account for expected heavy usage |
| Timeout-only | Can't distinguish slow from stuck |
| Heartbeat-only | Process can heartbeat while frozen |

#### Manager Health (Gate monitors Managers)

Gates monitor manager health to make intelligent DC routing decisions.

**Signal Definitions for Managers**:
| Signal | Question | Measurement | Failure Threshold |
|--------|----------|-------------|-------------------|
| Liveness | Is manager responding? | SWIM probe response | 3 consecutive misses |
| Readiness | Can accept jobs? | Has quorum + accepting jobs | `has_quorum=false` OR `accepting_jobs=false` |
| Progress | Is work flowing? | Job throughput + dispatch rate | `dispatch_rate < expected * 0.3` |

```python
@dataclass
class ManagerHealthState:
    """Three-signal health state for managers (monitored by gates)."""
    manager_id: str
    datacenter_id: str

    # Signal 1: Liveness
    last_liveness_response: float
    consecutive_liveness_failures: int

    # Signal 2: Readiness
    has_quorum: bool  # Can make authoritative decisions
    accepting_jobs: bool  # Self-reported
    active_worker_count: int  # Workers available for dispatch

    # Signal 3: Progress
    jobs_accepted_last_interval: int
    workflows_dispatched_last_interval: int
    expected_throughput: float  # Based on worker capacity

    @property
    def liveness(self) -> bool:
        time_since_response = time.monotonic() - self.last_liveness_response
        return (
            time_since_response < 30.0
            and self.consecutive_liveness_failures < 3
        )

    @property
    def readiness(self) -> bool:
        return (
            self.has_quorum
            and self.accepting_jobs
            and self.active_worker_count > 0
        )

    @property
    def progress_state(self) -> str:
        if self.jobs_accepted_last_interval == 0:
            return "idle"

        actual_rate = self.workflows_dispatched_last_interval
        if actual_rate >= self.expected_throughput * 0.8:
            return "normal"
        elif actual_rate >= self.expected_throughput * 0.3:
            return "slow"
        elif actual_rate > 0:
            return "degraded"
        else:
            return "stuck"

    def get_routing_decision(self) -> str:
        """Determine whether gate should route jobs to this manager."""
        if not self.liveness:
            return "evict"  # Remove from DC's active managers

        progress = self.progress_state

        if progress == "stuck" and self.jobs_accepted_last_interval > 0:
            return "evict"

        if progress in ("slow", "degraded"):
            return "investigate"

        if not self.readiness:
            return "drain"  # Don't send new jobs, let existing complete

        return "route"
```

**Integration with DC Health Classification (AD-16)**:
```
DC Health = f(manager_health_states)

If ALL managers NOT liveness → DC = UNHEALTHY
If MAJORITY managers NOT readiness → DC = DEGRADED
If ANY manager progress == "stuck" → DC = DEGRADED
If ALL managers readiness but NO capacity → DC = BUSY
Otherwise → DC = HEALTHY
```

#### Gate Health (Gates monitor peer Gates)

Gates monitor peer gate health for leader election and job forwarding decisions.

**Signal Definitions for Gates**:
| Signal | Question | Measurement | Failure Threshold |
|--------|----------|-------------|-------------------|
| Liveness | Is gate responding? | SWIM probe response | 3 consecutive misses |
| Readiness | Can handle jobs? | Has DC connectivity + not overloaded | `dc_connectivity=false` OR `overloaded=true` |
| Progress | Is work flowing? | Job forwarding rate + stats aggregation | `forward_rate < expected * 0.3` |

```python
@dataclass
class GateHealthState:
    """Three-signal health state for gates (monitored by peer gates)."""
    gate_id: str

    # Signal 1: Liveness
    last_liveness_response: float
    consecutive_liveness_failures: int

    # Signal 2: Readiness
    has_dc_connectivity: bool  # Can reach at least one DC
    connected_dc_count: int
    overload_state: str  # From HybridOverloadDetector

    # Signal 3: Progress
    jobs_forwarded_last_interval: int
    stats_aggregated_last_interval: int
    expected_forward_rate: float

    @property
    def liveness(self) -> bool:
        time_since_response = time.monotonic() - self.last_liveness_response
        return (
            time_since_response < 30.0
            and self.consecutive_liveness_failures < 3
        )

    @property
    def readiness(self) -> bool:
        return (
            self.has_dc_connectivity
            and self.connected_dc_count > 0
            and self.overload_state not in ("stressed", "overloaded")
        )

    @property
    def progress_state(self) -> str:
        if self.jobs_forwarded_last_interval == 0:
            return "idle"

        actual_rate = self.jobs_forwarded_last_interval
        if actual_rate >= self.expected_forward_rate * 0.8:
            return "normal"
        elif actual_rate >= self.expected_forward_rate * 0.3:
            return "slow"
        elif actual_rate > 0:
            return "degraded"
        else:
            return "stuck"

    def get_routing_decision(self) -> str:
        """Determine whether to forward jobs to this gate."""
        if not self.liveness:
            return "evict"  # Remove from peer list

        progress = self.progress_state

        if progress == "stuck" and self.jobs_forwarded_last_interval > 0:
            return "evict"

        if progress in ("slow", "degraded"):
            return "investigate"

        if not self.readiness:
            return "drain"

        return "route"

    def should_participate_in_election(self) -> bool:
        """Gates with poor health shouldn't become leaders."""
        return (
            self.liveness
            and self.readiness
            and self.progress_state in ("idle", "normal")
        )
```

#### Generic Node Health Infrastructure

```python
from typing import Generic, TypeVar, Protocol

class HealthSignals(Protocol):
    """Protocol for health signal providers."""
    @property
    def liveness(self) -> bool: ...
    @property
    def readiness(self) -> bool: ...
    @property
    def progress_state(self) -> str: ...

T = TypeVar("T", bound=HealthSignals)

class NodeHealthTracker(Generic[T]):
    """Generic health tracker for any node type."""

    def __init__(self, node_type: str):
        self._node_type = node_type
        self._states: dict[str, T] = {}
        self._history: dict[str, deque[str]] = {}  # node_id -> recent decisions

    def update_state(self, node_id: str, state: T) -> None:
        self._states[node_id] = state

    def get_routing_decision(self, node_id: str) -> str:
        if node_id not in self._states:
            return "unknown"
        return self._states[node_id].get_routing_decision()

    def get_healthy_nodes(self) -> list[str]:
        return [
            node_id for node_id, state in self._states.items()
            if state.liveness and state.readiness
        ]

    def should_evict(self, node_id: str) -> tuple[bool, str]:
        """
        Determine if node should be evicted with correlation check.
        Returns (should_evict, reason).
        """
        if node_id not in self._states:
            return False, "unknown node"

        state = self._states[node_id]
        decision = state.get_routing_decision()

        if decision != "evict":
            return False, "healthy"

        # Correlation check: are many nodes failing?
        total = len(self._states)
        failing = sum(
            1 for s in self._states.values()
            if s.get_routing_decision() == "evict"
        )

        if failing > total * 0.5:
            # More than half failing - likely systemic issue
            return False, "systemic failure detected, holding eviction"

        return True, "eviction criteria met"
```

#### SWIM Piggyback for Health State

Health signals are piggybacked on SWIM protocol messages for protocol efficiency:

```python
@dataclass
class HealthPiggyback:
    """Health state embedded in SWIM messages."""
    node_id: str
    node_type: str  # "worker" | "manager" | "gate"

    # Readiness signal
    accepting_work: bool
    capacity: int  # Available slots/cores

    # Progress signal (last interval)
    throughput: int  # Completions/dispatches/forwards
    expected_throughput: int

    # Overload signal (from AD-18)
    overload_state: str  # "healthy" | "busy" | "stressed" | "overloaded"
```

### AD-20: Cancellation Propagation

**Decision**: Implement four-phase cancellation: Client → Gate → Manager → Worker.

**Rationale**:
- Users need ability to stop long-running jobs
- Resources should be freed promptly
- Cancellation must be idempotent and handle partial failures
- Each layer confirms cancellation before propagating

**Cancellation Flow**:
```
┌─────────────────────────────────────────────────────────────────┐
│                    Cancellation Propagation                     │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  Client                Gate                Manager      Worker  │
│    │                    │                    │            │     │
│    │─ CancelJob(id) ───►│                    │            │     │
│    │                    │─ CancelJob(id) ───►│            │     │
│    │                    │                    │─ Cancel ──►│     │
│    │                    │                    │◄── Ack ────│     │
│    │                    │◄─── Ack ───────────│            │     │
│    │◄─── Ack ───────────│                    │            │     │
│    │                    │                    │            │     │
│  Phase 1: Request    Phase 2: Forward     Phase 3: Execute     │
│                      Phase 4: Confirm (reverse direction)       │
│                                                                 │
│  Timeout behavior:                                              │
│  - If Worker doesn't ACK: Manager retries, then marks failed   │
│  - If Manager doesn't ACK: Gate retries, then best-effort      │
│  - Client receives "cancellation requested" immediately        │
│  - Final status pushed when all DCs confirm                    │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

**Message Types**:
```python
@dataclass
class JobCancelRequest:
    job_id: str
    requester_id: str  # For audit trail
    timestamp: float
    fence_token: int  # Must match current job epoch

@dataclass
class JobCancelResponse:
    job_id: str
    success: bool
    cancelled_workflow_count: int
    error: str | None = None
```

**Idempotency**: Cancellation requests are idempotent - repeated requests return success if job is already cancelled or cancelling.

### AD-21: Unified Retry Framework with Jitter

**Decision**: Implement a unified retry framework with exponential backoff and jitter for all network operations.

**Rationale**:
- Scattered retry implementations lead to inconsistency
- Without jitter, retries cause thundering herd
- Different jitter strategies suit different scenarios
- Framework enables consistent timeout and backoff across codebase

**Jitter Strategies**:
```
┌─────────────────────────────────────────────────────────────────┐
│                       Jitter Strategies                         │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  Full Jitter (default for most operations):                    │
│  ├─ delay = random(0, min(cap, base * 2^attempt))              │
│  ├─ Best for independent clients                               │
│  └─ Maximum spread, minimum correlation                        │
│                                                                 │
│  Equal Jitter (for operations needing minimum delay):          │
│  ├─ temp = min(cap, base * 2^attempt)                          │
│  ├─ delay = temp/2 + random(0, temp/2)                         │
│  └─ Guarantees minimum delay while spreading                   │
│                                                                 │
│  Decorrelated Jitter (for AWS-style retries):                  │
│  ├─ delay = random(base, previous_delay * 3)                   │
│  ├─ Each retry depends on previous                             │
│  └─ Good spread with bounded growth                            │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

**Implementation**:
```python
class JitterStrategy(Enum):
    FULL = "full"
    EQUAL = "equal"
    DECORRELATED = "decorrelated"

@dataclass
class RetryConfig:
    """Configuration for retry behavior."""
    max_attempts: int = 3
    base_delay: float = 0.5  # seconds
    max_delay: float = 30.0  # cap
    jitter: JitterStrategy = JitterStrategy.FULL
    retryable_exceptions: tuple[type[Exception], ...] = (
        ConnectionError,
        TimeoutError,
        OSError,
    )

class RetryExecutor:
    """Unified retry execution with jitter."""

    def __init__(self, config: RetryConfig | None = None):
        self._config = config or RetryConfig()
        self._previous_delay: float = self._config.base_delay

    def calculate_delay(self, attempt: int) -> float:
        """Calculate delay with jitter for given attempt."""
        base = self._config.base_delay
        cap = self._config.max_delay

        if self._config.jitter == JitterStrategy.FULL:
            temp = min(cap, base * (2 ** attempt))
            return random.uniform(0, temp)

        elif self._config.jitter == JitterStrategy.EQUAL:
            temp = min(cap, base * (2 ** attempt))
            return temp / 2 + random.uniform(0, temp / 2)

        elif self._config.jitter == JitterStrategy.DECORRELATED:
            delay = random.uniform(base, self._previous_delay * 3)
            delay = min(cap, delay)
            self._previous_delay = delay
            return delay

        return base * (2 ** attempt)  # fallback: no jitter

    async def execute(
        self,
        operation: Callable[[], Awaitable[T]],
        operation_name: str = "operation",
    ) -> T:
        """Execute operation with retry and jitter."""
        last_exception: Exception | None = None

        for attempt in range(self._config.max_attempts):
            try:
                return await operation()
            except self._config.retryable_exceptions as exc:
                last_exception = exc
                if attempt < self._config.max_attempts - 1:
                    delay = self.calculate_delay(attempt)
                    await asyncio.sleep(delay)

        raise last_exception or RuntimeError(f"{operation_name} failed")
```

**Where Jitter Is Applied**:
- Health check intervals
- Retry delays
- Heartbeat timing
- State sync intervals
- Leader election timeouts
- Reconnection attempts

### AD-22: Load Shedding with Priority Queues

**Decision**: Implement load shedding using priority-based request classification.

**Rationale**:
- Under overload, processing all requests degrades all users
- Shedding low-priority work protects critical operations
- Priority should be explicit, not implicit
- Graceful degradation is better than complete failure

**Priority Levels**:
```
┌─────────────────────────────────────────────────────────────────┐
│                    Load Shedding Priority                       │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  Priority 0 (CRITICAL) - Never shed:                           │
│  ├─ Health checks / liveness probes                            │
│  ├─ Cancellation requests                                      │
│  ├─ Final result delivery                                      │
│  └─ Cluster membership (SWIM)                                  │
│                                                                 │
│  Priority 1 (HIGH) - Shed under severe overload:               │
│  ├─ Job submissions                                            │
│  ├─ Workflow dispatch                                          │
│  └─ State sync requests                                        │
│                                                                 │
│  Priority 2 (NORMAL) - Shed under moderate overload:           │
│  ├─ Progress updates                                           │
│  ├─ Stats queries                                              │
│  └─ Reconnection requests                                      │
│                                                                 │
│  Priority 3 (LOW) - Shed first:                                │
│  ├─ Detailed stats                                             │
│  ├─ Debug/diagnostic requests                                  │
│  └─ Non-essential sync                                         │
│                                                                 │
│  Shedding Thresholds (based on overload state):                │
│  ├─ healthy: shed nothing                                      │
│  ├─ busy: shed Priority 3                                      │
│  ├─ stressed: shed Priority 2-3                                │
│  └─ overloaded: shed Priority 1-3 (only CRITICAL processed)   │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

**Implementation**:
```python
class RequestPriority(Enum):
    CRITICAL = 0
    HIGH = 1
    NORMAL = 2
    LOW = 3

class LoadShedder:
    """Determines whether to shed requests based on priority and load."""

    def __init__(self, overload_detector: HybridOverloadDetector):
        self._detector = overload_detector

        # Map overload state to minimum priority processed
        self._shed_thresholds: dict[str, int] = {
            "healthy": 4,    # Process all (nothing shed)
            "busy": 3,       # Shed LOW
            "stressed": 2,   # Shed NORMAL and LOW
            "overloaded": 1, # Only CRITICAL (shed HIGH, NORMAL, LOW)
        }

    def should_shed(self, priority: RequestPriority) -> bool:
        """Return True if request should be shed."""
        state = self._detector.get_state()
        min_priority = self._shed_thresholds.get(state, 4)
        return priority.value >= min_priority

    def classify_request(self, message_type: str) -> RequestPriority:
        """Classify request by message type."""
        critical_types = {"ping", "cancel_job", "final_result", "swim_*"}
        high_types = {"job_submit", "workflow_dispatch", "state_sync"}
        normal_types = {"progress_update", "stats_query", "register_callback"}

        if message_type in critical_types:
            return RequestPriority.CRITICAL
        elif message_type in high_types:
            return RequestPriority.HIGH
        elif message_type in normal_types:
            return RequestPriority.NORMAL
        else:
            return RequestPriority.LOW
```

### AD-23: Backpressure for Stats Updates

**Decision**: Implement tiered stats retention with backpressure signaling.

**Rationale**:
- Unbounded stats history causes memory exhaustion
- Different retention needs for different data freshness
- Upstream should slow down when downstream is overwhelmed
- Explicit backpressure prevents silent data loss

**Tiered Retention**:
```
┌─────────────────────────────────────────────────────────────────┐
│                  Tiered Stats Retention                         │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  HOT (0-60 seconds):                                           │
│  ├─ Full resolution (every update)                             │
│  ├─ In-memory ring buffer                                      │
│  └─ Used for real-time dashboards                              │
│                                                                 │
│  WARM (1-60 minutes):                                          │
│  ├─ 10-second aggregates                                       │
│  ├─ Compressed in-memory                                       │
│  └─ Used for recent history                                    │
│                                                                 │
│  COLD (1-24 hours):                                            │
│  ├─ 1-minute aggregates                                        │
│  ├─ Spill to disk if needed                                    │
│  └─ Used for job post-mortems                                  │
│                                                                 │
│  ARCHIVE (> 24 hours):                                         │
│  ├─ Final summary only                                         │
│  └─ Persisted with job completion                              │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

**Backpressure Levels**:
```python
class BackpressureLevel(Enum):
    NONE = 0       # Accept all updates
    THROTTLE = 1   # Reduce update frequency
    BATCH = 2      # Only accept batched updates
    REJECT = 3     # Reject non-critical updates

@dataclass
class StatsBuffer:
    """Bounded stats buffer with backpressure."""
    max_hot_entries: int = 1000
    max_warm_entries: int = 360  # 1 hour at 10s intervals
    max_cold_entries: int = 1440  # 24 hours at 1m intervals

    hot: deque[StatsEntry]
    warm: deque[AggregatedStats]
    cold: deque[AggregatedStats]

    def get_backpressure_level(self) -> BackpressureLevel:
        """Determine backpressure based on buffer fill."""
        hot_fill = len(self.hot) / self.max_hot_entries

        if hot_fill < 0.7:
            return BackpressureLevel.NONE
        elif hot_fill < 0.85:
            return BackpressureLevel.THROTTLE
        elif hot_fill < 0.95:
            return BackpressureLevel.BATCH
        else:
            return BackpressureLevel.REJECT
```

### AD-24: Rate Limiting (Client and Server)

**Decision**: Implement token bucket rate limiting at both client and server sides.

**Rationale**:
- Prevents any single client from overwhelming the system
- Server-side is authoritative; client-side is cooperative
- Token bucket allows bursts while enforcing average rate
- Per-client tracking enables fair sharing

**Implementation**:
```
┌─────────────────────────────────────────────────────────────────┐
│                    Rate Limiting Architecture                   │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  Client-Side (cooperative):                                     │
│  ├─ Pre-flight check before sending                            │
│  ├─ Respects server's rate limit headers                       │
│  └─ Delays requests when approaching limit                     │
│                                                                 │
│  Server-Side (authoritative):                                   │
│  ├─ Per-client token buckets                                   │
│  ├─ Returns 429 with Retry-After when exceeded                 │
│  └─ Different limits for different operation types             │
│                                                                 │
│  Token Bucket Parameters:                                       │
│  ├─ bucket_size: Maximum burst capacity                        │
│  ├─ refill_rate: Tokens added per second                       │
│  └─ current_tokens: Available tokens                           │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

```python
class TokenBucket:
    """Token bucket rate limiter."""

    def __init__(self, bucket_size: int, refill_rate: float):
        self._bucket_size = bucket_size
        self._refill_rate = refill_rate
        self._tokens = float(bucket_size)
        self._last_refill = time.monotonic()
        self._lock = asyncio.Lock()

    async def acquire(self, tokens: int = 1) -> bool:
        """Try to acquire tokens. Returns False if rate limited."""
        async with self._lock:
            self._refill()
            if self._tokens >= tokens:
                self._tokens -= tokens
                return True
            return False

    def _refill(self) -> None:
        """Refill tokens based on elapsed time."""
        now = time.monotonic()
        elapsed = now - self._last_refill
        self._tokens = min(
            self._bucket_size,
            self._tokens + elapsed * self._refill_rate
        )
        self._last_refill = now

class ServerRateLimiter:
    """Server-side rate limiter with per-client buckets."""

    def __init__(self, default_config: RateLimitConfig):
        self._config = default_config
        self._buckets: dict[str, TokenBucket] = {}

    def check_rate_limit(self, client_id: str, operation: str) -> tuple[bool, float]:
        """Check if request is allowed. Returns (allowed, retry_after)."""
        bucket = self._get_or_create_bucket(client_id, operation)
        if bucket.acquire(1):
            return True, 0.0
        else:
            retry_after = 1.0 / bucket._refill_rate
            return False, retry_after
```

### AD-25: Version Skew Handling

**Decision**: Support rolling upgrades via protocol versioning and capability negotiation.

**Rationale**:
- Zero-downtime upgrades require version compatibility
- Nodes must handle messages from older/newer versions
- Unknown fields should be ignored, not rejected
- Capability advertisement enables gradual feature rollout

**Protocol Versioning**:
```
┌─────────────────────────────────────────────────────────────────┐
│                    Version Skew Handling                        │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  Version Format: MAJOR.MINOR                                    │
│  ├─ MAJOR: Breaking changes (must match)                       │
│  └─ MINOR: Additive changes (newer can talk to older)          │
│                                                                 │
│  Handshake includes:                                            │
│  ├─ protocol_version: "1.2"                                    │
│  ├─ capabilities: ["cancellation", "batched_stats", ...]       │
│  └─ node_version: "hyperscale-0.5.0" (informational)           │
│                                                                 │
│  Compatibility Rules:                                           │
│  ├─ Same MAJOR: compatible                                     │
│  ├─ Different MAJOR: reject connection                         │
│  ├─ Newer MINOR → older: use older's feature set               │
│  └─ Older MINOR → newer: newer ignores unknown capabilities    │
│                                                                 │
│  Message Handling:                                              │
│  ├─ Unknown fields: ignore (forward compatibility)             │
│  ├─ Missing optional fields: use defaults                      │
│  └─ Missing required fields: reject with clear error           │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

**Implementation**:
```python
@dataclass
class ProtocolVersion:
    major: int
    minor: int

    def is_compatible_with(self, other: "ProtocolVersion") -> bool:
        return self.major == other.major

    def supports_feature(self, other: "ProtocolVersion", feature: str) -> bool:
        """Check if feature is supported by both versions."""
        # Feature was added in version X.Y
        feature_versions = {
            "cancellation": (1, 0),
            "batched_stats": (1, 1),
            "client_reconnection": (1, 2),
            "fence_tokens": (1, 2),
        }
        required = feature_versions.get(feature, (999, 999))
        return (
            (self.major, self.minor) >= required
            and (other.major, other.minor) >= required
        )

@dataclass
class NodeCapabilities:
    protocol_version: ProtocolVersion
    capabilities: set[str]
    node_version: str  # Informational

    def negotiate(self, other: "NodeCapabilities") -> set[str]:
        """Return capabilities supported by both nodes."""
        return self.capabilities & other.capabilities
```

### AD-26: Adaptive Healthcheck Extensions

**Decision**: Allow healthcheck deadline extensions with logarithmic grant reduction.

**Rationale**:
- Long-running operations may legitimately need more time
- Unlimited extensions enable abuse
- Logarithmic reduction discourages repeated requests
- Extensions require active negotiation (not automatic)

**Extension Protocol**:
```
┌─────────────────────────────────────────────────────────────────┐
│              Adaptive Healthcheck Extensions                    │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  Base deadline: 30 seconds                                      │
│                                                                 │
│  Extension grants (logarithmic reduction):                      │
│  ├─ 1st extension: +30s (100% of base)                         │
│  ├─ 2nd extension: +15s (50% of base)                          │
│  ├─ 3rd extension: +7.5s (25% of base)                         │
│  ├─ 4th extension: +3.75s (12.5% of base)                      │
│  └─ ...converges to minimum (1s)                               │
│                                                                 │
│  Formula: grant = max(min_grant, base / (2^extension_count))   │
│                                                                 │
│  Extension request must include:                                │
│  ├─ reason: "long_workflow" | "gc_pause" | "resource_contention"│
│  ├─ estimated_completion: timestamp                            │
│  └─ current_progress: 0.0-1.0                                  │
│                                                                 │
│  Extension denied if:                                           │
│  ├─ No progress since last extension                           │
│  ├─ Total extensions exceed max (e.g., 5)                      │
│  └─ Node is already marked suspect                             │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

**Implementation**:
```python
@dataclass
class ExtensionTracker:
    """Tracks healthcheck extensions for a worker."""
    worker_id: str
    base_deadline: float = 30.0
    min_grant: float = 1.0
    max_extensions: int = 5

    extension_count: int = 0
    last_progress: float = 0.0
    total_extended: float = 0.0

    def request_extension(
        self,
        reason: str,
        current_progress: float,
    ) -> tuple[bool, float]:
        """
        Request deadline extension.
        Returns (granted, extension_seconds).
        """
        # Deny if too many extensions
        if self.extension_count >= self.max_extensions:
            return False, 0.0

        # Deny if no progress
        if current_progress <= self.last_progress and self.extension_count > 0:
            return False, 0.0

        # Calculate grant with logarithmic reduction
        grant = max(
            self.min_grant,
            self.base_deadline / (2 ** self.extension_count)
        )

        self.extension_count += 1
        self.last_progress = current_progress
        self.total_extended += grant

        return True, grant

    def reset(self) -> None:
        """Reset tracker when worker completes operation or recovers."""
        self.extension_count = 0
        self.last_progress = 0.0
        self.total_extended = 0.0
```

**Message Types**:
```python
@dataclass
class HealthcheckExtensionRequest:
    """Worker requests more time before being marked unhealthy."""
    worker_id: str
    reason: str  # "long_workflow" | "gc_pause" | "resource_contention"
    current_progress: float  # 0.0 to 1.0
    estimated_completion: float  # Unix timestamp
    active_workflow_count: int

@dataclass
class HealthcheckExtensionResponse:
    """Manager response to extension request."""
    granted: bool
    extension_seconds: float  # 0.0 if not granted
    new_deadline: float  # Unix timestamp of new deadline
    remaining_extensions: int  # How many more can be requested
    denial_reason: str | None = None  # If not granted
```

**Complete Protocol Flow Example**:
```
┌─────────────────────────────────────────────────────────────────┐
│           Healthcheck Extension Protocol Flow                   │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  Worker                                    Manager              │
│    │                                          │                 │
│    │◄──── Healthcheck probe ─────────────────│ (deadline: 30s) │
│    │                                          │                 │
│    │  [Running long workflow, needs more time]│                 │
│    │                                          │                 │
│    │─── ExtensionRequest(progress=0.3) ─────►│                 │
│    │                                          │                 │
│    │    [Manager: extension_count=0]          │                 │
│    │    [Grant: 30s / 2^0 = 30s]              │                 │
│    │                                          │                 │
│    │◄── ExtensionResponse(granted=True, 30s)─│ (deadline: 60s) │
│    │                                          │                 │
│    │  [Still working...]                      │                 │
│    │                                          │                 │
│    │─── ExtensionRequest(progress=0.6) ─────►│                 │
│    │                                          │                 │
│    │    [Manager: extension_count=1]          │                 │
│    │    [Grant: 30s / 2^1 = 15s]              │                 │
│    │                                          │                 │
│    │◄── ExtensionResponse(granted=True, 15s)─│ (deadline: 75s) │
│    │                                          │                 │
│    │─── ExtensionRequest(progress=0.6) ─────►│ [NO PROGRESS!]  │
│    │                                          │                 │
│    │◄── ExtensionResponse(granted=False) ────│ (denied)        │
│    │                                          │                 │
│    │  [Worker marked SUSPECT after deadline] │                 │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

**Manager-Side Integration**:
```python
class WorkerHealthManager:
    """Manages worker health with extension support."""

    def __init__(self):
        self._extension_trackers: dict[str, ExtensionTracker] = {}
        self._worker_deadlines: dict[str, float] = {}

    def handle_extension_request(
        self,
        request: HealthcheckExtensionRequest,
    ) -> HealthcheckExtensionResponse:
        """Process extension request from worker."""
        tracker = self._extension_trackers.setdefault(
            request.worker_id,
            ExtensionTracker(worker_id=request.worker_id)
        )

        granted, extension_seconds = tracker.request_extension(
            reason=request.reason,
            current_progress=request.current_progress,
        )

        if granted:
            current_deadline = self._worker_deadlines.get(
                request.worker_id,
                time.monotonic() + 30.0
            )
            new_deadline = current_deadline + extension_seconds
            self._worker_deadlines[request.worker_id] = new_deadline

            return HealthcheckExtensionResponse(
                granted=True,
                extension_seconds=extension_seconds,
                new_deadline=new_deadline,
                remaining_extensions=tracker.max_extensions - tracker.extension_count,
            )
        else:
            denial_reason = self._get_denial_reason(tracker, request)
            return HealthcheckExtensionResponse(
                granted=False,
                extension_seconds=0.0,
                new_deadline=self._worker_deadlines.get(request.worker_id, 0.0),
                remaining_extensions=max(0, tracker.max_extensions - tracker.extension_count),
                denial_reason=denial_reason,
            )

    def _get_denial_reason(
        self,
        tracker: ExtensionTracker,
        request: HealthcheckExtensionRequest,
    ) -> str:
        if tracker.extension_count >= tracker.max_extensions:
            return f"Maximum extensions ({tracker.max_extensions}) exceeded"
        if request.current_progress <= tracker.last_progress:
            return f"No progress since last extension (was {tracker.last_progress}, now {request.current_progress})"
        return "Extension denied"

    def on_worker_healthy(self, worker_id: str) -> None:
        """Reset extension tracker when worker completes successfully."""
        if worker_id in self._extension_trackers:
            self._extension_trackers[worker_id].reset()
```

**Grant Reduction Table**:
| Extension # | Formula | Grant (base=30s) | Cumulative |
|-------------|---------|------------------|------------|
| 1 | 30 / 2^0 | 30.0s | 30.0s |
| 2 | 30 / 2^1 | 15.0s | 45.0s |
| 3 | 30 / 2^2 | 7.5s | 52.5s |
| 4 | 30 / 2^3 | 3.75s | 56.25s |
| 5 | 30 / 2^4 | 1.875s → 1.0s (min) | 57.25s |
| 6+ | — | denied | — |

**Key Properties**:
- **Converging**: Total extension converges (geometric series)
- **Progress-gated**: Must show forward progress to get more time
- **Bounded**: Hard limit on extension count prevents indefinite delays
- **Self-limiting**: Diminishing returns discourage dependency on extensions

### AD-27: Gate Module Reorganization

**Decision**: Reorganize gate-related code into focused modules following manager patterns.

**Rationale**:
- Current gate.py is monolithic and hard to maintain
- Similar to manager refactoring already completed
- One class per file improves testability
- Clear module boundaries reduce coupling

**Proposed Structure**:
```
hyperscale/distributed_rewrite/
├── jobs/
│   ├── gates/                    # Gate-side job management
│   │   ├── __init__.py
│   │   ├── gate_job_manager.py   # Per-job state and locking
│   │   ├── job_forwarding.py     # Cross-gate job forwarding
│   │   └── consistent_hash.py    # Per-job gate ownership
│   │
│   ├── managers/                 # Manager-side (existing)
│   │   ├── __init__.py
│   │   ├── job_manager.py
│   │   ├── worker_pool.py
│   │   └── workflow_dispatcher.py
│   │
│   └── __init__.py
│
├── datacenters/                  # DC-level coordination
│   ├── __init__.py
│   ├── datacenter_health.py      # DatacenterHealthManager
│   ├── manager_dispatcher.py     # ManagerDispatcher
│   └── lease_manager.py          # DC lease management
│
├── reliability/                  # Cross-cutting reliability
│   ├── __init__.py
│   ├── retry.py                  # RetryExecutor
│   ├── circuit_breaker.py        # CircuitBreaker
│   ├── load_shedding.py          # LoadShedder
│   ├── backpressure.py           # BackpressureController
│   ├── rate_limiting.py          # TokenBucket, RateLimiter
│   ├── overload.py               # HybridOverloadDetector
│   └── jitter.py                 # Jitter utilities
│
├── health/                       # Health checking
│   ├── __init__.py
│   ├── worker_health.py          # WorkerHealthState, three-signal model
│   ├── extension_tracker.py      # Adaptive extensions
│   └── probes.py                 # Liveness/Readiness probe implementations
│
└── swim/
    └── gates/                    # Gate SWIM extensions
        ├── __init__.py
        └── peer_topology.py      # GatePeerTopology
```

**Migration Plan**:
1. Create new module directories
2. Extract classes one at a time (preserve behavior)
3. Update imports in gate.py incrementally
4. Add tests for each extracted class
5. Final cleanup of gate.py

---

### AD-28: Enhanced DNS Discovery with Peer Selection

**Decision**: Implement a robust, locality-aware peer discovery and selection system using Weighted Rendezvous Hashing combined with Adaptive EWMA-based selection, bounded connection pools, and comprehensive security validation.

**Rationale**:
- Current static seed approach doesn't scale for globally distributed deployments
- Need to prevent accidental cross-cluster and cross-environment joins
- Role-based security prevents workers from directly contacting gates or vice versa
- Locality awareness reduces latency by preferring same-DC peers
- Adaptive selection handles heterogeneous peer performance gracefully
- Sticky connections reduce connection churn while allowing health-based eviction

**Problem Statement**:
In a globally distributed performance testing framework, peers can:
1. Be in different datacenters with varying latencies (1ms same-DC vs 200ms cross-region)
2. Experience temporary overload during test execution
3. Crash and restart with different IPs (Kubernetes pod replacement)
4. Be misconfigured to accidentally join wrong cluster/environment
5. Attempt unauthorized role-based connections (worker→gate should be blocked)

#### Architecture Overview

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                     ENHANCED DNS DISCOVERY ARCHITECTURE                               │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                       │
│  ┌─────────────────────────────────────────────────────────────────────────────────┐ │
│  │                           LAYER 1: DNS RESOLUTION                                │ │
│  │                                                                                   │ │
│  │  ┌──────────────┐   ┌──────────────┐   ┌──────────────┐   ┌──────────────┐     │ │
│  │  │   Static     │   │     DNS      │   │   Negative   │   │   Positive   │     │ │
│  │  │   Seeds      │   │   Resolver   │   │    Cache     │   │    Cache     │     │ │
│  │  │              │   │              │   │              │   │              │     │ │
│  │  │ 10.0.1.5:9000│   │ SRV records  │   │ Failed hosts │   │ Resolved IPs │     │ │
│  │  │ 10.0.1.6:9000│   │ + A records  │   │ (30s TTL)    │   │ (DNS TTL)    │     │ │
│  │  └──────┬───────┘   └──────┬───────┘   └──────┬───────┘   └──────┬───────┘     │ │
│  │         │                  │                  │                  │              │ │
│  │         └──────────────────┴──────────────────┴──────────────────┘              │ │
│  │                                   │                                              │ │
│  │                                   ▼                                              │ │
│  │                        ┌─────────────────────┐                                  │ │
│  │                        │  Candidate Set      │                                  │ │
│  │                        │  (all discovered)   │                                  │ │
│  │                        └──────────┬──────────┘                                  │ │
│  └───────────────────────────────────┼──────────────────────────────────────────────┘ │
│                                      │                                                │
│  ┌───────────────────────────────────┼──────────────────────────────────────────────┐ │
│  │                           LAYER 2: SECURITY VALIDATION                           │ │
│  │                                   │                                              │ │
│  │                                   ▼                                              │ │
│  │                        ┌─────────────────────┐                                  │ │
│  │                        │  Cluster ID Check   │ ─── Reject if cluster_id ≠ ours  │ │
│  │                        └──────────┬──────────┘                                  │ │
│  │                                   │                                              │ │
│  │                                   ▼                                              │ │
│  │                        ┌─────────────────────┐                                  │ │
│  │                        │ Environment Check   │ ─── Reject if env_id ≠ ours      │ │
│  │                        └──────────┬──────────┘                                  │ │
│  │                                   │                                              │ │
│  │                                   ▼                                              │ │
│  │                        ┌─────────────────────┐                                  │ │
│  │                        │  Role Validation    │ ─── Check mTLS cert claims       │ │
│  │                        └──────────┬──────────┘                                  │ │
│  └───────────────────────────────────┼──────────────────────────────────────────────┘ │
│                                      │                                                │
│  ┌───────────────────────────────────┼──────────────────────────────────────────────┐ │
│  │                           LAYER 3: LOCALITY FILTER                               │ │
│  │                                   │                                              │ │
│  │                                   ▼                                              │ │
│  │     ┌─────────────────────────────────────────────────────────────────────┐    │ │
│  │     │                      LOCALITY TIERS                                   │    │ │
│  │     │                                                                       │    │ │
│  │     │   Tier 0 (preferred): Same datacenter         (latency < 2ms)        │    │ │
│  │     │   Tier 1 (fallback):  Same region             (latency < 50ms)       │    │ │
│  │     │   Tier 2 (emergency): Global (any DC)         (latency varies)       │    │ │
│  │     │                                                                       │    │ │
│  │     │   Selection: Try Tier 0 first. If < min_peers, add Tier 1, etc.      │    │ │
│  │     │                                                                       │    │ │
│  │     └─────────────────────────────────────────────────────────────────────┘    │ │
│  │                                   │                                              │ │
│  │                                   ▼                                              │ │
│  │                        ┌─────────────────────┐                                  │ │
│  │                        │  Locality-Filtered  │                                  │ │
│  │                        │   Candidate Set     │                                  │ │
│  │                        └──────────┬──────────┘                                  │ │
│  └───────────────────────────────────┼──────────────────────────────────────────────┘ │
│                                      │                                                │
│  ┌───────────────────────────────────┼──────────────────────────────────────────────┐ │
│  │                           LAYER 4: PEER SELECTION                                │ │
│  │                                   │                                              │ │
│  │                                   ▼                                              │ │
│  │     ┌─────────────────────────────────────────────────────────────────────┐    │ │
│  │     │           WEIGHTED RENDEZVOUS HASH + POWER OF TWO CHOICES            │    │ │
│  │     │                                                                       │    │ │
│  │     │  Step 1: Rendezvous Hash produces deterministic candidate ranking    │    │ │
│  │     │          score = hash(peer_id || selector_id || role) * health_weight│    │ │
│  │     │          → Top K candidates (K=8)                                    │    │ │
│  │     │                                                                       │    │ │
│  │     │  Step 2: Power of Two Choices for load balancing                     │    │ │
│  │     │          From K candidates, randomly sample 2                        │    │ │
│  │     │          Compare their EWMA latency scores                           │    │ │
│  │     │          Choose the one with lower latency                           │    │ │
│  │     │                                                                       │    │ │
│  │     │  Step 3: Maintain sticky primary (K=3) and backup (K=2) connections  │    │ │
│  │     │          Only switch when health degrades significantly              │    │ │
│  │     │                                                                       │    │ │
│  │     └─────────────────────────────────────────────────────────────────────┘    │ │
│  │                                   │                                              │ │
│  │                                   ▼                                              │ │
│  │                        ┌─────────────────────┐                                  │ │
│  │                        │   Selected Peers    │                                  │ │
│  │                        │   (3 primary +      │                                  │ │
│  │                        │    2 backup)        │                                  │ │
│  │                        └──────────┬──────────┘                                  │ │
│  └───────────────────────────────────┼──────────────────────────────────────────────┘ │
│                                      │                                                │
│  ┌───────────────────────────────────┼──────────────────────────────────────────────┐ │
│  │                           LAYER 5: CONNECTION POOL                               │ │
│  │                                   │                                              │ │
│  │                                   ▼                                              │ │
│  │     ┌─────────────────────────────────────────────────────────────────────┐    │ │
│  │     │                    STICKY CONNECTION POOL                             │    │ │
│  │     │                                                                       │    │ │
│  │     │  Primary Connections (3):                                            │    │ │
│  │     │  ┌─────────┐  ┌─────────┐  ┌─────────┐                              │    │ │
│  │     │  │ Peer A  │  │ Peer B  │  │ Peer C  │   Active connections        │    │ │
│  │     │  │ EWMA:2ms│  │ EWMA:3ms│  │ EWMA:5ms│   Round-robin for requests  │    │ │
│  │     │  └─────────┘  └─────────┘  └─────────┘                              │    │ │
│  │     │                                                                       │    │ │
│  │     │  Backup Connections (2):                                             │    │ │
│  │     │  ┌─────────┐  ┌─────────┐                                           │    │ │
│  │     │  │ Peer D  │  │ Peer E  │   Ready to promote on primary failure     │    │ │
│  │     │  │ EWMA:8ms│  │EWMA:10ms│                                           │    │ │
│  │     │  └─────────┘  └─────────┘                                           │    │ │
│  │     │                                                                       │    │ │
│  │     │  Eviction Policy:                                                    │    │ │
│  │     │  - error_rate > 5%  OR                                               │    │ │
│  │     │  - consecutive_failures > 3  OR                                      │    │ │
│  │     │  - latency > p99_baseline * 3                                        │    │ │
│  │     │                                                                       │    │ │
│  │     │  On eviction: Promote backup → primary, replenish from candidates    │    │ │
│  │     │                                                                       │    │ │
│  │     └─────────────────────────────────────────────────────────────────────┘    │ │
│  └──────────────────────────────────────────────────────────────────────────────────┘ │
│                                                                                       │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

#### Security: Cluster ID and Environment ID

Prevents accidental cross-cluster and cross-environment joins:

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                     CLUSTER/ENVIRONMENT ISOLATION                                    │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                       │
│  Problem: Misconfigured node in staging tries to join production cluster             │
│                                                                                       │
│  ┌────────────────────────────────────────────────────────────────────────────────┐ │
│  │                                                                                  │ │
│  │    STAGING NODE                        PRODUCTION CLUSTER                       │ │
│  │    cluster_id: "hyperscale-staging"    cluster_id: "hyperscale-prod"           │ │
│  │    env_id: "staging"                   env_id: "production"                    │ │
│  │                                                                                  │ │
│  │         │                                       │                               │ │
│  │         │──── Registration Request ────────────▶│                               │ │
│  │         │     cluster_id: "hyperscale-staging" │                               │ │
│  │         │                                       │                               │ │
│  │         │◀─── REJECT: cluster_id mismatch ─────│                               │ │
│  │         │     expected: "hyperscale-prod"       │                               │ │
│  │         │                                       │                               │ │
│  └────────────────────────────────────────────────────────────────────────────────┘ │
│                                                                                       │
│  Configuration:                                                                      │
│  ```python                                                                           │
│  @dataclass(slots=True)                                                             │
│  class DiscoveryConfig:                                                             │
│      cluster_id: str         # Required - unique cluster identifier                 │
│      environment_id: str     # Required - prod/staging/dev                          │
│      ...                                                                            │
│  ```                                                                                │
│                                                                                       │
│  Wire Protocol Addition:                                                            │
│  - All registration messages include cluster_id and environment_id                  │
│  - Receiver validates BEFORE processing any other fields                            │
│  - Mismatch results in immediate rejection with clear error message                 │
│                                                                                       │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

#### Security: Role-Based Connection Matrix

mTLS certificate claims enforce which node types can communicate:

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                        ROLE-BASED CONNECTION MATRIX                                  │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                       │
│  Certificate Claim Format:                                                           │
│  ┌────────────────────────────────────────────────────────────────────────────────┐ │
│  │  Subject Alternative Name (SAN):                                                │ │
│  │    URI: hyperscale://role/{worker|manager|gate|client}                         │ │
│  │    URI: hyperscale://cluster/{cluster_id}                                      │ │
│  │    URI: hyperscale://env/{environment_id}                                      │ │
│  │    URI: hyperscale://dc/{datacenter_id}                                        │ │
│  └────────────────────────────────────────────────────────────────────────────────┘ │
│                                                                                       │
│  Connection Matrix:                                                                  │
│  ┌────────────┬─────────────────────────────────────────────────────────────────┐  │
│  │  Initiator │                        Can Connect To                            │  │
│  ├────────────┼──────────┬──────────┬──────────┬──────────────────────────────────┤  │
│  │            │  Worker  │  Manager │   Gate   │   Client                       │  │
│  ├────────────┼──────────┼──────────┼──────────┼──────────────────────────────────┤  │
│  │  Client    │    ❌    │    ❌    │    ✅    │      ❌                        │  │
│  │            │          │          │ (submit) │                                │  │
│  ├────────────┼──────────┼──────────┼──────────┼──────────────────────────────────┤  │
│  │  Gate      │    ❌    │    ✅    │    ✅    │      ✅ (push)                 │  │
│  │            │          │ (forward)│ (peer)   │                                │  │
│  ├────────────┼──────────┼──────────┼──────────┼──────────────────────────────────┤  │
│  │  Manager   │    ✅    │    ✅    │    ✅    │      ✅ (push)                 │  │
│  │            │(dispatch)│  (peer)  │ (report) │                                │  │
│  ├────────────┼──────────┼──────────┼──────────┼──────────────────────────────────┤  │
│  │  Worker    │    ❌    │    ✅    │    ❌    │      ❌                        │  │
│  │            │          │(progress)│          │                                │  │
│  └────────────┴──────────┴──────────┴──────────┴──────────────────────────────────┘  │
│                                                                                       │
│  Example Rejection:                                                                  │
│  ┌────────────────────────────────────────────────────────────────────────────────┐ │
│  │  Worker (role=worker) attempts to connect to Gate (role=gate)                  │ │
│  │                                                                                  │ │
│  │  Gate extracts initiator role from mTLS cert: "worker"                         │ │
│  │  Gate checks: is "worker" in allowed_initiators? NO                            │ │
│  │  Gate rejects: "Connection denied: role 'worker' cannot connect to 'gate'"     │ │
│  └────────────────────────────────────────────────────────────────────────────────┘ │
│                                                                                       │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

#### Peer Selection Algorithm: Weighted Rendezvous Hash + Power of Two Choices

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                    PEER SELECTION ALGORITHM                                          │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                       │
│  STEP 1: WEIGHTED RENDEZVOUS HASH (for deterministic candidate ranking)             │
│  ─────────────────────────────────────────────────────────────────────────────────  │
│                                                                                       │
│  For each peer P in the locality-filtered candidate set:                            │
│                                                                                       │
│    base_score = hash(peer_id || selector_id || role)                                │
│    health_weight = 1.0 - (error_rate * 2) - (latency_factor * 0.5)                 │
│    weighted_score = base_score * max(0.1, health_weight)                            │
│                                                                                       │
│  Sort by weighted_score descending → Top K candidates (K=8)                         │
│                                                                                       │
│  Why Rendezvous Hash?                                                               │
│  - Deterministic: same inputs always produce same ranking (debuggable)              │
│  - Minimal disruption: adding/removing peer only affects that peer's connections    │
│  - No central coordination needed                                                    │
│                                                                                       │
│  ─────────────────────────────────────────────────────────────────────────────────  │
│  STEP 2: POWER OF TWO CHOICES (for load balancing among candidates)                 │
│  ─────────────────────────────────────────────────────────────────────────────────  │
│                                                                                       │
│  From K candidates, to select one connection:                                       │
│                                                                                       │
│    candidate_a = random.choice(candidates)                                          │
│    candidate_b = random.choice(candidates - {candidate_a})                          │
│    chosen = candidate_a if ewma_latency[a] < ewma_latency[b] else candidate_b       │
│                                                                                       │
│  Why Power of Two?                                                                   │
│  - Avoids thundering herd (not everyone picks the "best")                           │
│  - Automatically load balances across peers                                         │
│  - O(1) selection vs O(n) for finding global minimum                                │
│                                                                                       │
│  ─────────────────────────────────────────────────────────────────────────────────  │
│  STEP 3: ADAPTIVE EWMA LATENCY TRACKING                                             │
│  ─────────────────────────────────────────────────────────────────────────────────  │
│                                                                                       │
│  For each request to peer P:                                                        │
│                                                                                       │
│    measured_latency = response_time - request_time                                  │
│    ewma[P] = α * measured_latency + (1 - α) * ewma[P]                              │
│                                                                                       │
│  Where α = 0.2 (balance between responsiveness and stability)                       │
│                                                                                       │
│  Benefits:                                                                           │
│  - Smooths transient spikes (one slow request doesn't cause failover)               │
│  - Adapts to persistent degradation                                                 │
│  - Simple to compute and store                                                       │
│                                                                                       │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

#### Sticky Connections with Health-Based Eviction

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                    STICKY CONNECTION LIFECYCLE                                       │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                       │
│  Initial State:                                                                      │
│  ┌─────────────────────────────────────────────────────────────────────────────┐   │
│  │  PRIMARY (3)          BACKUP (2)           CANDIDATE POOL (K=8)            │   │
│  │  [A, B, C]            [D, E]               [A, B, C, D, E, F, G, H]         │   │
│  │  (active)             (warm standby)        (from rendezvous hash)          │   │
│  └─────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                       │
│  Request Routing:                                                                    │
│  - Round-robin across PRIMARY connections                                           │
│  - Track latency per request for EWMA                                               │
│  - Track errors per connection                                                       │
│                                                                                       │
│  Health Monitoring (per connection):                                                │
│  ┌────────────────────────────────────────────────────────────────────────────┐    │
│  │  Metric               │  Threshold        │  Action                        │    │
│  ├───────────────────────┼───────────────────┼─────────────────────────────────┤    │
│  │  error_rate           │  > 5%             │  Mark DEGRADED                 │    │
│  │  consecutive_failures │  > 3              │  Mark UNHEALTHY → evict        │    │
│  │  ewma_latency         │  > p99 * 3        │  Mark SLOW → evict             │    │
│  │  connection_age       │  > 1 hour         │  Consider refresh              │    │
│  └────────────────────────────────────────────────────────────────────────────┘    │
│                                                                                       │
│  Eviction Sequence:                                                                  │
│  ┌─────────────────────────────────────────────────────────────────────────────┐   │
│  │                                                                              │   │
│  │  t=0   PRIMARY: [A, B, C]    BACKUP: [D, E]                                 │   │
│  │        Peer B: consecutive_failures = 4 (threshold = 3)                     │   │
│  │                                                                              │   │
│  │  t=1   Evict B from PRIMARY                                                 │   │
│  │        PRIMARY: [A, _, C]    BACKUP: [D, E]                                 │   │
│  │                                                                              │   │
│  │  t=2   Promote D to PRIMARY                                                 │   │
│  │        PRIMARY: [A, D, C]    BACKUP: [_, E]                                 │   │
│  │                                                                              │   │
│  │  t=3   Replenish BACKUP from candidate pool (with jitter: 100-500ms)        │   │
│  │        Select F using Power of Two Choices                                  │   │
│  │        PRIMARY: [A, D, C]    BACKUP: [F, E]                                 │   │
│  │                                                                              │   │
│  └─────────────────────────────────────────────────────────────────────────────┘   │
│                                                                                       │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

#### Discovery Timing and Jitter

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                    TIMING CONFIGURATION                                              │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                       │
│  DNS Resolution:                                                                     │
│  ┌────────────────────────────────────────────────────────────────────────────┐    │
│  │  dns_timeout:           2.0 seconds                                        │    │
│  │  dns_cache_ttl:         Respect DNS TTL (or default 30s)                   │    │
│  │  negative_cache_ttl:    30 seconds (don't hammer failed lookups)           │    │
│  └────────────────────────────────────────────────────────────────────────────┘    │
│                                                                                       │
│  Peer Probing:                                                                       │
│  ┌────────────────────────────────────────────────────────────────────────────┐    │
│  │  probe_timeout:         500ms per probe                                    │    │
│  │  max_concurrent_probes: 10 (prevent socket exhaustion)                     │    │
│  │  probe_jitter:          0-100ms (prevent synchronized probing)             │    │
│  └────────────────────────────────────────────────────────────────────────────┘    │
│                                                                                       │
│  Backoff (when all probes fail):                                                    │
│  ┌────────────────────────────────────────────────────────────────────────────┐    │
│  │  initial_backoff:       500ms                                              │    │
│  │  max_backoff:           15 seconds                                         │    │
│  │  backoff_multiplier:    2.0                                                │    │
│  │  jitter_factor:         0.25 (25% randomization)                           │    │
│  └────────────────────────────────────────────────────────────────────────────┘    │
│                                                                                       │
│  Discovery Refresh:                                                                  │
│  ┌────────────────────────────────────────────────────────────────────────────┐    │
│  │  refresh_interval:      60 seconds (re-evaluate candidate set)             │    │
│  │  refresh_jitter:        0-5 seconds (prevent synchronized refresh)         │    │
│  └────────────────────────────────────────────────────────────────────────────┘    │
│                                                                                       │
│  Connection Pool:                                                                    │
│  ┌────────────────────────────────────────────────────────────────────────────┐    │
│  │  promotion_jitter:      100-500ms (prevent synchronized recovery)          │    │
│  │  connection_max_age:    3600 seconds (1 hour, then consider refresh)       │    │
│  │  ewma_alpha:            0.2 (balance responsiveness vs stability)          │    │
│  └────────────────────────────────────────────────────────────────────────────┘    │
│                                                                                       │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

#### Metrics and Observability

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                    DISCOVERY METRICS                                                 │
├─────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                       │
│  DNS Metrics:                                                                        │
│  ┌────────────────────────────────────────────────────────────────────────────┐    │
│  │  discovery_dns_lookups_total{datacenter, result}                           │    │
│  │    - result: "success" | "timeout" | "error" | "negative_cached"           │    │
│  │                                                                              │    │
│  │  discovery_dns_cache_hits_total{type}                                      │    │
│  │    - type: "positive" | "negative"                                         │    │
│  │                                                                              │    │
│  │  discovery_dns_resolution_duration_ms{datacenter}                          │    │
│  └────────────────────────────────────────────────────────────────────────────┘    │
│                                                                                       │
│  Selection Metrics:                                                                  │
│  ┌────────────────────────────────────────────────────────────────────────────┐    │
│  │  discovery_candidate_set_size{role, datacenter}                            │    │
│  │  discovery_candidate_set_changes_total{reason}                             │    │
│  │    - reason: "dns_update" | "health_change" | "peer_added" | "peer_removed"│    │
│  │                                                                              │    │
│  │  discovery_locality_tier_selected_total{tier}                              │    │
│  │    - tier: "same_dc" | "same_region" | "global"                            │    │
│  │                                                                              │    │
│  │  discovery_selection_duration_ms                                           │    │
│  └────────────────────────────────────────────────────────────────────────────┘    │
│                                                                                       │
│  Connection Pool Metrics:                                                            │
│  ┌────────────────────────────────────────────────────────────────────────────┐    │
│  │  discovery_pool_connections{state, role}                                   │    │
│  │    - state: "primary" | "backup"                                           │    │
│  │                                                                              │    │
│  │  discovery_pool_promotions_total{from_state, to_state}                     │    │
│  │  discovery_pool_evictions_total{reason}                                    │    │
│  │    - reason: "error_rate" | "consecutive_failures" | "latency" | "stale"   │    │
│  │                                                                              │    │
│  │  discovery_peer_ewma_latency_ms{peer_id, datacenter}                       │    │
│  │  discovery_peer_error_rate{peer_id}                                        │    │
│  └────────────────────────────────────────────────────────────────────────────┘    │
│                                                                                       │
│  Security Metrics:                                                                   │
│  ┌────────────────────────────────────────────────────────────────────────────┐    │
│  │  discovery_cluster_id_rejections_total{expected, received}                 │    │
│  │  discovery_environment_id_rejections_total{expected, received}             │    │
│  │  discovery_role_rejections_total{initiator_role, target_role}              │    │
│  └────────────────────────────────────────────────────────────────────────────┘    │
│                                                                                       │
└─────────────────────────────────────────────────────────────────────────────────────┘
```

#### Configuration

```python
@dataclass(slots=True)
class DiscoveryConfig:
    """Configuration for enhanced peer discovery."""

    # ===== Security (Required) =====
    cluster_id: str              # Unique cluster identifier (e.g., "hyperscale-prod")
    environment_id: str          # Environment (e.g., "production", "staging")

    # ===== DNS Configuration =====
    dns_names: list[str] = field(default_factory=list)  # SRV/A records to resolve
    static_seeds: list[str] = field(default_factory=list)  # Fallback addresses
    dns_timeout: float = 2.0
    dns_cache_ttl: float = 30.0  # Override if DNS doesn't provide TTL
    negative_cache_ttl: float = 30.0  # Don't re-resolve failed names

    # ===== Locality =====
    datacenter_id: str = ""      # This node's datacenter
    region_id: str = ""          # This node's region (group of DCs)
    prefer_same_dc: bool = True
    prefer_same_region: bool = True
    min_peers_per_tier: int = 3  # Minimum before falling back to next tier

    # ===== Peer Selection =====
    candidate_set_size: int = 8           # K for rendezvous hash
    primary_connections: int = 3          # Active connections
    backup_connections: int = 2           # Warm standby
    ewma_alpha: float = 0.2               # Latency smoothing factor

    # ===== Health Thresholds =====
    error_rate_threshold: float = 0.05    # 5% errors → concern
    consecutive_failure_limit: int = 3    # Hard failures → evict
    latency_multiplier_threshold: float = 3.0  # 3x baseline → evict

    # ===== Timing =====
    probe_timeout: float = 0.5            # 500ms per probe
    max_concurrent_probes: int = 10
    initial_backoff: float = 0.5          # 500ms
    max_backoff: float = 15.0             # 15 seconds
    backoff_multiplier: float = 2.0
    jitter_factor: float = 0.25           # 25% randomization
    refresh_interval: float = 60.0        # Re-evaluate candidates
    promotion_jitter: tuple[float, float] = (0.1, 0.5)  # 100-500ms
```

#### Module Structure

```
hyperscale/distributed_rewrite/discovery/
├── __init__.py                    # Public exports
├── discovery_service.py           # Main DiscoveryService orchestrator
│
├── dns/
│   ├── __init__.py
│   ├── resolver.py                # AsyncDNSResolver with caching
│   └── negative_cache.py          # NegativeCache for failed lookups
│
├── locality/
│   ├── __init__.py
│   ├── locality_filter.py         # LocalityFilter (DC/region preference)
│   └── locality_info.py           # LocalityInfo dataclass
│
├── selection/
│   ├── __init__.py
│   ├── rendezvous_hash.py         # WeightedRendezvousHash
│   ├── power_of_two.py            # PowerOfTwoSelector
│   └── ewma_tracker.py            # EWMALatencyTracker
│
├── pool/
│   ├── __init__.py
│   ├── connection_pool.py         # ConnectionPool with sticky connections
│   ├── peer_health.py             # PeerHealthTracker
│   └── promotion.py               # PromotionManager
│
├── security/
│   ├── __init__.py
│   ├── cluster_validator.py       # ClusterValidator (cluster_id/env_id)
│   └── role_validator.py          # RoleValidator (mTLS cert claims)
│
├── metrics/
│   ├── __init__.py
│   └── discovery_metrics.py       # DiscoveryMetrics
│
└── models/
    ├── __init__.py
    ├── discovery_config.py        # DiscoveryConfig dataclass
    ├── peer_info.py               # PeerInfo with health data
    ├── candidate_set.py           # CandidateSet dataclass
    └── connection_state.py        # ConnectionState enum
```

**Trade-offs**:
- (+) Deterministic peer selection via rendezvous hash (debuggable)
- (+) Load balancing via Power of Two Choices (avoids thundering herd)
- (+) Locality awareness reduces cross-DC traffic
- (+) Strong security boundaries prevent misconfiguration
- (+) Sticky connections reduce churn overhead
- (-) More complex than simple round-robin
- (-) Requires certificate infrastructure for role validation
- (-) EWMA requires per-peer state tracking

**Alternatives Considered**:
- Simple round-robin: Too naive, no health awareness
- Consistent hashing: Good but disrupts more on topology changes
- Central load balancer: Single point of failure, external dependency
- Random selection: No locality awareness, unpredictable behavior

---

### AD-29: Protocol-Level Peer Confirmation for Robust Initialization

**Decision**: Implement a "confirmed vs unconfirmed peer" model where failure detection only applies to peers we have successfully communicated with at least once. Peers from configuration start as "unconfirmed" and must receive a successful probe response, heartbeat, or other protocol message before they can transition to the failure detection state machine.

**Rationale**:
During cluster formation, nodes begin probing each other immediately. Due to network timing, async startup order, and other transient conditions, initial probes may fail even though all nodes are healthy. Without distinguishing "never reached" from "was reachable, now isn't", the SWIM failure detector triggers false positives, causing cascading "failures" that destabilize the cluster before it ever forms.

**Problem Statement**:
```
Timeline without peer confirmation:

T=0: Gate1, Gate2, Gate3 start simultaneously
T=0.1: Gate1 sends probe to Gate2 (Gate2 not yet listening)
T=1.1: Gate1 probe times out → Gate1 marks Gate2 as SUSPECT
T=2.5: Gate1 indirect probes fail → Gate1 marks Gate2 as DEAD
T=3.0: Gate2 finally ready, sends heartbeat to Gate1
T=3.1: Gate1 receives heartbeat but already removed Gate2 from active peers

Result: Cluster never stabilizes, continuous false failure detection
```

**Solution: Confirmed vs Unconfirmed Peers**

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                     PEER STATE MACHINE                                           │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                  │
│    ┌────────────────────┐                                                       │
│    │                    │                                                       │
│    │   UNCONFIRMED      │  ─── Peers from config, not yet reached              │
│    │                    │                                                       │
│    │  • No failure      │                                                       │
│    │    detection       │                                                       │
│    │  • Probe attempts  │                                                       │
│    │    continue        │                                                       │
│    │  • Not in active   │                                                       │
│    │    peer set        │                                                       │
│    │                    │                                                       │
│    └─────────┬──────────┘                                                       │
│              │                                                                   │
│              │ Successful communication:                                         │
│              │ • Probe ACK received                                              │
│              │ • Heartbeat received                                              │
│              │ • Any valid protocol message                                      │
│              │                                                                   │
│              ▼                                                                   │
│    ┌────────────────────┐                                                       │
│    │                    │                                                       │
│    │    CONFIRMED       │  ─── Successfully communicated at least once         │
│    │                    │                                                       │
│    │  • Normal SWIM     │      ┌──────────────────────────────────────────┐    │
│    │    failure         │      │                                          │    │
│    │    detection       │      │  SWIM State Machine (per Lifeguard)      │    │
│    │  • Added to        │      │                                          │    │
│    │    active peers    │      │  ALIVE ──timeout──► SUSPECT              │    │
│    │  • Participates    │      │    ▲                   │                 │    │
│    │    in gossip       │      │    │                   │ no refutation   │    │
│    │                    │      │    │ refutation        ▼                 │    │
│    │                    │      │    └─────────────── DEAD                 │    │
│    │                    │      │                                          │    │
│    └────────────────────┘      └──────────────────────────────────────────┘    │
│                                                                                  │
└─────────────────────────────────────────────────────────────────────────────────┘
```

**Implementation Details**:

1. **Data Structures**:
```python
class HealthAwareServer:
    # Peers we've successfully communicated with at least once
    _confirmed_peers: set[tuple[str, int]]

    # Peers we know about but haven't confirmed yet (from config)
    _unconfirmed_peers: set[tuple[str, int]]
```

2. **Peer Addition** (from config or discovery):
```python
async def _add_peer(self, peer: tuple[str, int]):
    """Peer from configuration starts as unconfirmed."""
    if peer not in self._confirmed_peers:
        self._unconfirmed_peers.add(peer)
    # Begin probing to confirm
```

3. **Peer Confirmation** (on ANY successful communication):
```python
async def _confirm_peer(self, peer: tuple[str, int]):
    """Mark peer as confirmed after successful communication."""
    if peer in self._unconfirmed_peers:
        self._unconfirmed_peers.discard(peer)
        self._confirmed_peers.add(peer)
        # NOW add to active peer tracking (e.g., _active_gate_peers)
        await self._on_peer_confirmed(peer)
```

4. **Failure Detection Guard**:
```python
async def _on_probe_timeout(self, peer: tuple[str, int]):
    if peer not in self._confirmed_peers:
        # Never reached this peer - log but don't escalate
        # Continue probing, eventually we'll reach them
        return

    # Confirmed peer didn't respond - THIS is meaningful
    await self._start_suspicion(peer)
```

5. **Recovery Re-confirmation**:
```python
async def _on_node_join(self, peer: tuple[str, int]):
    """Node rejoined - it's already confirmed from before."""
    # No need to re-confirm, just update state
    if peer in self._confirmed_peers:
        await self._handle_peer_recovery(peer)
```

**Events That Confirm a Peer**:
- Receiving an ACK to our probe
- Receiving a heartbeat message
- Receiving any valid protocol message (join, leave, alive, etc.)
- Receiving a response to indirect probe request

**Events That Do NOT Confirm**:
- Adding peer from configuration
- Receiving gossip ABOUT a peer from another node
- DNS resolution returning the peer's address

**Strict Lifeguard Compliance**:
This approach works IN CONJUNCTION with proper Lifeguard suspicion protocol:

1. Probe timeout → SUSPECT (never directly to DEAD)
2. SUSPECT → Broadcast suspicion, request indirect probes
3. SUSPECT + timeout without refutation → DEAD
4. Refutation received → Back to ALIVE

The key insight: **Suspicion only applies to CONFIRMED peers**. An unconfirmed peer cannot be "suspected" because we have no baseline expectation of their reachability.

**Sequence Diagram - Correct Initialization**:

```
Gate1                    Gate2                    Gate3
  │                        │                        │
  │ T=0: Start             │ T=0: Start             │ T=0: Start
  │                        │                        │
  │──── probe ────────────►│ (not ready yet)        │
  │     TIMEOUT            │                        │
  │     [unconfirmed, no   │                        │
  │      failure action]   │                        │
  │                        │                        │
  │                        │──── heartbeat ────────►│
  │                        │                        │
  │◄─────── heartbeat ─────│                        │
  │  [Gate2 CONFIRMED!]    │                        │
  │  [add to active peers] │                        │
  │                        │                        │
  │──── probe ────────────►│                        │
  │◄────── ACK ────────────│                        │
  │  [confirmed, ACK       │                        │
  │   reinforces health]   │                        │
  │                        │                        │
  │◄──────────────────────────── heartbeat ─────────│
  │  [Gate3 CONFIRMED!]    │                        │
  │                        │                        │
  ▼                        ▼                        ▼
All peers confirmed, cluster stable
```

**Sequence Diagram - Failure After Confirmation**:

```
Gate1                    Gate2 (crashes)           Gate3
  │                        │                        │
  │ [Gate2 confirmed]      │                        │
  │                        X crash                  │
  │                        │                        │
  │──── probe ────────────►│                        │
  │     TIMEOUT            │                        │
  │  [CONFIRMED peer       │                        │
  │   failed - start       │                        │
  │   SUSPICION]           │                        │
  │                        │                        │
  │──── ping-req ─────────────────────────────────►│
  │  [indirect probe       │                        │
  │   via Gate3]           │                        │──── probe ──►│ (dead)
  │                        │                        │   TIMEOUT    │
  │◄─────── NACK ──────────────────────────────────│
  │                        │                        │
  │  [no refutation after  │                        │
  │   suspicion timeout]   │                        │
  │                        │                        │
  │  Gate2 → DEAD          │                        │
  │  [remove from active]  │                        │
```

**Trade-offs**:
- (+) No arbitrary timeouts - behavior based on actual protocol state
- (+) Correct Lifeguard semantics - suspicion is meaningful
- (+) Self-healing - if peer comes up later, we'll reach them and confirm
- (+) No false positives during initialization
- (+) Memory efficient - just two sets, not per-peer epoch tracking
- (+) Works with any cluster size or topology
- (-) Initial probe failures are "silent" - may delay detection of config errors
- (-) Requires discipline to call _confirm_peer on all successful paths

**Mitigation for Silent Failures**:
Add logging/metrics for unconfirmed peers that remain unconfirmed after a threshold:
```python
if peer_unconfirmed_duration > 60.0:  # 1 minute
    log.warning(f"Peer {peer} still unconfirmed after 60s - check configuration")
```

**Files to Modify**:
- `hyperscale/distributed_rewrite/swim/health_aware_server.py` - Base SWIM implementation
- `hyperscale/distributed_rewrite/nodes/gate.py` - Gate peer tracking
- `hyperscale/distributed_rewrite/nodes/manager.py` - Manager peer tracking
- `hyperscale/distributed_rewrite/nodes/worker.py` - Worker manager tracking

**Alternatives Considered**:
1. **Grace Period**: Arbitrary timeout, masks real failures during startup
2. **Quorum-Based Init**: Deadlock potential if all nodes wait for quorum
3. **Two-Phase Bootstrap**: Good but doesn't handle dynamic peer discovery
4. **Epoch-Based Freshness**: More complex, higher memory overhead

**Testing Strategy**:
1. Unit tests for confirmed/unconfirmed state transitions
2. Integration test: 3+ gates starting simultaneously, verify no false failures
3. Integration test: Confirmed peer crash, verify proper SUSPECT→DEAD flow
4. Integration test: Unconfirmed peer never reachable, verify no DEAD transition

---

### AD-30: Hierarchical Failure Detection for Multi-Job Distributed Systems

**Decision**: Implement a two-layer hierarchical failure detection system that separates machine-level liveness (global layer) from job-specific responsiveness (job layer), solving timer starvation issues and enabling accurate result routing in multi-job environments.

**Rationale**:
The original SWIM + Lifeguard implementation suffered from **timer starvation** where rapid gossip confirmations caused suspicion timers to be continuously rescheduled before they could expire. In a globally distributed system with multiple concurrent jobs, we also need to distinguish between "machine is dead" (affects all jobs) and "node is slow for job X" (affects only that job).

**Problem Statement - Timer Starvation**:

```
Original SuspicionManager flow with confirmation-based rescheduling:

T=0.00: Node A fails probe to Node B → start_suspicion(B, timeout=5s)
T=0.05: Node C gossips "B is suspect" → confirm_suspicion(B) → RESCHEDULE timer
T=0.10: Node D gossips "B is suspect" → confirm_suspicion(B) → RESCHEDULE timer
T=0.15: Node E gossips "B is suspect" → confirm_suspicion(B) → RESCHEDULE timer
...
T=4.95: Node Z gossips "B is suspect" → confirm_suspicion(B) → RESCHEDULE timer
T=5.00: Timer should expire... but was just reset to 4.5s remaining!

Result: Timer NEVER expires. Node B is never declared dead even though
        it hasn't responded to probes for 5+ seconds.

Root cause: Each confirmation cancels the old timer and creates a new one.
            With gossip echo (O(log n) dissemination), confirmations arrive
            faster than the (now shorter) timeout can elapse.
```

**Problem Statement - Multi-Job Routing**:

```
Scenario: Manager M1 runs jobs A, B, C simultaneously

Job A: High CPU load (90%), responses slow
Job B: Normal load (30%), responses normal
Job C: Memory pressure (85%), responses slow

With single-layer detection:
- M1 is either "alive" or "dead" for ALL jobs
- Can't route Job A results away from slow M1
- Can't keep Job B results on healthy M1

Need: Per-job suspicion that tracks "is this node responsive for THIS job?"
```

**Solution: Two-Layer Hierarchical Detection**

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                   HIERARCHICAL FAILURE DETECTION                                 │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                  │
│  ┌───────────────────────────────────────────────────────────────────────────┐  │
│  │                      GLOBAL LAYER (TimingWheel)                            │  │
│  │                                                                            │  │
│  │   Question: "Is this MACHINE alive?"                                       │  │
│  │                                                                            │  │
│  │   Triggers: SWIM probe timeout (machine-level liveness)                    │  │
│  │   Timeout: 5-30 seconds (configurable)                                     │  │
│  │   Effect: Global death clears ALL job suspicions for that node             │  │
│  │                                                                            │  │
│  │   Implementation: Kafka-style hierarchical timing wheel                    │  │
│  │   - O(1) timer insertion and removal                                       │  │
│  │   - Single timer advancement (no per-suspicion timers)                     │  │
│  │   - Confirmation updates state, NOT timer                                  │  │
│  │                                                                            │  │
│  │   ┌─────────────────────────────────────────────────────────────────┐     │  │
│  │   │ Coarse Wheel (1s ticks)   │ Fine Wheel (100ms ticks)            │     │  │
│  │   │ ┌─┬─┬─┬─┬─┬─┬─┬─┬─┬─┐    │ ┌─┬─┬─┬─┬─┬─┬─┬─┬─┬─┐              │     │  │
│  │   │ │0│1│2│3│4│5│6│7│8│9│    │ │0│1│2│3│4│5│6│7│8│9│              │     │  │
│  │   │ └─┴─┴─┴─┴─┴─┴─┴─┴─┴─┘    │ └─┴─┴─┴─┴─┴─┴─┴─┴─┴─┘              │     │  │
│  │   │     ↑ current             │     ↑ current                       │     │  │
│  │   │                           │                                     │     │  │
│  │   │ Entries cascade from coarse to fine as they approach expiration │     │  │
│  │   └─────────────────────────────────────────────────────────────────┘     │  │
│  └───────────────────────────────────────────────────────────────────────────┘  │
│                                    │                                             │
│                                    │ Global death → Clear job suspicions         │
│                                    ▼                                             │
│  ┌───────────────────────────────────────────────────────────────────────────┐  │
│  │                       JOB LAYER (JobSuspicionManager)                      │  │
│  │                                                                            │  │
│  │   Question: "Is this node RESPONSIVE for THIS JOB?"                        │  │
│  │                                                                            │  │
│  │   Triggers: Job-specific communication timeout                             │  │
│  │   Timeout: 1-10 seconds (faster than global)                               │  │
│  │   Effect: Job-specific routing decisions                                   │  │
│  │                                                                            │  │
│  │   Implementation: Adaptive polling with LHM integration                    │  │
│  │   - Per (job_id, node) suspicion state                                     │  │
│  │   - Poll interval adapts: far (1s) → medium (250ms) → near (50ms)         │  │
│  │   - Confirmation updates state only (no timer reschedule)                  │  │
│  │   - LHM multiplier extends polling under load                              │  │
│  │                                                                            │  │
│  │   ┌─────────────────────────────────────────────────────────────────┐     │  │
│  │   │ Job A          │ Job B          │ Job C                         │     │  │
│  │   │ ┌────────────┐ │ ┌────────────┐ │ ┌────────────┐               │     │  │
│  │   │ │ Node1: OK  │ │ │ Node1: OK  │ │ │ Node1: SUSPECT            │     │  │
│  │   │ │ Node2: SUSP│ │ │ Node2: OK  │ │ │ Node2: OK                 │     │  │
│  │   │ │ Node3: OK  │ │ │ Node3: OK  │ │ │ Node3: SUSPECT            │     │  │
│  │   │ └────────────┘ │ └────────────┘ │ └────────────┘               │     │  │
│  │   │                │                │                               │     │  │
│  │   │ Independent suspicion per (job_id, node) pair                   │     │  │
│  │   └─────────────────────────────────────────────────────────────────┘     │  │
│  └───────────────────────────────────────────────────────────────────────────┘  │
│                                                                                  │
└─────────────────────────────────────────────────────────────────────────────────┘
```

**Component Architecture**:

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                      HierarchicalFailureDetector                                 │
│                                                                                  │
│  ┌─────────────────────────────────────────────────────────────────────────────┐│
│  │                              PUBLIC API                                      ││
│  ├─────────────────────────────────────────────────────────────────────────────┤│
│  │ start() / stop()              - Lifecycle management                        ││
│  │ suspect_global(node, inc)     - Start global suspicion                      ││
│  │ suspect_job(job, node, inc)   - Start job-specific suspicion                ││
│  │ confirm_global/job(...)       - Add confirmation (NO timer reschedule)      ││
│  │ refute_global/job(...)        - Clear suspicion (higher incarnation)        ││
│  │ is_alive_global(node)         - Query: machine up?                          ││
│  │ is_alive_for_job(job, node)   - Query: node responsive for job?             ││
│  │ clear_job(job_id)             - Cleanup when job completes                  ││
│  │ get_node_status(node)         - Comprehensive status query                  ││
│  └─────────────────────────────────────────────────────────────────────────────┘│
│                                    │                                             │
│          ┌────────────────────────┴─────────────────────────┐                   │
│          ▼                                                  ▼                   │
│  ┌───────────────────┐                            ┌───────────────────┐         │
│  │   TimingWheel     │                            │ JobSuspicionMgr   │         │
│  │                   │                            │                   │         │
│  │ • Coarse buckets  │                            │ • Per-job tracking│         │
│  │ • Fine buckets    │                            │ • Adaptive polling│         │
│  │ • Single tick     │                            │ • LHM integration │         │
│  │ • O(1) ops        │                            │ • Resource limits │         │
│  └───────────────────┘                            └───────────────────┘         │
│          │                                                  │                   │
│          │ on_expired(node, state)                          │ on_expired(job,   │
│          ▼                                                  ▼  node, inc)       │
│  ┌───────────────────────────────────────────────────────────────────────────┐ │
│  │                         CALLBACK HANDLERS                                  │ │
│  │                                                                            │ │
│  │  _handle_global_expiration:           _handle_job_expiration:              │ │
│  │  1. Mark node as globally dead        1. Record job-specific death         │ │
│  │  2. Clear ALL job suspicions          2. Invoke on_job_death callback      │ │
│  │  3. Invoke on_global_death callback   3. Update job routing state          │ │
│  │  4. Record failure event                                                   │ │
│  └───────────────────────────────────────────────────────────────────────────┘ │
│                                                                                  │
│  ┌───────────────────────────────────────────────────────────────────────────┐ │
│  │                        RECONCILIATION LOOP                                 │ │
│  │                                                                            │ │
│  │  Periodic (every 5s):                                                      │ │
│  │  - Clear job suspicions for globally-dead nodes                            │ │
│  │  - Detect inconsistencies between layers                                   │ │
│  │  - Log/escalate anomalies                                                  │ │
│  └───────────────────────────────────────────────────────────────────────────┘ │
└─────────────────────────────────────────────────────────────────────────────────┘
```

**Timing Wheel Design (Global Layer)**:

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                           TIMING WHEEL INTERNALS                                 │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                  │
│  Configuration:                                                                  │
│  • coarse_tick_ms: 1000 (1 second per coarse bucket)                            │
│  • fine_tick_ms: 100 (100ms per fine bucket)                                    │
│  • coarse_buckets: 64 (64 seconds max timeout in coarse wheel)                  │
│  • fine_buckets: 10 (1 second of fine-grained resolution)                       │
│                                                                                  │
│  ┌─────────────────────────────────────────────────────────────────────────────┐│
│  │                         COARSE WHEEL (1s resolution)                        ││
│  │                                                                             ││
│  │  Bucket 0    Bucket 1    Bucket 2    ...    Bucket 63                       ││
│  │  ┌──────┐    ┌──────┐    ┌──────┐          ┌──────┐                        ││
│  │  │Entry │    │      │    │Entry │          │      │                        ││
│  │  │  A   │    │      │    │  C   │          │      │                        ││
│  │  │Entry │    │      │    │      │          │      │                        ││
│  │  │  B   │    │      │    │      │          │      │                        ││
│  │  └──────┘    └──────┘    └──────┘          └──────┘                        ││
│  │     ▲                                                                       ││
│  │     │ current_coarse_idx                                                    ││
│  │                                                                             ││
│  │  When current bucket expires → cascade entries to fine wheel                ││
│  └─────────────────────────────────────────────────────────────────────────────┘│
│                                    │                                             │
│                                    │ cascade                                     │
│                                    ▼                                             │
│  ┌─────────────────────────────────────────────────────────────────────────────┐│
│  │                          FINE WHEEL (100ms resolution)                      ││
│  │                                                                             ││
│  │  Bucket 0    Bucket 1    Bucket 2    ...    Bucket 9                        ││
│  │  ┌──────┐    ┌──────┐    ┌──────┐          ┌──────┐                        ││
│  │  │Entry │    │Entry │    │      │          │      │                        ││
│  │  │  X   │    │  Y   │    │      │          │      │                        ││
│  │  └──────┘    └──────┘    └──────┘          └──────┘                        ││
│  │     ▲                                                                       ││
│  │     │ current_fine_idx                                                      ││
│  │                                                                             ││
│  │  When fine bucket expires → fire expiration callbacks                       ││
│  └─────────────────────────────────────────────────────────────────────────────┘│
│                                                                                  │
│  TICK ADVANCEMENT (single task, runs every fine_tick_ms):                       │
│                                                                                  │
│  async def _tick():                                                              │
│      # Advance fine wheel                                                        │
│      fine_idx = (fine_idx + 1) % fine_buckets                                   │
│      if fine_idx == 0:                                                           │
│          # Wrapped around - advance coarse wheel                                 │
│          coarse_idx = (coarse_idx + 1) % coarse_buckets                         │
│          # Cascade coarse bucket entries to fine wheel                          │
│          for entry in coarse_buckets[coarse_idx]:                               │
│              fine_target = calculate_fine_bucket(entry.expiration)              │
│              fine_buckets[fine_target].add(entry)                               │
│                                                                                  │
│      # Fire expired entries in current fine bucket                              │
│      for entry in fine_buckets[fine_idx]:                                       │
│          if entry.expiration <= now:                                            │
│              on_expired(entry.node, entry.state)                                │
│                                                                                  │
└─────────────────────────────────────────────────────────────────────────────────┘
```

**Adaptive Polling Design (Job Layer)**:

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                       ADAPTIVE POLLING ALGORITHM                                 │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                  │
│  Each JobSuspicion has a single polling task (NOT timer-per-suspicion):         │
│                                                                                  │
│  async def _poll_suspicion(suspicion):                                           │
│      while not suspicion.cancelled and running:                                  │
│          remaining = suspicion.time_remaining(n_members)                         │
│                                                                                  │
│          if remaining <= 0:                                                      │
│              # EXPIRED - declare dead                                            │
│              await _handle_expiration(suspicion)                                 │
│              return                                                              │
│                                                                                  │
│          # Calculate adaptive poll interval                                      │
│          poll_interval = _calculate_poll_interval(remaining)                     │
│          sleep_time = min(poll_interval, remaining)                              │
│                                                                                  │
│          await asyncio.sleep(sleep_time)                                         │
│          # Loop continues - if confirmations arrived, time_remaining shorter    │
│                                                                                  │
│  Poll Interval Selection:                                                        │
│  ┌─────────────────────────────────────────────────────────────────────────────┐│
│  │                                                                             ││
│  │  Time Remaining      Base Interval    After LHM (×2)                        ││
│  │  ──────────────      ─────────────    ──────────────                        ││
│  │  > 5 seconds         1000ms (far)     2000ms                                ││
│  │  1-5 seconds         250ms (medium)   500ms                                 ││
│  │  < 1 second          50ms (near)      100ms                                 ││
│  │                                                                             ││
│  │  ┌────────────────────────────────────────────────────────────────────┐    ││
│  │  │                                                                    │    ││
│  │  │  Poll    ┌─────┐   ┌────┐   ┌───┐  ┌──┐ ┌─┐┌─┐┌─┐┌─┐             │    ││
│  │  │  Rate    │     │   │    │   │   │  │  │ │ ││ ││ ││ │ EXPIRE      │    ││
│  │  │          │     │   │    │   │   │  │  │ │ ││ ││ ││ │   ↓         │    ││
│  │  │  ────────┴─────┴───┴────┴───┴───┴──┴──┴─┴─┴┴─┴┴─┴┴─┴──────►     │    ││
│  │  │  T=0          T=5s        T=9s   T=9.5s  T=10s                   │    ││
│  │  │                                                                    │    ││
│  │  │  Polls become more frequent as expiration approaches               │    ││
│  │  └────────────────────────────────────────────────────────────────────┘    ││
│  │                                                                             ││
│  └─────────────────────────────────────────────────────────────────────────────┘│
│                                                                                  │
│  KEY INSIGHT: Confirmations update suspicion STATE (confirmation_count).         │
│               The poll loop naturally picks up the shorter timeout on next poll. │
│               NO timer cancellation/rescheduling needed!                         │
│                                                                                  │
│  Before (timer starvation):           After (adaptive polling):                  │
│  ─────────────────────────           ──────────────────────────                 │
│  T=0: start_suspicion                 T=0: start_suspicion                       │
│  T=0.1: confirm → CANCEL + NEW timer  T=0.1: confirm → update count              │
│  T=0.2: confirm → CANCEL + NEW timer  T=0.2: confirm → update count              │
│  ...timer never expires...            T=0.5: poll → remaining=4.0s, sleep        │
│                                       T=1.0: poll → remaining=3.0s, sleep        │
│                                       ...                                        │
│                                       T=5.0: poll → remaining=0, EXPIRE          │
│                                                                                  │
└─────────────────────────────────────────────────────────────────────────────────┘
```

**Node Status State Machine**:

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                         NODE STATUS STATE MACHINE                                │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                  │
│  NodeStatus enum:                                                                │
│  ┌───────────────┐  ┌─────────────────────┐  ┌─────────────────┐                │
│  │    ALIVE      │  │  SUSPECTED_GLOBAL   │  │  SUSPECTED_JOB  │                │
│  │               │  │                     │  │                 │                │
│  │ Not suspected │  │ Suspected at global │  │ Suspected for   │                │
│  │ at any layer  │  │ layer (machine may  │  │ specific job(s) │                │
│  │               │  │ be down)            │  │ but not global  │                │
│  └───────┬───────┘  └──────────┬──────────┘  └────────┬────────┘                │
│          │                     │                      │                          │
│          │                     │                      │                          │
│          │                     ▼                      ▼                          │
│          │          ┌─────────────────────┐  ┌─────────────────┐                │
│          │          │    DEAD_GLOBAL      │  │    DEAD_JOB     │                │
│          │          │                     │  │                 │                │
│          │          │ Declared dead at    │  │ Declared dead   │                │
│          │          │ global level        │  │ for specific    │                │
│          │          │ (machine is down)   │  │ job only        │                │
│          │          └─────────────────────┘  └─────────────────┘                │
│          │                     │                                                 │
│          │                     │                                                 │
│          └─────────────────────┼────────────────────────────────────────────────│
│                                │                                                 │
│                                ▼                                                 │
│                    Global death clears all job suspicions                        │
│                                                                                  │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                  │
│  State Transitions:                                                              │
│                                                                                  │
│  ┌─────────┐    suspect_global()     ┌──────────────────┐                       │
│  │  ALIVE  │ ──────────────────────► │ SUSPECTED_GLOBAL │                       │
│  └─────────┘                          └────────┬─────────┘                       │
│       ▲                                        │                                 │
│       │ refute_global() or                     │ timeout without                 │
│       │ clear_global_death()                   │ refutation                      │
│       │                                        ▼                                 │
│       │                               ┌──────────────────┐                       │
│       └───────────────────────────────│   DEAD_GLOBAL    │                       │
│         (node rejoins with            └──────────────────┘                       │
│          higher incarnation)                   │                                 │
│                                                │ triggers                        │
│                                                ▼                                 │
│                                    Clear all job suspicions                      │
│                                    for this node                                 │
│                                                                                  │
│  ┌─────────┐      suspect_job()      ┌───────────────┐                          │
│  │  ALIVE  │ ──────────────────────► │ SUSPECTED_JOB │                          │
│  └─────────┘                          └───────┬───────┘                          │
│       ▲                                       │                                  │
│       │ refute_job()                          │ timeout without                  │
│       │                                       │ refutation                       │
│       │                                       ▼                                  │
│       │                               ┌───────────────┐                          │
│       └───────────────────────────────│   DEAD_JOB    │                          │
│                                       └───────────────┘                          │
│                                                                                  │
└─────────────────────────────────────────────────────────────────────────────────┘
```

**Lifecycle Diagram - HierarchicalFailureDetector**:

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                    HIERARCHICAL DETECTOR LIFECYCLE                               │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                  │
│  1. CONSTRUCTION                                                                 │
│  ────────────────                                                                │
│  detector = HierarchicalFailureDetector(                                         │
│      config=HierarchicalConfig(...),                                             │
│      on_global_death=handle_global_death,                                        │
│      on_job_death=handle_job_death,                                              │
│      get_n_members=lambda: len(active_nodes),                                    │
│      get_job_n_members=lambda job: len(job_nodes[job]),                          │
│      get_lhm_multiplier=lambda: local_health.get_multiplier(),                   │
│  )                                                                               │
│       │                                                                          │
│       │ Creates TimingWheel and JobSuspicionManager                              │
│       │ Initializes reconciliation state                                         │
│       ▼                                                                          │
│  ┌─────────────┐                                                                 │
│  │  CREATED    │                                                                 │
│  │             │                                                                 │
│  │ Wheel: idle │                                                                 │
│  │ Jobs: idle  │                                                                 │
│  │ Reconcile:  │                                                                 │
│  │   not run   │                                                                 │
│  └──────┬──────┘                                                                 │
│         │                                                                        │
│         │ await detector.start()                                                 │
│         ▼                                                                        │
│  2. STARTUP                                                                      │
│  ──────────                                                                      │
│  ┌─────────────┐                                                                 │
│  │  STARTING   │                                                                 │
│  │             │─── timing_wheel.start()                                         │
│  │             │    └── Creates tick advancement task                            │
│  │             │                                                                 │
│  │             │─── Starts reconciliation loop task                              │
│  │             │                                                                 │
│  └──────┬──────┘                                                                 │
│         │                                                                        │
│         │ _running = True                                                        │
│         ▼                                                                        │
│  ┌─────────────┐                                                                 │
│  │   RUNNING   │                                                                 │
│  │             │                                                                 │
│  │ Wheel: tick │◄────────────────────────────────────────────────────┐          │
│  │ Jobs: poll  │                                                     │          │
│  │ Reconcile:  │    suspect_global()  ──► Add to timing wheel        │          │
│  │   periodic  │    confirm_global()  ──► Update state (no reschedule)          │
│  │             │    suspect_job()     ──► Create job suspicion       │          │
│  │             │    confirm_job()     ──► Update confirmation count  │          │
│  │             │                                                     │          │
│  │             │    [Expiration]      ──► Callback + state update ───┘          │
│  │             │                                                                 │
│  └──────┬──────┘                                                                 │
│         │                                                                        │
│         │ await detector.stop()                                                  │
│         ▼                                                                        │
│  3. SHUTDOWN                                                                     │
│  ───────────                                                                     │
│  ┌─────────────┐                                                                 │
│  │  STOPPING   │                                                                 │
│  │             │─── _running = False                                             │
│  │             │                                                                 │
│  │             │─── Cancel reconciliation task                                   │
│  │             │                                                                 │
│  │             │─── timing_wheel.stop()                                          │
│  │             │    └── Cancels tick task, clears buckets                        │
│  │             │                                                                 │
│  │             │─── job_manager.shutdown()                                       │
│  │             │    └── Cancels all poll tasks, clears suspicions                │
│  │             │                                                                 │
│  └──────┬──────┘                                                                 │
│         │                                                                        │
│         ▼                                                                        │
│  ┌─────────────┐                                                                 │
│  │   STOPPED   │                                                                 │
│  │             │                                                                 │
│  │ All tasks   │                                                                 │
│  │ cancelled   │                                                                 │
│  │ All state   │                                                                 │
│  │ cleared     │                                                                 │
│  └─────────────┘                                                                 │
│                                                                                  │
└─────────────────────────────────────────────────────────────────────────────────┘
```

**Integration with HealthAwareServer**:

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                    HEALTHAWARESERVER INTEGRATION                                 │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                  │
│  class HealthAwareServer(MercurySyncBaseServer):                                 │
│      """Base SWIM server with optional hierarchical detection."""                │
│                                                                                  │
│      def __init__(self, ...):                                                    │
│          ...                                                                     │
│          # Optional hierarchical detector (initialized by subclasses)            │
│          self._hierarchical_detector: HierarchicalFailureDetector | None = None │
│                                                                                  │
│      # ─────────────────────────────────────────────────────────────────────── #
│      # Initialization (called by subclasses in their __init__)                  #
│      # ─────────────────────────────────────────────────────────────────────── #
│                                                                                  │
│      def init_hierarchical_detector(                                             │
│          self,                                                                   │
│          config: HierarchicalConfig | None = None,                               │
│          on_global_death: Callable[[tuple[str,int], int], None] | None = None,  │
│          on_job_death: Callable[[str, tuple[str,int], int], None] | None = None,│
│          get_job_n_members: Callable[[str], int] | None = None,                 │
│      ) -> HierarchicalFailureDetector:                                           │
│          """Initialize hierarchical detector with callbacks."""                  │
│          self._hierarchical_detector = HierarchicalFailureDetector(              │
│              config=config,                                                      │
│              on_global_death=on_global_death,                                    │
│              on_job_death=on_job_death,                                          │
│              get_n_members=self._get_member_count,   # From SWIM membership     │
│              get_job_n_members=get_job_n_members,                                │
│              get_lhm_multiplier=self._get_lhm_multiplier,  # From LHM           │
│          )                                                                       │
│          return self._hierarchical_detector                                      │
│                                                                                  │
│      # ─────────────────────────────────────────────────────────────────────── #
│      # Lifecycle (called by subclasses in start()/stop())                       #
│      # ─────────────────────────────────────────────────────────────────────── #
│                                                                                  │
│      async def start_hierarchical_detector(self) -> None:                        │
│          if self._hierarchical_detector:                                         │
│              await self._hierarchical_detector.start()                           │
│                                                                                  │
│      async def stop_hierarchical_detector(self) -> None:                         │
│          if self._hierarchical_detector:                                         │
│              await self._hierarchical_detector.stop()                            │
│                                                                                  │
│      # ─────────────────────────────────────────────────────────────────────── #
│      # Convenience methods (fail-open if detector not initialized)              #
│      # ─────────────────────────────────────────────────────────────────────── #
│                                                                                  │
│      async def suspect_node_global(self, node, inc, from_node) -> bool           │
│      async def suspect_node_for_job(self, job, node, inc, from_node) -> bool     │
│      async def is_node_alive_global(self, node) -> bool                          │
│      def is_node_alive_for_job(self, job, node) -> bool                          │
│      async def clear_job_suspicions(self, job_id) -> int                         │
│      async def get_node_hierarchical_status(self, node) -> NodeStatus | None     │
│                                                                                  │
└─────────────────────────────────────────────────────────────────────────────────┘
```

**Example Implementation - Manager with Hierarchical Detection**:

```python
class ManagerServer(HealthAwareServer):
    """Manager node with job-layer failure detection."""

    def __init__(self, ...):
        super().__init__(...)

        # Initialize hierarchical detector for job-aware failure tracking
        self.init_hierarchical_detector(
            config=HierarchicalConfig(
                # Longer global timeout for WAN latency
                global_min_timeout=10.0,
                global_max_timeout=60.0,
                # Shorter job timeout for responsiveness
                job_min_timeout=2.0,
                job_max_timeout=15.0,
            ),
            on_global_death=self._on_worker_globally_dead,
            on_job_death=self._on_worker_dead_for_job,
            get_job_n_members=self._get_job_worker_count,
        )

    async def start(self) -> None:
        await super().start()
        # Start hierarchical detection after SWIM is running
        await self.start_hierarchical_detector()

    async def stop(self, ...) -> None:
        # Stop hierarchical detection before SWIM shutdown
        await self.stop_hierarchical_detector()
        await super().stop(...)

    # ─────────────────────────────────────────────────────────────────────────
    # Callbacks
    # ─────────────────────────────────────────────────────────────────────────

    def _on_worker_globally_dead(
        self,
        worker_addr: tuple[str, int],
        incarnation: int,
    ) -> None:
        """Worker machine is dead - affects ALL jobs on that worker."""
        worker_id = self._worker_addr_to_id.get(worker_addr)
        if worker_id:
            # Remove from all job assignments
            self._job_manager.remove_worker_from_all_jobs(worker_id)
            # Trigger workflow reassignment
            self._task_runner.run(self._reassign_workflows_from_dead_worker, worker_id)

    def _on_worker_dead_for_job(
        self,
        job_id: str,
        worker_addr: tuple[str, int],
        incarnation: int,
    ) -> None:
        """Worker is unresponsive for specific job - reroute that job only."""
        worker_id = self._worker_addr_to_id.get(worker_addr)
        if worker_id:
            # Remove from this job's assignment only
            self._job_manager.remove_worker_from_job(job_id, worker_id)
            # Reroute pending workflows for this job
            self._task_runner.run(self._reroute_job_workflows, job_id, worker_id)

    def _get_job_worker_count(self, job_id: str) -> int:
        """Get number of workers assigned to a job."""
        return self._job_manager.get_worker_count(job_id)

    # ─────────────────────────────────────────────────────────────────────────
    # Usage in workflow dispatch
    # ─────────────────────────────────────────────────────────────────────────

    async def _select_worker_for_workflow(
        self,
        job_id: str,
        workflow: Workflow,
    ) -> tuple[str, int] | None:
        """Select a worker that's alive for this specific job."""
        candidates = self._job_manager.get_job_workers(job_id)

        for worker_id in candidates:
            worker_addr = self._get_worker_addr(worker_id)

            # Check job-specific liveness, not just global
            if self.is_node_alive_for_job(job_id, worker_addr):
                return worker_addr

        return None  # No healthy workers for this job

    # ─────────────────────────────────────────────────────────────────────────
    # Starting job-layer suspicion
    # ─────────────────────────────────────────────────────────────────────────

    async def _on_workflow_response_timeout(
        self,
        job_id: str,
        worker_addr: tuple[str, int],
    ) -> None:
        """Workflow response timed out - suspect worker for this job."""
        # Get worker's current incarnation
        incarnation = self._get_worker_incarnation(worker_addr)

        # Start job-specific suspicion (not global - machine may be fine)
        await self.suspect_node_for_job(
            job_id=job_id,
            node=worker_addr,
            incarnation=incarnation,
            from_node=self._get_self_udp_addr(),
        )

    # ─────────────────────────────────────────────────────────────────────────
    # Cleanup when job completes
    # ─────────────────────────────────────────────────────────────────────────

    async def _on_job_completed(self, job_id: str) -> None:
        """Job finished - clear all suspicions for that job."""
        cleared = await self.clear_job_suspicions(job_id)
        if cleared > 0:
            await self._log(f"Cleared {cleared} suspicions for completed job {job_id}")
```

**Example Implementation - Gate with Cross-DC Detection**:

```python
class GateServer(HealthAwareServer):
    """Gate node with datacenter-level failure detection."""

    def __init__(self, ...):
        super().__init__(...)

        # Initialize for cross-DC manager detection
        self.init_hierarchical_detector(
            config=HierarchicalConfig(
                # Very long timeout for WAN (cross-DC) latency
                global_min_timeout=30.0,
                global_max_timeout=120.0,
                # Per-DC "job" timeout (treat each DC as a "job")
                job_min_timeout=5.0,
                job_max_timeout=30.0,
            ),
            on_global_death=self._on_manager_globally_dead,
            on_job_death=self._on_manager_dead_for_dc,  # DC treated as "job"
            get_job_n_members=self._get_dc_manager_count,
        )

    async def _on_manager_heartbeat_timeout(
        self,
        dc_id: str,
        manager_addr: tuple[str, int],
    ) -> None:
        """Manager heartbeat timed out - suspect for this DC."""
        incarnation = self._get_manager_incarnation(manager_addr)

        # Suspect manager for this DC (job = DC)
        await self.suspect_node_for_job(
            job_id=dc_id,  # DC ID used as "job ID"
            node=manager_addr,
            incarnation=incarnation,
            from_node=self._get_self_udp_addr(),
        )

    async def _select_manager_for_dc(self, dc_id: str) -> tuple[str, int] | None:
        """Select a healthy manager for a datacenter."""
        managers = self._dc_managers.get(dc_id, [])

        for manager_addr in managers:
            # Check DC-specific health
            if self.is_node_alive_for_job(dc_id, manager_addr):
                return manager_addr

        return None
```

**Reconciliation Logic**:

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                         RECONCILIATION SCENARIOS                                 │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                  │
│  Scenario 1: Global death with lingering job suspicions                          │
│  ───────────────────────────────────────────────────────                         │
│                                                                                  │
│  State BEFORE:                    State AFTER reconciliation:                    │
│  ┌──────────────────────┐        ┌──────────────────────┐                       │
│  │ Global Layer         │        │ Global Layer         │                       │
│  │ Node A: DEAD         │        │ Node A: DEAD         │                       │
│  │                      │        │                      │                       │
│  │ Job Layer            │        │ Job Layer            │                       │
│  │ Job1/NodeA: SUSPECT  │───────►│ Job1/NodeA: CLEARED  │                       │
│  │ Job2/NodeA: SUSPECT  │        │ Job2/NodeA: CLEARED  │                       │
│  └──────────────────────┘        └──────────────────────┘                       │
│                                                                                  │
│  Reason: If machine is dead, all jobs are implicitly affected.                   │
│          Job suspicions are redundant and waste resources.                       │
│                                                                                  │
│  ────────────────────────────────────────────────────────────────────────────── │
│                                                                                  │
│  Scenario 2: Job death but global alive (job-specific issue)                     │
│  ───────────────────────────────────────────────────────────                     │
│                                                                                  │
│  State:                                                                          │
│  ┌──────────────────────┐                                                       │
│  │ Global Layer         │                                                       │
│  │ Node A: ALIVE        │  ◄── Machine is up (SWIM probes succeed)              │
│  │                      │                                                       │
│  │ Job Layer            │                                                       │
│  │ Job1/NodeA: DEAD     │  ◄── But unresponsive for Job1 (CPU saturated)        │
│  │ Job2/NodeA: ALIVE    │  ◄── Still responsive for Job2                        │
│  └──────────────────────┘                                                       │
│                                                                                  │
│  Action: Route Job1 workflows away from Node A.                                  │
│          Keep routing Job2 workflows to Node A.                                  │
│                                                                                  │
│  This is the KEY VALUE of hierarchical detection!                                │
│                                                                                  │
│  ────────────────────────────────────────────────────────────────────────────── │
│                                                                                  │
│  Scenario 3: Node rejoins (was globally dead)                                    │
│  ────────────────────────────────────────────                                    │
│                                                                                  │
│  Timeline:                                                                       │
│  T=0:   Node A marked DEAD_GLOBAL                                                │
│  T=10:  Node A restarts, sends heartbeat with higher incarnation                 │
│  T=10:  Receive heartbeat → clear_global_death(A)                                │
│  T=10:  Node A now ALIVE at both layers                                          │
│                                                                                  │
│  No job suspicions to clear (they were cleared when node died globally).         │
│                                                                                  │
└─────────────────────────────────────────────────────────────────────────────────┘
```

**Resource Limits and Bounds**:

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                           RESOURCE LIMITS                                        │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                  │
│  Global Layer (TimingWheel):                                                     │
│  ───────────────────────────                                                     │
│  • max_entries: 10,000 (default)                                                 │
│  • Memory per entry: ~200 bytes (SuspicionState + wheel bookkeeping)             │
│  • Max memory: ~2MB for 10K entries                                              │
│  • Single tick task: O(bucket_size) per tick                                     │
│                                                                                  │
│  Job Layer (JobSuspicionManager):                                                │
│  ────────────────────────────────                                                │
│  • max_suspicions_per_job: 1,000 (default)                                       │
│  • max_total_suspicions: 50,000 (default)                                        │
│  • Memory per suspicion: ~300 bytes (JobSuspicion + polling state)               │
│  • Max memory: ~15MB for 50K suspicions                                          │
│  • One poll task per active suspicion (lightweight, mostly sleeping)             │
│                                                                                  │
│  Graceful Degradation:                                                           │
│  ─────────────────────                                                           │
│  When limits are reached:                                                        │
│  • New suspicions are REJECTED (start_suspicion returns None/False)              │
│  • Existing suspicions continue to be tracked                                    │
│  • Cleanup runs periodically to remove expired entries                           │
│  • Metrics/logs indicate limit reached                                           │
│                                                                                  │
│  ┌─────────────────────────────────────────────────────────────────────────────┐│
│  │  if len(suspicions) >= max_total_suspicions:                                ││
│  │      # Try cleanup first                                                    ││
│  │      cleanup_orphaned()                                                     ││
│  │      if len(suspicions) >= max_total_suspicions:                            ││
│  │          return None  # Reject - at capacity                                ││
│  └─────────────────────────────────────────────────────────────────────────────┘│
│                                                                                  │
└─────────────────────────────────────────────────────────────────────────────────┘
```

**Files Modified/Created**:

| File | Description |
|------|-------------|
| `hyperscale/distributed_rewrite/swim/detection/timing_wheel.py` | Kafka-style hierarchical timing wheel for O(1) timer operations |
| `hyperscale/distributed_rewrite/swim/detection/job_suspicion_manager.py` | Per-job adaptive polling suspicion manager |
| `hyperscale/distributed_rewrite/swim/detection/hierarchical_failure_detector.py` | Coordinator for global + job layers |
| `hyperscale/distributed_rewrite/swim/detection/__init__.py` | Updated exports |
| `hyperscale/distributed_rewrite/swim/health_aware_server.py` | Integration methods for subclasses |
| `tests/integration/test_timing_wheel.py` | Comprehensive timing wheel tests |
| `tests/integration/test_job_suspicion_manager.py` | Job suspicion manager tests |
| `tests/integration/test_hierarchical_failure_detector.py` | End-to-end hierarchical detection tests |

**Testing Strategy**:

1. **Unit Tests** (per component):
   - TimingWheel: bucket operations, tick advancement, cascade, expiration
   - JobSuspicionManager: adaptive polling, confirmation handling, cleanup
   - HierarchicalFailureDetector: layer coordination, reconciliation

2. **Integration Tests**:
   - Timer starvation scenario (rapid confirmations)
   - Global death clears job suspicions
   - Job-specific failure with global alive
   - LHM adjustment propagation
   - Concurrent operations (asyncio correctness)

3. **Edge Cases**:
   - Max limits reached (graceful rejection)
   - Node rejoins after global death
   - Job completion during active suspicion
   - Network partition (some layers detect, others don't)

**Alternatives Considered**:

1. **Single Timer with Dynamic Timeout**: Simpler but still has reschedule overhead
2. **Confirmation Debouncing**: Delays confirmation propagation, affects protocol correctness
3. **Timeout Floor**: Minimum timeout regardless of confirmations, but wastes time when node is clearly dead
4. **Batch Confirmation Processing**: Reduces reschedules but adds latency
5. **Hierarchical Without Job Layer**: Loses per-job routing capability

**Trade-offs**:

| Aspect | Before | After |
|--------|--------|-------|
| Timer management | Per-suspicion timers | Single tick + adaptive polling |
| Confirmation handling | Cancel + reschedule | State update only |
| Memory overhead | Lower | Higher (two layers) |
| Complexity | Simpler | More complex |
| Job awareness | None | Full per-job tracking |
| Timer starvation | Vulnerable | Immune |
| Routing accuracy | Global only | Per-job granularity |

---

### AD-31: Gossip-Informed Callbacks for Failure Propagation

**Decision**: Invoke application-layer callbacks (`_on_node_dead_callbacks`) when SWIM gossip reports a node as dead, not just when direct failure detection occurs. This enables cluster-wide consistent failure response and proper job leadership transfer across all node relationships.

**Rationale**:
In a distributed system using SWIM protocol, failure detection can occur through two paths:
1. **Direct detection**: Node A probes Node B, timeout expires, A marks B dead
2. **Gossip propagation**: Node A learns from Node C's gossip that B is dead

The original implementation only invoked `_on_node_dead_callbacks` for direct detection. This caused inconsistent cluster views where nodes that learned about failures via gossip didn't update their application state (e.g., `_active_gate_peers`, job leadership tracking).

**Problem Statement - Inconsistent Failure Response**:

```
Scenario: 3-node gate cluster (Gate1, Gate2, Gate3)

T=0.0: Gate3 crashes
T=0.5: Gate1 directly detects Gate3 failure (probe timeout)
       → _on_node_dead_callbacks invoked on Gate1
       → Gate1._active_gate_peers removes Gate3 ✓
       → Gate1 takes over Gate3's job leadership ✓

T=0.6: Gate1 gossips "Gate3 is DEAD" to Gate2
       → Gate2.process_piggyback_data() receives update
       → Gate2 updates incarnation_tracker to DEAD
       → ❌ _on_node_dead_callbacks NOT invoked on Gate2
       → Gate2._active_gate_peers still contains Gate3!
       → Gate2 doesn't know Gate3's jobs transferred to Gate1

Result: Gate2 has stale view - may route requests to dead Gate3
        or conflict with Gate1's job leadership takeover
```

**Solution: Gossip-Informed Callbacks**

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    FAILURE DETECTION CALLBACK FLOW                           │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  PATH 1: DIRECT DETECTION                                                   │
│  ────────────────────────                                                   │
│                                                                              │
│  SWIM Probe Timeout                                                          │
│        │                                                                     │
│        ▼                                                                     │
│  start_suspicion(node)                                                       │
│        │                                                                     │
│        ▼                                                                     │
│  [Suspicion timer expires in TimingWheel]                                   │
│        │                                                                     │
│        ▼                                                                     │
│  _on_suspicion_expired(node)                                                │
│        │                                                                     │
│        ├─► update_node_state(node, DEAD)                                    │
│        ├─► queue_gossip_update('dead', node)    ──► propagate to cluster    │
│        └─► invoke _on_node_dead_callbacks(node)  ✓                          │
│                                                                              │
│  PATH 2: GOSSIP-INFORMED (NEW)                                              │
│  ─────────────────────────────                                              │
│                                                                              │
│  Receive gossip: "node X is DEAD"                                           │
│        │                                                                     │
│        ▼                                                                     │
│  process_piggyback_data(data)                                               │
│        │                                                                     │
│        ├─► Check: was node already DEAD?                                    │
│        │          │                                                          │
│        │          ├─► YES: skip (idempotent)                                │
│        │          │                                                          │
│        │          └─► NO: state transition detected                         │
│        │                   │                                                 │
│        ▼                   │                                                 │
│  update_node_state(node, DEAD)                                              │
│        │                   │                                                 │
│        │                   ▼                                                 │
│        │   invoke _on_node_dead_callbacks(node)  ✓ (NEW)                    │
│        │                                                                     │
│        └─► queue_gossip_update('dead', node)    ──► continue propagation    │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

**Key Implementation Details**:

1. **Idempotency**: Only invoke callbacks when state actually changes (NOT-DEAD → DEAD)
2. **Symmetry**: Mirrors existing DEAD→OK recovery detection in `update_node_state`
3. **Incarnation respect**: Only process gossip with fresh incarnation numbers
4. **Metrics**: Track `gossip_informed_deaths` separately from direct detections

**Code Change** (in `process_piggyback_data`):

```python
# Check previous state BEFORE updating
previous_state = self._incarnation_tracker.get_node_state(update.node)
was_dead = previous_state and previous_state.status == b'DEAD'

updated = self.update_node_state(update.node, status, update.incarnation, update.timestamp)

# Gossip-informed callback: invoke when learning about death via gossip
if updated and update.update_type in ('dead', 'leave') and not was_dead:
    self._metrics.increment('gossip_informed_deaths')
    self._probe_scheduler.remove_member(update.node)
    for callback in self._on_node_dead_callbacks:
        callback(update.node)
```

**Impact on Node Relationships**:

| Relationship | Before AD-31 | After AD-31 |
|--------------|--------------|-------------|
| Gate ↔ Gate | Only detector updates `_active_gate_peers` | All gates update consistently |
| Manager ↔ Manager | Only detector triggers job takeover | All managers see consistent state |
| Gate ↔ Manager | Managers don't learn about gate failures quickly | Managers can react to gate deaths |
| Manager ↔ Worker | Workers only react to direct detection | Workers respond to gossip too |

**Job Leadership Transfer Cascade**:

With gossip-informed callbacks, the failure propagation enables proper job leadership transfer:

```
Gate Failure → Job Leadership Transfer
──────────────────────────────────────
Gate1 (job leader) dies
    │
    ├─► Gate2 detects (direct or gossip)
    │       └─► _on_node_dead callback
    │               └─► _handle_gate_peer_failure
    │                       └─► _handle_job_leader_failure
    │                               └─► takeover_leadership(job_id)
    │                               └─► _broadcast_job_leadership (to gates)
    │                               └─► _notify_managers_of_leadership (NEW)
    │
    └─► Gate3 detects (gossip from Gate2)
            └─► _on_node_dead callback
                    └─► Updates _active_gate_peers
                    └─► Sees Gate2 already took over (via broadcast)

Manager Failure → Job Leadership Transfer
────────────────────────────────────────
Manager1 (job leader in DC) dies
    │
    ├─► Manager2 (cluster leader) detects
    │       └─► _on_node_dead callback
    │               └─► _handle_manager_peer_failure
    │                       └─► _handle_job_leader_failure
    │                               └─► Takes over job leadership
    │                               └─► Propagates via heartbeat
    │                               └─► _notify_gate_of_leadership (NEW)
    │                               └─► _notify_workers_of_leadership (NEW)
    │
    ├─► Workers detect (gossip)
    │       └─► _on_node_dead callback
    │               └─► _handle_manager_failure
    │                       └─► Selects new primary manager
    │                       └─► Receives leadership update via heartbeat
    │
    └─► Origin Gate learns (via manager notification)
            └─► Updates _job_dc_managers[job_id][dc_id]
```

**Safeguards**:

1. **Incarnation checking**: Stale gossip with old incarnation is rejected
2. **State transition check**: Only fire callback on actual NOT-DEAD → DEAD transition
3. **Fencing tokens**: Job leadership uses monotonic tokens to prevent stale leaders
4. **Idempotent handlers**: Application callbacks must handle duplicate invocations

**Testing Strategy**:

1. Unit test: Verify callbacks invoked for gossip-received deaths
2. Integration test: 3 gates, kill one, verify all gates update `_active_gate_peers`
3. Integration test: Job leadership transfers correctly when leader gate fails
4. Integration test: Manager cluster leader takes over jobs when non-leader fails
5. Integration test: Workers discover new job leader after manager failure

**Files Modified**:

- `hyperscale/distributed_rewrite/swim/health_aware_server.py`: Add gossip-informed callback invocation in `process_piggyback_data`
- `hyperscale/distributed_rewrite/nodes/gate.py`: Add manager notification after job leadership takeover
- `hyperscale/distributed_rewrite/nodes/manager.py`: Add gate and worker notification after job leadership takeover

---

### AD-32: Hybrid Bounded Execution with Priority Load Shedding

**Decision**: Implement a hybrid approach for bounded pending responses optimized for a globally distributed performance testing framework:

1. **Server-side (incoming requests)**: Priority-aware bounded immediate execution with load shedding
2. **Client-side (outgoing requests)**: RobustMessageQueue per destination with graduated backpressure

This prevents memory exhaustion while ensuring latency-critical messages (SWIM heartbeats) are never delayed by queue overhead, and slow destinations don't block fast ones.

**Rationale - Why Hybrid?**

In a globally distributed performance testing framework:
- **Extreme latency** between datacenters (50-300ms RTT)
- **Frequent stats updates** from workers (100+ updates/sec per worker)
- **Busy workers** with high CPU/memory, making interval-based cleanup unreliable
- **SWIM protocol** requires sub-millisecond response for accurate failure detection

| Approach | Server-Side Problem | Client-Side Problem |
|----------|--------------------|--------------------|
| Queue-only | Consumer loop adds latency even at 0% load - deadly for SWIM | Works well |
| Counter-only | Works well | Head-of-line blocking on slow destinations |
| **Hybrid** | Immediate execution, priority discrimination | Per-destination isolation |

---

## Part 1: Server-Side Priority-Aware Bounded Immediate Execution

**Problem Statement - Unbounded Hot Path Queues**:

```
Original Flow (Vulnerable):

Incoming TCP/UDP Message (sync callback)
        │
        ▼
self._pending_responses.append(        ◄── UNBOUNDED DEQUE
    asyncio.ensure_future(
        self.process_*_request(...)
    )
)

Problem Scenarios:

1. MANAGER under load:
   - 1000 workers push stats at 100 updates/second each
   - 100,000 tasks created per second
   - Cleanup runs every 100ms → 10,000 tasks accumulate
   - Memory grows linearly with load

2. GATE under retry storm:
   - 10 datacenters × 50 retries × 100 concurrent jobs
   - 50,000 pending tasks during network partition recovery
   - No bound → potential OOM

3. WORKER under CPU pressure:
   - High CPU utilization delays event loop
   - Cleanup interval becomes unreliable
   - Tasks accumulate faster than they're cleaned
```

**Solution: Priority-Aware InFlightTracker**

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│           SERVER-SIDE: PRIORITY-AWARE BOUNDED IMMEDIATE EXECUTION                │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                  │
│  Incoming Message (sync callback from protocol)                                  │
│          │                                                                       │
│          ▼                                                                       │
│  ┌───────────────────────────────────────────────────────────────────────────┐  │
│  │                     MESSAGE PRIORITY CLASSIFICATION                        │  │
│  │                                                                            │  │
│  │   CRITICAL (0) │ SWIM probe/ack, leadership, failure detection            │  │
│  │   HIGH (1)     │ Job dispatch, workflow commands, state sync              │  │
│  │   NORMAL (2)   │ Status updates, heartbeats (non-SWIM)                    │  │
│  │   LOW (3)      │ Metrics, stats, telemetry, logs                          │  │
│  └───────────────────────────────────────────────────────────────────────────┘  │
│          │                                                                       │
│          ▼                                                                       │
│  ┌───────────────────────────────────────────────────────────────────────────┐  │
│  │                     IN-FLIGHT TRACKER CHECK                                │  │
│  │                                                                            │  │
│  │   tracker.try_acquire(priority) → bool                                     │  │
│  │                                                                            │  │
│  │   Priority Limits (per-priority bounded):                                  │  │
│  │   ┌──────────────────────────────────────────────────────────────────┐    │  │
│  │   │ Priority   │ Limit │ Current │ Available │ Status              │    │  │
│  │   ├──────────────────────────────────────────────────────────────────┤    │  │
│  │   │ CRITICAL   │   ∞   │    5    │     ∞     │ Always allowed      │    │  │
│  │   │ HIGH       │  500  │   480   │    20     │ ✓ Allowed           │    │  │
│  │   │ NORMAL     │  300  │   300   │     0     │ ✗ At limit          │    │  │
│  │   │ LOW        │  200  │   200   │     0     │ ✗ At limit, shed    │    │  │
│  │   └──────────────────────────────────────────────────────────────────┘    │  │
│  │                                                                            │  │
│  │   Global Limit: 1000 (sum of all priorities)                              │  │
│  └───────────────────────────────────────────────────────────────────────────┘  │
│          │                           │                                          │
│       ACQUIRED                    REJECTED                                      │
│          │                           │                                          │
│          ▼                           ▼                                          │
│  ┌───────────────────┐    ┌───────────────────────────────────────────────────┐│
│  │ Immediate Execute │    │              LOAD SHEDDING                         ││
│  │                   │    │                                                    ││
│  │ 1. Create task    │    │   Priority-based discrimination:                   ││
│  │ 2. Add callback   │    │                                                    ││
│  │ 3. Execute NOW    │    │   • LOW: Silent drop, increment counter           ││
│  │                   │    │   • NORMAL: Drop if HIGH/CRITICAL pressure        ││
│  │ No queue latency! │    │   • HIGH: Only drop if CRITICAL overwhelmed       ││
│  │                   │    │   • CRITICAL: NEVER drop, always execute          ││
│  └───────────────────┘    │                                                    ││
│          │                │   Response varies by protocol:                     ││
│          │                │   • UDP: Silent drop (no guarantee anyway)        ││
│          │                │   • TCP: Error response with Retry-After          ││
│          │                └───────────────────────────────────────────────────────┘│
│          │                                                                       │
│          ▼                                                                       │
│  ┌───────────────────────────────────────────────────────────────────────────┐  │
│  │                        TASK DONE CALLBACK                                  │  │
│  │                                                                            │  │
│  │  1. tracker.release(priority)  # Decrement priority-specific counter      │  │
│  │  2. Retrieve exception (prevent memory leak)                              │  │
│  │  3. Remove from tracking deque                                            │  │
│  └───────────────────────────────────────────────────────────────────────────┘  │
│                                                                                  │
└─────────────────────────────────────────────────────────────────────────────────┘
```

**State Diagram - Priority Load Shedding**:

```
                              ┌─────────────────────────────────────────────┐
                              │              SYSTEM STATE                    │
                              └─────────────────────────────────────────────┘
                                                │
        ┌───────────────────────────────────────┼───────────────────────────────────────┐
        │                                       │                                       │
        ▼                                       ▼                                       ▼
┌───────────────────┐                ┌───────────────────┐                ┌───────────────────┐
│     HEALTHY       │                │    PRESSURED      │                │    OVERLOADED     │
│                   │                │                   │                │                   │
│ All priorities    │                │ LOW at limit      │                │ NORMAL at limit   │
│ have capacity     │                │ Others OK         │                │ Only HIGH+CRIT OK │
│                   │                │                   │                │                   │
│ Actions:          │                │ Actions:          │                │ Actions:          │
│ • Accept all      │                │ • Shed LOW        │                │ • Shed LOW+NORMAL │
│                   │                │ • Accept others   │                │ • Accept HIGH+CRIT│
└───────────────────┘                └───────────────────┘                └───────────────────┘
        │                                       │                                       │
        │                                       │                                       │
        ▼                                       ▼                                       ▼
┌─────────────────────────────────────────────────────────────────────────────────────────────┐
│                                      CRITICAL                                                │
│                                                                                              │
│  CRITICAL priority messages ALWAYS execute immediately, regardless of system state.         │
│  This ensures SWIM probes/acks are never delayed, maintaining accurate failure detection.   │
└─────────────────────────────────────────────────────────────────────────────────────────────┘
```

**InFlightTracker Implementation**:

```python
from enum import IntEnum
from dataclasses import dataclass, field
from typing import Dict
import asyncio


class MessagePriority(IntEnum):
    """Priority levels for incoming messages."""
    CRITICAL = 0  # SWIM probes/acks - NEVER shed
    HIGH = 1      # Job dispatch, workflow commands
    NORMAL = 2    # Status updates, non-SWIM heartbeats
    LOW = 3       # Metrics, stats, telemetry


@dataclass(slots=True)
class PriorityLimits:
    """Per-priority concurrency limits."""
    critical: int = 0      # 0 = unlimited
    high: int = 500
    normal: int = 300
    low: int = 200
    global_limit: int = 1000


@dataclass
class InFlightTracker:
    """
    Tracks in-flight tasks by priority with bounded execution.

    Thread-safety: All operations are sync-safe (GIL-protected integers).
    Called from sync protocol callbacks.
    """
    limits: PriorityLimits = field(default_factory=PriorityLimits)

    # Per-priority counters
    _counts: Dict[MessagePriority, int] = field(default_factory=lambda: {
        MessagePriority.CRITICAL: 0,
        MessagePriority.HIGH: 0,
        MessagePriority.NORMAL: 0,
        MessagePriority.LOW: 0,
    })

    # Metrics
    _acquired_total: Dict[MessagePriority, int] = field(default_factory=lambda: {
        MessagePriority.CRITICAL: 0,
        MessagePriority.HIGH: 0,
        MessagePriority.NORMAL: 0,
        MessagePriority.LOW: 0,
    })
    _shed_total: Dict[MessagePriority, int] = field(default_factory=lambda: {
        MessagePriority.CRITICAL: 0,
        MessagePriority.HIGH: 0,
        MessagePriority.NORMAL: 0,
        MessagePriority.LOW: 0,
    })

    def try_acquire(self, priority: MessagePriority) -> bool:
        """
        Try to acquire a slot for the given priority.

        Returns True if acquired (execute immediately).
        Returns False if rejected (apply load shedding).

        CRITICAL priority ALWAYS succeeds.
        """
        # CRITICAL never shed
        if priority == MessagePriority.CRITICAL:
            self._counts[priority] += 1
            self._acquired_total[priority] += 1
            return True

        # Check global limit
        total = sum(self._counts.values())
        if total >= self.limits.global_limit:
            self._shed_total[priority] += 1
            return False

        # Check per-priority limit
        limit = self._get_limit(priority)
        if limit > 0 and self._counts[priority] >= limit:
            self._shed_total[priority] += 1
            return False

        self._counts[priority] += 1
        self._acquired_total[priority] += 1
        return True

    def release(self, priority: MessagePriority) -> None:
        """Release a slot for the given priority."""
        if self._counts[priority] > 0:
            self._counts[priority] -= 1

    def _get_limit(self, priority: MessagePriority) -> int:
        """Get limit for priority. 0 means unlimited."""
        if priority == MessagePriority.CRITICAL:
            return self.limits.critical  # Usually 0 (unlimited)
        elif priority == MessagePriority.HIGH:
            return self.limits.high
        elif priority == MessagePriority.NORMAL:
            return self.limits.normal
        else:  # LOW
            return self.limits.low

    @property
    def total_in_flight(self) -> int:
        """Total tasks currently in flight."""
        return sum(self._counts.values())

    def get_stats(self) -> dict:
        """Get current stats for observability."""
        return {
            "in_flight": dict(self._counts),
            "total_in_flight": self.total_in_flight,
            "acquired_total": dict(self._acquired_total),
            "shed_total": dict(self._shed_total),
            "limits": {
                "critical": self.limits.critical,
                "high": self.limits.high,
                "normal": self.limits.normal,
                "low": self.limits.low,
                "global": self.limits.global_limit,
            }
        }
```

**Integration with MercurySyncBaseServer**:

```python
class MercurySyncBaseServer:
    def __init__(self, ...):
        # ... existing init ...

        # AD-32: Priority-aware bounded execution
        self._tcp_tracker = InFlightTracker(
            limits=PriorityLimits(
                critical=0,  # Unlimited
                high=env.PENDING_RESPONSE_HIGH_LIMIT,
                normal=env.PENDING_RESPONSE_NORMAL_LIMIT,
                low=env.PENDING_RESPONSE_LOW_LIMIT,
                global_limit=env.PENDING_RESPONSE_MAX_CONCURRENT,
            )
        )
        self._udp_tracker = InFlightTracker(limits=...)

    def _spawn_tcp_response(
        self,
        coro: Coroutine,
        priority: MessagePriority = MessagePriority.NORMAL
    ) -> bool:
        """
        Spawn a TCP response task with priority-aware bounded execution.

        Returns True if task spawned, False if shed.
        Called from sync protocol callback.
        """
        if not self._tcp_tracker.try_acquire(priority):
            # Load shedding - log and return
            self._tcp_shed_count += 1
            return False

        task = asyncio.ensure_future(coro)
        task.add_done_callback(
            lambda t: self._on_tcp_task_done(t, priority)
        )
        self._pending_tcp_server_responses.append(task)
        return True

    def _on_tcp_task_done(
        self,
        task: asyncio.Task,
        priority: MessagePriority
    ) -> None:
        """Done callback - release slot and cleanup."""
        # Retrieve exception to prevent memory leak
        try:
            task.exception()
        except (asyncio.CancelledError, asyncio.InvalidStateError):
            pass
        except Exception:
            pass  # Logged elsewhere

        # Release the priority slot
        self._tcp_tracker.release(priority)
```

---

## Part 2: Client-Side RobustMessageQueue for Slow Destinations

**Problem Statement - Head-of-Line Blocking**:

```
Client sending to multiple destinations:

┌─────────────────────────────────────────────────────────────────────────────────┐
│                    PROBLEM: SINGLE QUEUE FOR ALL DESTINATIONS                    │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                  │
│  Outgoing Messages:                                                              │
│  ┌─────────────────────────────────────────────────────────────────────────┐    │
│  │  [DC-Asia:msg1] [DC-Asia:msg2] [DC-EU:msg1] [DC-US:msg1] [DC-Asia:msg3] │    │
│  └─────────────────────────────────────────────────────────────────────────┘    │
│                      ▲                                                          │
│                      │                                                          │
│              Asia DC has 300ms latency + packet loss                            │
│              EU and US are fast (50ms)                                          │
│                                                                                  │
│  Result: All messages blocked behind slow Asia connection                       │
│          Fast destinations starved                                              │
│                                                                                  │
└─────────────────────────────────────────────────────────────────────────────────┘
```

**Solution: Per-Destination RobustMessageQueue**:

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│              CLIENT-SIDE: PER-DESTINATION ROBUSTMESSAGEQUEUE                     │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                  │
│  Outgoing Request Manager:                                                       │
│                                                                                  │
│  ┌─────────────────────────────────────────────────────────────────────────┐    │
│  │                    PER-DESTINATION QUEUES                                │    │
│  │                                                                          │    │
│  │  ┌──────────────────┐  ┌──────────────────┐  ┌──────────────────┐       │    │
│  │  │    DC-Asia       │  │     DC-EU        │  │     DC-US        │       │    │
│  │  │  RobustQueue     │  │  RobustQueue     │  │  RobustQueue     │       │    │
│  │  │                  │  │                  │  │                  │       │    │
│  │  │ [msg1][msg2][m3] │  │ [msg1]           │  │ [msg1]           │       │    │
│  │  │                  │  │                  │  │                  │       │    │
│  │  │ State: THROTTLED │  │ State: HEALTHY   │  │ State: HEALTHY   │       │    │
│  │  │ Consumer: slow   │  │ Consumer: fast   │  │ Consumer: fast   │       │    │
│  │  └──────────────────┘  └──────────────────┘  └──────────────────┘       │    │
│  │          │                     │                     │                   │    │
│  │          ▼                     ▼                     ▼                   │    │
│  │  ┌──────────────────┐  ┌──────────────────┐  ┌──────────────────┐       │    │
│  │  │ Consumer Loop    │  │ Consumer Loop    │  │ Consumer Loop    │       │    │
│  │  │ (per destination)│  │ (per destination)│  │ (per destination)│       │    │
│  │  │                  │  │                  │  │                  │       │    │
│  │  │ await send()     │  │ await send()     │  │ await send()     │       │    │
│  │  │ (blocking on     │  │ (fast)           │  │ (fast)           │       │    │
│  │  │  slow network)   │  │                  │  │                  │       │    │
│  │  └──────────────────┘  └──────────────────┘  └──────────────────┘       │    │
│  │                                                                          │    │
│  └─────────────────────────────────────────────────────────────────────────┘    │
│                                                                                  │
│  Benefits:                                                                       │
│  1. Slow DC doesn't block fast DCs                                              │
│  2. Per-destination backpressure (THROTTLE → BATCH → OVERFLOW)                  │
│  3. Overflow ring buffer preserves newest messages on burst                      │
│  4. Metrics per destination for observability                                   │
│                                                                                  │
└─────────────────────────────────────────────────────────────────────────────────┘
```

**State Diagram - Per-Destination Queue States**:

```
                            ┌─────────────────────────────────────────┐
                            │        ROBUSTMESSAGEQUEUE STATES        │
                            └─────────────────────────────────────────┘
                                            │
        ┌───────────────────────────────────┼───────────────────────────────────────┐
        │                                   │                                       │
        ▼                                   ▼                                       ▼
┌───────────────┐                  ┌───────────────┐                       ┌───────────────┐
│   HEALTHY     │ fill < 70%       │  THROTTLED    │ 70% ≤ fill < 85%     │   BATCHING    │
│               │ ─────────────────│               │ ─────────────────────│               │
│ • No delay    │                  │ • 50ms delay  │                       │ • 200ms delay │
│ • Full speed  │                  │ • Slow down   │                       │ • Batch only  │
└───────────────┘                  └───────────────┘                       └───────────────┘
        ▲                                   │                                       │
        │                                   │                                       │
        │  fill < 70%                       │ 85% ≤ fill < 95%                      │
        └───────────────────────────────────┼───────────────────────────────────────┘
                                            │
                                            ▼
                                   ┌───────────────┐
                                   │   OVERFLOW    │ fill ≥ 95% or primary full
                                   │               │
                                   │ • 100ms delay │
                                   │ • Using ring  │
                                   │ • Drop oldest │
                                   └───────────────┘
                                            │
                                            │ overflow also full
                                            ▼
                                   ┌───────────────┐
                                   │   SATURATED   │
                                   │               │
                                   │ • 500ms delay │
                                   │ • Reject new  │
                                   │ • Critical    │
                                   └───────────────┘
```

**OutgoingRequestManager Implementation**:

```python
from hyperscale.distributed_rewrite.reliability import (
    RobustMessageQueue,
    RobustQueueConfig,
    QueueState,
)
from dataclasses import dataclass, field
from typing import Dict, Tuple, Any, Callable, Awaitable
import asyncio


@dataclass(slots=True)
class OutgoingRequest:
    """Represents an outgoing request to a destination."""
    destination: Tuple[str, int]
    data: bytes
    priority: MessagePriority = MessagePriority.NORMAL
    created_at: float = field(default_factory=time.monotonic)


class OutgoingRequestManager:
    """
    Manages outgoing requests with per-destination queuing.

    Uses RobustMessageQueue per destination to:
    1. Isolate slow destinations from fast ones
    2. Provide graduated backpressure per destination
    3. Preserve newest messages during overload

    Usage:
        manager = OutgoingRequestManager(send_func=self._send_to_destination)

        # Enqueue a request
        result = manager.enqueue(destination, data, priority)
        if result.backpressure.level != BackpressureLevel.NONE:
            # Sender should slow down for this destination
            pass
    """

    def __init__(
        self,
        send_func: Callable[[Tuple[str, int], bytes], Awaitable[None]],
        config: RobustQueueConfig | None = None,
        max_destinations: int = 1000,
    ):
        self._send_func = send_func
        self._config = config or RobustQueueConfig(
            maxsize=500,
            overflow_size=100,
            throttle_threshold=0.70,
            batch_threshold=0.85,
            reject_threshold=0.95,
        )
        self._max_destinations = max_destinations

        # Per-destination queues and consumers
        self._queues: Dict[Tuple[str, int], RobustMessageQueue[OutgoingRequest]] = {}
        self._consumers: Dict[Tuple[str, int], asyncio.Task] = {}
        self._running = False

        # LRU eviction for destinations
        self._destination_access_order: list[Tuple[str, int]] = []

    def enqueue(
        self,
        destination: Tuple[str, int],
        data: bytes,
        priority: MessagePriority = MessagePriority.NORMAL
    ) -> QueuePutResult:
        """
        Enqueue a request to a destination.

        Returns QueuePutResult with backpressure information.
        Caller can use result.backpressure to decide whether to slow down.
        """
        queue = self._get_or_create_queue(destination)

        request = OutgoingRequest(
            destination=destination,
            data=data,
            priority=priority,
        )

        return queue.put_nowait(request)

    def _get_or_create_queue(
        self,
        destination: Tuple[str, int]
    ) -> RobustMessageQueue[OutgoingRequest]:
        """Get or create queue for destination, with LRU eviction."""
        if destination in self._queues:
            # Update LRU order
            if destination in self._destination_access_order:
                self._destination_access_order.remove(destination)
            self._destination_access_order.append(destination)
            return self._queues[destination]

        # Evict LRU if at capacity
        while len(self._queues) >= self._max_destinations:
            oldest = self._destination_access_order.pop(0)
            self._evict_destination(oldest)

        # Create new queue and consumer
        queue = RobustMessageQueue[OutgoingRequest](self._config)
        self._queues[destination] = queue
        self._destination_access_order.append(destination)

        # Start consumer for this destination
        if self._running:
            self._consumers[destination] = asyncio.create_task(
                self._consume_destination(destination)
            )

        return queue

    async def _consume_destination(self, destination: Tuple[str, int]) -> None:
        """Consumer loop for a single destination."""
        queue = self._queues.get(destination)
        if not queue:
            return

        while self._running and destination in self._queues:
            try:
                request = await queue.get()
                await self._send_func(request.destination, request.data)
            except asyncio.CancelledError:
                break
            except Exception as e:
                # Log and continue - don't let one failure stop the consumer
                pass

    async def start(self) -> None:
        """Start all consumer loops."""
        self._running = True
        for destination in list(self._queues.keys()):
            if destination not in self._consumers:
                self._consumers[destination] = asyncio.create_task(
                    self._consume_destination(destination)
                )

    async def stop(self) -> None:
        """Stop all consumer loops gracefully."""
        self._running = False
        for task in self._consumers.values():
            task.cancel()
        await asyncio.gather(*self._consumers.values(), return_exceptions=True)
        self._consumers.clear()

    def _evict_destination(self, destination: Tuple[str, int]) -> None:
        """Evict a destination (LRU cleanup)."""
        if destination in self._consumers:
            self._consumers[destination].cancel()
            del self._consumers[destination]
        if destination in self._queues:
            del self._queues[destination]

    def get_destination_stats(self, destination: Tuple[str, int]) -> dict | None:
        """Get stats for a specific destination."""
        queue = self._queues.get(destination)
        if queue:
            return queue.get_metrics()
        return None

    def get_all_stats(self) -> dict:
        """Get stats for all destinations."""
        return {
            "destination_count": len(self._queues),
            "destinations": {
                f"{host}:{port}": queue.get_metrics()
                for (host, port), queue in self._queues.items()
            }
        }
```

---

## Part 3: Applicability Matrix

| Component | Server-Side (Incoming) | Client-Side (Outgoing) | Notes |
|-----------|------------------------|------------------------|-------|
| **MercurySyncBaseServer** | ✅ InFlightTracker | ✅ OutgoingRequestManager | Both patterns apply |
| **UDPProtocol (jobs)** | ✅ InFlightTracker | ✅ OutgoingRequestManager | Same pattern for job protocol |
| **HealthAwareServer** | ✅ Inherits | ✅ Inherits | Extends MercurySyncBaseServer |
| **RemoteGraphController** | ✅ Inherits | ✅ Inherits | Extends UDPProtocol |
| **Gate** | ✅ Via inheritance | ✅ For DC communication | Cross-DC coordination |
| **Manager** | ✅ Via inheritance | ✅ For worker communication | Stats from workers |
| **Worker** | ✅ Via inheritance | ✅ For manager communication | Lower priority limits |
| **WorkflowRunner** | ❌ | ❌ | Already has `_max_pending_workflows` |
| **RemoteGraphManager** | ❌ | ❌ | Different pattern (workflow queuing) |

---

## Part 4: Configuration

**Environment Variables (env.py)**:

```python
# AD-32: Priority-Aware Bounded Execution Settings
PENDING_RESPONSE_MAX_CONCURRENT: StrictInt = 1000      # Global limit
PENDING_RESPONSE_HIGH_LIMIT: StrictInt = 500           # HIGH priority limit
PENDING_RESPONSE_NORMAL_LIMIT: StrictInt = 300         # NORMAL priority limit
PENDING_RESPONSE_LOW_LIMIT: StrictInt = 200            # LOW priority limit (shed first)
PENDING_RESPONSE_WARN_THRESHOLD: StrictFloat = 0.8     # Log warning at 80%

# AD-32: Client-Side Queue Settings
OUTGOING_QUEUE_SIZE: StrictInt = 500                   # Per-destination queue size
OUTGOING_OVERFLOW_SIZE: StrictInt = 100                # Overflow ring buffer size
OUTGOING_MAX_DESTINATIONS: StrictInt = 1000            # Max tracked destinations
```

**Per-Node Type Recommendations**:

| Node Type | GLOBAL | HIGH | NORMAL | LOW | QUEUE_SIZE | Rationale |
|-----------|--------|------|--------|-----|------------|-----------|
| Gate | 2000 | 1000 | 600 | 400 | 1000 | Cross-DC coordination, high volume |
| Manager | 5000 | 2500 | 1500 | 1000 | 500 | Highest load from worker stats |
| Worker | 500 | 250 | 150 | 100 | 250 | Lower limit, focus on execution |

---

## Part 5: Observability

**Logging Models**:

```python
@dataclass
class PriorityLoadStats(ServerInfo):
    """Tracks priority-aware load shedding stats."""
    # Per-priority in-flight counts
    critical_in_flight: int
    high_in_flight: int
    normal_in_flight: int
    low_in_flight: int
    total_in_flight: int

    # Per-priority acquired totals
    critical_acquired: int
    high_acquired: int
    normal_acquired: int
    low_acquired: int

    # Per-priority shed totals
    critical_shed: int  # Should always be 0!
    high_shed: int
    normal_shed: int
    low_shed: int

    # Limits
    global_limit: int
    high_limit: int
    normal_limit: int
    low_limit: int


@dataclass
class DestinationQueueStats(ServerInfo):
    """Tracks per-destination queue stats."""
    destination_host: str
    destination_port: int
    primary_size: int
    overflow_size: int
    state: str  # HEALTHY, THROTTLED, BATCHING, OVERFLOW, SATURATED
    total_enqueued: int
    total_dropped: int
    backpressure_level: str
```

**Alert Conditions**:

```python
# Critical: CRITICAL priority messages being shed (should never happen)
if priority_stats.critical_shed > 0:
    log.error("CRITICAL: SWIM messages being shed - cluster stability at risk!")

# Warning: HIGH priority at limit
if priority_stats.high_in_flight >= high_limit * 0.9:
    log.warn(f"HIGH priority at {pct}% - job dispatch may be delayed")

# Info: Destination in overflow
if destination_stats.state in ("OVERFLOW", "SATURATED"):
    log.warn(f"Destination {host}:{port} in {state} - slow connection")
```

---

## Part 6: Testing Strategy

**Server-Side (InFlightTracker)**:

1. **Unit test**: CRITICAL always acquired regardless of load
2. **Unit test**: LOW shed before NORMAL before HIGH
3. **Unit test**: Per-priority limits enforced independently
4. **Unit test**: Release correctly decrements counters
5. **Integration test**: Manager under 10K updates/second sheds LOW, keeps CRITICAL
6. **Chaos test**: SWIM probes never dropped even at 100% saturation

**Client-Side (OutgoingRequestManager)**:

1. **Unit test**: Per-destination queue isolation
2. **Unit test**: LRU eviction when max destinations reached
3. **Unit test**: Backpressure signals propagate correctly
4. **Integration test**: Slow destination doesn't block fast destinations
5. **Integration test**: Overflow preserves newest messages
6. **Load test**: Memory bounded under sustained cross-DC traffic

---

## Part 7: Files Modified

| File | Change |
|------|--------|
| `hyperscale/distributed_rewrite/server/server/mercury_sync_base_server.py` | Add InFlightTracker, _spawn_tcp_response, _spawn_udp_response |
| `hyperscale/core/jobs/protocols/udp_protocol.py` | Add InFlightTracker for UDPProtocol._pending_responses |
| `hyperscale/distributed_rewrite/env/env.py` | Add priority limit and queue configuration |
| `hyperscale/distributed_rewrite/server/protocol/in_flight_tracker.py` | NEW: InFlightTracker, MessagePriority, PriorityLimits |
| `hyperscale/distributed_rewrite/server/protocol/outgoing_request_manager.py` | NEW: OutgoingRequestManager using RobustMessageQueue |
| `hyperscale/logging/hyperscale_logging_models.py` | Add PriorityLoadStats, DestinationQueueStats |

---

## Architecture

### Node Types

#### Gate Nodes (Optional)

Cross-datacenter coordinators that manage global job state and DC-level retries.

```
┌─────────────────────────────────────────────────────────────────┐
│                         GATE NODE                                │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  ┌──────────────────┐    ┌──────────────────┐                   │
│  │   SWIM UDP       │    │   TCP Protocol   │                   │
│  │   (Healthcheck)  │    │   (Job/Status)   │                   │
│  │                  │    │                  │                   │
│  │ • Probe/Ack      │    │ • Job Submission │                   │
│  │ • Suspicion      │    │ • Status Relay   │                   │
│  │ • Leadership     │    │ • State Sync     │                   │
│  │ • State Embed    │    │ • Lease Transfer │                   │
│  └──────────────────┘    └──────────────────┘                   │
│           │                      │                               │
│           ▼                      ▼                               │
│  ┌─────────────────────────────────────────────────────────┐    │
│  │                    Gate State                            │    │
│  │  • _jobs: GlobalJobStatus per job                       │    │
│  │  • _leases: DatacenterLease per job:dc                  │    │
│  │  • _datacenter_status: ManagerHeartbeat per DC          │    │
│  │  • _versioned_clock: Per-entity Lamport timestamps      │    │
│  └─────────────────────────────────────────────────────────┘    │
│                                                                  │
│  Responsibilities:                                               │
│  • Accept job submissions from clients                          │
│  • Select target datacenters for job execution                  │
│  • Create leases for at-most-once semantics                     │
│  • Aggregate status from managers across DCs                    │
│  • Handle DC-level failure and retry (lease-based)              │
│  • Leader election among gates                                   │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

#### Manager Nodes

Orchestrate workflow execution within a datacenter.

```
┌─────────────────────────────────────────────────────────────────┐
│                       MANAGER NODE                               │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  ┌──────────────────┐    ┌──────────────────┐                   │
│  │   SWIM UDP       │    │   TCP Protocol   │                   │
│  │   (Healthcheck)  │    │   (Workflows)    │                   │
│  │                  │    │                  │                   │
│  │ • Probe Workers  │    │ • Job Dispatch   │                   │
│  │ • Probe Managers │    │ • Quorum Confirm │                   │
│  │ • Worker HB Recv │    │ • State Sync     │                   │
│  │ • Manager HB Send│    │ • Progress Recv  │                   │
│  └──────────────────┘    └──────────────────┘                   │
│           │                      │                               │
│           ▼                      ▼                               │
│  ┌─────────────────────────────────────────────────────────┐    │
│  │                   Manager State                          │    │
│  │  • _workers: WorkerRegistration per node_id             │    │
│  │  • _worker_status: WorkerHeartbeat per node_id          │    │
│  │  • _worker_addr_to_id: (host,port) → node_id reverse    │    │
│  │  • _jobs: JobProgress per job_id                        │    │
│  │  • _workflow_assignments: workflow_id → worker_node_id  │    │
│  │  • _workflow_retries: Retry tracking with dispatch data │    │
│  │  • _versioned_clock: Per-entity Lamport timestamps      │    │
│  └─────────────────────────────────────────────────────────┘    │
│                                                                  │
│  Responsibilities:                                               │
│  • Register workers and track their capacity                    │
│  • Select workers for workflow dispatch (crypto-random)         │
│  • Request quorum confirmation before provisioning              │
│  • Retry failed workflows on different workers                  │
│  • Aggregate progress from workers                              │
│  • Report status to gates (via SWIM heartbeat embedding)        │
│  • State sync on leader election                                │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

#### Worker Nodes

Execute actual workflow code on CPU cores.

```
┌─────────────────────────────────────────────────────────────────┐
│                        WORKER NODE                               │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  ┌──────────────────┐    ┌──────────────────┐                   │
│  │   SWIM UDP       │    │   TCP Protocol   │                   │
│  │   (Healthcheck)  │    │   (Workflows)    │                   │
│  │                  │    │                  │                   │
│  │ • Respond Probes │    │ • Recv Dispatch  │                   │
│  │ • Worker HB Send │    │ • Send Progress  │                   │
│  │ • State Embed    │    │ • State Sync     │                   │
│  └──────────────────┘    └──────────────────┘                   │
│           │                      │                               │
│           ▼                      ▼                               │
│  ┌─────────────────────────────────────────────────────────┐    │
│  │                    Worker State                          │    │
│  │  • _total_cores / _available_cores: Core capacity       │    │
│  │  • _core_assignments: core_idx → workflow_id            │    │
│  │  • _workflow_cores: workflow_id → [core_idx, ...]       │    │
│  │  • _active_workflows: workflow_id → WorkflowProgress    │    │
│  │  • _workflow_tokens: workflow_id → TaskRunner token     │    │
│  │  • _workflow_cancel_events: workflow_id → asyncio.Event │    │
│  └─────────────────────────────────────────────────────────┘    │
│                                                                  │
│  Responsibilities:                                               │
│  • Track per-core workflow assignments                          │
│  • Execute workflows via TaskRunner                             │
│  • Send throttled progress updates to manager                   │
│  • Respond to cancellation requests                             │
│  • Report state via SWIM heartbeat embedding                    │
│  • Provide state snapshots for manager sync                     │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

### Communication Protocols

```
┌─────────────────────────────────────────────────────────────────┐
│                    PROTOCOL SEPARATION                           │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│                         UDP (SWIM)                               │
│                    ┌─────────────────┐                          │
│                    │   HEALTHCHECK   │                          │
│                    │   ONLY          │                          │
│                    └─────────────────┘                          │
│                            │                                     │
│     ┌──────────────────────┼──────────────────────┐             │
│     │                      │                      │             │
│     ▼                      ▼                      ▼             │
│  ┌──────┐              ┌──────┐              ┌──────┐           │
│  │Probe │              │ Ack  │              │Gossip│           │
│  │      │              │      │              │      │           │
│  │+ HB  │◄────────────►│+ HB  │              │      │           │
│  │embed │              │embed │              │      │           │
│  └──────┘              └──────┘              └──────┘           │
│                                                                  │
│    Serf-style: Heartbeat data embedded in probe/ack responses   │
│                                                                  │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│                         TCP (Data)                               │
│                    ┌─────────────────┐                          │
│                    │   STATE SYNC    │                          │
│                    │   JOB SUBMIT    │                          │
│                    │   PROGRESS      │                          │
│                    └─────────────────┘                          │
│                            │                                     │
│     ┌──────────────────────┼──────────────────────┐             │
│     │                      │                      │             │
│     ▼                      ▼                      ▼             │
│  ┌────────┐          ┌──────────┐          ┌──────────┐         │
│  │Workflow│          │ Quorum   │          │  State   │         │
│  │Dispatch│          │ Confirm  │          │   Sync   │         │
│  └────────┘          └──────────┘          └──────────┘         │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

### TCP Length-Prefixed Framing

TCP is a stream protocol, not a message protocol. Data can arrive fragmented across multiple `data_received` callbacks, especially for large payloads like cloudpickled workflow classes. To ensure reliable message delivery, all TCP messages use **length-prefixed framing**:

```
┌─────────────────────────────────────────────────────────────────┐
│                    TCP MESSAGE FRAMING                           │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  Wire Format:                                                    │
│  ┌──────────────┬────────────────────────────────────────────┐  │
│  │ Length (4B)  │              Payload (N bytes)              │  │
│  │  big-endian  │  [encrypted(compressed(addr<action<data))]  │  │
│  └──────────────┴────────────────────────────────────────────┘  │
│                                                                  │
│  Processing Flow:                                                │
│  ┌────────────────────────────────────────────────────────────┐ │
│  │                                                             │ │
│  │  1. Buffer incoming data in ReceiveBuffer                  │ │
│  │  2. Read 4-byte length prefix (big-endian uint32)          │ │
│  │  3. Wait for complete message (length prefix + payload)    │ │
│  │  4. Extract and process complete message                   │ │
│  │  5. Repeat for any remaining buffered data                 │ │
│  │                                                             │ │
│  └────────────────────────────────────────────────────────────┘ │
│                                                                  │
│  Design Rationale:                                               │
│  ┌────────────────────────────────────────────────────────────┐ │
│  │                                                             │ │
│  │  • 4-byte prefix supports messages up to ~4GB              │ │
│  │  • Handles arbitrary-sized cloudpickled classes            │ │
│  │  • Prevents pickle truncation on large payloads            │ │
│  │  • Applied after compression/encryption (framing is outer) │ │
│  │                                                             │ │
│  └────────────────────────────────────────────────────────────┘ │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

### Leadership Election

Hierarchical lease-based leadership with LHM (Local Health Multiplier) eligibility:

```
┌─────────────────────────────────────────────────────────────────┐
│                    LEADERSHIP ELECTION                          │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  ┌────────────────────────────────────────────────────────────┐ │
│  │                    Eligibility Check                        │ │
│  │                                                             │ │
│  │  1. LHM Score ≤ max_leader_lhm (default: 4.0)              │ │
│  │  2. Node is ALIVE in SWIM cluster                          │ │
│  │  3. Priority factor (configurable per node)                │ │
│  └────────────────────────────────────────────────────────────┘ │
│                            │                                     │
│                            ▼                                     │
│  ┌────────────────────────────────────────────────────────────┐ │
│  │                    Pre-Voting Phase                         │ │
│  │                                                             │ │
│  │  • Candidate requests pre-votes from all members           │ │
│  │  • Members compare candidate eligibility vs current leader │ │
│  │  • Prevents split-brain from network partitions            │ │
│  └────────────────────────────────────────────────────────────┘ │
│                            │                                     │
│                            ▼                                     │
│  ┌────────────────────────────────────────────────────────────┐ │
│  │                    Term-Based Resolution                    │ │
│  │                                                             │ │
│  │  • Each election increments term number                    │ │
│  │  • Higher term always wins conflicts                       │ │
│  │  • Fencing tokens derived from term for at-most-once       │ │
│  └────────────────────────────────────────────────────────────┘ │
│                            │                                     │
│                            ▼                                     │
│  ┌────────────────────────────────────────────────────────────┐ │
│  │                    Flapping Detection                       │ │
│  │                                                             │ │
│  │  • Track leadership changes in sliding window              │ │
│  │  • Cooldown period if too many changes detected            │ │
│  │  • Prevents oscillation under unstable conditions          │ │
│  └────────────────────────────────────────────────────────────┘ │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

---

## Component Diagrams

### SWIM Protocol Implementation

```
┌─────────────────────────────────────────────────────────────────┐
│                      SWIM + LIFEGUARD                            │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  ┌─────────────────────────────────────────────────────────┐    │
│  │                  Local Health Multiplier                 │    │
│  │  ┌─────────────────────────────────────────────────────┐│    │
│  │  │ score = 0 (healthy) → 8 (degraded)                  ││    │
│  │  │ timeout_multiplier = 1 + (score × factor)           ││    │
│  │  │ Incremented on: failed probes, event loop lag       ││    │
│  │  │ Decremented on: successful probes, recovery         ││    │
│  │  └─────────────────────────────────────────────────────┘│    │
│  └─────────────────────────────────────────────────────────┘    │
│                            │                                     │
│           ┌────────────────┼────────────────┐                   │
│           ▼                ▼                ▼                   │
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐              │
│  │   Direct    │  │  Indirect   │  │  Suspicion  │              │
│  │   Probe     │  │   Probe     │  │  Protocol   │              │
│  │             │  │  (Ping-Req) │  │             │              │
│  │ timeout =   │  │             │  │ timeout =   │              │
│  │ base × LHM  │  │ via random  │  │ fn(n, LHM)  │              │
│  │             │  │ proxy node  │  │             │              │
│  └─────────────┘  └─────────────┘  └─────────────┘              │
│           │                │                │                    │
│           └────────────────┼────────────────┘                   │
│                            ▼                                     │
│  ┌─────────────────────────────────────────────────────────┐    │
│  │                  Incarnation Tracker                     │    │
│  │  • Per-node incarnation numbers                         │    │
│  │  • Higher incarnation = fresher state                   │    │
│  │  • Refutation: increment own incarnation to clear       │    │
│  │    suspicion                                            │    │
│  └─────────────────────────────────────────────────────────┘    │
│                            │                                     │
│                            ▼                                     │
│  ┌─────────────────────────────────────────────────────────┐    │
│  │                    Gossip Buffer                         │    │
│  │  • Piggybacked membership updates                       │    │
│  │  • Priority: JOIN > LEAVE > ALIVE > SUSPECT > DEAD      │    │
│  │  • Bounded size with overflow callback                  │    │
│  │  • Efficient encoding within UDP MTU                    │    │
│  └─────────────────────────────────────────────────────────┘    │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

### State Embedder (Serf-Style Heartbeats)

```
┌─────────────────────────────────────────────────────────────────┐
│                    STATE EMBEDDER PATTERN                        │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  Protocol (Composition over Inheritance):                        │
│  ┌─────────────────────────────────────────────────────────┐    │
│  │  class StateEmbedder(Protocol):                          │    │
│  │      def get_state(self) -> bytes | None                 │    │
│  │      def process_state(self, data: bytes, addr) -> None  │    │
│  └─────────────────────────────────────────────────────────┘    │
│                            │                                     │
│        ┌───────────────────┼───────────────────┐                │
│        ▼                   ▼                   ▼                │
│  ┌───────────┐      ┌───────────┐      ┌───────────┐            │
│  │  Worker   │      │  Manager  │      │   Gate    │            │
│  │  Embedder │      │  Embedder │      │  Embedder │            │
│  └───────────┘      └───────────┘      └───────────┘            │
│        │                   │                   │                 │
│        ▼                   ▼                   ▼                 │
│  ┌───────────┐      ┌───────────┐      ┌───────────┐            │
│  │ Worker    │      │ Manager   │      │  (none)   │            │
│  │ Heartbeat │      │ Heartbeat │      │           │            │
│  │ • cores   │      │ • DC      │      │ Gates are │            │
│  │ • queue   │      │ • workers │      │ receivers │            │
│  │ • cpu %   │      │ • jobs    │      │ only      │            │
│  │ • mem %   │      │ • leader? │      │           │            │
│  └───────────┘      └───────────┘      └───────────┘            │
│                                                                  │
│  Flow:                                                           │
│  ┌──────────────────────────────────────────────────────────┐   │
│  │                                                           │   │
│  │   Worker ──probe─→ Manager                               │   │
│  │   Worker ←─ack+WorkerHeartbeat── Manager                 │   │
│  │                                                           │   │
│  │   Manager ──probe─→ Gate                                 │   │
│  │   Manager ←─ack+ManagerHeartbeat── Gate                  │   │
│  │                                                           │   │
│  │   (State learned passively via SWIM protocol)            │   │
│  │                                                           │   │
│  └──────────────────────────────────────────────────────────┘   │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

### Worker Core Allocation & Execution Cycle

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                      WORKER NODE - CORE ALLOCATION                           │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  Physical/Virtual Cores:                                                     │
│  ┌───┬───┬───┬───┬───┬───┬───┬───┐                                          │
│  │ 0 │ 1 │ 2 │ 3 │ 4 │ 5 │ 6 │ 7 │  (8-core worker example)                 │
│  └───┴───┴───┴───┴───┴───┴───┴───┘                                          │
│    │   │   │       │   │   │                                                 │
│    │   └───┴───────┘   └───┴──────► wf-456 (3 cores: 1,2,5,6)               │
│    │                                                                         │
│    └──────────────────────────────► wf-123 (1 core: 0)                      │
│                                                                              │
│  ┌───────────────────────────────────────────────────────────────────────┐  │
│  │                     _core_assignments                                  │  │
│  │  {0: "wf-123", 1: "wf-456", 2: "wf-456", 3: None,                    │  │
│  │   4: None, 5: "wf-456", 6: "wf-456", 7: None}                        │  │
│  └───────────────────────────────────────────────────────────────────────┘  │
│                                                                              │
│  ┌───────────────────────────────────────────────────────────────────────┐  │
│  │                     _workflow_cores                                    │  │
│  │  {"wf-123": [0], "wf-456": [1, 2, 5, 6]}                             │  │
│  └───────────────────────────────────────────────────────────────────────┘  │
│                                                                              │
│  Allocation Algorithm (_allocate_cores):                                     │
│  ┌───────────────────────────────────────────────────────────────────────┐  │
│  │  1. Scan _core_assignments for cores where value is None              │  │
│  │  2. Take first N available cores (requested vus)                      │  │
│  │  3. Mark cores as assigned to workflow_id                             │  │
│  │  4. Add to _workflow_cores mapping                                    │  │
│  │  5. Return list of allocated core indices                             │  │
│  └───────────────────────────────────────────────────────────────────────┘  │
│                                                                              │
│  Deallocation (_free_cores):                                                 │
│  ┌───────────────────────────────────────────────────────────────────────┐  │
│  │  1. Look up cores from _workflow_cores[workflow_id]                   │  │
│  │  2. Set each core to None in _core_assignments                        │  │
│  │  3. Remove workflow_id from _workflow_cores                           │  │
│  │  4. Cancel running task via TaskRunner token                          │  │
│  └───────────────────────────────────────────────────────────────────────┘  │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Worker Execution Cycle

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                      WORKER REQUEST/EXECUTION CYCLE                          │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │  INBOUND: receive_workflow_dispatch (TCP)                            │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│                              │                                               │
│                              ▼                                               │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │  1. Deserialize WorkflowDispatch                                     │    │
│  │  2. Check capacity: available_cores >= vus                           │    │
│  │  3. If insufficient → return WorkflowDispatchAck(accepted=False)     │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│                              │                                               │
│                              ▼                                               │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │  4. _allocate_cores(workflow_id, vus) → [core_indices]               │    │
│  │  5. Deserialize Workflow class from cloudpickle                      │    │
│  │  6. Create WorkflowProgress tracker                                  │    │
│  │  7. Store in _active_workflows[workflow_id]                          │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│                              │                                               │
│                              ▼                                               │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │  8. Submit to TaskRunner:                                            │    │
│  │     token = _task_runner.run(_execute_workflow, workflow, ...)       │    │
│  │  9. Store token: _workflow_tokens[workflow_id] = token               │    │
│  │ 10. Return WorkflowDispatchAck(accepted=True, cores_assigned=N)      │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│                              │                                               │
│                              ▼                                               │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │                    WORKFLOW EXECUTION LOOP                           │    │
│  │  ┌─────────────────────────────────────────────────────────────┐    │    │
│  │  │  while not cancel_event.is_set():                           │    │    │
│  │  │      execute_action()                                        │    │    │
│  │  │      update_progress()                                       │    │    │
│  │  │                                                              │    │    │
│  │  │      # Throttled TCP progress updates (every 100ms)         │    │    │
│  │  │      if int(elapsed * 10) % 10 == 0:                        │    │    │
│  │  │          send_progress_to_manager()                         │    │    │
│  │  └─────────────────────────────────────────────────────────────┘    │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│                              │                                               │
│                    ┌─────────┴─────────┐                                    │
│                    ▼                   ▼                                    │
│  ┌─────────────────────┐   ┌─────────────────────┐                          │
│  │  COMPLETION         │   │  CANCELLATION       │                          │
│  │  ───────────        │   │  ────────────       │                          │
│  │  1. Update status   │   │  1. cancel_event    │                          │
│  │  2. Send final      │   │     .set()          │                          │
│  │     progress        │   │  2. TaskRunner      │                          │
│  │  3. _free_cores()   │   │     .cancel(token)  │                          │
│  │  4. Cleanup maps    │   │  3. _free_cores()   │                          │
│  └─────────────────────┘   └─────────────────────┘                          │
│                                                                              │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │  PARALLEL: SWIM UDP Probe Response                                   │    │
│  │  • Embed WorkerHeartbeat in ack (via StateEmbedder)                 │    │
│  │  • Fields: node_id, state, available_cores, queue_depth,            │    │
│  │           cpu_percent, memory_percent, version, active_workflows    │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Manager Request Cycle

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                        MANAGER REQUEST CYCLE                                 │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  ┌────────────────────────────────────────────────────────────────────────┐ │
│  │ INBOUND: receive_job_submission (TCP from Gate or Client)              │ │
│  │          JobSubmission { job_id, workflows (pickled), vus, timeout }   │ │
│  └────────────────────────────────────────────────────────────────────────┘ │
│                                     │                                        │
│                                     ▼                                        │
│  ┌────────────────────────────────────────────────────────────────────────┐ │
│  │ 1. Leader Check: if not self.is_leader() → forward to leader          │ │
│  │ 2. Deserialize workflows list from cloudpickle                        │ │
│  │ 3. Create JobProgress tracker for job_id                              │ │
│  └────────────────────────────────────────────────────────────────────────┘ │
│                                     │                                        │
│           ┌─────────────────────────┴─────────────────────────┐             │
│           │            FOR EACH WORKFLOW IN JOB:              │             │
│           ▼                                                    │             │
│  ┌─────────────────────────────────────────────────────────┐  │             │
│  │  WORKER SELECTION (crypto-random for security)          │  │             │
│  │  ───────────────────────────────────────────────────────│  │             │
│  │  1. Get all registered workers from _workers            │  │             │
│  │  2. Filter by health: HEALTHY or DEGRADED (not DRAINING)│  │             │
│  │  3. Filter by capacity: available_cores >= vus          │  │             │
│  │  4. Apply backpressure: queue_depth < soft_limit        │  │             │
│  │  5. Use secrets.SystemRandom().choice() for selection   │  │             │
│  └─────────────────────────────────────────────────────────┘  │             │
│                         │                                      │             │
│                         ▼                                      │             │
│  ┌─────────────────────────────────────────────────────────┐  │             │
│  │  QUORUM CONFIRMATION (if manager cluster size > 1)      │  │             │
│  │  ───────────────────────────────────────────────────────│  │             │
│  │  1. Create ProvisionRequest { workflow_id, worker, ... }│  │             │
│  │  2. Send to all peer managers                           │  │             │
│  │  3. Wait for quorum: (n // 2) + 1 confirmations         │  │             │
│  │  4. Timeout → reject provisioning                       │  │             │
│  │  5. Quorum achieved → proceed to commit                 │  │             │
│  └─────────────────────────────────────────────────────────┘  │             │
│                         │                                      │             │
│                         ▼                                      │             │
│  ┌─────────────────────────────────────────────────────────┐  │             │
│  │  DISPATCH TO WORKER (TCP)                               │  │             │
│  │  ───────────────────────────────────────────────────────│  │             │
│  │  1. Create WorkflowDispatch { fence_token, ... }        │  │             │
│  │  2. Store in _workflow_assignments[workflow_id]         │  │             │
│  │  3. Store pickled bytes in _workflow_retries for retry  │  │             │
│  │  4. Send via send_tcp(worker_addr, "dispatch", data)    │  │             │
│  │  5. Wait for WorkflowDispatchAck                        │  │             │
│  └─────────────────────────────────────────────────────────┘  │             │
│                         │                                      │             │
│           └─────────────┴──────────────────────────────────────┘             │
│                         │                                                    │
│                         ▼                                                    │
│  ┌────────────────────────────────────────────────────────────────────────┐ │
│  │ OUTBOUND: JobAck { job_id, accepted, workflows_dispatched }            │ │
│  └────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
│  ═══════════════════════════════════════════════════════════════════════════│
│                                                                              │
│  ┌────────────────────────────────────────────────────────────────────────┐ │
│  │ INBOUND: receive_workflow_progress (TCP from Worker)                   │ │
│  │          WorkflowProgress { job_id, workflow_id, status, stats... }    │ │
│  └────────────────────────────────────────────────────────────────────────┘ │
│                                     │                                        │
│                                     ▼                                        │
│  ┌────────────────────────────────────────────────────────────────────────┐ │
│  │ 1. Stale Check: _versioned_clock.is_entity_stale()                    │ │
│  │ 2. Update _jobs[job_id] with workflow progress                        │ │
│  │ 3. Check status:                                                       │ │
│  │    • COMPLETED → _cleanup_workflow(), cleanup retry info              │ │
│  │    • FAILED    → _handle_workflow_failure() (retry or mark failed)    │ │
│  │ 4. Aggregate job-level stats                                          │ │
│  └────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
│  ═══════════════════════════════════════════════════════════════════════════│
│                                                                              │
│  ┌────────────────────────────────────────────────────────────────────────┐ │
│  │ PARALLEL: SWIM UDP Operations                                          │ │
│  │                                                                         │ │
│  │ 1. Receive WorkerHeartbeat (via StateEmbedder from worker probes)     │ │
│  │    → Update _worker_status[node_id]                                    │ │
│  │    → Passive capacity/health monitoring                                │ │
│  │                                                                         │ │
│  │ 2. Embed ManagerHeartbeat in probe acks (to Gates)                    │ │
│  │    → Fields: node_id, datacenter, is_leader, term, job/workflow counts│ │
│  │                                                                         │ │
│  │ 3. Node death callback → _on_node_dead(worker_addr)                   │ │
│  │    → Trigger workflow retry on different workers                       │ │
│  └────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Gate Request Cycle

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                          GATE REQUEST CYCLE                                  │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  ┌────────────────────────────────────────────────────────────────────────┐ │
│  │ INBOUND: receive_job_submission (TCP from Client)                      │ │
│  │          JobSubmission { job_id, workflows, vus, datacenter_count }    │ │
│  └────────────────────────────────────────────────────────────────────────┘ │
│                                     │                                        │
│                                     ▼                                        │
│  ┌────────────────────────────────────────────────────────────────────────┐ │
│  │ 1. Leader Check: if not self.is_leader() → forward to leader          │ │
│  │ 2. Create GlobalJobStatus tracker                                      │ │
│  │ 3. Select target datacenters:                                          │ │
│  │    • If datacenters specified → use those                              │ │
│  │    • Else → select N available DCs with healthy managers              │ │
│  └────────────────────────────────────────────────────────────────────────┘ │
│                                     │                                        │
│           ┌─────────────────────────┴─────────────────────────┐             │
│           │            FOR EACH TARGET DATACENTER:            │             │
│           ▼                                                    │             │
│  ┌─────────────────────────────────────────────────────────┐  │             │
│  │  LEASE CREATION (at-most-once semantics)                │  │             │
│  │  ───────────────────────────────────────────────────────│  │             │
│  │  1. Generate fence_token (monotonic, derived from term) │  │             │
│  │  2. Create DatacenterLease {                            │  │             │
│  │       job_id, datacenter, lease_holder: self.node_id,  │  │             │
│  │       fence_token, expires_at: now + timeout           │  │             │
│  │     }                                                   │  │             │
│  │  3. Store in _leases[(job_id, datacenter)]             │  │             │
│  └─────────────────────────────────────────────────────────┘  │             │
│                         │                                      │             │
│                         ▼                                      │             │
│  ┌─────────────────────────────────────────────────────────┐  │             │
│  │  DISPATCH TO MANAGER (TCP)                              │  │             │
│  │  ───────────────────────────────────────────────────────│  │             │
│  │  1. Find leader manager for datacenter                  │  │             │
│  │     (from _datacenter_status ManagerHeartbeats)         │  │             │
│  │  2. Send JobSubmission with fence_token                 │  │             │
│  │  3. Wait for JobAck                                     │  │             │
│  │  4. If failed → mark DC as failed, continue to others   │  │             │
│  └─────────────────────────────────────────────────────────┘  │             │
│                         │                                      │             │
│           └─────────────┴──────────────────────────────────────┘             │
│                         │                                                    │
│                         ▼                                                    │
│  ┌────────────────────────────────────────────────────────────────────────┐ │
│  │ OUTBOUND: JobAck { job_id, accepted, datacenters_dispatched }          │ │
│  └────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
│  ═══════════════════════════════════════════════════════════════════════════│
│                                                                              │
│  ┌────────────────────────────────────────────────────────────────────────┐ │
│  │ PARALLEL: Status Aggregation                                           │ │
│  │                                                                         │ │
│  │ 1. Receive ManagerHeartbeat (via StateEmbedder from SWIM probes)      │ │
│  │    → Update _datacenter_status[datacenter]                             │ │
│  │    → Passive monitoring of DC health                                   │ │
│  │                                                                         │ │
│  │ 2. Receive JobProgress (TCP from Managers)                            │ │
│  │    → Update _jobs[job_id].datacenters[dc]                              │ │
│  │    → Aggregate totals: completed, failed, rate                         │ │
│  │                                                                         │ │
│  │ 3. Lease Management (_lease_cleanup_loop via TaskRunner)              │ │
│  │    → Check expired leases every cleanup_interval                       │ │
│  │    → Expired lease → mark DC as FAILED for that job                   │ │
│  │    → No retry (explicit failure to client)                             │ │
│  └────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
│  ═══════════════════════════════════════════════════════════════════════════│
│                                                                              │
│  ┌────────────────────────────────────────────────────────────────────────┐ │
│  │ CLIENT STATUS QUERY: get_job_status(job_id) → GlobalJobStatus         │ │
│  │                                                                         │ │
│  │ GlobalJobStatus {                                                       │ │
│  │   job_id: "job-123"                                                    │ │
│  │   status: RUNNING                                                       │ │
│  │   datacenters: [                                                        │ │
│  │     JobProgress { dc: "us-east-1", completed: 10000, rate: 5000/s },  │ │
│  │     JobProgress { dc: "eu-west-1", completed: 8500, rate: 4200/s },   │ │
│  │   ]                                                                     │ │
│  │   total_completed: 18500                                                │ │
│  │   overall_rate: 9200/s                                                  │ │
│  │   elapsed_seconds: 42.5                                                 │ │
│  │   completed_datacenters: 0                                              │ │
│  │   failed_datacenters: 0                                                 │ │
│  │ }                                                                       │ │
│  └────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Complete Request Flow (End-to-End)

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                        END-TO-END JOB EXECUTION                              │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  CLIENT                                                                      │
│    │                                                                         │
│    │ ① JobSubmission (workflows, vus, dc_count)                             │
│    ▼                                                                         │
│  GATE (Leader)                                                               │
│    │                                                                         │
│    ├─► Create leases for target DCs                                          │
│    │                                                                         │
│    │ ② JobSubmission + fence_token (per DC)                                 │
│    ├──────────────────┬──────────────────┐                                  │
│    ▼                  ▼                  ▼                                  │
│  MANAGER-A          MANAGER-B          MANAGER-C     (DC leaders)           │
│    │                  │                  │                                   │
│    ├─► Quorum        ├─► Quorum        ├─► Quorum                          │
│    │   confirm       │   confirm       │   confirm                          │
│    │                  │                  │                                   │
│    │ ③ WorkflowDispatch (per workflow)                                      │
│    ├───┬───┬───┐     ├───┬───┬───┐     ├───┬───┬───┐                       │
│    ▼   ▼   ▼   ▼     ▼   ▼   ▼   ▼     ▼   ▼   ▼   ▼                       │
│   W1  W2  W3  W4    W5  W6  W7  W8    W9 W10 W11 W12  (Workers)             │
│    │   │   │   │     │   │   │   │     │   │   │   │                        │
│    │   │   │   │     │   │   │   │     │   │   │   │                        │
│    ├───┴───┴───┘     ├───┴───┴───┘     ├───┴───┴───┘                        │
│    │                  │                  │                                   │
│    │ ④ WorkflowProgress (throttled TCP, every 100ms)                        │
│    ▼                  ▼                  ▼                                  │
│  MANAGER-A          MANAGER-B          MANAGER-C                            │
│    │                  │                  │                                   │
│    │ ⑤ JobProgress (aggregated)                                             │
│    ├──────────────────┴──────────────────┘                                  │
│    ▼                                                                         │
│  GATE (Leader)                                                               │
│    │                                                                         │
│    │ ⑥ GlobalJobStatus (aggregated across DCs)                              │
│    ▼                                                                         │
│  CLIENT                                                                      │
│                                                                              │
│  ═══════════════════════════════════════════════════════════════════════════│
│                                                                              │
│  PARALLEL SWIM UDP FLOW (Healthcheck + Passive Discovery):                  │
│                                                                              │
│      Workers ◄──probe──► Managers ◄──probe──► Gates                         │
│              └─ack+HB─┘            └─ack+HB─┘                               │
│                                                                              │
│      WorkerHeartbeat               ManagerHeartbeat                          │
│      • available_cores             • datacenter                              │
│      • queue_depth                 • is_leader                               │
│      • cpu/mem percent             • job/workflow counts                     │
│      • active_workflows            • worker_count                            │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

## State Machines

### SWIM Node States

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                         SWIM NODE STATE MACHINE                              │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│                              ┌─────────┐                                     │
│                              │ UNKNOWN │                                     │
│                              └────┬────┘                                     │
│                                   │                                          │
│                          join / probe response                               │
│                                   │                                          │
│                                   ▼                                          │
│  ┌────────────────────────────────────────────────────────────────────────┐ │
│  │                                                                         │ │
│  │   ┌───────────────────────────────────────────────────────────────┐    │ │
│  │   │                         ALIVE                                  │    │ │
│  │   │                                                                │    │ │
│  │   │  • Responds to probes                                         │    │ │
│  │   │  • Participates in gossip                                     │    │ │
│  │   │  • Eligible for work dispatch                                 │    │ │
│  │   └───────────────────────────────┬───────────────────────────────┘    │ │
│  │                                   │                                     │ │
│  │                    probe timeout / suspect message                      │ │
│  │                    (incarnation ≥ current)                              │ │
│  │                                   │                                     │ │
│  │                                   ▼                                     │ │
│  │   ┌───────────────────────────────────────────────────────────────┐    │ │
│  │   │                        SUSPECT                                 │    │ │
│  │   │                                                                │    │ │
│  │   │  • Suspicion timer started: T = k × log(n) × LHM              │    │ │
│  │   │  • Can be refuted with higher incarnation                     │    │ │
│  │   │  • Confirmations accelerate timeout                           │    │ │
│  │   └──────────┬─────────────────────────────────┬──────────────────┘    │ │
│  │              │                                 │                        │ │
│  │   refutation (higher incarnation)     suspicion timeout expired         │ │
│  │   or alive message                    (no refutation received)          │ │
│  │              │                                 │                        │ │
│  │              ▼                                 ▼                        │ │
│  │   ┌─────────────────┐               ┌─────────────────┐                │ │
│  │   │     ALIVE       │               │      DEAD       │                │ │
│  │   │   (restored)    │               │                 │                │ │
│  │   └─────────────────┘               │  • Removed from │                │ │
│  │                                     │    membership   │                │ │
│  │                                     │  • Gossip DEAD  │                │ │
│  │                                     │    propagated   │                │ │
│  │                                     └────────┬────────┘                │ │
│  │                                              │                          │ │
│  └──────────────────────────────────────────────┼──────────────────────────┘ │
│                                                 │                            │
│                                      cleanup after TTL                       │
│                                                 │                            │
│                                                 ▼                            │
│                                          ┌───────────┐                       │
│                                          │  REMOVED  │                       │
│                                          │ (garbage  │                       │
│                                          │ collected)│                       │
│                                          └───────────┘                       │
│                                                                              │
│  Transitions:                                                                │
│  ┌────────────────────────────────────────────────────────────────────────┐ │
│  │  UNKNOWN → ALIVE    : First probe response or join acknowledgment      │ │
│  │  ALIVE   → SUSPECT  : Probe timeout OR suspect gossip with inc ≥ curr  │ │
│  │  SUSPECT → ALIVE    : Refutation with incarnation > current            │ │
│  │  SUSPECT → DEAD     : Suspicion timer expires without refutation       │ │
│  │  DEAD    → REMOVED  : Cleanup task removes after TTL                   │ │
│  │  DEAD    → ALIVE    : Rejoin with higher incarnation (rare)            │ │
│  └────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Worker States

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                         WORKER STATE MACHINE                                 │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│                           ┌──────────────┐                                   │
│                           │  REGISTERING │                                   │
│                           └──────┬───────┘                                   │
│                                  │                                           │
│                      manager acknowledges registration                       │
│                                  │                                           │
│                                  ▼                                           │
│  ┌────────────────────────────────────────────────────────────────────────┐ │
│  │                                                                         │ │
│  │   ┌───────────────────────────────────────────────────────────────┐    │ │
│  │   │                        HEALTHY                                 │    │ │
│  │   │                                                                │    │ │
│  │   │  Conditions:                                                   │    │ │
│  │   │  • CPU < 80%                                                  │    │ │
│  │   │  • Memory < 85%                                               │    │ │
│  │   │  • Queue depth < soft_limit                                   │    │ │
│  │   │  • LHM score < 4                                              │    │ │
│  │   │                                                                │    │ │
│  │   │  Behavior: Accepts new workflows normally                     │    │ │
│  │   └────────────────────────────┬──────────────────────────────────┘    │ │
│  │                                │                                        │ │
│  │              resource pressure increases                                │ │
│  │              (CPU ≥ 80% OR memory ≥ 85% OR queue ≥ soft_limit)         │ │
│  │                                │                                        │ │
│  │                                ▼                                        │ │
│  │   ┌───────────────────────────────────────────────────────────────┐    │ │
│  │   │                       DEGRADED                                 │    │ │
│  │   │                                                                │    │ │
│  │   │  Conditions:                                                   │    │ │
│  │   │  • CPU 80-95% OR Memory 85-95% OR Queue at soft_limit         │    │ │
│  │   │  • LHM score 4-6                                              │    │ │
│  │   │                                                                │    │ │
│  │   │  Behavior:                                                     │    │ │
│  │   │  • Accepts work with backpressure signaling                   │    │ │
│  │   │  • Manager deprioritizes in worker selection                  │    │ │
│  │   │  • Extended timeouts via LHM                                  │    │ │
│  │   └──────────┬─────────────────────────────────┬──────────────────┘    │ │
│  │              │                                 │                        │ │
│  │    pressure relieved                  pressure critical                 │ │
│  │    (metrics return to normal)         (CPU > 95% OR OOM risk)          │ │
│  │              │                                 │                        │ │
│  │              ▼                                 ▼                        │ │
│  │   ┌─────────────────┐               ┌─────────────────┐                │ │
│  │   │     HEALTHY     │               │    DRAINING     │                │ │
│  │   │   (restored)    │               │                 │                │ │
│  │   └─────────────────┘               │  • No new work  │                │ │
│  │                                     │  • Complete     │                │ │
│  │          ▲                          │    existing     │                │ │
│  │          │                          │  • Report drain │                │ │
│  │   all work completed                │    to manager   │                │ │
│  │   AND healthy metrics               └────────┬────────┘                │ │
│  │          │                                   │                          │ │
│  │          │                        shutdown requested OR                 │ │
│  │          │                        unrecoverable error                   │ │
│  │          │                                   │                          │ │
│  │          │                                   ▼                          │ │
│  │          │                          ┌─────────────────┐                │ │
│  │          └──────────────────────────│    OFFLINE      │                │ │
│  │                                     │                 │                │ │
│  │                                     │  • Not in SWIM  │                │ │
│  │                                     │  • Cleanup done │                │ │
│  │                                     └─────────────────┘                │ │
│  │                                                                         │ │
│  └─────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
│  State reported in WorkerHeartbeat.state for manager visibility             │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Job Lifecycle

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                           JOB STATE MACHINE                                  │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  Client submits JobSubmission                                                │
│            │                                                                 │
│            ▼                                                                 │
│  ┌─────────────────┐                                                        │
│  │    SUBMITTED    │  Job received by Gate/Manager                          │
│  └────────┬────────┘                                                        │
│           │                                                                  │
│           │ validate & queue                                                 │
│           ▼                                                                  │
│  ┌─────────────────┐                                                        │
│  │     QUEUED      │  Waiting for resources                                 │
│  └────────┬────────┘                                                        │
│           │                                                                  │
│           │ resources available, begin dispatch                              │
│           ▼                                                                  │
│  ┌─────────────────┐                                                        │
│  │   DISPATCHING   │  Workflows being sent to workers                       │
│  │                 │  (quorum confirmation in progress)                     │
│  └────────┬────────┘                                                        │
│           │                                                                  │
│           │ all workflows dispatched                                         │
│           ▼                                                                  │
│  ┌─────────────────┐                                                        │
│  │     RUNNING     │  Workflows executing on workers                        │
│  │                 │  Progress updates flowing                              │
│  └────────┬────────┘                                                        │
│           │                                                                  │
│           ├─────────────────────────────────────────┐                       │
│           │                                         │                       │
│           │ all workflows complete                  │ user cancellation     │
│           ▼                                         ▼                       │
│  ┌─────────────────┐                       ┌─────────────────┐              │
│  │   COMPLETING    │                       │   CANCELLING    │              │
│  │                 │                       │                 │              │
│  │ Aggregating     │                       │ Sending cancel  │              │
│  │ final results   │                       │ to all workers  │              │
│  └────────┬────────┘                       └────────┬────────┘              │
│           │                                         │                       │
│           │ results aggregated                      │ all cancelled         │
│           ▼                                         ▼                       │
│  ┌─────────────────┐                       ┌─────────────────┐              │
│  │    COMPLETED    │                       │    CANCELLED    │              │
│  │                 │                       │                 │              │
│  │ Success!        │                       │ User stopped    │              │
│  │ Results ready   │                       │                 │              │
│  └─────────────────┘                       └─────────────────┘              │
│                                                                              │
│           │ (alternate paths from RUNNING)                                  │
│           │                                                                  │
│           ├─────────────────────────────────────────┐                       │
│           │                                         │                       │
│           │ unrecoverable errors                    │ timeout exceeded      │
│           ▼                                         ▼                       │
│  ┌─────────────────┐                       ┌─────────────────┐              │
│  │     FAILED      │                       │     TIMEOUT     │              │
│  │                 │                       │                 │              │
│  │ Max retries     │                       │ Exceeded        │              │
│  │ exhausted       │                       │ timeout_seconds │              │
│  └─────────────────┘                       └─────────────────┘              │
│                                                                              │
│  Terminal states: COMPLETED, CANCELLED, FAILED, TIMEOUT                     │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Workflow Lifecycle

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                        WORKFLOW STATE MACHINE                                │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  Part of Job dispatching                                                     │
│            │                                                                 │
│            ▼                                                                 │
│  ┌─────────────────┐                                                        │
│  │     PENDING     │  Workflow created, not yet dispatched                  │
│  └────────┬────────┘                                                        │
│           │                                                                  │
│           │ worker selected, dispatch sent                                   │
│           ▼                                                                  │
│  ┌─────────────────┐                                                        │
│  │    ASSIGNED     │  Sent to worker, awaiting ack                          │
│  └────────┬────────┘                                                        │
│           │                                                                  │
│           ├─────────────────────────────────────────┐                       │
│           │                                         │                       │
│           │ worker accepts (cores allocated)        │ worker rejects        │
│           ▼                                         ▼                       │
│  ┌─────────────────┐                       ┌─────────────────┐              │
│  │     RUNNING     │                       │   RE-DISPATCH   │              │
│  │                 │                       │                 │              │
│  │ Executing on    │                       │ Select another  │──┐           │
│  │ allocated cores │                       │ worker          │  │           │
│  │                 │                       └─────────────────┘  │           │
│  │ Progress:       │                              ▲              │           │
│  │ • completed_cnt │                              │              │           │
│  │ • failed_cnt    │                              │              │           │
│  │ • rate/second   │                              │              │           │
│  │ • step_stats[]  │                              │ retry < max  │           │
│  └────────┬────────┘                              │              │           │
│           │                                       │              │           │
│           ├─────────────────────────────────┬─────┘              │           │
│           │                                 │                    │           │
│           │ all actions complete            │ worker fails       │           │
│           │ successfully                    │ (SWIM DEAD)        │           │
│           ▼                                 ▼                    │           │
│  ┌─────────────────┐               ┌─────────────────┐          │           │
│  │    COMPLETED    │               │  WORKER_FAILED  │──────────┘           │
│  │                 │               │                 │                      │
│  │ Success!        │               │ Retry on        │                      │
│  │ Results in      │               │ different       │                      │
│  │ WorkflowProgress│               │ worker          │                      │
│  └─────────────────┘               └────────┬────────┘                      │
│                                             │                                │
│                                             │ retry >= max                   │
│                                             ▼                                │
│                                    ┌─────────────────┐                      │
│                                    │     FAILED      │                      │
│                                    │                 │                      │
│                                    │ Max retries     │                      │
│                                    │ exhausted       │                      │
│                                    └─────────────────┘                      │
│                                                                              │
│  Also from RUNNING:                                                          │
│  ┌─────────────────┐                                                        │
│  │    CANCELLED    │  ← Cancel request received                             │
│  └─────────────────┘                                                        │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Leadership States

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                      LEADERSHIP STATE MACHINE                                │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│                              ┌──────────────┐                                │
│                              │   INITIAL    │                                │
│                              └──────┬───────┘                                │
│                                     │                                        │
│                          join cluster / startup                              │
│                                     │                                        │
│                                     ▼                                        │
│  ┌────────────────────────────────────────────────────────────────────────┐ │
│  │                                                                         │ │
│  │   ┌───────────────────────────────────────────────────────────────┐    │ │
│  │   │                        FOLLOWER                                │    │ │
│  │   │                                                                │    │ │
│  │   │  • Accepts leader heartbeats                                  │    │ │
│  │   │  • Forwards requests to leader                                │    │ │
│  │   │  • Responds to pre-vote requests                              │    │ │
│  │   │  • Monitors leader liveness                                   │    │ │
│  │   └────────────────────────────┬──────────────────────────────────┘    │ │
│  │                                │                                        │ │
│  │              leader timeout expired AND                                 │ │
│  │              self is eligible (LHM ≤ max_leader_lhm)                   │ │
│  │                                │                                        │ │
│  │                                ▼                                        │ │
│  │   ┌───────────────────────────────────────────────────────────────┐    │ │
│  │   │                      PRE_CANDIDATE                             │    │ │
│  │   │                                                                │    │ │
│  │   │  • Sends pre-vote requests to all members                     │    │ │
│  │   │  • Collects pre-vote responses                                │    │ │
│  │   │  • Does NOT increment term yet (prevents disruption)          │    │ │
│  │   │  • Timeout: pre_vote_timeout                                  │    │ │
│  │   └──────────┬─────────────────────────────────┬──────────────────┘    │ │
│  │              │                                 │                        │ │
│  │   pre-vote majority granted              pre-vote denied OR             │ │
│  │   (> n/2 nodes agree)                    timeout OR higher term         │ │
│  │              │                                 │                        │ │
│  │              ▼                                 ▼                        │ │
│  │   ┌─────────────────┐               ┌─────────────────┐                │ │
│  │   │    CANDIDATE    │               │    FOLLOWER     │                │ │
│  │   │                 │               │   (step down)   │                │ │
│  │   │ • Increment term│               └─────────────────┘                │ │
│  │   │ • Vote for self │                                                   │ │
│  │   │ • Request votes │                                                   │ │
│  │   │   from peers    │                                                   │ │
│  │   └────────┬────────┘                                                   │ │
│  │            │                                                            │ │
│  │            ├─────────────────────────────────────────┐                 │ │
│  │            │                                         │                 │ │
│  │   vote majority granted                     vote denied OR             │ │
│  │   (> n/2 votes for self)                    higher term seen           │ │
│  │            │                                         │                 │ │
│  │            ▼                                         ▼                 │ │
│  │   ┌─────────────────┐                       ┌─────────────────┐        │ │
│  │   │     LEADER      │                       │    FOLLOWER     │        │ │
│  │   │                 │                       │   (step down)   │        │ │
│  │   │ • Broadcast win │                       └─────────────────┘        │ │
│  │   │ • Send heartbeat│                                                   │ │
│  │   │ • Handle requests                                                   │ │
│  │   │ • State sync    │                                                   │ │
│  │   └────────┬────────┘                                                   │ │
│  │            │                                                            │ │
│  │   ┌────────┴────────────────────────────────────────────┐              │ │
│  │   │                                                      │              │ │
│  │   │ LHM exceeds threshold     higher term         network partition    │ │
│  │   │ (unhealthy leader)        discovered          (loses majority)     │ │
│  │   │                                                      │              │ │
│  │   ▼                           ▼                          ▼              │ │
│  │   ┌──────────────────────────────────────────────────────────────┐     │ │
│  │   │                        FOLLOWER                               │     │ │
│  │   │                       (step down)                             │     │ │
│  │   └──────────────────────────────────────────────────────────────┘     │ │
│  │                                                                         │ │
│  └─────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
│  Flapping Protection:                                                        │
│  ┌────────────────────────────────────────────────────────────────────────┐ │
│  │  If leadership changes > threshold in window → cooldown period         │ │
│  │  During cooldown: no new elections initiated                           │ │
│  └────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

## Data Flow

### Job Submission Flow

```
┌─────────────────────────────────────────────────────────────────┐
│                    JOB SUBMISSION FLOW                           │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  Client                                                          │
│    │                                                             │
│    │ TCP: JobSubmission                                          │
│    ▼                                                             │
│  Gate (Leader)                                                   │
│    │                                                             │
│    ├──► Create DatacenterLease (fence_token)                    │
│    │                                                             │
│    │ TCP: JobSubmission (with lease)                            │
│    ▼                                                             │
│  Manager (Leader)                                                │
│    │                                                             │
│    ├──► Deserialize workflows                                   │
│    │                                                             │
│    │    For each workflow:                                      │
│    │    ┌────────────────────────────────────────────────┐      │
│    │    │ 1. Select eligible worker (crypto-random)      │      │
│    │    │ 2. Create ProvisionRequest (fence_token)       │      │
│    │    │ 3. Request quorum confirmation from peers      │      │
│    │    │ 4. On quorum: commit and dispatch              │      │
│    │    └────────────────────────────────────────────────┘      │
│    │                                                             │
│    │ TCP: WorkflowDispatch                                      │
│    ▼                                                             │
│  Worker                                                          │
│    │                                                             │
│    ├──► Allocate cores via _allocate_cores()                    │
│    ├──► Create WorkflowProgress tracker                         │
│    ├──► Execute via TaskRunner                                  │
│    │                                                             │
│    │ TCP: WorkflowDispatchAck                                   │
│    ▼                                                             │
│  Manager                                                         │
│    │                                                             │
│    │ TCP: JobAck                                                │
│    ▼                                                             │
│  Gate → Client                                                   │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

### Progress Update Flow

```
┌─────────────────────────────────────────────────────────────────┐
│                   PROGRESS UPDATE FLOW                           │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  Two parallel flows:                                             │
│                                                                  │
│  ┌───────────────────────────────────────────────────────────┐  │
│  │ 1. ACTIVE UPDATES (TCP, throttled to 1/sec)               │  │
│  │                                                            │  │
│  │    Worker ──WorkflowProgress──► Manager                   │  │
│  │             (TCP, explicit)                                │  │
│  │                                                            │  │
│  │    • completed_count, failed_count                        │  │
│  │    • rate_per_second, elapsed_seconds                     │  │
│  │    • per-step stats                                       │  │
│  │    • assigned_cores list                                  │  │
│  └───────────────────────────────────────────────────────────┘  │
│                                                                  │
│  ┌───────────────────────────────────────────────────────────┐  │
│  │ 2. PASSIVE DISCOVERY (UDP, via SWIM heartbeats)           │  │
│  │                                                            │  │
│  │    Worker ←─probe/ack─► Manager                           │  │
│  │    (WorkerHeartbeat embedded)                             │  │
│  │                                                            │  │
│  │    Manager ←─probe/ack─► Gate                             │  │
│  │    (ManagerHeartbeat embedded)                            │  │
│  │                                                            │  │
│  │    • Capacity, queue depth, resource utilization          │  │
│  │    • Active job/workflow counts                           │  │
│  │    • Leadership status                                    │  │
│  └───────────────────────────────────────────────────────────┘  │
│                                                                  │
│  Aggregation:                                                    │
│  ┌───────────────────────────────────────────────────────────┐  │
│  │                                                            │  │
│  │   Worker Progress → Manager JobProgress → Gate GlobalJob  │  │
│  │                                                            │  │
│  │   GlobalJobStatus {                                       │  │
│  │     job_id, status                                        │  │
│  │     datacenters: [JobProgress, ...]                       │  │
│  │     total_completed, total_failed                         │  │
│  │     overall_rate, elapsed_seconds                         │  │
│  │     completed_datacenters, failed_datacenters             │  │
│  │   }                                                       │  │
│  │                                                            │  │
│  └───────────────────────────────────────────────────────────┘  │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

---

## Timing Diagrams

### SWIM Probe Cycle

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                         SWIM PROBE CYCLE TIMING                              │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  Time ─────────────────────────────────────────────────────────────────────► │
│                                                                              │
│  Node A          Node B          Node C (proxy)         Node D               │
│    │                │                  │                   │                 │
│    │ ① probe        │                  │                   │                 │
│    │───────────────►│                  │                   │                 │
│    │                │                  │                   │                 │
│    │                │                  │                   │                 │
│    │ ② ack + HB     │                  │                   │                 │
│    │◄───────────────│                  │                   │                 │
│    │                │                  │                   │                 │
│  ──┴────────────────┴──────────────────┴───────────────────┴──────────────── │
│                                                                              │
│    SUCCESSFUL PROBE: base_timeout × LHM_multiplier                          │
│                                                                              │
│  ═══════════════════════════════════════════════════════════════════════════ │
│                                                                              │
│  Time ─────────────────────────────────────────────────────────────────────► │
│                                                                              │
│  Node A          Node B (slow)       Node C (proxy)         Node D           │
│    │                │                     │                   │              │
│    │ ① probe        │                     │                   │              │
│    │───────────────►│                     │                   │              │
│    │                │                     │                   │              │
│    │      ┌─────────┼─────────────────────┼───────────────────┼────┐         │
│    │      │ TIMEOUT │ (no response)       │                   │    │         │
│    │      └─────────┼─────────────────────┼───────────────────┼────┘         │
│    │                │                     │                   │              │
│    │ ② ping-req (indirect probe)         │                   │              │
│    │─────────────────────────────────────►│                   │              │
│    │                │                     │                   │              │
│    │                │    ③ probe          │                   │              │
│    │                │◄────────────────────│                   │              │
│    │                │                     │                   │              │
│    │                │    ④ ack            │                   │              │
│    │                │────────────────────►│                   │              │
│    │                │                     │                   │              │
│    │ ⑤ ack (indirect)                     │                   │              │
│    │◄─────────────────────────────────────│                   │              │
│    │                │                     │                   │              │
│  ──┴────────────────┴─────────────────────┴───────────────────┴───────────── │
│                                                                              │
│    INDIRECT PROBE SUCCESS: Node B is alive but slow                          │
│                                                                              │
│  ═══════════════════════════════════════════════════════════════════════════ │
│                                                                              │
│  Time ─────────────────────────────────────────────────────────────────────► │
│                                                                              │
│  Node A          Node B (dead)       Node C (proxy)         Node D           │
│    │                ╳                     │                   │              │
│    │ ① probe        ╳                     │                   │              │
│    │───────────────►╳                     │                   │              │
│    │                ╳                     │                   │              │
│    │      ┌─────────┼─────────────────────┼────┐              │              │
│    │      │ TIMEOUT │                     │    │              │              │
│    │      └─────────┼─────────────────────┼────┘              │              │
│    │                ╳                     │                   │              │
│    │ ② ping-req     ╳                     │                   │              │
│    │─────────────────────────────────────►│                   │              │
│    │                ╳                     │                   │              │
│    │                ╳    ③ probe          │                   │              │
│    │                ╳◄────────────────────│                   │              │
│    │                ╳                     │                   │              │
│    │                ╳     ┌───────────────┼────┐              │              │
│    │                ╳     │ TIMEOUT       │    │              │              │
│    │                ╳     └───────────────┼────┘              │              │
│    │                ╳                     │                   │              │
│    │ ④ nack (indirect failed)             │                   │              │
│    │◄─────────────────────────────────────│                   │              │
│    │                ╳                     │                   │              │
│    │ ⑤ START SUSPICION                    │                   │              │
│    │ broadcast suspect msg                │                   │              │
│    │─────────────────────────────────────►│──────────────────►│              │
│    │                ╳                     │                   │              │
│    │      ┌─────────┼─────────────────────┼───────────────────┼────┐         │
│    │      │ SUSPICION TIMEOUT             │                   │    │         │
│    │      │ T = k × log(n) × LHM          │                   │    │         │
│    │      └─────────┼─────────────────────┼───────────────────┼────┘         │
│    │                ╳                     │                   │              │
│    │ ⑥ MARK DEAD    ╳                     │                   │              │
│    │ broadcast dead msg                   │                   │              │
│    │─────────────────────────────────────►│──────────────────►│              │
│    │                ╳                     │                   │              │
│  ──┴────────────────╳─────────────────────┴───────────────────┴───────────── │
│                                                                              │
│    FAILURE DETECTION: Direct → Indirect → Suspicion → Dead                   │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Quorum Confirmation

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                      QUORUM CONFIRMATION TIMING                              │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  Time ─────────────────────────────────────────────────────────────────────► │
│                                                                              │
│  Manager 1         Manager 2 (★)        Manager 3           Worker           │
│  (follower)        (leader)             (follower)                           │
│     │                  │                    │                   │            │
│     │                  │ ① Job received     │                   │            │
│     │                  │◄═══════════════════│                   │            │
│     │                  │                    │                   │            │
│     │                  │ Select worker      │                   │            │
│     │                  │ Create provision   │                   │            │
│     │                  │                    │                   │            │
│     │ ② ProvisionReq   │                    │                   │            │
│     │◄─────────────────│                    │                   │            │
│     │                  │ ② ProvisionReq     │                   │            │
│     │                  │───────────────────►│                   │            │
│     │                  │                    │                   │            │
│     │ Validate:        │                    │ Validate:         │            │
│     │ • Worker alive?  │                    │ • Worker alive?   │            │
│     │ • Version fresh? │                    │ • Version fresh?  │            │
│     │ • Capacity ok?   │                    │ • Capacity ok?    │            │
│     │                  │                    │                   │            │
│     │ ③ ProvisionConf  │                    │                   │            │
│     │─────────────────►│                    │                   │            │
│     │                  │ ③ ProvisionConf    │                   │            │
│     │                  │◄───────────────────│                   │            │
│     │                  │                    │                   │            │
│     │                  │ QUORUM ACHIEVED    │                   │            │
│     │                  │ (2/3 = majority)   │                   │            │
│     │                  │                    │                   │            │
│     │ ④ ProvisionCommit│                    │                   │            │
│     │◄─────────────────│                    │                   │            │
│     │                  │ ④ ProvisionCommit  │                   │            │
│     │                  │───────────────────►│                   │            │
│     │                  │                    │                   │            │
│     │                  │ ⑤ WorkflowDispatch │                   │            │
│     │                  │────────────────────────────────────────►            │
│     │                  │                    │                   │            │
│     │                  │ ⑥ DispatchAck      │                   │            │
│     │                  │◄────────────────────────────────────────            │
│     │                  │                    │                   │            │
│  ───┴──────────────────┴────────────────────┴───────────────────┴─────────── │
│                                                                              │
│    SUCCESS: Quorum (n/2 + 1) confirmations → commit → dispatch               │
│                                                                              │
│  ═══════════════════════════════════════════════════════════════════════════ │
│                                                                              │
│  TIMEOUT SCENARIO:                                                           │
│                                                                              │
│  Manager 1         Manager 2 (★)        Manager 3 (slow)      Worker         │
│     │                  │                    │                   │            │
│     │ ② ProvisionReq   │                    │                   │            │
│     │◄─────────────────│                    │                   │            │
│     │                  │ ② ProvisionReq     │                   │            │
│     │                  │───────────────────►│                   │            │
│     │                  │                    │                   │            │
│     │ ③ ProvisionConf  │                    │                   │            │
│     │─────────────────►│                    │ (processing...)   │            │
│     │                  │                    │                   │            │
│     │                  │      ┌─────────────┼────┐              │            │
│     │                  │      │ TIMEOUT     │    │              │            │
│     │                  │      └─────────────┼────┘              │            │
│     │                  │                    │                   │            │
│     │                  │ Only 1/3 confirm   │                   │            │
│     │                  │ (no quorum)        │                   │            │
│     │                  │                    │                   │            │
│     │ ④ ProvisionAbort │                    │                   │            │
│     │◄─────────────────│                    │                   │            │
│     │                  │                    │                   │            │
│     │                  │ Retry with         │                   │            │
│     │                  │ different worker   │                   │            │
│     │                  │                    │                   │            │
│  ───┴──────────────────┴────────────────────┴───────────────────┴─────────── │
│                                                                              │
│    FAILURE: Quorum timeout → abort → retry (different worker if available)  │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Leader Election Sequence

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                      LEADER ELECTION SEQUENCE                                │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  Time ─────────────────────────────────────────────────────────────────────► │
│                                                                              │
│  TERM: 5           Node A (★ old)      Node B           Node C               │
│                       │                   │                │                 │
│                       ╳ CRASH             │                │                 │
│                       ╳                   │                │                 │
│                       ╳                   │                │                 │
│                       ╳     ┌─────────────┼────────────────┼────┐            │
│                       ╳     │ LEADER      │                │    │            │
│                       ╳     │ TIMEOUT     │                │    │            │
│                       ╳     └─────────────┼────────────────┼────┘            │
│                       ╳                   │                │                 │
│  ─────────────────────╳───────────────────┴────────────────┴──────────────── │
│                       ╳                                                      │
│  PRE-VOTE PHASE       ╳                                                      │
│                       ╳                                                      │
│  TERM: 5 (unchanged)  ╳    Node B             Node C                         │
│                       ╳       │                  │                           │
│                       ╳       │ Check eligibility│                           │
│                       ╳       │ (LHM ≤ 4.0 ✓)    │                           │
│                       ╳       │                  │                           │
│                       ╳       │ ① pre-vote-req (term=5)                      │
│                       ╳       │─────────────────►│                           │
│                       ╳       │                  │                           │
│                       ╳       │                  │ Compare:                  │
│                       ╳       │                  │ • No current leader       │
│                       ╳       │                  │ • B is eligible           │
│                       ╳       │                  │                           │
│                       ╳       │ ② pre-vote-grant │                           │
│                       ╳       │◄─────────────────│                           │
│                       ╳       │                  │                           │
│                       ╳       │ Pre-vote majority│                           │
│                       ╳       │ (2/2 = 100%)     │                           │
│                       ╳       │                  │                           │
│  ─────────────────────╳───────┴──────────────────┴────────────────────────── │
│                       ╳                                                      │
│  VOTE PHASE           ╳                                                      │
│                       ╳                                                      │
│  TERM: 6 (incremented)╳    Node B             Node C                         │
│                       ╳       │                  │                           │
│                       ╳       │ Increment term   │                           │
│                       ╳       │ Vote for self    │                           │
│                       ╳       │                  │                           │
│                       ╳       │ ③ vote-req (term=6)                          │
│                       ╳       │─────────────────►│                           │
│                       ╳       │                  │                           │
│                       ╳       │                  │ Term 6 > my term 5        │
│                       ╳       │                  │ Grant vote                │
│                       ╳       │                  │                           │
│                       ╳       │ ④ vote-grant     │                           │
│                       ╳       │◄─────────────────│                           │
│                       ╳       │                  │                           │
│                       ╳       │ Vote majority    │                           │
│                       ╳       │ (2/2 = 100%)     │                           │
│                       ╳       │                  │                           │
│  ─────────────────────╳───────┴──────────────────┴────────────────────────── │
│                       ╳                                                      │
│  LEADER ANNOUNCEMENT  ╳                                                      │
│                       ╳                                                      │
│  TERM: 6              ╳    Node B (★ new)     Node C                         │
│                       ╳       │                  │                           │
│                       ╳       │ ⑤ leader-announce│                           │
│                       ╳       │─────────────────►│                           │
│                       ╳       │                  │                           │
│                       ╳       │ Trigger:         │                           │
│                       ╳       │ _on_become_leader│                           │
│                       ╳       │                  │ Trigger:                  │
│                       ╳       │                  │ _on_leader_change         │
│                       ╳       │                  │                           │
│                       ╳       │ Begin state sync │                           │
│                       ╳       │ from workers     │                           │
│                       ╳       │                  │                           │
│  ─────────────────────╳───────┴──────────────────┴────────────────────────── │
│                                                                              │
│  SPLIT-BRAIN PREVENTION:                                                     │
│  • Pre-vote phase doesn't increment term (prevents term explosion)           │
│  • Candidate must get pre-vote majority before real election                │
│  • Nodes only grant pre-vote if no current leader OR candidate is better    │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

## Failure Handling

### Worker Failure

```
┌─────────────────────────────────────────────────────────────────┐
│                    WORKER FAILURE HANDLING                       │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  Detection (SWIM UDP):                                           │
│  ┌───────────────────────────────────────────────────────────┐  │
│  │ 1. Direct probe times out (LHM-adjusted timeout)          │  │
│  │ 2. Indirect probe via random proxy                        │  │
│  │ 3. Suspicion timer starts (confirmation-based)            │  │
│  │ 4. No refutation → Node marked DEAD                       │  │
│  └───────────────────────────────────────────────────────────┘  │
│                            │                                     │
│                            ▼                                     │
│  Manager._on_node_dead() callback:                               │
│  ┌───────────────────────────────────────────────────────────┐  │
│  │ 1. O(1) lookup via _worker_addr_to_id                     │  │
│  │ 2. Clean up: _workers, _worker_status, _worker_last_status│  │
│  │ 3. Find workflows assigned to failed worker               │  │
│  │ 4. For each workflow:                                     │  │
│  │    • Get/create retry info (_workflow_retries)            │  │
│  │    • Add failed worker to exclusion set                   │  │
│  │    • If retries < max: select new worker, re-dispatch     │  │
│  │    • If retries >= max: mark workflow FAILED              │  │
│  └───────────────────────────────────────────────────────────┘  │
│                                                                  │
│  Retry Logic:                                                    │
│  ┌───────────────────────────────────────────────────────────┐  │
│  │ _workflow_retries: {                                      │  │
│  │   workflow_id: (                                          │  │
│  │     retry_count: int,                                     │  │
│  │     original_dispatch_bytes: bytes,  # preserved          │  │
│  │     failed_workers: set[str],        # exclusion list     │  │
│  │   )                                                       │  │
│  │ }                                                         │  │
│  │                                                            │  │
│  │ New dispatch:                                             │  │
│  │ • Deserialize original WorkflowDispatch                   │  │
│  │ • Create new dispatch with new fence_token                │  │
│  │ • Select worker excluding failed_workers set              │  │
│  │ • Increment retry_count                                   │  │
│  └───────────────────────────────────────────────────────────┘  │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

### Manager Failure

```
┌─────────────────────────────────────────────────────────────────┐
│                   MANAGER FAILURE HANDLING                       │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  Detection: SWIM cluster among managers                          │
│                                                                  │
│  ┌───────────────────────────────────────────────────────────┐  │
│  │ PEER TRACKING (each manager maintains):                   │  │
│  │                                                            │  │
│  │ _manager_udp_to_tcp: dict[(host,port) → (host,port)]     │  │
│  │   Maps SWIM UDP addresses to TCP addresses                │  │
│  │                                                            │  │
│  │ _active_manager_peers: set[(host,port)]                   │  │
│  │   Currently live peer managers (updated via callbacks)    │  │
│  │                                                            │  │
│  │ _on_node_dead() checks BOTH:                              │  │
│  │   • _worker_addr_to_id (for worker failure)               │  │
│  │   • _manager_udp_to_tcp (for peer manager failure)        │  │
│  └───────────────────────────────────────────────────────────┘  │
│                                                                  │
│  New Leader Election:                                            │
│  ┌───────────────────────────────────────────────────────────┐  │
│  │ 1. Leader failure detected via SWIM                       │  │
│  │ 2. Leader's heartbeats stop → lease expires on followers  │  │
│  │ 3. Pre-voting phase among eligible managers               │  │
│  │ 4. Candidate with lowest LHM + highest priority wins      │  │
│  │ 5. New leader announces with new term number              │  │
│  │                                                            │  │
│  │ Note: Leadership re-election is AUTOMATIC via lease       │  │
│  │ expiry in LocalLeaderElection - no manual intervention    │  │
│  └───────────────────────────────────────────────────────────┘  │
│                            │                                     │
│                            ▼                                     │
│  Peer Manager Failure:                                           │
│  ┌───────────────────────────────────────────────────────────┐  │
│  │ _handle_manager_peer_failure():                           │  │
│  │                                                            │  │
│  │ 1. Remove from _active_manager_peers                      │  │
│  │ 2. Check if dead peer was the leader                      │  │
│  │ 3. Log quorum status for monitoring                       │  │
│  │                                                            │  │
│  │ Quorum calculation:                                        │  │
│  │ • Uses CONFIGURED peer count (prevents split-brain)       │  │
│  │ • _has_quorum_available() checks ACTIVE vs required       │  │
│  └───────────────────────────────────────────────────────────┘  │
│                            │                                     │
│                            ▼                                     │
│  Peer Manager Recovery:                                          │
│  ┌───────────────────────────────────────────────────────────┐  │
│  │ _handle_manager_peer_recovery() (via on_node_join):       │  │
│  │                                                            │  │
│  │ 1. Add back to _active_manager_peers                      │  │
│  │ 2. Log recovery and quorum status                         │  │
│  │ 3. Quorum capacity restored                               │  │
│  └───────────────────────────────────────────────────────────┘  │
│                            │                                     │
│                            ▼                                     │
│  State Synchronization (new leader only):                        │
│  ┌───────────────────────────────────────────────────────────┐  │
│  │ _on_manager_become_leader() callback:                     │  │
│  │                                                            │  │
│  │ 1. Request StateSyncRequest from all registered workers   │  │
│  │ 2. Workers respond with WorkerStateSnapshot               │  │
│  │    • active_workflows: dict[workflow_id → progress]       │  │
│  │    • Core allocations, version                            │  │
│  │ 3. New leader rebuilds authoritative state from workers   │  │
│  │    (Workers are source of truth)                          │  │
│  └───────────────────────────────────────────────────────────┘  │
│                                                                  │
│  In-Flight Work:                                                 │
│  ┌───────────────────────────────────────────────────────────┐  │
│  │ • Pending provisions: timeout and client retries          │  │
│  │ • Running workflows: continue on workers (unaffected)     │  │
│  │ • Progress updates: resume after new leader sync          │  │
│  └───────────────────────────────────────────────────────────┘  │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

### Worker Manager Failover

```
┌─────────────────────────────────────────────────────────────────┐
│                 WORKER MANAGER FAILOVER                          │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  When a worker detects its assigned manager has failed:          │
│                                                                  │
│  ┌───────────────────────────────────────────────────────────┐  │
│  │ _handle_manager_failure() (via on_node_dead callback):    │  │
│  │                                                            │  │
│  │ 1. Check if dead node is current manager                  │  │
│  │ 2. Clear _current_manager reference                       │  │
│  │ 3. Iterate through _manager_addrs backup list             │  │
│  │ 4. Skip the failed manager                                │  │
│  │ 5. Attempt registration with each alternative             │  │
│  │ 6. On success: set _current_manager, report workflows     │  │
│  └───────────────────────────────────────────────────────────┘  │
│                            │                                     │
│                            ▼                                     │
│  Report Active Workflows:                                        │
│  ┌───────────────────────────────────────────────────────────┐  │
│  │ _report_active_workflows_to_manager():                    │  │
│  │                                                            │  │
│  │ For each workflow in _active_workflows:                   │  │
│  │   • Send WorkflowProgress to new manager                  │  │
│  │   • Ensures new manager is aware of in-flight work        │  │
│  │   • No workflow interruption during failover              │  │
│  └───────────────────────────────────────────────────────────┘  │
│                                                                  │
│  Timeline:                                                       │
│  ┌───────────────────────────────────────────────────────────┐  │
│  │                                                            │  │
│  │  Manager A dies                                            │  │
│  │       │                                                    │  │
│  │       ▼                                                    │  │
│  │  SWIM detects (probe → indirect → suspicion → DEAD)       │  │
│  │       │                                                    │  │
│  │       ▼                                                    │  │
│  │  Worker._on_node_dead(Manager A addr)                     │  │
│  │       │                                                    │  │
│  │       ▼                                                    │  │
│  │  _handle_manager_failure() runs                           │  │
│  │       │                                                    │  │
│  │       ▼                                                    │  │
│  │  Try Manager B from _manager_addrs                        │  │
│  │       │                                                    │  │
│  │       ▼                                                    │  │
│  │  Registration succeeds → _current_manager = B             │  │
│  │       │                                                    │  │
│  │       ▼                                                    │  │
│  │  _report_active_workflows_to_manager()                    │  │
│  │       │                                                    │  │
│  │       ▼                                                    │  │
│  │  Normal operation resumes with Manager B                  │  │
│  │                                                            │  │
│  └───────────────────────────────────────────────────────────┘  │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

### Datacenter Failure

```
┌─────────────────────────────────────────────────────────────────┐
│                  DATACENTER FAILURE HANDLING                     │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  Detection (at Gate):                                            │
│  ┌───────────────────────────────────────────────────────────┐  │
│  │ • No ManagerHeartbeat received (SWIM timeout)             │  │
│  │ • All managers in DC marked DEAD                          │  │
│  │ • DC marked unavailable                                   │  │
│  └───────────────────────────────────────────────────────────┘  │
│                            │                                     │
│                            ▼                                     │
│  Gate Handling:                                                  │
│  ┌───────────────────────────────────────────────────────────┐  │
│  │ Lease-based at-most-once:                                 │  │
│  │                                                            │  │
│  │ • If lease expired → Job marked FAILED for that DC        │  │
│  │ • If lease valid → Wait for recovery or timeout           │  │
│  │                                                            │  │
│  │ User-facing: Gate returns job failure to client           │  │
│  │ (No automatic cross-DC retry - explicit decision)         │  │
│  └───────────────────────────────────────────────────────────┘  │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

### Failure Recovery Flows

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                       FAILURE RECOVERY MATRIX                                │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  ┌────────────────┬─────────────────────┬──────────────────────────────────┐│
│  │ FAILURE TYPE   │ DETECTION           │ RECOVERY ACTION                  ││
│  ├────────────────┼─────────────────────┼──────────────────────────────────┤│
│  │                │                     │                                  ││
│  │ Worker crash   │ SWIM probe timeout  │ Retry workflow on another worker ││
│  │                │ + indirect probe    │ Exclude failed worker from retry ││
│  │                │ + suspicion expiry  │ Mark workflow FAILED if max retry││
│  │                │                     │                                  ││
│  ├────────────────┼─────────────────────┼──────────────────────────────────┤│
│  │                │                     │                                  ││
│  │ Worker         │ WorkerHeartbeat     │ Deprioritize in worker selection ││
│  │ overloaded     │ state = DEGRADED    │ Apply backpressure signaling     ││
│  │                │ OR queue_depth high │ Extend timeouts via LHM          ││
│  │                │                     │                                  ││
│  ├────────────────┼─────────────────────┼──────────────────────────────────┤│
│  │                │                     │                                  ││
│  │ Manager        │ SWIM detects DEAD   │ Pre-vote → elect new leader      ││
│  │ leader crash   │ among manager peers │ New leader syncs state from      ││
│  │                │                     │ all workers (source of truth)    ││
│  │                │                     │                                  ││
│  ├────────────────┼─────────────────────┼──────────────────────────────────┤│
│  │                │                     │                                  ││
│  │ Manager        │ Quorum timeout      │ Retry with original quorum       ││
│  │ follower crash │ for confirmation    │ If quorum impossible → abort job ││
│  │                │                     │ New manager syncs when joins     ││
│  │                │                     │                                  ││
│  ├────────────────┼─────────────────────┼──────────────────────────────────┤│
│  │                │                     │                                  ││
│  │ Gate leader    │ SWIM among gates    │ New gate leader elected          ││
│  │ crash          │                     │ Lease transfer to new leader     ││
│  │                │                     │ Jobs continue with new gate      ││
│  │                │                     │                                  ││
│  ├────────────────┼─────────────────────┼──────────────────────────────────┤│
│  │                │                     │                                  ││
│  │ Datacenter     │ All managers DEAD   │ Gate marks DC as failed          ││
│  │ total failure  │ No ManagerHeartbeat │ Lease expires → job FAILED       ││
│  │                │                     │ Return failure to client         ││
│  │                │                     │                                  ││
│  ├────────────────┼─────────────────────┼──────────────────────────────────┤│
│  │                │                     │                                  ││
│  │ Network        │ Partial SWIM        │ Pre-vote prevents split-brain    ││
│  │ partition      │ connectivity        │ Minority partition steps down    ││
│  │                │                     │ Majority continues operation     ││
│  │                │                     │                                  ││
│  └────────────────┴─────────────────────┴──────────────────────────────────┘│
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Network Partition Handling

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                     NETWORK PARTITION SCENARIOS                              │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  SCENARIO 1: Manager Cluster Partition (2+1)                                 │
│  ════════════════════════════════════════════                                │
│                                                                              │
│     ┌─────────────────────────┐       ║      ┌─────────────────┐            │
│     │      PARTITION A        │       ║      │   PARTITION B   │            │
│     │   (majority: 2 nodes)   │       ║      │ (minority: 1)   │            │
│     │                         │       ║      │                 │            │
│     │   ┌────┐     ┌────┐    │       ║      │     ┌────┐      │            │
│     │   │ M1 │◄───►│ M2 │    │       ║      │     │ M3 │      │            │
│     │   │ ★  │     │    │    │       ║      │     │    │      │            │
│     │   └────┘     └────┘    │       ║      │     └────┘      │            │
│     │                         │       ║      │                 │            │
│     │   Maintains leadership  │       ║      │  Steps down     │            │
│     │   Continues operation   │       ║      │  (no majority)  │            │
│     │                         │       ║      │                 │            │
│     └─────────────────────────┘       ║      └─────────────────┘            │
│                                       ║                                      │
│                              NETWORK PARTITION                               │
│                                                                              │
│  Behavior:                                                                   │
│  • M3 cannot reach M1/M2, loses leader heartbeats                           │
│  • M3 starts pre-vote, but cannot get majority (only self)                  │
│  • M3 remains follower, does not disrupt cluster                            │
│  • M1 (leader) continues with M2 (2/3 = quorum for confirmations)           │
│                                                                              │
│  ═══════════════════════════════════════════════════════════════════════════ │
│                                                                              │
│  SCENARIO 2: Worker Isolation                                                │
│  ════════════════════════════════                                            │
│                                                                              │
│     ┌─────────────────────────┐       ║      ┌─────────────────┐            │
│     │      MANAGER SIDE       │       ║      │  ISOLATED       │            │
│     │                         │       ║      │  WORKER         │            │
│     │   ┌────┐     ┌────┐    │       ║      │                 │            │
│     │   │ M1 │     │ M2 │    │       ║      │     ┌────┐      │            │
│     │   │ ★  │     │    │    │       ║      │     │ W3 │      │            │
│     │   └──┬─┘     └────┘    │       ║      │     │    │      │            │
│     │      │                  │       ║      │     └────┘      │            │
│     │      ▼                  │       ║      │                 │            │
│     │   ┌────┐     ┌────┐    │       ║      │  Continues      │            │
│     │   │ W1 │     │ W2 │    │       ║      │  executing      │            │
│     │   └────┘     └────┘    │       ║      │  (timeout will  │            │
│     │                         │       ║      │  eventually     │            │
│     │   Reschedule W3 work   │       ║      │  cancel)        │            │
│     │   on W1 or W2          │       ║      │                 │            │
│     └─────────────────────────┘       ║      └─────────────────┘            │
│                                       ║                                      │
│                                                                              │
│  Behavior:                                                                   │
│  • Manager probes W3 → timeout → indirect probe → suspicion → DEAD          │
│  • Manager triggers _on_node_dead callback                                  │
│  • Workflows on W3 are retried on W1/W2 (excluding W3)                      │
│  • If partition heals before W3 timeout, W3 may complete redundantly        │
│  • Fence tokens prevent duplicate commits                                   │
│                                                                              │
│  ═══════════════════════════════════════════════════════════════════════════ │
│                                                                              │
│  SCENARIO 3: Gate-to-DC Partition                                            │
│  ════════════════════════════════════                                        │
│                                                                              │
│     ┌─────────────────┐               ║      ┌─────────────────┐            │
│     │   GATE CLUSTER  │               ║      │  DATACENTER A   │            │
│     │                 │               ║      │                 │            │
│     │   ┌────┐       │               ║      │   ┌────┐        │            │
│     │   │ G1 │        │               ║      │   │ M1 │        │            │
│     │   │ ★  │        │               ║      │   │ ★  │        │            │
│     │   └────┘        │               ║      │   └──┬─┘        │            │
│     │                 │               ║      │      ▼          │            │
│     │   Jobs for DC-A │               ║      │   ┌────┐        │            │
│     │   marked FAILED │               ║      │   │ W1 │        │            │
│     │   (lease expiry)│               ║      │   └────┘        │            │
│     │                 │               ║      │                 │            │
│     └─────────────────┘               ║      │ DC continues    │            │
│                                       ║      │ until timeout   │            │
│                                       ║      └─────────────────┘            │
│                                                                              │
│  Behavior:                                                                   │
│  • Gate stops receiving ManagerHeartbeat from DC-A                          │
│  • Gate marks DC-A managers as DEAD via SWIM                                │
│  • Lease for DC-A jobs expires                                              │
│  • Gate returns job failure to client (no cross-DC retry)                   │
│  • DC-A workflows eventually timeout or complete (ignored by gate)          │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Cascading Failure Protection

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                     CASCADING FAILURE PROTECTION                             │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  PROTECTION MECHANISMS:                                                      │
│                                                                              │
│  ┌────────────────────────────────────────────────────────────────────────┐ │
│  │ 1. LOCAL HEALTH MULTIPLIER (LHM)                                       │ │
│  │                                                                         │ │
│  │    ┌──────────────────────────────────────────────────────────────┐    │ │
│  │    │                                                               │    │ │
│  │    │   Probe fails ──► LHM increases ──► Timeouts extend           │    │ │
│  │    │        ▲                                    │                 │    │ │
│  │    │        │                                    ▼                 │    │ │
│  │    │        └────────── Prevents ◄─── False positives reduced      │    │ │
│  │    │                    cascade                                    │    │ │
│  │    │                                                               │    │ │
│  │    └──────────────────────────────────────────────────────────────┘    │ │
│  │                                                                         │ │
│  │    If one node is slow, we don't mark it dead prematurely              │ │
│  │    → Prevents triggering retry storm on healthy workers                │ │
│  └────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
│  ┌────────────────────────────────────────────────────────────────────────┐ │
│  │ 2. GRACEFUL DEGRADATION                                                │ │
│  │                                                                         │ │
│  │    Load Level     │ Action                                             │ │
│  │    ───────────────┼─────────────────────────────────────────────────── │ │
│  │    NORMAL         │ Full operation                                     │ │
│  │    ELEVATED       │ Reduce gossip frequency                            │ │
│  │    HIGH           │ Skip non-essential probes                          │ │
│  │    SEVERE         │ Leader considers stepping down                     │ │
│  │    CRITICAL       │ Reject new work, focus on completing existing      │ │
│  │                                                                         │ │
│  │    Prevents: Overloaded node being marked dead due to slow responses   │ │
│  └────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
│  ┌────────────────────────────────────────────────────────────────────────┐ │
│  │ 3. BACKPRESSURE SIGNALING                                              │ │
│  │                                                                         │ │
│  │    Worker queue_depth ──► Embedded in WorkerHeartbeat                  │ │
│  │                                    │                                    │ │
│  │                                    ▼                                    │ │
│  │                           Manager respects soft_limit                   │ │
│  │                                    │                                    │ │
│  │                                    ▼                                    │ │
│  │                           New work → other workers                      │ │
│  │                                                                         │ │
│  │    Prevents: Overloading already-stressed workers                      │ │
│  └────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
│  ┌────────────────────────────────────────────────────────────────────────┐ │
│  │ 4. RETRY LIMITS & EXCLUSION                                            │ │
│  │                                                                         │ │
│  │    Workflow fails on Worker A                                          │ │
│  │         │                                                               │ │
│  │         ▼                                                               │ │
│  │    Retry 1: Select from {B, C, D} (A excluded)                         │ │
│  │         │                                                               │ │
│  │    Fails on Worker B                                                    │ │
│  │         │                                                               │ │
│  │         ▼                                                               │ │
│  │    Retry 2: Select from {C, D} (A, B excluded)                         │ │
│  │         │                                                               │ │
│  │    Fails on Worker C                                                    │ │
│  │         │                                                               │ │
│  │         ▼                                                               │ │
│  │    max_retries reached → FAILED (no more attempts)                      │ │
│  │                                                                         │ │
│  │    Prevents: Infinite retry loops, same worker repeated failure        │ │
│  └────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
│  ┌────────────────────────────────────────────────────────────────────────┐ │
│  │ 5. CIRCUIT BREAKERS                                                    │ │
│  │                                                                         │ │
│  │    ErrorHandler tracks errors by category:                              │ │
│  │                                                                         │ │
│  │    NETWORK errors ──► threshold exceeded ──► circuit OPEN              │ │
│  │                                                   │                     │ │
│  │                                                   ▼                     │ │
│  │                                           Fail fast (no retry)          │ │
│  │                                                   │                     │ │
│  │                                           cooldown period               │ │
│  │                                                   │                     │ │
│  │                                                   ▼                     │ │
│  │                                           circuit HALF-OPEN             │ │
│  │                                                   │                     │ │
│  │                                           test request                  │ │
│  │                                                   │                     │ │
│  │                                    success ──► CLOSED   failure ──► OPEN│ │
│  │                                                                         │ │
│  │    Prevents: Repeated attempts to failing resources                    │ │
│  └────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
│  ┌────────────────────────────────────────────────────────────────────────┐ │
│  │ 6. FLAPPING DETECTION                                                  │ │
│  │                                                                         │ │
│  │    Leadership changes in sliding window:                                │ │
│  │                                                                         │ │
│  │    Time: ─────────[change]───[change]───[change]───[change]─────►       │ │
│  │                                                          │              │ │
│  │                                            4 changes in 60s             │ │
│  │                                                          │              │ │
│  │                                                          ▼              │ │
│  │                                               COOLDOWN ACTIVATED        │ │
│  │                                               (no new elections)        │ │
│  │                                                          │              │ │
│  │                                               cooldown expires          │ │
│  │                                                          │              │ │
│  │                                                          ▼              │ │
│  │                                               Normal operation          │ │
│  │                                                                         │ │
│  │    Prevents: Leadership oscillation under unstable conditions          │ │
│  └────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

## Zombie Job Prevention & Detection

This section documents the mechanisms for detecting, preventing, and cleaning up "zombie" jobs - jobs that become stuck, orphaned, or fail to complete properly.

### Zombie Job Lifecycle Diagram

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    ZOMBIE JOB LIFECYCLE & PREVENTION                         │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  What is a "Zombie Job"?                                                     │
│  ───────────────────────                                                     │
│  A job that:                                                                 │
│  • Consumes resources without making progress                                │
│  • Has no live owner/manager tracking it                                     │
│  • Cannot be cancelled via normal means                                      │
│  • Prevents completion of parent job                                         │
│                                                                              │
│  ┌────────────────────────────────────────────────────────────────────────┐ │
│  │                    ZOMBIE CREATION SCENARIOS                           │ │
│  │                                                                        │ │
│  │  Scenario 1: Worker Dies Mid-Workflow                                  │ │
│  │  ─────────────────────────────────────────                             │ │
│  │  Worker ──[executing workflow]──► CRASH! ──► Workflow state lost       │ │
│  │                                                                        │ │
│  │  Scenario 2: Manager Dies After Dispatch                               │ │
│  │  ─────────────────────────────────────────                             │ │
│  │  Manager ──[dispatch]──► Worker ──► Manager CRASH ──► No result recv   │ │
│  │                                                                        │ │
│  │  Scenario 3: Network Partition                                         │ │
│  │  ─────────────────────────────────────────                             │ │
│  │  Manager ◄──X──► Worker   (both think workflow is running)             │ │
│  │                                                                        │ │
│  │  Scenario 4: Workflow Execution Hang                                   │ │
│  │  ─────────────────────────────────────────                             │ │
│  │  Worker ──[workflow.execute() hangs indefinitely]──► Never completes   │ │
│  │                                                                        │ │
│  │  Scenario 5: Result Delivery Failure                                   │ │
│  │  ─────────────────────────────────────────                             │ │
│  │  Worker ──► Result ──X──► Manager   (result lost, no retry)            │ │
│  └────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Detection Mechanisms

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    ZOMBIE DETECTION MECHANISMS                               │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  ┌────────────────────────────────────────────────────────────────────────┐ │
│  │ 1. WORKFLOW TIMEOUT DETECTION (WorkflowDispatcher)                     │ │
│  │                                                                        │ │
│  │    Location: hyperscale/distributed_rewrite/jobs/workflow_dispatcher.py│ │
│  │                                                                        │ │
│  │    ┌─────────────────────────────────────────────────────────────────┐ │ │
│  │    │                                                                  │ │ │
│  │    │    WorkflowDispatcher.check_timeouts()                          │ │ │
│  │    │           │                                                      │ │ │
│  │    │           ▼                                                      │ │ │
│  │    │    for pending in self._pending:                                │ │ │
│  │    │        age = now - pending.registered_at                        │ │ │
│  │    │        │                                                         │ │ │
│  │    │        ├── if age > pending.timeout_seconds:                    │ │ │
│  │    │        │       └── EVICT (reason: "timeout")                    │ │ │
│  │    │        │                                                         │ │ │
│  │    │        └── if pending.dispatch_attempts > max_attempts:         │ │ │
│  │    │                └── EVICT (reason: "max_dispatch_attempts")       │ │ │
│  │    │                                                                  │ │ │
│  │    │    Default timeout_seconds: 300 (5 minutes)                     │ │ │
│  │    │    Default max_dispatch_attempts: 5                             │ │ │
│  │    │    Check interval: 30 seconds (via _job_cleanup_loop)            │ │ │
│  │    │                                                                  │ │ │
│  │    └─────────────────────────────────────────────────────────────────┘ │ │
│  │                                                                        │ │
│  │    Callbacks Invoked:                                                  │ │
│  │    • on_workflow_evicted(job_id, workflow_id, reason)                 │ │
│  │    • on_dispatch_failed(job_id, workflow_id)                          │ │
│  └────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
│  ┌────────────────────────────────────────────────────────────────────────┐ │
│  │ 2. DEAD WORKER DETECTION (SWIM Protocol + Callbacks)                   │ │
│  │                                                                        │ │
│  │    Detection Flow:                                                     │ │
│  │                                                                        │ │
│  │    SWIM Probe ──► Timeout ──► Indirect Probe ──► Timeout               │ │
│  │                                    │                                   │ │
│  │                                    ▼                                   │ │
│  │                           Enter SUSPECT state                          │ │
│  │                                    │                                   │ │
│  │                           No refutation (30s)                          │ │
│  │                                    │                                   │ │
│  │                                    ▼                                   │ │
│  │                           Mark DEAD ──► _on_node_dead() callback       │ │
│  │                                    │                                   │ │
│  │                                    ▼                                   │ │
│  │    Manager identifies all workflows assigned to dead worker            │ │
│  │    │                                                                   │ │
│  │    ├── Retry count < max: Re-dispatch to new worker                   │ │
│  │    │       └── Failed worker added to exclusion set                   │ │
│  │    │                                                                   │ │
│  │    └── Retry count >= max: Mark workflow FAILED                       │ │
│  │                                                                        │ │
│  └────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
│  ┌────────────────────────────────────────────────────────────────────────┐ │
│  │ 3. PROGRESS-BASED HEALTH DETECTION (AD-19 Three-Signal Model)          │ │
│  │                                                                        │ │
│  │    Location: hyperscale/distributed_rewrite/health/                    │ │
│  │                                                                        │ │
│  │    ProgressState Assessment:                                           │ │
│  │    ┌─────────────────────────────────────────────────────────────────┐ │ │
│  │    │ State     │ Criteria                  │ Implication             │ │ │
│  │    │───────────┼───────────────────────────┼─────────────────────────│ │ │
│  │    │ IDLE      │ No active workflows       │ Normal - no work        │ │ │
│  │    │ NORMAL    │ completion_rate >= expected │ Healthy operation     │ │ │
│  │    │ SLOW      │ completion_rate < 50%     │ Possible contention     │ │ │
│  │    │ DEGRADED  │ completion_rate < 25%     │ Significant slowdown    │ │ │
│  │    │ STUCK     │ No progress for threshold │ Potential zombie        │ │ │
│  │    └─────────────────────────────────────────────────────────────────┘ │ │
│  │                                                                        │ │
│  │    Routing Decision Based on Health:                                   │ │
│  │    • ROUTE: Send new work                                              │ │
│  │    • DRAIN: Stop sending work, let existing complete                   │ │
│  │    • INVESTIGATE: Suspect issue, check more signals                    │ │
│  │    • EVICT: Remove from routing, assume dead/zombie                    │ │
│  └────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
│  ┌────────────────────────────────────────────────────────────────────────┐ │
│  │ 4. LEASE EXPIRY DETECTION (Gate Layer)                                 │ │
│  │                                                                        │ │
│  │    Location: hyperscale/distributed_rewrite/leases/job_lease.py        │ │
│  │                                                                        │ │
│  │    Job Lease Lifecycle:                                                │ │
│  │                                                                        │ │
│  │    Gate-1 acquires lease ──► lease.expires_at = now + 30s              │ │
│  │          │                                                             │ │
│  │          ├── Renew: lease.expires_at += renewal_period                 │ │
│  │          │                                                             │ │
│  │          └── Fail to renew (crash/partition):                          │ │
│  │                  │                                                     │ │
│  │                  ▼                                                     │ │
│  │          Lease expires ──► Gate-2 can claim ──► fence_token++          │ │
│  │                                   │                                    │ │
│  │                                   ▼                                    │ │
│  │          Old results with stale fence_token are REJECTED               │ │
│  │                                                                        │ │
│  │    Default lease_timeout: 30 seconds                                   │ │
│  └────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Prevention Mechanisms

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    ZOMBIE PREVENTION MECHANISMS                              │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  ┌────────────────────────────────────────────────────────────────────────┐ │
│  │ 1. FENCE TOKENS (At-Most-Once Dispatch Semantics)                      │ │
│  │                                                                        │ │
│  │    Location: Worker._workflow_fence_tokens                             │ │
│  │                                                                        │ │
│  │    Purpose: Prevent duplicate/stale dispatches from creating zombies   │ │
│  │                                                                        │ │
│  │    ┌────────────────────────────────────────────────────────────────┐  │ │
│  │    │                                                                 │  │ │
│  │    │  Worker receives WorkflowDispatch(workflow_id, fence_token=5)  │  │ │
│  │    │              │                                                  │  │ │
│  │    │              ▼                                                  │  │ │
│  │    │  current = _workflow_fence_tokens.get(workflow_id, -1)         │  │ │
│  │    │              │                                                  │  │ │
│  │    │   ┌──────────┴──────────┐                                       │  │ │
│  │    │   │                     │                                       │  │ │
│  │    │   ▼                     ▼                                       │  │ │
│  │    │ fence_token <= current  fence_token > current                  │  │ │
│  │    │   │                     │                                       │  │ │
│  │    │   ▼                     ▼                                       │  │ │
│  │    │ REJECT (stale)        ACCEPT                                   │  │ │
│  │    │ Return NACK            │                                       │  │ │
│  │    │                        ▼                                       │  │ │
│  │    │              _workflow_fence_tokens[workflow_id] = fence_token │  │ │
│  │    │              Execute workflow                                  │  │ │
│  │    │                                                                 │  │ │
│  │    └────────────────────────────────────────────────────────────────┘  │ │
│  │                                                                        │ │
│  │    Prevents:                                                           │ │
│  │    • Duplicate execution from retry storms                            │ │
│  │    • Stale dispatches from recovered old manager                      │ │
│  │    • Split-brain double execution                                      │ │
│  └────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
│  ┌────────────────────────────────────────────────────────────────────────┐ │
│  │ 2. VERSIONED STATE CLOCK (Stale Update Rejection)                      │ │
│  │                                                                        │ │
│  │    Location: hyperscale/distributed_rewrite/swim/versioned_clock.py    │ │
│  │                                                                        │ │
│  │    Purpose: Reject out-of-order updates that could create             │ │
│  │             inconsistent state                                         │ │
│  │                                                                        │ │
│  │    VersionedStateClock {                                               │ │
│  │        _entity_versions: dict[str, (version, timestamp)]              │ │
│  │                                                                        │ │
│  │        is_entity_stale(entity_id, incoming_version) -> bool           │ │
│  │        check_and_update(entity_id, incoming_version) -> bool          │ │
│  │        cleanup_old_entities(max_age) -> None                          │ │
│  │    }                                                                   │ │
│  │                                                                        │ │
│  │    Used at:                                                            │ │
│  │    • Manager receiving WorkerHeartbeat                                │ │
│  │    • Manager receiving WorkflowProgress                               │ │
│  │    • Gate receiving ManagerHeartbeat                                  │ │
│  └────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
│  ┌────────────────────────────────────────────────────────────────────────┐ │
│  │ 3. CANCELLATION POLLING (Fallback When Push Fails)                     │ │
│  │                                                                        │ │
│  │    Location: Worker._cancellation_poll_loop()                          │ │
│  │                                                                        │ │
│  │    Problem: Cancellation push from manager might not reach worker      │ │
│  │    Solution: Worker periodically polls manager for cancellation status │ │
│  │                                                                        │ │
│  │    ┌────────────────────────────────────────────────────────────────┐  │ │
│  │    │                                                                 │  │ │
│  │    │  while running:                                                 │  │ │
│  │    │      await sleep(poll_interval)  # Default: 5-10s              │  │ │
│  │    │                                                                 │  │ │
│  │    │      for workflow_id in active_workflows:                      │  │ │
│  │    │          │                                                      │  │ │
│  │    │          ▼                                                      │  │ │
│  │    │      Send WorkflowCancellationQuery to manager                 │  │ │
│  │    │          │                                                      │  │ │
│  │    │          ▼                                                      │  │ │
│  │    │      if response.is_cancelled:                                 │  │ │
│  │    │          _cancel_workflow(workflow_id, "poll_detected")        │  │ │
│  │    │                                                                 │  │ │
│  │    └────────────────────────────────────────────────────────────────┘  │ │
│  │                                                                        │ │
│  │    Ensures: Cancellations are never "lost" due to network issues      │ │
│  └────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
│  ┌────────────────────────────────────────────────────────────────────────┐ │
│  │ 4. ADAPTIVE HEALTHCHECK EXTENSIONS (AD-26)                             │ │
│  │                                                                        │ │
│  │    Location: hyperscale/distributed_rewrite/health/extension_tracker.py│ │
│  │                                                                        │ │
│  │    Problem: Long-running workflows might be killed as "stuck"         │ │
│  │    Solution: Allow legitimate slow workers to request deadline extensions│
│  │                                                                        │ │
│  │    Extension Request Flow:                                             │ │
│  │                                                                        │ │
│  │    Worker ──► Heartbeat with extension_requested=True ──► Manager      │ │
│  │                    │                                                   │ │
│  │                    ▼                                                   │ │
│  │    ExtensionTracker.request_extension(reason, current_progress)        │ │
│  │                    │                                                   │ │
│  │        ┌───────────┴───────────┐                                       │ │
│  │        │                       │                                       │ │
│  │        ▼                       ▼                                       │ │
│  │    GRANTED                  DENIED                                     │ │
│  │    (extension_seconds)      (denial_reason)                            │ │
│  │                                                                        │ │
│  │    Grant Decay (Logarithmic):                                          │ │
│  │    ┌────────────────────────────────────────────────────────────────┐  │ │
│  │    │ Grant # │ Formula        │ Example (base=30s) │                │  │ │
│  │    │─────────┼────────────────┼────────────────────│                │  │ │
│  │    │ 1       │ base / 2       │ 15s                │                │  │ │
│  │    │ 2       │ base / 4       │ 7.5s               │                │  │ │
│  │    │ 3       │ base / 8       │ 3.75s              │                │  │ │
│  │    │ 4       │ base / 16      │ 1.875s             │                │  │ │
│  │    │ 5       │ min_grant      │ 1s (capped)        │                │  │ │
│  │    └────────────────────────────────────────────────────────────────┘  │ │
│  │                                                                        │ │
│  │    Denial Reasons:                                                     │ │
│  │    • "max_extensions_exceeded" - Already used all extensions          │ │
│  │    • "no_progress" - Progress same as last request (stuck)            │ │
│  └────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Cleanup Mechanisms

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    ZOMBIE CLEANUP MECHANISMS                                 │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  ┌────────────────────────────────────────────────────────────────────────┐ │
│  │ 1. MANAGER JOB CLEANUP LOOP                                            │ │
│  │                                                                        │ │
│  │    Location: Manager._job_cleanup_loop() (manager.py:6225)             │ │
│  │                                                                        │ │
│  │    Interval: MERCURY_SYNC_CLEANUP_INTERVAL (default: 30s)              │ │
│  │                                                                        │ │
│  │    ┌────────────────────────────────────────────────────────────────┐  │ │
│  │    │                                                                 │  │ │
│  │    │  while running:                                                 │  │ │
│  │    │      await sleep(cleanup_interval)                             │  │ │
│  │    │                                                                 │  │ │
│  │    │      # 1. Check workflow timeouts via dispatcher               │  │ │
│  │    │      evicted = await _workflow_dispatcher.check_timeouts()     │  │ │
│  │    │      for (job_id, workflow_id, reason) in evicted:             │  │ │
│  │    │          mark_workflow_failed(job_id, workflow_id, reason)     │  │ │
│  │    │                                                                 │  │ │
│  │    │      # 2. Clean completed jobs after retention period          │  │ │
│  │    │      for job_id, job in _jobs.items():                         │  │ │
│  │    │          if job.status == COMPLETED:                           │  │ │
│  │    │              if age > _completed_job_max_age:  # ~30 min       │  │ │
│  │    │                  cleanup_job(job_id)                           │  │ │
│  │    │                                                                 │  │ │
│  │    │      # 3. Clean failed/cancelled/timeout jobs                  │  │ │
│  │    │      for job_id, job in _jobs.items():                         │  │ │
│  │    │          if job.status in [FAILED, CANCELLED, TIMEOUT]:        │  │ │
│  │    │              if age > _failed_job_max_age:  # longer retention │  │ │
│  │    │                  cleanup_job(job_id)                           │  │ │
│  │    │                                                                 │  │ │
│  │    └────────────────────────────────────────────────────────────────┘  │ │
│  └────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
│  ┌────────────────────────────────────────────────────────────────────────┐ │
│  │ 2. DEAD NODE REAP LOOP                                                 │ │
│  │                                                                        │ │
│  │    Location: Manager._dead_node_reap_loop() (manager.py:6380)          │ │
│  │                                                                        │ │
│  │    ┌────────────────────────────────────────────────────────────────┐  │ │
│  │    │                                                                 │  │ │
│  │    │  Reap Intervals:                                                │  │ │
│  │    │  ├── Dead workers: MANAGER_DEAD_WORKER_REAP_INTERVAL (~24h)    │  │ │
│  │    │  ├── Dead peers:   MANAGER_DEAD_PEER_REAP_INTERVAL   (~24h)    │  │ │
│  │    │  └── Dead gates:   MANAGER_DEAD_GATE_REAP_INTERVAL   (~24h)    │  │ │
│  │    │                                                                 │  │ │
│  │    │  For each dead node past reap interval:                        │  │ │
│  │    │  ├── Remove from _dead_workers / _dead_peers / _dead_gates     │  │ │
│  │    │  ├── Remove from all tracking structures                       │  │ │
│  │    │  └── Free any resources/leases associated                      │  │ │
│  │    │                                                                 │  │ │
│  │    └────────────────────────────────────────────────────────────────┘  │ │
│  │                                                                        │ │
│  │    Note: 24h is conservative for debugging. In production,            │ │
│  │    consider reducing to 1-2h via environment variables.               │ │
│  └────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
│  ┌────────────────────────────────────────────────────────────────────────┐ │
│  │ 3. WORKER WORKFLOW CLEANUP (finally block)                             │ │
│  │                                                                        │ │
│  │    Location: Worker._execute_workflow() finally block                  │ │
│  │                                                                        │ │
│  │    ┌────────────────────────────────────────────────────────────────┐  │ │
│  │    │                                                                 │  │ │
│  │    │  async def _execute_workflow(...):                             │  │ │
│  │    │      try:                                                       │  │ │
│  │    │          # Execute workflow                                    │  │ │
│  │    │          result = await remote_manager.execute(...)            │  │ │
│  │    │                                                                 │  │ │
│  │    │      except CancelledError:                                    │  │ │
│  │    │          # Handle cancellation                                 │  │ │
│  │    │                                                                 │  │ │
│  │    │      except Exception:                                         │  │ │
│  │    │          # Handle failure                                      │  │ │
│  │    │                                                                 │  │ │
│  │    │      finally:                                                   │  │ │
│  │    │          # ALWAYS cleanup - prevents resource leaks            │  │ │
│  │    │          await _core_allocator.free(workflow_id)  ◄── Free CPU │  │ │
│  │    │          _workflow_tokens.pop(workflow_id)        ◄── Remove   │  │ │
│  │    │          _workflow_cancel_events.pop(workflow_id) ◄── tracking │  │ │
│  │    │          _active_workflows.pop(workflow_id)       ◄── state    │  │ │
│  │    │          _workflow_fence_tokens.pop(workflow_id)  ◄── data     │  │ │
│  │    │          _remote_manger.start_server_cleanup()    ◄── Cleanup  │  │ │
│  │    │                                                                 │  │ │
│  │    └────────────────────────────────────────────────────────────────┘  │ │
│  │                                                                        │ │
│  │    Guarantees: Workflow resources are ALWAYS freed, regardless of     │ │
│  │    success, failure, or cancellation.                                 │ │
│  └────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
│  ┌────────────────────────────────────────────────────────────────────────┐ │
│  │ 4. GATE LEASE CLEANUP LOOP                                             │ │
│  │                                                                        │ │
│  │    Location: Gate._lease_cleanup_loop()                                │ │
│  │                                                                        │ │
│  │    ┌────────────────────────────────────────────────────────────────┐  │ │
│  │    │                                                                 │  │ │
│  │    │  while running:                                                 │  │ │
│  │    │      await sleep(cleanup_interval)                             │  │ │
│  │    │                                                                 │  │ │
│  │    │      for lease_key, lease in _leases.items():                  │  │ │
│  │    │          if time.monotonic() > lease.expires_at:               │  │ │
│  │    │              │                                                  │  │ │
│  │    │              ▼                                                  │  │ │
│  │    │          Mark job's DC as FAILED                               │  │ │
│  │    │          │                                                      │  │ │
│  │    │          ▼                                                      │  │ │
│  │    │          Remove expired lease                                  │  │ │
│  │    │          │                                                      │  │ │
│  │    │          ▼                                                      │  │ │
│  │    │          Notify client of partial failure                      │  │ │
│  │    │                                                                 │  │ │
│  │    └────────────────────────────────────────────────────────────────┘  │ │
│  │                                                                        │ │
│  │    Ensures: Jobs with dead datacenters don't hang forever             │ │
│  └────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Cancellation Flow (Killing Zombie Jobs)

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    CANCELLATION PROPAGATION FLOW                             │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  User Request: client.cancel_job(job_id)                                     │
│                                                                              │
│  ┌─────────────────────────────────────────────────────────────────────────┐ │
│  │                                                                          │ │
│  │  CLIENT                                                                  │ │
│  │    │                                                                     │ │
│  │    │ JobCancelRequest(job_id, fence_token, reason)                      │ │
│  │    │                                                                     │ │
│  │    ▼                                                                     │ │
│  │  GATE                                                                    │ │
│  │    │                                                                     │ │
│  │    ├── Validate fence_token (reject stale)                              │ │
│  │    ├── Check lease ownership (am I responsible?)                        │ │
│  │    │                                                                     │ │
│  │    │ FOR EACH datacenter with active workflows:                         │ │
│  │    │    │                                                                │ │
│  │    │    │ WorkflowCancelRequest(job_id, workflow_ids)                   │ │
│  │    │    │                                                                │ │
│  │    │    ▼                                                                │ │
│  │  MANAGER                                                                 │ │
│  │    │                                                                     │ │
│  │    ├── Update job status to CANCELLING                                  │ │
│  │    ├── Update workflow status to CANCELLED                              │ │
│  │    │                                                                     │ │
│  │    │ FOR EACH worker with workflow:                                     │ │
│  │    │    │                                                                │ │
│  │    │    │ WorkflowCancelRequest(workflow_id, fence_token)               │ │
│  │    │    │                                                                │ │
│  │    │    ▼                                                                │ │
│  │  WORKER                                                                  │ │
│  │    │                                                                     │ │
│  │    ├── Set _workflow_cancel_events[workflow_id]                         │ │
│  │    ├── TaskRunner.cancel(workflow_token)                                │ │
│  │    ├── RemoteGraphManager.cancel_workflow(run_id)                       │ │
│  │    │                                                                     │ │
│  │    │ RESPONSE PROPAGATION (reverse):                                    │ │
│  │    │                                                                     │ │
│  │    ▼                                                                     │ │
│  │  WorkflowCancelResponse(success=True, cancelled_count=N)                │ │
│  │    │                                                                     │ │
│  │    ▼                                                                     │ │
│  │  JobCancelResponse(success=True, cancelled_workflow_count=M)            │ │
│  │    │                                                                     │ │
│  │    ▼                                                                     │ │
│  │  CLIENT receives confirmation                                            │ │
│  │                                                                          │ │
│  └─────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
│  Fallback Mechanism (if push fails):                                         │ │
│  ┌─────────────────────────────────────────────────────────────────────────┐ │
│  │                                                                          │ │
│  │  Worker._cancellation_poll_loop():                                       │ │
│  │                                                                          │ │
│  │    Every 5-10 seconds:                                                   │ │
│  │    ├── For each active workflow                                          │ │
│  │    │    │                                                                │ │
│  │    │    │ WorkflowCancellationQuery(workflow_id)                        │ │
│  │    │    │                                                                │ │
│  │    │    ▼                                                                │ │
│  │    │    Manager checks if cancelled ──► Response                        │ │
│  │    │                                        │                            │ │
│  │    │    ┌───────────────────────────────────┘                            │ │
│  │    │    │                                                                │ │
│  │    │    ├── is_cancelled=True → _cancel_workflow()                      │ │
│  │    │    └── is_cancelled=False → continue execution                     │ │
│  │    │                                                                     │ │
│  │    Ensures: Even if manager→worker push is lost, worker will            │ │
│  │    discover cancellation within poll_interval seconds                   │ │
│  │                                                                          │ │
│  └─────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Complete Zombie Prevention State Machine

```
┌─────────────────────────────────────────────────────────────────────────────┐
│               ZOMBIE PREVENTION STATE MACHINE (per workflow)                 │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│                                                                              │
│                           ┌──────────────┐                                  │
│                           │   PENDING    │                                  │
│                           │   (queued)   │                                  │
│                           └──────┬───────┘                                  │
│                                  │                                          │
│                    ┌─────────────┼─────────────┐                            │
│                    │             │             │                            │
│                    ▼             ▼             ▼                            │
│           ┌────────────┐ ┌────────────┐ ┌────────────┐                      │
│           │  TIMEOUT   │ │ DISPATCHED │ │ MAX_RETRY  │                      │
│           │ (evicted)  │ │            │ │ (evicted)  │                      │
│           └─────┬──────┘ └──────┬─────┘ └──────┬─────┘                      │
│                 │               │              │                            │
│                 │               ▼              │                            │
│                 │        ┌────────────┐        │                            │
│                 │        │  RUNNING   │        │                            │
│                 │        │ (on worker)│        │                            │
│                 │        └──────┬─────┘        │                            │
│                 │               │              │                            │
│        ┌────────┼───────────────┼──────────────┼────────┐                   │
│        │        │               │              │        │                   │
│        ▼        ▼               ▼              ▼        ▼                   │
│  ┌──────────┐ ┌──────────┐ ┌──────────┐ ┌──────────┐ ┌──────────┐          │
│  │ COMPLETED│ │  FAILED  │ │CANCELLED │ │ TIMEOUT  │ │WORKER_DIE│          │
│  │          │ │(internal)│ │ (user)   │ │(runtime) │ │(detected)│          │
│  └────┬─────┘ └────┬─────┘ └────┬─────┘ └────┬─────┘ └────┬─────┘          │
│       │            │            │            │            │                 │
│       │            │            │            │            │                 │
│       │            │            │            │      ┌─────┴─────┐           │
│       │            │            │            │      │           │           │
│       │            │            │            │      ▼           ▼           │
│       │            │            │            │  RETRY #N   MAX_RETRY        │
│       │            │            │            │  (redispatch) (failed)       │
│       │            │            │            │      │           │           │
│       │            │            │            │      │           │           │
│       ▼            ▼            ▼            ▼      ▼           ▼           │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │                        CLEANUP (always)                              │   │
│  │  • Free cores: _core_allocator.free(workflow_id)                    │   │
│  │  • Remove tracking: _workflow_tokens, _active_workflows, etc.       │   │
│  │  • Send result/status to manager                                    │   │
│  │  • RemoteGraphManager.start_server_cleanup()                        │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                              │
│  Legend:                                                                     │
│  ───────                                                                     │
│  • Timeout paths prevent indefinite waiting                                 │
│  • Worker death triggers immediate retry or failure                         │
│  • All paths lead to CLEANUP (no resource leaks)                           │
│  • Fence tokens prevent duplicate execution on retry                        │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Mechanism Summary Table

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    ZOMBIE PREVENTION MECHANISM SUMMARY                       │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  ┌──────────────────────┬───────────────┬──────────────────────────────────┐│
│  │ Mechanism            │ Location      │ Protects Against                 ││
│  ├──────────────────────┼───────────────┼──────────────────────────────────┤│
│  │ Workflow Timeout     │ Dispatcher    │ Hung pending workflows           ││
│  │ (check_timeouts)     │               │ (default: 300s)                  ││
│  ├──────────────────────┼───────────────┼──────────────────────────────────┤│
│  │ SWIM Dead Detection  │ All nodes     │ Dead workers/managers/gates      ││
│  │ (_on_node_dead)      │               │ (suspicion: ~30s)                ││
│  ├──────────────────────┼───────────────┼──────────────────────────────────┤│
│  │ Progress Health      │ Manager       │ Stuck workers without progress   ││
│  │ (AD-19)              │               │ (STUCK state detection)          ││
│  ├──────────────────────┼───────────────┼──────────────────────────────────┤│
│  │ Lease Expiry         │ Gate          │ Jobs orphaned by gate failure    ││
│  │ (job_lease)          │               │ (default: 30s)                   ││
│  ├──────────────────────┼───────────────┼──────────────────────────────────┤│
│  │ Fence Tokens         │ Worker        │ Duplicate/stale dispatches       ││
│  │                      │               │ (at-most-once semantics)         ││
│  ├──────────────────────┼───────────────┼──────────────────────────────────┤│
│  │ Versioned Clock      │ Manager/Gate  │ Out-of-order state updates       ││
│  │                      │               │ (stale update rejection)         ││
│  ├──────────────────────┼───────────────┼──────────────────────────────────┤│
│  │ Cancel Polling       │ Worker        │ Lost cancellation messages       ││
│  │                      │               │ (poll interval: 5-10s)           ││
│  ├──────────────────────┼───────────────┼──────────────────────────────────┤│
│  │ Extension Tracking   │ Manager       │ Legitimate slow work killed      ││
│  │ (AD-26)              │               │ (max 5 extensions, decay)        ││
│  ├──────────────────────┼───────────────┼──────────────────────────────────┤│
│  │ Job Cleanup Loop     │ Manager       │ Resource accumulation            ││
│  │                      │               │ (interval: 30s)                  ││
│  ├──────────────────────┼───────────────┼──────────────────────────────────┤│
│  │ Dead Node Reaping    │ Manager       │ Stale dead node tracking         ││
│  │                      │               │ (interval: ~24h)                 ││
│  ├──────────────────────┼───────────────┼──────────────────────────────────┤│
│  │ finally Cleanup      │ Worker        │ Resource leaks on any exit       ││
│  │ (_execute_workflow)  │               │ (always runs)                    ││
│  └──────────────────────┴───────────────┴──────────────────────────────────┘│
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Known Gaps and Future Improvements

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    KNOWN GAPS & FUTURE IMPROVEMENTS                          │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  ┌────────────────────────────────────────────────────────────────────────┐ │
│  │ GAP 1: NO RUNTIME EXECUTION TIMEOUT                                    │ │
│  │                                                                        │ │
│  │ Current: timeout_seconds only affects dispatch eligibility             │ │
│  │ Problem: Workflow can run indefinitely if execution hangs             │ │
│  │                                                                        │ │
│  │ Recommendation: Add execution_timeout at RemoteGraphManager level     │ │
│  │   • asyncio.wait_for() wrapper with hard timeout                      │ │
│  │   • Separate from dispatch timeout (dispatch_timeout vs exec_timeout) │ │
│  └────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
│  ┌────────────────────────────────────────────────────────────────────────┐ │
│  │ GAP 2: LONG DEAD NODE REAP INTERVAL                                    │ │
│  │                                                                        │ │
│  │ Current: 24h default for dead node reaping                            │ │
│  │ Problem: Dead worker tracking accumulates memory                       │ │
│  │                                                                        │ │
│  │ Recommendation: Reduce to 1-2h in production                          │ │
│  │   • Configure via MANAGER_DEAD_WORKER_REAP_INTERVAL                   │ │
│  │   • Keep 24h for debugging/development only                           │ │
│  └────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
│  ┌────────────────────────────────────────────────────────────────────────┐ │
│  │ GAP 3: NO HARD KILL SIGNAL                                             │ │
│  │                                                                        │ │
│  │ Current: Cancellation relies on workflow respecting cancel event       │ │
│  │ Problem: Misbehaving workflow can ignore cancellation                  │ │
│  │                                                                        │ │
│  │ Recommendation: Add process-level kill capability                      │ │
│  │   • Track workflow PID at execution start                             │ │
│  │   • SIGKILL after grace period if cancel not acknowledged             │ │
│  │   • May require process isolation (subprocess vs thread)              │ │
│  └────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
│  ┌────────────────────────────────────────────────────────────────────────┐ │
│  │ GAP 4: NO ORPHAN JOB SCANNER                                           │ │
│  │                                                                        │ │
│  │ Current: Rely on timeout and heartbeat for detection                   │ │
│  │ Problem: Jobs can be orphaned if all tracking state lost              │ │
│  │                                                                        │ │
│  │ Recommendation: Add periodic reconciliation scan                       │ │
│  │   • Manager queries all workers for active workflow list              │ │
│  │   • Compare with manager's tracking → find orphans                    │ │
│  │   • Clean up or re-adopt orphaned workflows                           │ │
│  └────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
│  ┌────────────────────────────────────────────────────────────────────────┐ │
│  │ GAP 5: EXTENSION EXHAUSTION HARD CUTOFF                                │ │
│  │                                                                        │ │
│  │ Current: After max extensions, no more time granted                    │ │
│  │ Problem: Legitimate slow work killed abruptly                          │ │
│  │                                                                        │ │
│  │ Recommendation: Graceful degradation                                   │ │
│  │   • Notify workflow of impending timeout                              │ │
│  │   • Allow checkpoint/save before kill                                 │ │
│  │   • Configurable behavior (kill vs pause vs notify)                   │ │
│  └────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Configuration Reference

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    ZOMBIE PREVENTION CONFIGURATION                           │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  Environment Variables:                                                      │
│                                                                              │
│  ┌────────────────────────────────────┬──────────┬────────────────────────┐ │
│  │ Variable                           │ Default  │ Description            │ │
│  ├────────────────────────────────────┼──────────┼────────────────────────┤ │
│  │ MERCURY_SYNC_CLEANUP_INTERVAL      │ 30s      │ Job cleanup loop freq  │ │
│  │ MANAGER_DEAD_WORKER_REAP_INTERVAL  │ 86400s   │ Dead worker reap (24h) │ │
│  │ MANAGER_DEAD_PEER_REAP_INTERVAL    │ 86400s   │ Dead peer reap (24h)   │ │
│  │ MANAGER_DEAD_GATE_REAP_INTERVAL    │ 86400s   │ Dead gate reap (24h)   │ │
│  │ WORKER_CANCELLATION_POLL_INTERVAL  │ 5s       │ Cancel poll frequency  │ │
│  │ SWIM_SUSPICION_TIMEOUT             │ 30s      │ Time before DEAD       │ │
│  └────────────────────────────────────┴──────────┴────────────────────────┘ │
│                                                                              │
│  Per-Job Configuration:                                                      │
│                                                                              │
│  ┌────────────────────────────────────┬──────────┬────────────────────────┐ │
│  │ Parameter                          │ Default  │ Description            │ │
│  ├────────────────────────────────────┼──────────┼────────────────────────┤ │
│  │ timeout_seconds                    │ 300s     │ Workflow dispatch time │ │
│  │ max_dispatch_attempts              │ 5        │ Retries before fail    │ │
│  │ max_extensions                     │ 5        │ Deadline extensions    │ │
│  │ lease_timeout                      │ 30s      │ Gate job lease duration│ │
│  └────────────────────────────────────┴──────────┴────────────────────────┘ │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

## Backpressure & Degradation

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    BACKPRESSURE & GRACEFUL DEGRADATION                       │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  DEGRADATION LEVELS:                                                         │
│                                                                              │
│  ┌─────────────────────────────────────────────────────────────────────────┐│
│  │                                                                          ││
│  │  Level      │ LHM   │ Event Loop │ Actions                              ││
│  │             │ Score │ Lag Ratio  │                                       ││
│  │  ───────────┼───────┼────────────┼────────────────────────────────────── ││
│  │  NORMAL     │ 0-2   │ < 0.5      │ Full operation                       ││
│  │             │       │            │                                       ││
│  │  ELEVATED   │ 2-4   │ 0.5-1.0    │ • Extend timeouts by 1.25x           ││
│  │             │       │            │ • Reduce gossip rate                 ││
│  │             │       │            │                                       ││
│  │  HIGH       │ 4-6   │ 1.0-2.0    │ • Extend timeouts by 1.5x            ││
│  │             │       │            │ • Skip 25% of probes                 ││
│  │             │       │            │ • Reduce piggyback size              ││
│  │             │       │            │                                       ││
│  │  SEVERE     │ 6-7   │ 2.0-4.0    │ • Extend timeouts by 2x              ││
│  │             │       │            │ • Skip 50% of probes                 ││
│  │             │       │            │ • Consider leadership stepdown       ││
│  │             │       │            │                                       ││
│  │  CRITICAL   │ 7-8   │ > 4.0      │ • Extend timeouts by 3x              ││
│  │             │       │            │ • Skip all non-essential probes      ││
│  │             │       │            │ • Force leadership stepdown          ││
│  │             │       │            │ • Reject new work                    ││
│  │                                                                          ││
│  └─────────────────────────────────────────────────────────────────────────┘│
│                                                                              │
│  BACKPRESSURE FLOW:                                                          │
│                                                                              │
│  ┌─────────────────────────────────────────────────────────────────────────┐│
│  │                                                                          ││
│  │   Worker              Manager              Gate                Client   ││
│  │     │                    │                   │                   │      ││
│  │     │ WorkerHeartbeat    │                   │                   │      ││
│  │     │ {queue_depth: 45}  │                   │                   │      ││
│  │     │───────────────────►│                   │                   │      ││
│  │     │                    │                   │                   │      ││
│  │     │                    │ Check soft_limit  │                   │      ││
│  │     │                    │ (e.g., 50)        │                   │      ││
│  │     │                    │                   │                   │      ││
│  │     │                    │ Worker approaching│                   │      ││
│  │     │                    │ limit - depriori- │                   │      ││
│  │     │                    │ tize in selection │                   │      ││
│  │     │                    │                   │                   │      ││
│  │     │                    │◄──────────────────│ New job           │      ││
│  │     │                    │                   │                   │      ││
│  │     │                    │ Select different  │                   │      ││
│  │     │                    │ worker with lower │                   │      ││
│  │     │                    │ queue_depth       │                   │      ││
│  │     │                    │                   │                   │      ││
│  │  ───┴────────────────────┴───────────────────┴───────────────────┴───── ││
│  │                                                                          ││
│  │   If ALL workers at capacity:                                            ││
│  │                                                                          ││
│  │   Worker 1            Worker 2            Worker 3                       ││
│  │   queue: 50           queue: 48           queue: 50                      ││
│  │   (at limit)          (near limit)        (at limit)                     ││
│  │       │                   │                   │                          ││
│  │       └───────────────────┼───────────────────┘                          ││
│  │                           │                                              ││
│  │                           ▼                                              ││
│  │                    Manager rejects                                        ││
│  │                    new workflow with                                      ││
│  │                    backpressure error                                     ││
│  │                           │                                              ││
│  │                           ▼                                              ││
│  │                    Gate/Client receives                                   ││
│  │                    "capacity exceeded"                                    ││
│  │                           │                                              ││
│  │                           ▼                                              ││
│  │                    Client implements                                      ││
│  │                    exponential backoff                                    ││
│  │                                                                          ││
│  └─────────────────────────────────────────────────────────────────────────┘│
│                                                                              │
│  LHM ADJUSTMENT FLOW:                                                        │
│                                                                              │
│  ┌─────────────────────────────────────────────────────────────────────────┐│
│  │                                                                          ││
│  │   Event                        │ LHM Change                             ││
│  │   ─────────────────────────────┼──────────────────────────────────────  ││
│  │   Probe success                │ Decrement by 1 (min 0)                 ││
│  │   Probe failure                │ Increment by 1                         ││
│  │   Indirect probe required      │ Increment by 1                         ││
│  │   Event loop lag detected      │ Increment by 1-2                       ││
│  │   Event loop recovered         │ Decrement by 1                         ││
│  │   Suspicion started            │ Increment by 1                         ││
│  │   Refutation successful        │ Decrement by 1                         ││
│  │                                                                          ││
│  │   Timeout Calculation:                                                   ││
│  │   effective_timeout = base_timeout × (1 + LHM_score × 0.25)             ││
│  │                                                                          ││
│  │   Example (base_timeout = 500ms):                                        ││
│  │   • LHM 0 → 500ms                                                        ││
│  │   • LHM 2 → 750ms                                                        ││
│  │   • LHM 4 → 1000ms                                                       ││
│  │   • LHM 8 → 1500ms                                                       ││
│  │                                                                          ││
│  └─────────────────────────────────────────────────────────────────────────┘│
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

## Scaling Operations

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                         SCALING OPERATIONS                                   │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  ADDING A WORKER:                                                            │
│                                                                              │
│  ┌─────────────────────────────────────────────────────────────────────────┐│
│  │                                                                          ││
│  │   New Worker              Manager (Leader)                               ││
│  │      │                          │                                        ││
│  │      │ ① TCP: WorkerRegistration│                                        ││
│  │      │ {node, total_cores, ...} │                                        ││
│  │      │─────────────────────────►│                                        ││
│  │      │                          │                                        ││
│  │      │                          │ Add to _workers                        ││
│  │      │                          │ Add to probe_scheduler                 ││
│  │      │                          │                                        ││
│  │      │ ② TCP: RegistrationAck   │                                        ││
│  │      │◄─────────────────────────│                                        ││
│  │      │                          │                                        ││
│  │      │ ③ UDP: Join SWIM cluster │                                        ││
│  │      │─────────────────────────►│                                        ││
│  │      │                          │                                        ││
│  │      │ ④ UDP: Ack + member list │                                        ││
│  │      │◄─────────────────────────│                                        ││
│  │      │                          │                                        ││
│  │      │      ════════════════════│═══════════════════                     ││
│  │      │         Worker now ACTIVE and receiving work                      ││
│  │      │                          │                                        ││
│  │                                                                          ││
│  │   Time: ~1-2 seconds from registration to first workflow                ││
│  │                                                                          ││
│  └─────────────────────────────────────────────────────────────────────────┘│
│                                                                              │
│  REMOVING A WORKER (GRACEFUL):                                               │
│                                                                              │
│  ┌─────────────────────────────────────────────────────────────────────────┐│
│  │                                                                          ││
│  │   Worker                    Manager (Leader)                             ││
│  │      │                          │                                        ││
│  │      │ ① Set state = DRAINING   │                                        ││
│  │      │                          │                                        ││
│  │      │ ② UDP: WorkerHeartbeat   │                                        ││
│  │      │ {state: DRAINING}        │                                        ││
│  │      │─────────────────────────►│                                        ││
│  │      │                          │                                        ││
│  │      │                          │ Stop sending new work                  ││
│  │      │                          │                                        ││
│  │      │ ③ Complete existing workflows                                    ││
│  │      │                          │                                        ││
│  │      │ ④ TCP: All workflows done│                                        ││
│  │      │─────────────────────────►│                                        ││
│  │      │                          │                                        ││
│  │      │ ⑤ UDP: Leave message     │                                        ││
│  │      │─────────────────────────►│                                        ││
│  │      │                          │                                        ││
│  │      │                          │ Remove from _workers                   ││
│  │      │                          │ Gossip leave to cluster                ││
│  │      │                          │                                        ││
│  │      ╳ Shutdown                 │                                        ││
│  │                                                                          ││
│  └─────────────────────────────────────────────────────────────────────────┘│
│                                                                              │
│  ADDING A MANAGER:                                                           │
│                                                                              │
│  ┌─────────────────────────────────────────────────────────────────────────┐│
│  │                                                                          ││
│  │   New Manager           Existing Managers                                ││
│  │      │                       │                                           ││
│  │      │ ① UDP: Join SWIM cluster                                         ││
│  │      │──────────────────────►│                                           ││
│  │      │                       │                                           ││
│  │      │ ② UDP: Ack + members  │                                           ││
│  │      │◄──────────────────────│                                           ││
│  │      │                       │                                           ││
│  │      │ ★ CURRENT: Immediately joins quorum                              ││
│  │      │ ★ FUTURE: STATE: SYNCING (not in quorum until sync done)        ││
│  │      │                       │                                           ││
│  │      │ ③ TCP: StateSyncRequest (NOT YET IMPLEMENTED)                    ││
│  │      │──────────────────────►│ (to leader, should get manager state)    ││
│  │      │                       │                                           ││
│  │      │ ④ TCP: ManagerStateSnapshot (NOT YET IMPLEMENTED)                ││
│  │      │◄──────────────────────│                                           ││
│  │      │                       │                                           ││
│  │      │ Apply state snapshot  │                                           ││
│  │      │ Verify consistency    │                                           ││
│  │      │                       │                                           ││
│  │      │ STATE: ACTIVE         │                                           ││
│  │      │ (counted in quorum)   │                                           ││
│  │      │                       │                                           ││
│  │      │      ════════════════════════════════════                         ││
│  │      │         New manager now participates in quorum                    ││
│  │      │         (n/2 + 1 threshold recalculated)                         ││
│  │      │                       │                                           ││
│  │                                                                          ││
│  └─────────────────────────────────────────────────────────────────────────┘│
│                                                                              │
│  REMOVING A MANAGER (GRACEFUL):                                              │
│                                                                              │
│  ┌─────────────────────────────────────────────────────────────────────────┐│
│  │                                                                          ││
│  │   Leaving Manager           Other Managers                               ││
│  │      │                          │                                        ││
│  │      │ ① STATE: LEAVING         │                                        ││
│  │      │                          │                                        ││
│  │      │ If leader:               │                                        ││
│  │      │ ② Trigger pre-vote for   │                                        ││
│  │      │    new leader            │                                        ││
│  │      │──────────────────────────►│                                        ││
│  │      │                          │                                        ││
│  │      │ ③ Wait for new leader    │                                        ││
│  │      │◄──────────────────────────│                                        ││
│  │      │                          │                                        ││
│  │      │ ④ Confirm pending work   │                                        ││
│  │      │    completes or transfers│                                        ││
│  │      │                          │                                        ││
│  │      │ ⑤ UDP: Leave message     │                                        ││
│  │      │──────────────────────────►│                                        ││
│  │      │                          │                                        ││
│  │      │                          │ Recalculate quorum                     ││
│  │      │                          │ (new work uses new quorum)             ││
│  │      │                          │                                        ││
│  │      ╳ Shutdown                 │                                        ││
│  │                                                                          ││
│  │   Note: In-flight work uses original quorum until completion            ││
│  │                                                                          ││
│  └─────────────────────────────────────────────────────────────────────────┘│
│                                                                              │
│  ADDING A GATE:                                                              │
│                                                                              │
│  ┌─────────────────────────────────────────────────────────────────────────┐│
│  │                                                                          ││
│  │   New Gate              Existing Gates                                   ││
│  │      │                       │                                           ││
│  │      │ ① UDP: Join SWIM cluster                                         ││
│  │      │──────────────────────►│                                           ││
│  │      │                       │                                           ││
│  │      │ ② TCP: StateSyncRequest                                          ││
│  │      │──────────────────────►│ (to leader)                               ││
│  │      │                       │                                           ││
│  │      │ ③ TCP: GlobalJobStatus[]│                                         ││
│  │      │◄──────────────────────│ + DatacenterLease[]                       ││
│  │      │                       │                                           ││
│  │      │ Apply state           │                                           ││
│  │      │                       │                                           ││
│  │      │ STATE: ACTIVE         │                                           ││
│  │      │ (can become leader)   │                                           ││
│  │      │                       │                                           ││
│  └─────────────────────────────────────────────────────────────────────────┘│
│                                                                              │
│  REMOVING A GATE (GRACEFUL):                                                 │
│                                                                              │
│  ┌─────────────────────────────────────────────────────────────────────────┐│
│  │                                                                          ││
│  │   Leaving Gate (★)          Other Gates                                  ││
│  │      │                          │                                        ││
│  │      │ ① Transfer leases        │                                        ││
│  │      │    to new leader         │                                        ││
│  │      │──────────────────────────►│                                        ││
│  │      │                          │                                        ││
│  │      │ ② LeaseTransfer ack      │                                        ││
│  │      │◄──────────────────────────│                                        ││
│  │      │                          │                                        ││
│  │      │ ③ Update registry        │                                        ││
│  │      │    (clients should       │                                        ││
│  │      │    reconnect to new gate)│                                        ││
│  │      │                          │                                        ││
│  │      │ ④ UDP: Leave message     │                                        ││
│  │      │──────────────────────────►│                                        ││
│  │      │                          │                                        ││
│  │      ╳ Shutdown                 │                                        ││
│  │                                                                          ││
│  └─────────────────────────────────────────────────────────────────────────┘│
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

## State Management

### Versioned Lamport Clock

```
┌─────────────────────────────────────────────────────────────────┐
│                  VERSIONED STATE CLOCK                           │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  Purpose: Reject stale updates from workers/managers             │
│                                                                  │
│  VersionedStateClock {                                           │
│    _entity_versions: dict[str, tuple[int, float]]               │
│    # entity_id → (last_version, last_update_time)               │
│  }                                                               │
│                                                                  │
│  Operations:                                                     │
│  ┌───────────────────────────────────────────────────────────┐  │
│  │ is_entity_stale(entity_id, incoming_version) -> bool      │  │
│  │   • True if incoming_version <= tracked version           │  │
│  │   • False if incoming_version > tracked version           │  │
│  │                                                            │  │
│  │ update_entity(entity_id, new_version) -> None             │  │
│  │   • Updates tracked version if new > current              │  │
│  │   • Records update timestamp                              │  │
│  │                                                            │  │
│  │ should_accept_update(entity_id, version) -> bool          │  │
│  │   • Combined check + update in one atomic operation       │  │
│  │                                                            │  │
│  │ cleanup_old_entities(max_age: float) -> None              │  │
│  │   • Remove entities not updated for > max_age seconds     │  │
│  └───────────────────────────────────────────────────────────┘  │
│                                                                  │
│  Usage:                                                          │
│  ┌───────────────────────────────────────────────────────────┐  │
│  │ # In Manager, receiving WorkerHeartbeat:                  │  │
│  │ if self._versioned_clock.is_entity_stale(                 │  │
│  │     heartbeat.node_id, heartbeat.version                  │  │
│  │ ):                                                        │  │
│  │     return  # Discard stale update                        │  │
│  │                                                            │  │
│  │ # Accept update                                           │  │
│  │ self._worker_status[heartbeat.node_id] = heartbeat        │  │
│  │ self._versioned_clock.update_entity(                      │  │
│  │     heartbeat.node_id, heartbeat.version                  │  │
│  │ )                                                         │  │
│  └───────────────────────────────────────────────────────────┘  │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

### Per-Core Workflow Assignment

```
┌─────────────────────────────────────────────────────────────────┐
│                  PER-CORE WORKFLOW TRACKING                      │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  Worker State:                                                   │
│  ┌───────────────────────────────────────────────────────────┐  │
│  │ _total_cores: int = os.cpu_count()                        │  │
│  │ _available_cores: int (computed)                          │  │
│  │                                                            │  │
│  │ _core_assignments: dict[int, str | None]                  │  │
│  │   # core_index → workflow_id (or None if free)            │  │
│  │   {0: None, 1: "wf-123", 2: "wf-123", 3: None, ...}       │  │
│  │                                                            │  │
│  │ _workflow_cores: dict[str, list[int]]                     │  │
│  │   # workflow_id → [core_indices]                          │  │
│  │   {"wf-123": [1, 2], "wf-456": [5, 6, 7]}                │  │
│  └───────────────────────────────────────────────────────────┘  │
│                                                                  │
│  Operations:                                                     │
│  ┌───────────────────────────────────────────────────────────┐  │
│  │ _allocate_cores(workflow_id, num_cores) -> list[int]      │  │
│  │   • Find num_cores free cores                             │  │
│  │   • Update _core_assignments                              │  │
│  │   • Update _workflow_cores                                │  │
│  │   • Return allocated core indices                         │  │
│  │                                                            │  │
│  │ _free_cores(workflow_id) -> None                          │  │
│  │   • Look up cores in _workflow_cores                      │  │
│  │   • Mark all as None in _core_assignments                 │  │
│  │   • Remove from _workflow_cores                           │  │
│  │                                                            │  │
│  │ stop_workflows_on_cores(core_indices) -> list[str]        │  │
│  │   • Hierarchical stop for specific cores                  │  │
│  │   • Returns workflow_ids that were cancelled              │  │
│  └───────────────────────────────────────────────────────────┘  │
│                                                                  │
│  Reported in WorkflowProgress.assigned_cores for visibility     │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

---

## Security

### Encryption & Authentication

```
┌─────────────────────────────────────────────────────────────────┐
│                   SECURITY ARCHITECTURE                          │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  AES-256-GCM Encryption:                                         │
│  ┌───────────────────────────────────────────────────────────┐  │
│  │ • HKDF key derivation from shared secret                  │  │
│  │ • Per-message salt (never reuse nonces)                   │  │
│  │ • Key rotation via MERCURY_SYNC_AUTH_SECRET_PREVIOUS      │  │
│  │ • Weak secret detection and rejection                     │  │
│  └───────────────────────────────────────────────────────────┘  │
│                                                                  │
│  Replay Protection:                                              │
│  ┌───────────────────────────────────────────────────────────┐  │
│  │ • Snowflake IDs with embedded timestamps                  │  │
│  │ • Sliding window detection (configurable)                 │  │
│  │ • Rejects duplicate and stale messages                    │  │
│  └───────────────────────────────────────────────────────────┘  │
│                                                                  │
│  Rate Limiting:                                                  │
│  ┌───────────────────────────────────────────────────────────┐  │
│  │ • Token bucket per source address                         │  │
│  │ • Configurable tokens and refill rate                     │  │
│  │ • Prevents DoS from flooding                              │  │
│  └───────────────────────────────────────────────────────────┘  │
│                                                                  │
│  Message Size Limits:                                            │
│  ┌───────────────────────────────────────────────────────────┐  │
│  │ • MAX_MESSAGE_SIZE: 1MB (compressed)                      │  │
│  │ • MAX_DECOMPRESSED_SIZE: 50MB                             │  │
│  │ • Compression bomb detection (max ratio: 100x)            │  │
│  │ • Large enough for cloudpickled workflow classes          │  │
│  └───────────────────────────────────────────────────────────┘  │
│                                                                  │
│  Serialization Security:                                         │
│  ┌───────────────────────────────────────────────────────────┐  │
│  │ • RestrictedUnpickler with explicit allowlist             │  │
│  │ • Blocks dangerous modules (os, subprocess, sys)          │  │
│  │ • Allows hyperscale.*, cloudpickle, and dependencies      │  │
│  │ • Sanitized error responses (no stack traces)             │  │
│  └───────────────────────────────────────────────────────────┘  │
│                                                                  │
│  TLS Configuration:                                              │
│  ┌───────────────────────────────────────────────────────────┐  │
│  │ • MERCURY_SYNC_TLS_VERIFY_HOSTNAME: true/false            │  │
│  │ • Certificate-based authentication available              │  │
│  │ • Configurable for local vs production environments       │  │
│  └───────────────────────────────────────────────────────────┘  │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

---

## Module Structure

```
hyperscale/distributed_rewrite/
├── README.md                 # This documentation
│
├── nodes/                    # Node implementations
│   ├── worker.py            # WorkerServer
│   ├── manager.py           # ManagerServer
│   └── gate.py              # GateServer
│
├── models/                   # Data models
│   ├── distributed.py       # Distributed message types
│   ├── message.py           # Base Message class
│   ├── restricted_unpickler.py  # Security: allowlist unpickler
│   └── ...
│
├── swim/                     # SWIM + Lifeguard protocol
│   ├── udp_server.py        # Base SWIM server
│   ├── core/                # Core types and utilities
│   │   ├── state_embedder.py   # Serf-style heartbeat embedding
│   │   ├── node_id.py          # Node identification
│   │   ├── errors.py           # Error hierarchy
│   │   ├── error_handler.py    # Circuit breakers, recovery
│   │   ├── metrics.py          # Protocol metrics
│   │   ├── audit.py            # Membership audit log
│   │   └── ...
│   ├── detection/           # Failure detection
│   │   ├── incarnation_tracker.py
│   │   ├── suspicion_manager.py
│   │   ├── indirect_probe_manager.py
│   │   └── probe_scheduler.py
│   ├── gossip/              # Gossip protocol
│   │   ├── gossip_buffer.py
│   │   └── piggyback_update.py
│   ├── health/              # Health monitoring
│   │   ├── local_health_multiplier.py
│   │   ├── health_monitor.py
│   │   └── graceful_degradation.py
│   └── leadership/          # Leader election
│       ├── local_leader_election.py
│       ├── leader_eligibility.py
│       ├── leader_state.py
│       └── flapping_detector.py
│
├── server/                   # Base server infrastructure
│   ├── server/
│   │   ├── mercury_sync_base_server.py
│   │   └── mercury_sync_server.py
│   ├── protocol/            # Network protocols
│   │   ├── mercury_sync_tcp_protocol.py
│   │   ├── mercury_sync_udp_protocol.py
│   │   └── security.py      # ReplayGuard, RateLimiter
│   ├── hooks/               # Decorators for TCP/UDP
│   │   ├── tcp/
│   │   └── udp/
│   ├── events/              # Logical clocks
│   │   ├── lamport_clock.py
│   │   └── versioned_state_clock.py
│   └── context/
│
├── taskex/                   # Task execution
│   ├── task_runner.py       # Async task management
│   ├── task.py
│   └── snowflake/           # ID generation
│
├── encryption/               # Cryptography
│   └── aes_gcm.py           # AESGCMFernet with key rotation
│
└── env/                      # Configuration
    └── env.py               # Environment variables
```

---

## Configuration

### Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `MERCURY_SYNC_AUTH_SECRET` | (required) | Shared secret for encryption (min 16 chars) |
| `MERCURY_SYNC_AUTH_SECRET_PREVIOUS` | None | Previous secret for key rotation |
| `MERCURY_SYNC_TLS_VERIFY_HOSTNAME` | `true` | TLS hostname verification |
| `MERCURY_SYNC_CLEANUP_INTERVAL` | `30s` | Background cleanup interval |
| `MERCURY_SYNC_TASK_RUNNER_MAX_THREADS` | 4 | TaskRunner thread pool size |

### Node Configuration

```python
# Worker example
worker = WorkerServer(
    host="0.0.0.0",
    tcp_port=8001,
    udp_port=8002,
    env=Env(),
    dc_id="us-east-1",
    manager_addrs=[("manager1.local", 9001)],
)

# Manager example
manager = ManagerServer(
    host="0.0.0.0",
    tcp_port=9001,
    udp_port=9002,
    env=Env(),
    dc_id="us-east-1",
    gate_addrs=[("gate1.local", 10001)],
    manager_peers=[("manager2.local", 9001)],
    quorum_timeout=5.0,
    max_workflow_retries=3,
)

# Gate example
gate = GateServer(
    host="0.0.0.0",
    tcp_port=10001,
    udp_port=10002,
    env=Env(),
    dc_id="global",
    datacenter_managers={
        "us-east-1": [("manager1.us-east.local", 9001)],
        "eu-west-1": [("manager1.eu-west.local", 9001)],
    },
)
```

---

## Message Protocol Reference

### TCP Messages (Data Transfer)

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                         TCP MESSAGE TYPES                                    │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  ┌────────────────────────────────────────────────────────────────────────┐ │
│  │ JOB LIFECYCLE MESSAGES                                                 │ │
│  ├────────────────────────────────────────────────────────────────────────┤ │
│  │                                                                         │ │
│  │  JobSubmission                                                          │ │
│  │  ├─ job_id: str                    # Unique job identifier              │ │
│  │  ├─ workflows: bytes               # Cloudpickled Workflow classes      │ │
│  │  ├─ vus: int                       # Cores per workflow                 │ │
│  │  ├─ timeout_seconds: float         # Max execution time                 │ │
│  │  ├─ datacenter_count: int = 1      # Target DC count (gates only)       │ │
│  │  └─ datacenters: list[str] = []    # Specific DCs (empty = auto)        │ │
│  │                                                                         │ │
│  │  JobAck                                                                  │ │
│  │  ├─ job_id: str                    # Job identifier                     │ │
│  │  ├─ accepted: bool                 # Whether accepted                   │ │
│  │  ├─ error: str | None = None       # Error if rejected                  │ │
│  │  └─ queued_position: int = 0       # Queue position                     │ │
│  │                                                                         │ │
│  │  CancelJob                                                               │ │
│  │  ├─ job_id: str                    # Job to cancel                      │ │
│  │  ├─ reason: str = ""               # Cancellation reason                │ │
│  │  └─ fence_token: int = 0           # Fencing token                      │ │
│  │                                                                         │ │
│  │  CancelAck                                                               │ │
│  │  ├─ job_id: str                    # Job identifier                     │ │
│  │  ├─ cancelled: bool                # Success                            │ │
│  │  ├─ workflows_cancelled: int = 0   # Count stopped                      │ │
│  │  └─ error: str | None = None       # Error if failed                    │ │
│  │                                                                         │ │
│  └────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
│  ┌────────────────────────────────────────────────────────────────────────┐ │
│  │ WORKFLOW DISPATCH MESSAGES                                             │ │
│  ├────────────────────────────────────────────────────────────────────────┤ │
│  │                                                                         │ │
│  │  WorkflowDispatch                                                        │ │
│  │  ├─ job_id: str                    # Parent job                         │ │
│  │  ├─ workflow_id: str               # Unique workflow instance           │ │
│  │  ├─ workflow: bytes                # Cloudpickled Workflow class        │ │
│  │  ├─ context: bytes                 # Cloudpickled context dict          │ │
│  │  ├─ vus: int                       # Cores to use                       │ │
│  │  ├─ timeout_seconds: float         # Execution timeout                  │ │
│  │  └─ fence_token: int               # At-most-once fencing               │ │
│  │                                                                         │ │
│  │  WorkflowDispatchAck                                                     │ │
│  │  ├─ workflow_id: str               # Workflow identifier                │ │
│  │  ├─ accepted: bool                 # Whether accepted                   │ │
│  │  ├─ error: str | None = None       # Error if rejected                  │ │
│  │  └─ cores_assigned: int = 0        # Actual cores                       │ │
│  │                                                                         │ │
│  └────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
│  ┌────────────────────────────────────────────────────────────────────────┐ │
│  │ PROGRESS & STATUS MESSAGES                                             │ │
│  ├────────────────────────────────────────────────────────────────────────┤ │
│  │                                                                         │ │
│  │  StepStats                                                               │ │
│  │  ├─ step_name: str                 # Step method name                   │ │
│  │  ├─ completed_count: int = 0       # Successful executions              │ │
│  │  ├─ failed_count: int = 0          # Failed executions                  │ │
│  │  └─ total_count: int = 0           # Total attempts                     │ │
│  │                                                                         │ │
│  │  WorkflowProgress                                                        │ │
│  │  ├─ job_id: str                    # Parent job                         │ │
│  │  ├─ workflow_id: str               # Workflow instance                  │ │
│  │  ├─ workflow_name: str             # Workflow class name                │ │
│  │  ├─ status: str                    # WorkflowStatus value               │ │
│  │  ├─ completed_count: int           # Actions completed                  │ │
│  │  ├─ failed_count: int              # Actions failed                     │ │
│  │  ├─ rate_per_second: float         # Current rate                       │ │
│  │  ├─ elapsed_seconds: float         # Time since start                   │ │
│  │  ├─ step_stats: list[StepStats]    # Per-step breakdown                 │ │
│  │  ├─ timestamp: float = 0.0         # Monotonic timestamp                │ │
│  │  └─ assigned_cores: list[int] = [] # Core indices                       │ │
│  │                                                                         │ │
│  │  JobProgress                                                             │ │
│  │  ├─ job_id: str                    # Job identifier                     │ │
│  │  ├─ datacenter: str                # Reporting DC                       │ │
│  │  ├─ status: str                    # JobStatus value                    │ │
│  │  ├─ workflows: list[WorkflowProgress]  # Per-workflow                   │ │
│  │  ├─ total_completed: int = 0       # Total actions                      │ │
│  │  ├─ total_failed: int = 0          # Total failed                       │ │
│  │  ├─ overall_rate: float = 0.0      # Aggregate rate                     │ │
│  │  ├─ elapsed_seconds: float = 0.0   # Job runtime                        │ │
│  │  └─ timestamp: float = 0.0         # Monotonic timestamp                │ │
│  │                                                                         │ │
│  │  GlobalJobStatus                                                         │ │
│  │  ├─ job_id: str                    # Job identifier                     │ │
│  │  ├─ status: str                    # JobStatus value                    │ │
│  │  ├─ datacenters: list[JobProgress] # Per-DC progress                    │ │
│  │  ├─ total_completed: int = 0       # Global total                       │ │
│  │  ├─ total_failed: int = 0          # Global failed                      │ │
│  │  ├─ overall_rate: float = 0.0      # Global rate                        │ │
│  │  ├─ elapsed_seconds: float = 0.0   # Since submission                   │ │
│  │  ├─ completed_datacenters: int = 0 # DCs finished                       │ │
│  │  └─ failed_datacenters: int = 0    # DCs failed                         │ │
│  │                                                                         │ │
│  └────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
│  ┌────────────────────────────────────────────────────────────────────────┐ │
│  │ QUORUM & PROVISIONING MESSAGES                                         │ │
│  ├────────────────────────────────────────────────────────────────────────┤ │
│  │                                                                         │ │
│  │  ProvisionRequest                                                        │ │
│  │  ├─ job_id: str                    # Job identifier                     │ │
│  │  ├─ workflow_id: str               # Workflow to provision              │ │
│  │  ├─ target_worker: str             # Selected worker node_id            │ │
│  │  ├─ cores_required: int            # Cores needed                       │ │
│  │  ├─ fence_token: int               # Fencing token                      │ │
│  │  └─ version: int                   # State version                      │ │
│  │                                                                         │ │
│  │  ProvisionConfirm                                                        │ │
│  │  ├─ job_id: str                    # Job identifier                     │ │
│  │  ├─ workflow_id: str               # Workflow                           │ │
│  │  ├─ confirming_node: str           # Confirming manager                 │ │
│  │  ├─ confirmed: bool                # Whether confirmed                  │ │
│  │  ├─ version: int                   # Node's version                     │ │
│  │  └─ error: str | None = None       # Error if not confirmed             │ │
│  │                                                                         │ │
│  │  ProvisionCommit                                                         │ │
│  │  ├─ job_id: str                    # Job identifier                     │ │
│  │  ├─ workflow_id: str               # Workflow                           │ │
│  │  ├─ target_worker: str             # Final worker                       │ │
│  │  ├─ cores_assigned: int            # Cores allocated                    │ │
│  │  ├─ fence_token: int               # Fencing token                      │ │
│  │  └─ committed_version: int         # Version at commit                  │ │
│  │                                                                         │ │
│  └────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
│  ┌────────────────────────────────────────────────────────────────────────┐ │
│  │ STATE SYNC MESSAGES                                                    │ │
│  ├────────────────────────────────────────────────────────────────────────┤ │
│  │                                                                         │ │
│  │  StateSyncRequest                                                        │ │
│  │  ├─ requester_id: str              # Requesting node                    │ │
│  │  ├─ requester_role: str            # NodeRole value                     │ │
│  │  └─ since_version: int = 0         # Only updates after this            │ │
│  │                                                                         │ │
│  │  StateSyncResponse                                                       │ │
│  │  ├─ responder_id: str              # Responding node                    │ │
│  │  ├─ current_version: int           # Current state version              │ │
│  │  ├─ worker_state: WorkerStateSnapshot | None  # If worker               │ │
│  │  └─ manager_state: ManagerStateSnapshot | None # If manager             │ │
│  │                                                                         │ │
│  │  WorkerStateSnapshot                                                     │ │
│  │  ├─ node_id: str                   # Worker identifier                  │ │
│  │  ├─ state: str                     # WorkerState value                  │ │
│  │  ├─ total_cores: int               # Total cores                        │ │
│  │  ├─ available_cores: int           # Free cores                         │ │
│  │  ├─ version: int                   # State version                      │ │
│  │  └─ active_workflows: dict[str, WorkflowProgress]                       │ │
│  │                                                                         │ │
│  │  ManagerStateSnapshot                                                    │ │
│  │  ├─ node_id: str                   # Manager identifier                 │ │
│  │  ├─ datacenter: str                # Datacenter                         │ │
│  │  ├─ is_leader: bool                # Leadership status                  │ │
│  │  ├─ term: int                      # Current term                       │ │
│  │  ├─ version: int                   # State version                      │ │
│  │  ├─ workers: list[WorkerStateSnapshot]  # Registered workers            │ │
│  │  └─ jobs: dict[str, JobProgress]   # Active jobs                        │ │
│  │                                                                         │ │
│  └────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
│  ┌────────────────────────────────────────────────────────────────────────┐ │
│  │ LEASE MESSAGES (Gates only)                                            │ │
│  ├────────────────────────────────────────────────────────────────────────┤ │
│  │                                                                         │ │
│  │  DatacenterLease                                                         │ │
│  │  ├─ job_id: str                    # Job identifier                     │ │
│  │  ├─ datacenter: str                # Datacenter holding lease           │ │
│  │  ├─ lease_holder: str              # Gate node_id                       │ │
│  │  ├─ fence_token: int               # Fencing token                      │ │
│  │  ├─ expires_at: float              # Monotonic expiration               │ │
│  │  └─ version: int                   # Lease version                      │ │
│  │                                                                         │ │
│  │  LeaseTransfer                                                           │ │
│  │  ├─ job_id: str                    # Job identifier                     │ │
│  │  ├─ datacenter: str                # Datacenter                         │ │
│  │  ├─ from_gate: str                 # Current holder                     │ │
│  │  ├─ to_gate: str                   # New holder                         │ │
│  │  ├─ new_fence_token: int           # New fencing token                  │ │
│  │  └─ version: int                   # Transfer version                   │ │
│  │                                                                         │ │
│  └────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

### UDP Messages (SWIM Protocol)

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                         UDP MESSAGE TYPES                                    │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  ┌────────────────────────────────────────────────────────────────────────┐ │
│  │ PROBE MESSAGES                                                         │ │
│  ├────────────────────────────────────────────────────────────────────────┤ │
│  │                                                                         │ │
│  │  Format: message_type>target_host:target_port[#base64_state]            │ │
│  │                                                                         │ │
│  │  probe>192.168.1.10:8001                                                │ │
│  │  └───┬─┘└────────────────┘                                              │ │
│  │      │        │                                                         │ │
│  │      │        └─ Target address                                         │ │
│  │      └─ Message type                                                    │ │
│  │                                                                         │ │
│  │  ack>192.168.1.5:8000#eyJub2RlX2lkIjoiLi4uIn0=                          │ │
│  │  └─┬┘└──────────────┘ └────────────────────────┘                        │ │
│  │    │        │                    │                                      │ │
│  │    │        │                    └─ Base64-encoded embedded state       │ │
│  │    │        └─ Sender address                                           │ │
│  │    └─ Message type                                                      │ │
│  │                                                                         │ │
│  │  ping-req>192.168.1.15:8002                                             │ │
│  │  └──────┘ (indirect probe via proxy node)                               │ │
│  │                                                                         │ │
│  └────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
│  ┌────────────────────────────────────────────────────────────────────────┐ │
│  │ MEMBERSHIP MESSAGES                                                    │ │
│  ├────────────────────────────────────────────────────────────────────────┤ │
│  │                                                                         │ │
│  │  join>192.168.1.5:8000                                                  │ │
│  │  └──┘ (request to join cluster)                                         │ │
│  │                                                                         │ │
│  │  leave>192.168.1.5:8000                                                 │ │
│  │  └───┘ (graceful departure)                                             │ │
│  │                                                                         │ │
│  │  alive:5>192.168.1.5:8000                                               │ │
│  │  └───┘ │ (refutation with incarnation 5)                                │ │
│  │        │                                                                │ │
│  │        └─ Incarnation number                                            │ │
│  │                                                                         │ │
│  │  suspect:3>192.168.1.10:8001                                            │ │
│  │  └─────┘ │ (suspicion with incarnation 3)                               │ │
│  │          │                                                              │ │
│  │          └─ Target node's last known incarnation                        │ │
│  │                                                                         │ │
│  │  dead:3>192.168.1.10:8001                                               │ │
│  │  └──┘ (node marked dead after suspicion expired)                        │ │
│  │                                                                         │ │
│  └────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
│  ┌────────────────────────────────────────────────────────────────────────┐ │
│  │ LEADERSHIP MESSAGES                                                    │ │
│  ├────────────────────────────────────────────────────────────────────────┤ │
│  │                                                                         │ │
│  │  pre-vote:5>192.168.1.5:8000                                            │ │
│  │  └──────┘ │ (pre-vote request for term 5)                               │ │
│  │           │                                                             │ │
│  │           └─ Proposed term                                              │ │
│  │                                                                         │ │
│  │  pre-vote-response:5:true>192.168.1.10:8001                             │ │
│  │                    │  │                                                 │ │
│  │                    │  └─ Granted (true/false)                           │ │
│  │                    └─ Term                                              │ │
│  │                                                                         │ │
│  │  vote-req:6>192.168.1.5:8000                                            │ │
│  │  └──────┘ (vote request for term 6)                                     │ │
│  │                                                                         │ │
│  │  vote-response:6:true>192.168.1.10:8001                                 │ │
│  │  (vote granted for term 6)                                              │ │
│  │                                                                         │ │
│  │  leader:6>192.168.1.5:8000                                              │ │
│  │  └────┘ (leader announcement for term 6)                                │ │
│  │                                                                         │ │
│  │  heartbeat:6>192.168.1.5:8000                                           │ │
│  │  └───────┘ (leader heartbeat for term 6)                                │ │
│  │                                                                         │ │
│  └────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
│  ┌────────────────────────────────────────────────────────────────────────┐ │
│  │ GOSSIP PIGGYBACK FORMAT                                                │ │
│  ├────────────────────────────────────────────────────────────────────────┤ │
│  │                                                                         │ │
│  │  Piggybacked updates are appended to messages:                          │ │
│  │                                                                         │ │
│  │  ack>192.168.1.5:8000|J:192.168.1.20:8003:0|A:192.168.1.10:8001:5       │ │
│  │                     └─────────────────────────────────────────────┘     │ │
│  │                                         │                               │ │
│  │                           Piggybacked gossip updates                    │ │
│  │                                                                         │ │
│  │  Update format: TYPE:HOST:PORT:INCARNATION                              │ │
│  │                                                                         │ │
│  │  Types:                                                                  │ │
│  │  • J = JOIN (highest priority)                                          │ │
│  │  • L = LEAVE                                                            │ │
│  │  • A = ALIVE                                                            │ │
│  │  • S = SUSPECT                                                          │ │
│  │  • D = DEAD (lowest priority)                                           │ │
│  │                                                                         │ │
│  │  Priority ensures important updates propagate first when space limited  │ │
│  │                                                                         │ │
│  └────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
│  ┌────────────────────────────────────────────────────────────────────────┐ │
│  │ EMBEDDED STATE (Serf-style Heartbeats)                                 │ │
│  ├────────────────────────────────────────────────────────────────────────┤ │
│  │                                                                         │ │
│  │  State embedded in ack responses after '#' separator:                   │ │
│  │                                                                         │ │
│  │  ack>192.168.1.5:8000#eyJub2RlX2lkIjogIndvcmtlci0xIiwgLi4ufQ==          │ │
│  │                      └────────────────────────────────────────┘         │ │
│  │                                   Base64(cloudpickle(Heartbeat))        │ │
│  │                                                                         │ │
│  │  WorkerHeartbeat (embedded by workers):                                 │ │
│  │  ├─ node_id: str                                                        │ │
│  │  ├─ state: str                   # HEALTHY|DEGRADED|DRAINING|OFFLINE    │ │
│  │  ├─ available_cores: int                                                │ │
│  │  ├─ queue_depth: int                                                    │ │
│  │  ├─ cpu_percent: float                                                  │ │
│  │  ├─ memory_percent: float                                               │ │
│  │  ├─ version: int                                                        │ │
│  │  └─ active_workflows: dict[str, str]  # workflow_id → status            │ │
│  │                                                                         │ │
│  │  ManagerHeartbeat (embedded by managers):                               │ │
│  │  ├─ node_id: str                                                        │ │
│  │  ├─ datacenter: str                                                     │ │
│  │  ├─ is_leader: bool                                                     │ │
│  │  ├─ term: int                                                           │ │
│  │  ├─ version: int                                                        │ │
│  │  ├─ active_jobs: int                                                    │ │
│  │  ├─ active_workflows: int                                               │ │
│  │  ├─ worker_count: int                                                   │ │
│  │  └─ available_cores: int                                                │ │
│  │                                                                         │ │
│  └────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Enums Reference

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                           ENUM VALUES                                        │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  NodeRole           │ Description                                           │
│  ───────────────────┼────────────────────────────────────────────────────── │
│  GATE               │ Cross-DC coordination node                            │
│  MANAGER            │ Datacenter workflow orchestrator                      │
│  WORKER             │ Workflow execution node                               │
│                                                                              │
│  ───────────────────────────────────────────────────────────────────────────│
│                                                                              │
│  JobStatus          │ Description                                           │
│  ───────────────────┼────────────────────────────────────────────────────── │
│  SUBMITTED          │ Job received, not yet dispatched                      │
│  QUEUED             │ Waiting for resources                                 │
│  DISPATCHING        │ Workflows being sent to workers                       │
│  RUNNING            │ Active execution                                      │
│  COMPLETING         │ Gathering final results                               │
│  COMPLETED          │ Successfully finished                                 │
│  FAILED             │ Failed (max retries exhausted)                        │
│  CANCELLED          │ User cancelled                                        │
│  TIMEOUT            │ Exceeded timeout_seconds                              │
│                                                                              │
│  ───────────────────────────────────────────────────────────────────────────│
│                                                                              │
│  WorkflowStatus     │ Description                                           │
│  ───────────────────┼────────────────────────────────────────────────────── │
│  PENDING            │ Not yet started                                       │
│  ASSIGNED           │ Sent to worker, awaiting ack                          │
│  RUNNING            │ Executing on worker                                   │
│  COMPLETED          │ Finished successfully                                 │
│  FAILED             │ Failed                                                │
│  CANCELLED          │ Cancelled                                             │
│                                                                              │
│  ───────────────────────────────────────────────────────────────────────────│
│                                                                              │
│  WorkerState        │ Description                                           │
│  ───────────────────┼────────────────────────────────────────────────────── │
│  HEALTHY            │ Normal operation, accepts work                        │
│  DEGRADED           │ High load, accepts with backpressure                  │
│  DRAINING           │ Not accepting new work                                │
│  OFFLINE            │ Not responding / shutdown                             │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

## Known Limitations & Future Work

### New Manager Join Process (✅ Implemented)

New managers join the cluster in a SYNCING state before becoming ACTIVE:

**Implementation**:
1. New manager joins SWIM cluster → State = SYNCING
2. SYNCING managers are NOT counted in quorum (`_has_quorum_available()` returns false)
3. Manager starts leader election
4. If leader: immediately transitions to ACTIVE (syncs state via `_on_manager_become_leader`)
5. If not leader: requests state sync from current leader via `_complete_startup_sync()`
6. After sync completes (or times out): State = ACTIVE → now counted in quorum

**Key Components**:
- `ManagerState` enum: SYNCING, ACTIVE, DRAINING
- `_manager_state` field tracks current state
- `ManagerHeartbeat.state` field broadcasts state to peers
- `_complete_startup_sync()` handles non-leader state sync on startup
- `_has_quorum_available()` excludes SYNCING managers from quorum count

### Quorum Timeout Handling (✅ Implemented)

When quorum cannot be achieved (e.g., too many managers down), operations fail fast with clear errors.

**Implementation**:
- Circuit breaker pattern prevents cascading failures during degraded cluster state
- Three specific quorum error types provide clear diagnostics:
  - `QuorumUnavailableError`: Not enough active managers (structural issue)
  - `QuorumTimeoutError`: Managers available but didn't respond in time
  - `QuorumCircuitOpenError`: Too many recent failures, failing fast
- Circuit breaker settings: Opens after 3 failures in 30s window, recovers after 10s
- `get_quorum_status()` method provides observability into circuit state

**Error Flow**:
1. Check circuit breaker first → `QuorumCircuitOpenError` if OPEN
2. Check if quorum possible → `QuorumUnavailableError` if insufficient managers
3. Attempt quorum → `QuorumTimeoutError` if timeout without enough confirmations
4. Record success/failure for circuit breaker state transitions

### New Gate Join Process (✅ Implemented)

Same pattern as managers - gates join in SYNCING state before becoming ACTIVE:

**Implementation**:
1. New gate joins SWIM cluster → State = SYNCING
2. SYNCING gates are NOT counted in quorum (`_has_quorum_available()` returns false)
3. Gate starts leader election
4. If leader: immediately transitions to ACTIVE
5. If not leader: requests state sync from current leader via `_complete_startup_sync()`
6. After sync completes (or times out): State = ACTIVE → now counted in quorum

**Key Components**:
- `GateState` enum: SYNCING, ACTIVE, DRAINING
- `_gate_state` field tracks current state
- `GateHeartbeat.state` field broadcasts state to peers
- `_complete_startup_sync()` handles non-leader state sync on startup
- `_has_quorum_available()` excludes SYNCING gates from quorum count

### Gate Quorum Timeout Handling (✅ Implemented)

Gates use the same circuit breaker pattern as managers for fail-fast behavior.

**Implementation**:
- `_quorum_circuit` ErrorStats instance tracks failures
- `_quorum_size()` calculates required quorum (majority of gates)
- `_has_quorum_available()` checks gate state and active peer count
- `get_quorum_status()` returns circuit state and gate metrics
- `receive_job_submission()` checks circuit breaker before accepting jobs
- `_dispatch_job_to_datacenters()` records success/failure for circuit breaker

**Job Submission Flow**:
1. Check if leader (only leader accepts jobs)
2. Check circuit breaker state → reject if OPEN
3. Check quorum availability → reject if insufficient active gates
4. Select datacenters and dispatch job
5. Record success/failure for circuit breaker transitions

### Worker ↔ Manager Communication Resilience (✅ Implemented)

All Worker ↔ Manager communication now uses retries with exponential backoff and circuit breakers.

#### Worker → Manager Communication

**Circuit Breaker**:
- `_manager_circuit`: ErrorStats tracking failures to managers
- `_is_manager_circuit_open()`: Check if circuit is open (fail-fast mode)
- `get_manager_circuit_status()`: Observability endpoint
- Settings: Opens after 3 failures in 30s, recovers after 10s

**Registration with Retries**:
```python
_register_with_manager(manager_addr, max_retries=3, base_delay=0.5)
# Delays: 0.5s → 1.0s → 2.0s
# Checks circuit breaker before attempting
# Records success/error for circuit state
```

**Progress Updates with Retries**:
```python
_send_progress_update(progress, max_retries=2, base_delay=0.2)
# Delays: 0.2s → 0.4s (shorter for frequent updates)
# Checks circuit breaker before attempting
# Records success/error for circuit state
```

#### Manager → Worker Communication

**Per-Worker Circuit Breakers**:
- `_worker_circuits: dict[str, ErrorStats]`: One circuit per worker
- `_get_worker_circuit()`: Get or create circuit for a worker
- `_is_worker_circuit_open()`: Check if worker's circuit is open
- `get_worker_circuit_status()`: Status for specific worker
- `get_all_worker_circuit_status()`: Status for all workers

**Worker Selection**:
- `_select_worker_for_workflow()`: Skips workers with open circuits
- `_select_worker_for_workflow_excluding()`: Skips workers with open circuits

**Workflow Dispatch with Retries**:
```python
_dispatch_workflow_to_worker(worker_id, dispatch, max_retries=2, base_delay=0.3)
# Delays: 0.3s → 0.6s
# Checks per-worker circuit before attempting
# Records success/error for per-worker circuit
# Worker rejection (not accepted) does NOT trigger retry
```

**Benefits**:
- Transient network failures are retried automatically
- Persistent failures trigger circuit breaker (fail-fast)
- Per-worker circuits prevent one bad worker from affecting others
- Exponential backoff prevents thundering herd on recovery

### Manager ↔ Gate Communication Resilience (✅ Implemented)

All Manager ↔ Gate communication now uses retries with exponential backoff and circuit breakers.

#### Manager → Gate Communication

**Circuit Breaker**:
- `_gate_circuit`: ErrorStats tracking failures to gates
- `_is_gate_circuit_open()`: Check if circuit is open (fail-fast mode)
- `get_gate_circuit_status()`: Observability endpoint
- Settings: Opens after 3 failures in 30s, recovers after 10s

**Registration with Retries**:
```python
_try_register_with_gate(gate_addr, max_retries=3, base_delay=0.5)
# Delays: 0.5s → 1.0s → 2.0s
# Checks circuit breaker before attempting
# Records success/error for circuit state
# Gate rejection (not accepted) does NOT trigger retry
```

**Job Progress with Retries**:
```python
_send_job_progress_to_gate(job, max_retries=2, base_delay=0.2)
# Delays: 0.2s → 0.4s (shorter for frequent updates)
# Checks circuit breaker before attempting
# Records success/error for circuit state
```

#### Gate → Manager Communication

**Per-Manager Circuit Breakers**:
- `_manager_circuits: dict[tuple[str, int], ErrorStats]`: One circuit per manager
- `_get_manager_circuit()`: Get or create circuit for a manager
- `_is_manager_circuit_open()`: Check if manager's circuit is open
- `get_manager_circuit_status()`: Status for specific manager
- `get_all_manager_circuit_status()`: Status for all managers

**Dispatch with Retries**:
```python
_try_dispatch_to_manager(manager_addr, submission, max_retries=2, base_delay=0.3)
# Delays: 0.3s → 0.6s
# Checks per-manager circuit before attempting
# Records success/error for per-manager circuit
# Manager rejection (not accepted, not busy) does NOT trigger retry
# BUSY response treated as success (job will be queued)
```

**DC-Level Dispatch**:
- `_try_dispatch_to_dc()`: Iterates managers, uses `_try_dispatch_to_manager`
- `_dispatch_job_with_fallback()`: Handles DC-level fallback chain
- Per-manager failures don't affect other managers in same DC
- If all managers in DC fail, tries fallback DCs

**Benefits**:
- Transient network failures retried automatically
- Per-manager circuits prevent one bad manager from affecting others
- DC-level fallback ensures jobs reach healthy DCs
- Exponential backoff prevents thundering herd on recovery

### Client Push Notifications (Implemented)

Client push notifications allow Gates and Managers to push job status updates directly to clients, eliminating the need for polling.

**Architecture**:

```
┌──────────────────────────────────────────────────────────────┐
│                   Push Notification Flow                     │
├──────────────────────────────────────────────────────────────┤
│  1. Client starts a TCP listener                             │
│  2. Client → Gate/Manager: JobSubmission(callback_addr=...)  │
│  3. Gate/Manager stores callback in _job_callbacks           │
│  4. On Tier 1 events (completion/failure):                   │
│     Gate/Manager → Client: JobStatusPush                     │
│  5. On Tier 2 interval (every 2s):                           │
│     Gate/Manager → Client: JobBatchPush                      │
└──────────────────────────────────────────────────────────────┘
```

**Message Types**:

- `JobStatusPush`: Tier 1 immediate updates for critical events (started, completed, failed)
  - `job_id`, `status`, `message`, `total_completed`, `total_failed`, `overall_rate`, `elapsed_seconds`, `is_final`
- `JobBatchPush`: Tier 2 periodic updates with aggregated stats
  - `job_id`, `status`, `step_stats[]`, `total_completed`, `total_failed`, `overall_rate`, `elapsed_seconds`

**JobSubmission Extension**:

```python
@dataclass
class JobSubmission(Message):
    job_id: str
    workflows: bytes
    # ... other fields ...
    callback_addr: tuple[str, int] | None = None  # Optional push callback
```

**Gate Implementation** (`GateServer`):

- `_job_callbacks: dict[str, tuple[str, int]]` - Stores callbacks by job_id
- `_send_immediate_update()` - Pushes `JobStatusPush` on critical events
- `_batch_stats_update()` - Pushes `JobBatchPush` to all callbacks for running jobs

**Manager Implementation** (`ManagerServer`):

- `_job_callbacks: dict[str, tuple[str, int]]` - Stores callbacks by job_id
- `_push_job_status_to_client()` - Pushes `JobStatusPush` on critical events
- `_push_batch_stats_to_clients()` - Pushes `JobBatchPush` periodically
- `_client_batch_push_loop()` - Background loop for Tier 2 updates (only when no gates)
- `_check_job_completion()` - Detects job completion and triggers push

**Client Implementation**:

Clients that want push notifications must implement TCP receivers:

```python
class JobStatusClient(MercurySyncBaseServer):
    @tcp.receive()
    async def receive_job_status_push(self, addr, data, clock_time):
        status = JobStatusPush.load(data)
        # Handle immediate status update
        return b'ok'
    
    @tcp.receive()
    async def receive_job_batch_push(self, addr, data, clock_time):
        batch = JobBatchPush.load(data)
        # Handle batched progress update
        return b'ok'
```

**Behavior**:

- Gate mode: Gates push to clients, managers forward to gates
- Direct mode: Managers push directly to clients (when no gates configured)
- Callbacks are automatically cleaned up when jobs reach final state

---

## Testing

Run the test suite:

```bash
python examples/test_distributed_rewrite.py
```

Current test coverage: 254+ tests covering:
- SWIM protocol (probing, suspicion, gossip)
- Leadership election (pre-voting, flapping)
- State embedding (heartbeat serialization)
- Distributed messages (all message types)
- Worker/Manager/Gate functionality
- State sync with retry mechanisms
- Per-core workflow assignment
- Worker/Manager failure handling
- Manager peer failure/recovery
- Gate split-brain prevention
- CRDTs (GCounter, LWWRegister, LWWMap, JobStatsCRDT)
- Datacenter health classification (HEALTHY/BUSY/DEGRADED/UNHEALTHY)
- Smart dispatch with fallback chain
- Tiered update strategy
- Client push notifications (JobStatusPush, JobBatchPush)
- Gate state management (SYNCING/ACTIVE/DRAINING)
- Gate quorum circuit breaker
- Worker circuit breaker for manager communication
- Worker retries with exponential backoff (registration, progress)
- Manager per-worker circuit breakers
- Manager retries with exponential backoff (workflow dispatch)
- Manager circuit breaker for gate communication
- Manager retries with exponential backoff (gate registration, job progress)
- Gate per-manager circuit breakers
- Gate retries with exponential backoff (manager dispatch)

---

## Manager Workflow Execution Architecture

This section documents how Managers handle workflow execution, mirroring the `RemoteGraphManager` architecture for distributed execution.

### Overview

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                     MANAGER WORKFLOW EXECUTION FLOW                          │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  JobSubmission                                                               │
│       │                                                                      │
│       ▼                                                                      │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │ 1. WORKFLOW CLASSIFICATION                                           │    │
│  │    • Detect test workflows (have HookType.TEST hooks)               │    │
│  │    • Build dependency graph (DependentWorkflow relationships)       │    │
│  │    • Determine execution order (BFS traversal)                      │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│       │                                                                      │
│       ▼                                                                      │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │ 2. PRIORITY-BASED THREAD ALLOCATION                                  │    │
│  │    • Calculate thread range from TOTAL pool (not available)         │    │
│  │    • Use StagePriority.get_worker_allocation_range()                │    │
│  │    • Provisioner.partion_by_priority() returns batches              │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│       │                                                                      │
│       ▼                                                                      │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │ 3. VU PROVISIONING                                                   │    │
│  │    • vus_per_thread = workflow.vus / threads                        │    │
│  │    • Distribute remainder to last thread                            │    │
│  │    • Store workflow_vus: dict[workflow_name, list[int]]            │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│       │                                                                      │
│       ▼                                                                      │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │ 4. CAPACITY CHECK & WORKER SELECTION                                 │    │
│  │    • Check if workers have enough AVAILABLE cores for threads      │    │
│  │    • Select workers via crypto-random (avoid bias)                  │    │
│  │    • If insufficient capacity → queue job (BUSY) or fail (DEGRADED)│    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│       │                                                                      │
│       ▼                                                                      │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │ 5. QUORUM CONFIRMATION & DISPATCH                                    │    │
│  │    • Request quorum confirmation from peer managers                 │    │
│  │    • On quorum: commit provisioning                                 │    │
│  │    • Dispatch WorkflowDispatch to selected workers                  │    │
│  │    • Include context for dependent workflows                        │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│       │                                                                      │
│       ▼                                                                      │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │ 6. EXECUTION & CONTEXT SYNCHRONIZATION                               │    │
│  │    • Workers execute workflows                                      │    │
│  │    • Workers send WorkflowProgress with context updates             │    │
│  │    • Manager syncs context updates to peers                         │    │
│  │    • Dependent workflows receive context from predecessors          │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Step 1: Workflow Classification

A workflow is classified as a **test workflow** if it has at least one hook with `HookType.TEST`. This classification is **critical** because it determines how many CPU cores the workflow receives:

- **Test workflows**: Get cores based on priority (can use up to 100% of pool)
- **Non-test workflows**: Always get 1 core (they don't parallelize load testing)

---

#### How HookType.TEST is Determined

A hook's type is set automatically by the `Hook` class based on the **return type annotation** of the decorated method.

**The Hook Type Decision Tree** (from `hook.py` lines 161-189):

```python
# Simplified logic from Hook.__init__()
if is_test and self.return_type in CallResult.__subclasses__():
    self.hook_type = HookType.TEST        # ← Test action (load testing)
    
elif is_test and self.return_type in CustomResult.__subclasses__():
    self.hook_type = HookType.TEST        # ← Custom test action
    
elif is_check:
    self.hook_type = HookType.CHECK       # ← Validation/assertion
    
elif is_metric:
    self.hook_type = HookType.METRIC      # ← Custom metric collection
    
else:
    self.hook_type = HookType.ACTION      # ← General action (setup/teardown)
```

**Key Insight**: The `@step()` decorator alone does NOT make a test workflow. The **return type** must be a `CallResult` subclass (like `HTTPResponse`, `GraphQLResponse`, etc.) for the hook to become `HookType.TEST`.

---

#### CallResult Subclasses (Test Return Types)

These return types indicate the method is a load test action:

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                     CALLRESULT SUBCLASSES (TEST TYPES)                       │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  HTTP Testing:                                                               │
│  • HTTPResponse      - Standard HTTP response                               │
│  • HTTP2Response     - HTTP/2 response                                      │
│  • HTTP3Response     - HTTP/3 (QUIC) response                               │
│                                                                              │
│  API Testing:                                                                │
│  • GraphQLResponse   - GraphQL query response                               │
│  • GRPCResponse      - gRPC call response                                   │
│                                                                              │
│  Database Testing:                                                           │
│  • MySQLResponse     - MySQL query response                                 │
│  • PostgresResponse  - PostgreSQL query response                            │
│  • MongoDBResponse   - MongoDB operation response                           │
│  • RedisResponse     - Redis command response                               │
│                                                                              │
│  Messaging Testing:                                                          │
│  • KafkaResponse     - Kafka produce/consume response                       │
│  • RabbitMQResponse  - RabbitMQ message response                            │
│                                                                              │
│  WebSocket/Realtime:                                                         │
│  • WebsocketResponse - WebSocket message response                           │
│  • UDPResponse       - UDP packet response                                  │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

#### Complete Example: Test vs Non-Test Workflows

```python
from hyperscale.graph import Workflow, step, action, depends
from hyperscale.testing import URL, HTTPResponse, Headers


class LoadTestWorkflow(Workflow):
    """
    TEST WORKFLOW - Gets multiple cores based on priority.
    
    This is a test workflow because:
    1. Has @step() decorated method
    2. Return type is HTTPResponse (a CallResult subclass)
    3. Calls self.client.http.get() which returns HTTPResponse
    
    Result: HookType.TEST → participates in priority-based core allocation
    """
    vus = 10000           # Virtual users (can be large!)
    duration = "5m"
    priority = "high"     # Optional: LOW, NORMAL, HIGH, EXCLUSIVE, AUTO (default)
    
    @step()
    async def test_api_endpoint(
        self,
        url: URL = 'https://api.example.com/users',
        headers: Headers = {'Authorization': 'Bearer token123'}
    ) -> HTTPResponse:    # ← This return type makes it HookType.TEST
        """Load test the users API endpoint."""
        return await self.client.http.get(url, headers=headers)
    
    @step()
    async def test_post_data(
        self,
        url: URL = 'https://api.example.com/data',
    ) -> HTTPResponse:    # ← Also HookType.TEST
        """Load test data submission."""
        return await self.client.http.post(url, json={"key": "value"})


class SetupWorkflow(Workflow):
    """
    NON-TEST WORKFLOW - Always gets 1 core.
    
    This is NOT a test workflow because:
    1. Uses @action() decorator (not @step())
    2. Return type is None (not a CallResult)
    
    Result: HookType.ACTION → single core, runs sequentially
    """
    vus = 1
    duration = "30s"
    
    @action()
    async def setup_test_data(self) -> None:  # ← None return = HookType.ACTION
        """Prepare test data before load testing."""
        # This runs on a single core
        self.context['api_key'] = 'test-key-123'
        self.context['base_url'] = 'https://api.example.com'


class UtilityWorkflow(Workflow):
    """
    NON-TEST WORKFLOW - @step() with dict return.
    
    This is NOT a test workflow because:
    1. Has @step() decorated method
    2. BUT return type is dict (NOT a CallResult subclass)
    
    Result: HookType.ACTION → single core
    """
    vus = 1000  # VUs don't matter - still gets 1 core
    duration = "1m"
    
    @step()
    async def process_data(self) -> dict:     # ← dict return = HookType.ACTION
        """Process data - not a load test."""
        await asyncio.sleep(0.1)
        return {"processed": True, "count": 100}


@depends('SetupWorkflow')
class DependentLoadTest(Workflow):
    """
    TEST WORKFLOW with dependency.
    
    This workflow:
    1. Waits for SetupWorkflow to complete
    2. Receives context from SetupWorkflow
    3. Is a test workflow (HTTPResponse return)
    """
    vus = 5000
    duration = "3m"
    
    @step()
    async def authenticated_request(
        self,
        url: URL = 'https://api.example.com/protected',
    ) -> HTTPResponse:
        """Use context from SetupWorkflow."""
        api_key = self.context.get('api_key', '')
        return await self.client.http.get(
            url, 
            headers={'X-API-Key': api_key}
        )
```

---

#### Detection Logic in Distributed Manager

```python
def _is_test_workflow(self, workflow) -> bool:
    """
    Determine if a workflow is a test workflow.
    
    A workflow is a test workflow if it has ANY hook with
    hook_type == HookType.TEST. The Hook class sets this
    automatically based on return type annotations.
    """
    import inspect
    from hyperscale.core.hooks import Hook
    
    for name, member in inspect.getmembers(workflow):
        if isinstance(member, Hook) and member.hook_type == HookType.TEST:
            return True
    return False
```

---

#### Core Allocation Summary

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                      CORE ALLOCATION BY WORKFLOW TYPE                        │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  ┌────────────────────────────────────────────────────────────────────────┐ │
│  │ TEST WORKFLOW                                                          │ │
│  │                                                                         │ │
│  │  Detection:                                                             │ │
│  │  • @step() decorator + CallResult return type (HTTPResponse, etc.)     │ │
│  │  • Hook.hook_type == HookType.TEST                                      │ │
│  │                                                                         │ │
│  │  Core Allocation:                                                       │ │
│  │  • Based on workflow.priority (default: AUTO)                          │ │
│  │  • AUTO: 1 to 100% of pool (single workflow gets all cores)            │ │
│  │  • LOW:  1 to 25% of pool                                               │ │
│  │  • NORMAL: 25% to 75% of pool                                           │ │
│  │  • HIGH: 75% to 100% of pool                                            │ │
│  │  • EXCLUSIVE: 100% of pool                                              │ │
│  │                                                                         │ │
│  │  Example: pool=8 cores, priority=NORMAL, vus=50000                      │ │
│  │  → Gets 2-6 cores (25-75% of 8)                                         │ │
│  │  → 50000 VUs distributed across cores (e.g., ~8333 VUs/core)           │ │
│  └────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
│  ┌────────────────────────────────────────────────────────────────────────┐ │
│  │ NON-TEST WORKFLOW                                                       │ │
│  │                                                                         │ │
│  │  Detection:                                                             │ │
│  │  • @step() with non-CallResult return (dict, None, etc.)               │ │
│  │  • @action(), @check(), @metric() decorators                            │ │
│  │  • Hook.hook_type != HookType.TEST                                      │ │
│  │                                                                         │ │
│  │  Core Allocation:                                                       │ │
│  │  • ALWAYS 1 core (regardless of vus, priority, or pool size)           │ │
│  │  • Non-test workflows don't parallelize load testing                   │ │
│  │  • Used for setup, teardown, data processing, etc.                     │ │
│  │                                                                         │ │
│  │  Example: pool=8 cores, vus=10000                                       │ │
│  │  → Gets 1 core (VUs don't affect allocation for non-test)              │ │
│  └────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
│  ⚠️  IMPORTANT: VUs ≠ Cores                                                 │
│      VUs (virtual users) can be 50,000+ and are distributed across cores.   │
│      Core allocation is determined by priority, NOT by VU count.            │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

#### WorkflowDispatch Message Structure

The manager sends both `vus` and `cores` to workers:

```python
@dataclass(slots=True)
class WorkflowDispatch(Message):
    """Dispatch a single workflow to a worker."""
    job_id: str              # Parent job identifier
    workflow_id: str         # Unique workflow instance ID
    workflow: bytes          # Cloudpickled Workflow class
    context: bytes           # Cloudpickled context dict
    vus: int                 # Virtual users (can be 50k+)
    cores: int               # CPU cores to allocate (from priority)
    timeout_seconds: float   # Execution timeout
    fence_token: int         # Fencing token for at-most-once
    context_version: int     # Layer version for staleness detection
    dependency_context: bytes # Context from dependencies
```

Workers allocate `cores` CPU cores and distribute `vus` virtual users across them.

---

### Virtual Users (VUs) and Steps

#### What is a Virtual User (VU)?

A **Virtual User (VU)** represents a single, continuously looping instance of a workflow. Each VU simulates one user performing a sequence of steps repeatedly for the duration of the test.

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                           VIRTUAL USER CONCEPT                               │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  workflow.vus = 10000    →   10,000 simulated users                         │
│  workflow.duration = "5m" →   Each user runs for 5 minutes                  │
│                                                                              │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │  VU #1                                                              │    │
│  │  ┌────────────────────────────────────────────────────────────────┐ │    │
│  │  │  Loop until duration expires:                                   │ │    │
│  │  │    → Execute @step() method 1                                   │ │    │
│  │  │    → Execute @step() method 2                                   │ │    │
│  │  │    → Execute @step() method N                                   │ │    │
│  │  │    → Record metrics (latency, status, etc.)                     │ │    │
│  │  │    → Repeat...                                                  │ │    │
│  │  └────────────────────────────────────────────────────────────────┘ │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│                                                                              │
│  ... × 10,000 concurrent virtual users                                      │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

**VU Distribution Across Cores**:

When a test workflow gets multiple cores (based on priority), VUs are evenly distributed:

```
Example: vus=10000, cores=4

  Core 0: 2,500 VUs running in parallel
  Core 1: 2,500 VUs running in parallel
  Core 2: 2,500 VUs running in parallel
  Core 3: 2,500 VUs running in parallel
  ─────────────────────────────────────
  Total:  10,000 VUs across 4 cores
```

---

#### What is a Step?

A **Step** is an async method decorated with `@step()` that defines a single action in the workflow loop. Steps are the building blocks of load tests.

```python
from hyperscale.graph import Workflow, step
from hyperscale.testing import URL, Headers, HTTPResponse


class APILoadTest(Workflow):
    vus = 5000
    duration = "3m"
    
    @step()
    async def get_users(
        self,
        url: URL = 'https://api.example.com/users',
    ) -> HTTPResponse:
        """Each VU calls this step repeatedly for 3 minutes."""
        return await self.client.http.get(url)
    
    @step()
    async def get_user_details(
        self,
        url: URL = 'https://api.example.com/users/1',
    ) -> HTTPResponse:
        """Called after get_users, then loop restarts."""
        return await self.client.http.get(url)
```

**Step Execution Order**:

```
VU Loop Iteration:
  1. Execute get_users() → record metrics
  2. Execute get_user_details() → record metrics
  3. Repeat until duration expires
```

---

#### Optimized Args (Performance Optimization)

**The Problem**: Load testing overhead can skew results. When testing API performance, we don't want to measure:
- DNS lookup time
- Header serialization time
- JSON encoding time
- SSL handshake overhead (for new connections)

These are test infrastructure costs, not the target system's actual performance.

**The Solution**: Hyperscale uses **Optimized Args** - special type-annotated keyword arguments that are pre-processed BEFORE the test loop starts.

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                         OPTIMIZED ARGS CONCEPT                               │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  BEFORE WORKFLOW EXECUTION (once, at startup):                              │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │  1. Parse @step() method signatures                                  │    │
│  │  2. Find keyword args with Optimized type hints (URL, Headers, etc.)│    │
│  │  3. Extract default values                                           │    │
│  │  4. Call .optimize() on each:                                        │    │
│  │     • URL: DNS lookup, address resolution                           │    │
│  │     • Headers: Serialize to bytes/string format                     │    │
│  │     • Data: JSON encode, compute content-length                     │    │
│  │  5. Store optimized values for reuse                                 │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│                            │                                                 │
│                            ▼                                                 │
│  DURING WORKFLOW EXECUTION (every loop iteration):                          │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │  1. Use pre-resolved IP address (skip DNS)                          │    │
│  │  2. Use pre-serialized headers (skip encoding)                      │    │
│  │  3. Use pre-encoded data (skip JSON serialization)                  │    │
│  │  4. Measure ONLY the actual HTTP request/response time              │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│                                                                              │
│  Result: Metrics reflect true target system performance,                    │
│          not test infrastructure overhead.                                   │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

#### Available Optimized Arg Types

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                        OPTIMIZED ARG TYPES                                   │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  HTTP/Network:                                                               │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │  URL       │ Pre-resolves DNS, caches IP address                    │    │
│  │            │ Supports HTTP, HTTP2, HTTP3, GraphQL, WebSocket, etc. │    │
│  │            │                                                         │    │
│  │  Headers   │ Pre-serializes headers to wire format                  │    │
│  │            │ HTTP/1.1: "Key: Value\r\n" string                      │    │
│  │            │ HTTP/2+: [(b'key', b'value'), ...] tuples              │    │
│  │            │                                                         │    │
│  │  Data      │ Pre-encodes request body                               │    │
│  │            │ dict/list → JSON bytes (via orjson)                    │    │
│  │            │ Pydantic model → JSON bytes                            │    │
│  │            │ Computes content-length and content-type               │    │
│  │            │                                                         │    │
│  │  Params    │ Pre-encodes URL query parameters                       │    │
│  │            │ {"key": "value"} → "?key=value"                        │    │
│  │            │                                                         │    │
│  │  Cookies   │ Pre-formats cookie header                              │    │
│  │            │ {"session": "abc"} → "session=abc"                     │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│                                                                              │
│  Authentication:                                                             │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │  Auth      │ Pre-computes authentication headers                    │    │
│  │            │ Basic, Bearer, OAuth, etc.                             │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│                                                                              │
│  GraphQL:                                                                    │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │  Query     │ Pre-validates and formats GraphQL query                │    │
│  │  Mutation  │ Pre-validates and formats GraphQL mutation             │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│                                                                              │
│  File Transfer:                                                              │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │  File      │ Pre-reads file content, computes metadata              │    │
│  │  Directory │ Pre-scans directory structure                          │    │
│  │  FileGlob  │ Pre-resolves glob patterns                             │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│                                                                              │
│  gRPC/Protobuf:                                                              │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │  Protobuf  │ Pre-validates protobuf message structure               │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│                                                                              │
│  Email/SMTP:                                                                 │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │  Email     │ Pre-formats email message (MIME encoding, etc.)        │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

#### Complete Example with Optimized Args

```python
from hyperscale.graph import Workflow, step
from hyperscale.testing import (
    URL,
    Headers,
    Data,
    Params,
    Cookies,
    Auth,
    HTTPResponse,
)


class OptimizedAPITest(Workflow):
    """
    Load test demonstrating all major optimized arg types.
    
    BEFORE execution starts, Hyperscale:
    1. Resolves 'api.example.com' → 93.184.216.34
    2. Serializes headers → "Authorization: Bearer token\r\n..."
    3. JSON-encodes data → b'{"action":"create"}'
    4. Encodes params → "?page=1&limit=100"
    5. Formats cookies → "session=abc123"
    
    DURING execution:
    - Uses cached IP (no DNS lookup per request)
    - Uses pre-serialized headers (no encoding per request)
    - Uses pre-encoded JSON (no serialization per request)
    - Metrics measure ONLY actual HTTP latency
    """
    vus = 10000
    duration = "5m"
    priority = "high"
    
    @step()
    async def get_with_auth(
        self,
        url: URL = 'https://api.example.com/users',
        headers: Headers = {
            'Authorization': 'Bearer eyJhbGciOiJIUzI1NiIs...',
            'Accept': 'application/json',
        },
        params: Params = {'page': 1, 'limit': 100},
    ) -> HTTPResponse:
        """
        GET request with pre-optimized:
        - URL (DNS pre-resolved)
        - Headers (pre-serialized)
        - Query params (pre-encoded)
        """
        return await self.client.http.get(
            url,
            headers=headers,
            params=params,
        )
    
    @step()
    async def post_json_data(
        self,
        url: URL = 'https://api.example.com/actions',
        headers: Headers = {
            'Content-Type': 'application/json',
            'Authorization': 'Bearer eyJhbGciOiJIUzI1NiIs...',
        },
        data: Data = {
            'action': 'create',
            'resource': 'user',
            'metadata': {'source': 'load_test'},
        },
    ) -> HTTPResponse:
        """
        POST request with pre-optimized:
        - URL (DNS pre-resolved)
        - Headers (pre-serialized)
        - JSON body (pre-encoded via orjson)
        - Content-Length (pre-computed)
        """
        return await self.client.http.post(
            url,
            headers=headers,
            data=data,
        )
    
    @step()
    async def request_with_cookies(
        self,
        url: URL = 'https://api.example.com/session',
        cookies: Cookies = {
            'session_id': 'abc123xyz',
            'user_pref': 'dark_mode',
        },
    ) -> HTTPResponse:
        """
        Request with pre-formatted cookies.
        """
        return await self.client.http.get(url, cookies=cookies)
```

---

#### How Optimization Works (URL Example)

```python
# From hyperscale/core/testing/models/url/url.py

class URL(OptimizedArg):
    def __init__(self, url: str):
        self.data = url
        self.optimized: Optional[OptimizedUrl] = None
    
    async def optimize(self, request_type: RequestType):
        """Called ONCE before workflow execution starts."""
        if self.optimized is not None:
            return  # Already optimized, skip
        
        # Create optimized URL with correct protocol
        self.optimized = OptimizedUrl(
            self.data,
            family=address_family,  # IPv4/IPv6
            protocol=protocol,       # TCP/UDP/QUIC
        )
        
        # Pre-resolve DNS based on request type
        match request_type:
            case RequestType.HTTP | RequestType.HTTP2 | RequestType.HTTP3:
                await self.optimized.lookup()  # DNS → IP address
            case RequestType.FTP:
                await self.optimized.lookup_ftp()
            case RequestType.SMTP:
                await self.optimized.lookup_smtp()
            # ... etc.
```

**Timeline**:

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                        OPTIMIZATION TIMELINE                                 │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  T=0: Workflow submitted                                                     │
│       │                                                                      │
│       ▼                                                                      │
│  T=1: Parse @step() signatures, extract optimized args                      │
│       │                                                                      │
│       ▼                                                                      │
│  T=2: url.optimize() → DNS lookup (50-200ms typically)                      │
│       headers.optimize() → serialize (< 1ms)                                │
│       data.optimize() → JSON encode (< 1ms)                                 │
│       │                                                                      │
│       ▼                                                                      │
│  T=3: START metrics collection                                              │
│       │                                                                      │
│       ├──► VU 1:    HTTP GET (uses cached IP, pre-serialized headers)      │
│       │              └─ Latency: 15ms (measured)                            │
│       │                                                                      │
│       ├──► VU 2:    HTTP GET (uses cached IP, pre-serialized headers)      │
│       │              └─ Latency: 12ms (measured)                            │
│       │                                                                      │
│       └──► VU N:    ... (all use same optimized values)                     │
│                                                                              │
│  DNS lookup cost (200ms) paid ONCE, not per request.                        │
│  With 10,000 VUs × 100 requests each = 1,000,000 requests                   │
│  Savings: 200ms × 1,000,000 = 55+ hours of DNS overhead eliminated!         │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

### Step Dependencies and the Step DAG

Steps within a workflow can depend on other steps, forming a **Directed Acyclic Graph (DAG)**. This determines execution order within each VU's loop.

#### Declaring Step Dependencies

Pass string names of other steps to `@step()`:

```python
from hyperscale.graph import Workflow, step
from hyperscale.testing import URL, HTTPResponse


class APITestWorkflow(Workflow):
    """
    Step DAG:
    
            authenticate
            /          \
       get_users    get_config    ← Run in parallel (same dependency)
            \          /
           process_data
    """
    vus = 5000
    duration = "3m"
    
    @step()  # No args = root step (no dependencies)
    async def authenticate(
        self,
        url: URL = 'https://api.example.com/auth',
    ) -> HTTPResponse:
        """First step - authenticates and returns token."""
        return await self.client.http.post(url, json={"user": "test"})
    
    @step('authenticate')  # Depends on 'authenticate'
    async def get_users(
        self,
        url: URL = 'https://api.example.com/users',
        authenticate: HTTPResponse | None = None,  # ← Gets authenticate's result!
    ) -> HTTPResponse:
        """Runs after authenticate. Can access auth response via kwarg."""
        # authenticate kwarg contains the HTTPResponse from authenticate step
        token = authenticate.json().get('token') if authenticate else None
        return await self.client.http.get(url)
    
    @step('authenticate')  # Also depends on 'authenticate' (parallel to get_users)
    async def get_config(
        self,
        url: URL = 'https://api.example.com/config',
    ) -> HTTPResponse:
        """Runs in parallel with get_users (both depend only on authenticate)."""
        return await self.client.http.get(url)
    
    @step('get_users', 'get_config')  # Depends on BOTH get_users AND get_config
    async def process_data(
        self,
        url: URL = 'https://api.example.com/process',
        get_users: HTTPResponse | None = None,    # ← Gets get_users result
        get_config: HTTPResponse | None = None,   # ← Gets get_config result
    ) -> HTTPResponse:
        """Final step - waits for both parallel steps to complete."""
        # Can access results from both previous steps
        users = get_users.json() if get_users else []
        config = get_config.json() if get_config else {}
        return await self.client.http.post(url, json={"users": users})
```

---

#### DAG Execution Order

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                         STEP DAG EXECUTION                                   │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  Each VU executes the DAG in topological order:                             │
│                                                                              │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │  Layer 0: [authenticate]           ← Execute, store result          │    │
│  │                  │                                                   │    │
│  │                  ▼                                                   │    │
│  │  Layer 1: [get_users, get_config]  ← Execute in parallel            │    │
│  │                  │                                                   │    │
│  │                  ▼                                                   │    │
│  │  Layer 2: [process_data]           ← Wait for both, then execute    │    │
│  │                  │                                                   │    │
│  │                  ▼                                                   │    │
│  │  Loop back to Layer 0 until duration expires                        │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│                                                                              │
│  Steps in the same layer (same dependencies) run concurrently.              │
│  Metrics are collected for each step separately.                            │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

#### Dependency Rules

| Pattern | Meaning |
|---------|---------|
| `@step()` | Root step, no dependencies |
| `@step('a')` | Depends on step `a` |
| `@step('a', 'b')` | Depends on BOTH `a` AND `b` |
| `@step('a')` + `@step('a')` | Both depend on `a`, run in parallel |

**Important Constraints**:

1. **Workflow Islands**: Steps can ONLY reference other steps within the SAME workflow class. Cross-workflow data sharing uses `@state()` methods only.

2. **Acyclic Only**: Dependencies must form a DAG. Circular dependencies will cause errors.

3. **String Names**: Dependencies are the **function names** as strings, not the functions themselves.

---

### VU-Isolated Context and Step Data Passing

#### Each VU Gets an Isolated Context Copy

When a VU starts its loop iteration, it receives a **shallow copy** of the workflow context:

```python
# From WorkflowRunner._spawn_vu()
context: Dict[str, Any] = dict(context)  # ← Fresh copy for this VU
```

This ensures:
- **No cross-VU interference**: VU #1's step results don't affect VU #2
- **Clean slate each iteration**: Each loop starts fresh
- **Thread safety**: No shared mutable state between concurrent VUs

---

#### Step Results Stored Under Function Name

After each step completes, its result is stored in the VU's context under the step's function name:

```python
# From WorkflowRunner._spawn_vu()
for complete in completed:
    step_name = complete.get_name()       # e.g., "authenticate"
    result = complete.result()            # HTTPResponse object
    context[step_name] = result           # context["authenticate"] = HTTPResponse
```

---

#### Accessing Previous Step Data

Subsequent steps access previous results via **keyword arguments** with matching names:

```python
# Hyperscale matches kwarg names to context keys
for hook in hook_set.values():
    hook.context_args.update(
        {key: context[key] for key in context if key in hook.kwarg_names}
    )
```

**Example**:

```python
@step('authenticate')
async def get_users(
    self,
    url: URL = 'https://api.example.com/users',
    authenticate: HTTPResponse | None = None,  # ← Matches context['authenticate']
) -> HTTPResponse:
    """
    The 'authenticate' kwarg will receive the HTTPResponse
    from the authenticate() step because:
    1. 'authenticate' is in hook.kwarg_names
    2. context['authenticate'] exists (from previous step)
    3. Hyperscale passes context['authenticate'] to this kwarg
    """
    if authenticate and authenticate.status_code == 200:
        token = authenticate.json().get('token')
        # Use token in this request
    return await self.client.http.get(url)
```

---

#### Optimized Args Override Context

**Important**: If a keyword argument has an `OptimizedArg` type hint (`URL`, `Headers`, `Data`, etc.), the optimized value takes precedence over context lookup.

```python
@step('step_one')
async def step_two(
    self,
    url: URL = 'https://api.example.com',  # ← OptimizedArg - NOT from context!
    step_one: HTTPResponse | None = None,   # ← From context (not OptimizedArg type)
) -> HTTPResponse:
    # 'url' uses the pre-optimized URL value
    # 'step_one' gets the HTTPResponse from step_one's execution
    return await self.client.http.get(url)
```

---

#### Complete Data Flow Example

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    VU DATA FLOW THROUGH STEP DAG                             │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  VU #42 Loop Iteration:                                                     │
│                                                                              │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │  1. VU starts with fresh context copy:                              │    │
│  │     context = {}                                                     │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│                            │                                                 │
│                            ▼                                                 │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │  2. Execute authenticate():                                          │    │
│  │     result = HTTPResponse(status=200, body={"token": "abc123"})     │    │
│  │     context["authenticate"] = result                                 │    │
│  │                                                                      │    │
│  │     context = {"authenticate": HTTPResponse(...)}                   │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│                            │                                                 │
│                            ▼                                                 │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │  3. Execute get_users(authenticate=context["authenticate"]):        │    │
│  │     # authenticate kwarg receives the HTTPResponse from step 2      │    │
│  │     result = HTTPResponse(status=200, body=[{user1}, {user2}])      │    │
│  │     context["get_users"] = result                                   │    │
│  │                                                                      │    │
│  │  3. Execute get_config() in PARALLEL:                               │    │
│  │     result = HTTPResponse(status=200, body={"theme": "dark"})       │    │
│  │     context["get_config"] = result                                  │    │
│  │                                                                      │    │
│  │     context = {                                                      │    │
│  │       "authenticate": HTTPResponse(...),                            │    │
│  │       "get_users": HTTPResponse(...),                               │    │
│  │       "get_config": HTTPResponse(...)                               │    │
│  │     }                                                                │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│                            │                                                 │
│                            ▼                                                 │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │  4. Execute process_data(                                            │    │
│  │       get_users=context["get_users"],                               │    │
│  │       get_config=context["get_config"]                              │    │
│  │     ):                                                               │    │
│  │     # Both kwargs receive results from parallel steps               │    │
│  │     result = HTTPResponse(status=201, body={"processed": True})     │    │
│  │     context["process_data"] = result                                │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│                            │                                                 │
│                            ▼                                                 │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │  5. Loop complete - VU #42 starts fresh iteration                   │    │
│  │     (context reset for next loop)                                   │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│                                                                              │
│  Meanwhile, VU #1, #2, ... #41, #43, ... #5000 are doing the same thing    │
│  with their own isolated context copies.                                    │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

#### One Client Return Per Test Step

Each test step can make multiple client calls, but only **ONE** response can be returned for metrics:

```python
@step()
async def multi_call_step(
    self,
    url1: URL = 'https://api.example.com/check',
    url2: URL = 'https://api.example.com/data',
) -> HTTPResponse:
    """
    Can call multiple clients, but only return one for metrics.
    """
    # Call 1 - not measured (result discarded for metrics)
    check_response = await self.client.http.get(url1)
    
    if check_response.status_code != 200:
        # Early exit - still need to return HTTPResponse
        return check_response
    
    # Call 2 - THIS is what gets measured (returned)
    return await self.client.http.post(url2, json={"checked": True})
```

**Best Practice**: One client call per step for clear metrics.

---

#### Workflows Are Islands

Steps can ONLY depend on other steps within the **same workflow class**:

```python
class WorkflowA(Workflow):
    @step()
    async def step_a(self) -> HTTPResponse: ...

class WorkflowB(Workflow):
    @step('step_a')  # ❌ ERROR: Can't reference WorkflowA's step
    async def step_b(self) -> HTTPResponse: ...
```

**Cross-workflow communication** uses `@state()` methods and workflow-level `Context`:

```python
class WorkflowA(Workflow):
    @step()
    async def get_data(self) -> HTTPResponse:
        return await self.client.http.get(url)
    
    @state('WorkflowB')  # Share state TO WorkflowB
    def share_token(self) -> Provide[str]:
        return self.context.get('token', '')


@depends('WorkflowA')
class WorkflowB(Workflow):
    @state('WorkflowA')  # Receive state FROM WorkflowA
    def receive_token(self, share_token: str | None = None) -> Use[str]:
        return share_token
    
    @step()
    async def use_token(self) -> HTTPResponse:
        token = self.context.get('share_token', '')  # From WorkflowA
        return await self.client.http.get(url, headers={'Auth': token})
```

---

### Step 2: Priority-Based Thread Allocation

**Critical**: Thread allocation is calculated from the **TOTAL pool size** (all registered workers' cores), NOT available cores. This determines how many cores the workflow MAY request.

**StagePriority Allocation Ranges**:

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    PRIORITY → THREAD ALLOCATION                              │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  TOTAL_POOL = sum(worker.total_cores for all registered workers)           │
│                                                                              │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │  Priority    │ Min Threads         │ Max Threads                    │    │
│  │  ────────────┼─────────────────────┼────────────────────────────────│    │
│  │  LOW         │ 1                   │ ceil(TOTAL_POOL × 0.25)       │    │
│  │  NORMAL      │ ceil(TOTAL_POOL×0.25)│ ceil(TOTAL_POOL × 0.75)      │    │
│  │  HIGH        │ ceil(TOTAL_POOL×0.75)│ TOTAL_POOL                   │    │
│  │  EXCLUSIVE   │ TOTAL_POOL          │ TOTAL_POOL (100%)             │    │
│  │  AUTO        │ 1                   │ TOTAL_POOL                    │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│                                                                              │
│  Example: TOTAL_POOL = 24 cores (3 workers × 8 cores each)                  │
│                                                                              │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │  Priority    │ Min Threads │ Max Threads                            │    │
│  │  ────────────┼─────────────┼────────────────────────────────────────│    │
│  │  LOW         │ 1           │ 6  (25% of 24)                         │    │
│  │  NORMAL      │ 6           │ 18 (75% of 24)                         │    │
│  │  HIGH        │ 18          │ 24 (100% of 24)                        │    │
│  │  EXCLUSIVE   │ 24          │ 24 (takes all cores)                   │    │
│  │  AUTO        │ 1           │ 24                                     │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│                                                                              │
│  ⚠️  IMPORTANT: This is the ALLOCATION RANGE, not the final count.         │
│      The Provisioner bins multiple workflows into batches that fit          │
│      within TOTAL_POOL, distributing threads within these ranges.           │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

**Provisioner.partion_by_priority() Algorithm**:

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    PROVISIONER PARTITIONING                                  │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  Input: List of workflow configs:                                           │
│  [                                                                           │
│    {"workflow_name": "LoadTest", "priority": HIGH, "is_test": True},        │
│    {"workflow_name": "DataLoad", "priority": AUTO, "is_test": False},       │
│    {"workflow_name": "Metrics",  "priority": LOW,  "is_test": True},        │
│  ]                                                                           │
│                                                                              │
│  Algorithm:                                                                  │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │ 1. Sort by priority (HIGH first), then by is_test                   │    │
│  │                                                                      │    │
│  │ 2. Non-test workflows → bypass batch (threads = 0, run sequentially)│    │
│  │                                                                      │    │
│  │ 3. For test workflows:                                               │    │
│  │    a. Calculate min/max threads from priority + TOTAL_POOL          │    │
│  │    b. Group workflows into batches that fit within TOTAL_POOL       │    │
│  │    c. Higher priority gets more threads within range                │    │
│  │    d. Distribute remaining threads to higher priority workflows     │    │
│  │                                                                      │    │
│  │ 4. Return: List[List[Tuple[workflow_name, priority, threads]]]      │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│                                                                              │
│  Output Example (TOTAL_POOL = 24):                                          │
│  [                                                                           │
│    [("DataLoad", AUTO, 0)],           # Non-test: bypass batch             │
│    [("LoadTest", HIGH, 18),           # HIGH gets 18 threads               │
│     ("Metrics", LOW, 6)],             # LOW gets remaining 6               │
│  ]                                                                           │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Step 3: VU Provisioning

After thread allocation, VUs are distributed among the allocated threads.

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                         VU PROVISIONING                                      │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  Formula:                                                                    │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │  vus_per_thread = workflow.vus // threads                           │    │
│  │  remainder_vus  = workflow.vus % threads                            │    │
│  │                                                                      │    │
│  │  # Each thread gets vus_per_thread                                  │    │
│  │  # Last thread gets vus_per_thread + remainder_vus                  │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│                                                                              │
│  Example: workflow.vus = 2000, threads = 6                                  │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │  vus_per_thread = 2000 // 6 = 333                                   │    │
│  │  remainder_vus  = 2000 % 6  = 2                                     │    │
│  │                                                                      │    │
│  │  workflow_vus = [333, 333, 333, 333, 333, 335]                      │    │
│  │                  ↑    ↑    ↑    ↑    ↑    ↑                         │    │
│  │                  T1   T2   T3   T4   T5   T6 (gets remainder)       │    │
│  │                                                                      │    │
│  │  Total: 333×5 + 335 = 1665 + 335 = 2000 ✓                          │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│                                                                              │
│  Result Structure:                                                           │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │  workflow_vus: Dict[str, List[int]] = {                             │    │
│  │      "LoadTest": [333, 333, 333, 333, 333, 335],  # 6 threads       │    │
│  │      "Metrics":  [166, 167],                       # 2 threads       │    │
│  │  }                                                                   │    │
│  │                                                                      │    │
│  │  Each list entry = VUs for that thread/worker                       │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Step 4: Dependency Graph & Execution Order

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                     DEPENDENCY GRAPH CONSTRUCTION                            │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  Input Workflows:                                                            │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │  Workflow("Setup")                                                   │    │
│  │  Workflow("LoadTest")                                                │    │
│  │  DependentWorkflow(                                                  │    │
│  │      workflow=Workflow("Validate"),                                  │    │
│  │      dependencies=["LoadTest"]                                       │    │
│  │  )                                                                   │    │
│  │  DependentWorkflow(                                                  │    │
│  │      workflow=Workflow("Report"),                                    │    │
│  │      dependencies=["Validate", "LoadTest"]                           │    │
│  │  )                                                                   │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│                                                                              │
│  Constructed Graph (networkx.DiGraph):                                      │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │                                                                      │    │
│  │      Setup ─────────┐                                               │    │
│  │                     │                                               │    │
│  │      LoadTest ──────┼──────► Validate ──────► Report                │    │
│  │            │        │              │              ▲                 │    │
│  │            │        │              │              │                 │    │
│  │            └────────┼──────────────┼──────────────┘                 │    │
│  │                     │                                               │    │
│  │  Sources: [Setup, LoadTest]                                         │    │
│  │                                                                      │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│                                                                              │
│  BFS Traversal Order:                                                        │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │  Layer 0: {Setup, LoadTest}     # Run in parallel (no deps)         │    │
│  │  Layer 1: {Validate}            # Waits for LoadTest                │    │
│  │  Layer 2: {Report}              # Waits for Validate + LoadTest     │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│                                                                              │
│  Execution:                                                                  │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │  Time ────────────────────────────────────────────────────────────► │    │
│  │                                                                      │    │
│  │  Layer 0:  [Setup]──────►                                           │    │
│  │            [LoadTest]────────────►                                  │    │
│  │                                                                      │    │
│  │  Layer 1:                         [Validate]──────►                 │    │
│  │                                   (receives LoadTest context)       │    │
│  │                                                                      │    │
│  │  Layer 2:                                          [Report]──────►  │    │
│  │                                                    (receives both)  │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Step 5: Context Management

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                        CONTEXT FLOW                                          │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  Context Structure:                                                          │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │  class Context:                                                      │    │
│  │      _context: Dict[str, WorkflowContext]                           │    │
│  │      # workflow_name → {key: value, ...}                            │    │
│  │                                                                      │    │
│  │  class WorkflowContext:                                              │    │
│  │      _values: Dict[str, Tuple[Any, int]]  # key → (value, timestamp)│    │
│  │                                                                      │    │
│  │  # Timestamps ensure LWW (Last-Write-Wins) for conflict resolution  │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│                                                                              │
│  Context Hooks (Using @context() decorator):                                 │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │                                                                      │    │
│  │  class LoadTestWorkflow(Workflow):                                  │    │
│  │                                                                      │    │
│  │      @context()                                                      │    │
│  │      async def provide_results(self) -> Provide[Dict]:              │    │
│  │          # StateAction.PROVIDE - writes to context                  │    │
│  │          return {"total_requests": 10000, "success_rate": 0.99}     │    │
│  │                                                                      │    │
│  │  class ValidateWorkflow(Workflow):                                  │    │
│  │                                                                      │    │
│  │      @context(workflows=["LoadTestWorkflow"])                       │    │
│  │      async def use_results(self, *, data: Dict) -> Use[bool]:       │    │
│  │          # StateAction.USE - reads from specified workflow context  │    │
│  │          return data["success_rate"] > 0.95                         │    │
│  │                                                                      │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│                                                                              │
│  Flow:                                                                       │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │                                                                      │    │
│  │  Worker 1                Manager                Worker 2             │    │
│  │     │                       │                      │                 │    │
│  │     │ Run LoadTest          │                      │                 │    │
│  │     │                       │                      │                 │    │
│  │     │ ① Workflow completes  │                      │                 │    │
│  │     │    context updated    │                      │                 │    │
│  │     │                       │                      │                 │    │
│  │     │ ② WorkflowProgress    │                      │                 │    │
│  │     │   + context_updates   │                      │                 │    │
│  │     │──────────────────────►│                      │                 │    │
│  │     │                       │                      │                 │    │
│  │     │                       │ ③ Store context      │                 │    │
│  │     │                       │    Sync to peers     │                 │    │
│  │     │                       │────────────────────► │                 │    │
│  │     │                       │    (ContextUpdate)   │                 │    │
│  │     │                       │                      │                 │    │
│  │     │                       │ ④ Dispatch Validate  │                 │    │
│  │     │                       │    + LoadTest context│                 │    │
│  │     │                       │─────────────────────►│                 │    │
│  │     │                       │                      │                 │    │
│  │     │                       │                      │ ⑤ Validate runs │    │
│  │     │                       │                      │    uses context │    │
│  │     │                       │                      │                 │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Manager State for Workflow Execution

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    MANAGER WORKFLOW EXECUTION STATE                          │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  class ManagerServer:                                                        │
│                                                                              │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │ # Core tracking                                                      │    │
│  │ _jobs: Dict[str, JobProgress]                                       │    │
│  │   # job_id → aggregated progress                                    │    │
│  │                                                                      │    │
│  │ _workflow_assignments: Dict[str, str]                               │    │
│  │   # workflow_id → worker_node_id                                    │    │
│  │                                                                      │    │
│  │ _workflow_retries: Dict[str, Tuple[int, bytes, set[str]]]          │    │
│  │   # workflow_id → (retry_count, original_dispatch, failed_workers)  │    │
│  │   # NOTE: Only for WORKER FAILURE (SWIM dead), NOT workflow errors  │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│                                                                              │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │ # NEW: Provisioning state                                            │    │
│  │                                                                      │    │
│  │ _provisioner: Provisioner                                           │    │
│  │   # Thread allocation calculator (uses TOTAL pool)                  │    │
│  │                                                                      │    │
│  │ _total_pool_size: int                                               │    │
│  │   # Sum of total_cores from all registered workers (cached)        │    │
│  │   # Updated on worker registration/death                            │    │
│  │                                                                      │    │
│  │ _job_workflow_configs: Dict[str, Dict[str, WorkflowConfig]]        │    │
│  │   # job_id → {workflow_name: config}                                │    │
│  │   # Config: {priority, is_test, threads, vus_per_thread}           │    │
│  │                                                                      │    │
│  │ _job_dependency_graphs: Dict[str, List[Dict[str, Workflow]]]       │    │
│  │   # job_id → execution layers (BFS traversal order)                 │    │
│  │                                                                      │    │
│  │ _job_current_layer: Dict[str, int]                                  │    │
│  │   # job_id → current executing layer index                          │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│                                                                              │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │ # NEW: Context state                                                 │    │
│  │                                                                      │    │
│  │ _job_contexts: Dict[str, Context]                                   │    │
│  │   # job_id → Context object (shared across workflows in job)       │    │
│  │                                                                      │    │
│  │ _context_clock: Dict[str, Dict[str, int]]                           │    │
│  │   # job_id → {workflow_name: lamport_timestamp}                     │    │
│  │   # For conflict resolution in context updates                      │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Complete Job Execution State Machine

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    JOB EXECUTION STATE MACHINE (Manager)                     │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│                           ┌──────────────────┐                               │
│                           │    SUBMITTED     │                               │
│                           │                  │                               │
│                           │ • Receive job    │                               │
│                           │ • Parse workflows│                               │
│                           └────────┬─────────┘                               │
│                                    │                                         │
│                         classify & provision                                 │
│                                    │                                         │
│                                    ▼                                         │
│                           ┌──────────────────┐                               │
│                           │   CLASSIFYING    │                               │
│                           │                  │                               │
│                           │ • Detect is_test │                               │
│                           │ • Build dep graph│                               │
│                           │ • BFS traversal  │                               │
│                           └────────┬─────────┘                               │
│                                    │                                         │
│                                    ▼                                         │
│                           ┌──────────────────┐                               │
│                           │   PROVISIONING   │                               │
│                           │                  │                               │
│                           │ • Calc threads   │                               │
│                           │   from TOTAL pool│                               │
│                           │ • Calc VUs/thread│                               │
│                           └────────┬─────────┘                               │
│                                    │                                         │
│                   ┌────────────────┼────────────────┐                        │
│                   │                │                │                        │
│                   ▼                ▼                ▼                        │
│          ┌──────────────┐  ┌──────────────┐  ┌──────────────┐               │
│          │   QUEUED     │  │  DISPATCHING │  │   FAILED     │               │
│          │              │  │              │  │              │               │
│          │ Insufficient │  │ Capacity OK  │  │ No workers   │               │
│          │ capacity     │  │ • Quorum req │  │ available    │               │
│          └──────┬───────┘  │ • Dispatch   │  └──────────────┘               │
│                 │          └──────┬───────┘                                  │
│                 │                 │                                          │
│        capacity available         │                                          │
│                 │                 ▼                                          │
│                 └────────► ┌──────────────────┐                              │
│                            │     RUNNING      │                              │
│                            │                  │                              │
│                            │ • Per-layer exec │                              │
│                            │ • Context sync   │                              │
│                            │ • Progress track │                              │
│                            └────────┬─────────┘                              │
│                                     │                                        │
│                   ┌─────────────────┼─────────────────┐                      │
│                   │                 │                 │                      │
│                   ▼                 ▼                 ▼                      │
│          ┌──────────────┐  ┌──────────────┐  ┌──────────────┐               │
│          │  COMPLETING  │  │   FAILED     │  │  CANCELLED   │               │
│          │              │  │              │  │              │               │
│          │ All layers   │  │ Workflow     │  │ User cancel  │               │
│          │ complete     │  │ error        │  │              │               │
│          └──────┬───────┘  └──────────────┘  └──────────────┘               │
│                 │                                                            │
│                 ▼                                                            │
│          ┌──────────────┐                                                    │
│          │  COMPLETED   │                                                    │
│          │              │                                                    │
│          │ Success!     │                                                    │
│          │ Results ready│                                                    │
│          └──────────────┘                                                    │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Layer-Based Execution Flow

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    LAYER-BASED EXECUTION FLOW                                │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  Manager executes dependency layers sequentially, workflows within          │
│  each layer in parallel:                                                     │
│                                                                              │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │                                                                      │    │
│  │  async def _execute_job(self, job_id: str):                         │    │
│  │      layers = self._job_dependency_graphs[job_id]                   │    │
│  │      context = self._job_contexts[job_id]                           │    │
│  │                                                                      │    │
│  │      for layer_idx, layer_workflows in enumerate(layers):           │    │
│  │          self._job_current_layer[job_id] = layer_idx                │    │
│  │                                                                      │    │
│  │          # Dispatch all workflows in layer (parallel)               │    │
│  │          dispatch_tasks = []                                         │    │
│  │          for workflow_name, workflow in layer_workflows.items():    │    │
│  │              # Get predecessor context for dependent workflows      │    │
│  │              dep_context = self._get_dependency_context(            │    │
│  │                  job_id, workflow                                    │    │
│  │              )                                                       │    │
│  │                                                                      │    │
│  │              # Dispatch with VUs from provisioning                  │    │
│  │              config = self._job_workflow_configs[job_id][name]     │    │
│  │              dispatch_tasks.append(                                  │    │
│  │                  self._dispatch_workflow(                            │    │
│  │                      job_id, workflow, config, dep_context          │    │
│  │                  )                                                   │    │
│  │              )                                                       │    │
│  │                                                                      │    │
│  │          # Wait for all workflows in layer to complete              │    │
│  │          await asyncio.gather(*dispatch_tasks)                      │    │
│  │                                                                      │    │
│  │          # Sync context updates from completed workflows            │    │
│  │          await self._sync_layer_context(job_id, layer_idx)          │    │
│  │                                                                      │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│                                                                              │
│  Example Timeline (3 workers, 24 cores total):                              │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │  Time ────────────────────────────────────────────────────────────► │    │
│  │                                                                      │    │
│  │  Layer 0:                                                            │    │
│  │  ┌────────────────────────────────────────────────────┐              │    │
│  │  │ Setup (1 thread, non-test) ─────►                  │              │    │
│  │  │ LoadTest (18 threads, HIGH, 333 VUs/thread) ──────────────►│     │    │
│  │  │ Analytics (6 threads, LOW, 166 VUs/thread) ───────────────►│     │    │
│  │  └────────────────────────────────────────────────────┘              │    │
│  │                                     ↓ context synced                 │    │
│  │  Layer 1:                                                            │    │
│  │  ┌──────────────────────────────────────────────────────────────┐   │    │
│  │  │ Validate (6 threads, receives LoadTest context) ──────►      │   │    │
│  │  └──────────────────────────────────────────────────────────────┘   │    │
│  │                                     ↓ context synced                 │    │
│  │  Layer 2:                                                            │    │
│  │  ┌──────────────────────────────────────────────────────────────┐   │    │
│  │  │ Report (1 thread, receives all context) ─────►               │   │    │
│  │  └──────────────────────────────────────────────────────────────┘   │    │
│  │                                                                      │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Cross-Manager Context Synchronization

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    CROSS-MANAGER CONTEXT SYNC                                │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  When a workflow completes, its context updates must be synchronized        │
│  to peer managers for fault tolerance:                                       │
│                                                                              │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │                                                                      │    │
│  │  ContextUpdate (new message type):                                  │    │
│  │  ┌────────────────────────────────────────────────────────────────┐ │    │
│  │  │  job_id: str                                                    │ │    │
│  │  │  workflow_name: str                                             │ │    │
│  │  │  context_values: Dict[str, Tuple[Any, int]]  # key→(val, ts)  │ │    │
│  │  │  source_manager: str                                            │ │    │
│  │  │  lamport_clock: int                                             │ │    │
│  │  └────────────────────────────────────────────────────────────────┘ │    │
│  │                                                                      │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│                                                                              │
│  Sync Flow:                                                                  │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │                                                                      │    │
│  │  Manager 1 (Leader)        Manager 2            Manager 3           │    │
│  │       │                        │                    │               │    │
│  │       │ Workflow completes     │                    │               │    │
│  │       │ with context update    │                    │               │    │
│  │       │                        │                    │               │    │
│  │       │ ① Update local context │                    │               │    │
│  │       │    with timestamp      │                    │               │    │
│  │       │                        │                    │               │    │
│  │       │ ② ContextUpdate        │                    │               │    │
│  │       │───────────────────────►│                    │               │    │
│  │       │                        │                    │               │    │
│  │       │ ② ContextUpdate        │                    │               │    │
│  │       │────────────────────────────────────────────►│               │    │
│  │       │                        │                    │               │    │
│  │       │                        │ ③ Apply if ts >    │               │    │
│  │       │                        │    current ts      │               │    │
│  │       │                        │                    │ ③ Apply if    │    │
│  │       │                        │                    │    ts > curr  │    │
│  │       │                        │                    │               │    │
│  │                                                                      │    │
│  │  Conflict Resolution: Last-Write-Wins (LWW) using Lamport timestamps│    │
│  │                                                                      │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│                                                                              │
│  Handler in Manager:                                                         │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │                                                                      │    │
│  │  @tcp.receive()                                                      │    │
│  │  async def context_update(self, addr, data, clock_time):            │    │
│  │      update = ContextUpdate.load(data)                              │    │
│  │                                                                      │    │
│  │      # Only apply if newer than our current context                 │    │
│  │      current_ts = self._context_clock.get(                          │    │
│  │          update.job_id, {}                                          │    │
│  │      ).get(update.workflow_name, 0)                                 │    │
│  │                                                                      │    │
│  │      if update.lamport_clock > current_ts:                          │    │
│  │          context = self._job_contexts[update.job_id]                │    │
│  │          for key, (value, ts) in update.context_values.items():     │    │
│  │              await context.update(                                   │    │
│  │                  update.workflow_name, key, value, timestamp=ts     │    │
│  │              )                                                       │    │
│  │          self._context_clock[update.job_id][update.workflow_name] = │    │
│  │              update.lamport_clock                                    │    │
│  │                                                                      │    │
│  │      return b'ok'                                                    │    │
│  │                                                                      │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Implementation Order

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    IMPLEMENTATION ORDER                                      │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  Phase 1: Workflow Classification & Provisioning                            │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │ 1.1 Add _classify_workflow() method to detect test workflows        │    │
│  │     • Inspect hooks for HookType.TEST                               │    │
│  │     • Return bool indicating is_test                                │    │
│  │                                                                      │    │
│  │ 1.2 Add _calculate_total_pool_size() method                         │    │
│  │     • Sum total_cores from all registered workers                   │    │
│  │     • Cache in _total_pool_size, update on worker changes           │    │
│  │                                                                      │    │
│  │ 1.3 Add _provision_workflows() method                               │    │
│  │     • Create configs with is_test, priority                         │    │
│  │     • Call Provisioner.partion_by_priority(configs)                 │    │
│  │     • Calculate VUs per thread for each workflow                    │    │
│  │     • Store in _job_workflow_configs                                │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│                                                                              │
│  Phase 2: Dependency Graph & Execution Order                                │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │ 2.1 Add _build_dependency_graph() method                            │    │
│  │     • Parse DependentWorkflow relationships                         │    │
│  │     • Build networkx.DiGraph                                        │    │
│  │     • BFS traversal to get execution layers                         │    │
│  │     • Store in _job_dependency_graphs                               │    │
│  │                                                                      │    │
│  │ 2.2 Update job_submission handler                                   │    │
│  │     • Classify workflows                                            │    │
│  │     • Build dependency graph                                        │    │
│  │     • Provision threads and VUs                                     │    │
│  │     • Check capacity before accepting                               │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│                                                                              │
│  Phase 3: Context Management                                                 │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │ 3.1 Add ContextUpdate message type                                  │    │
│  │     • job_id, workflow_name, context_values, lamport_clock          │    │
│  │                                                                      │    │
│  │ 3.2 Add context_update handler                                      │    │
│  │     • Receive from peer managers                                    │    │
│  │     • Apply with LWW conflict resolution                            │    │
│  │                                                                      │    │
│  │ 3.3 Update workflow_progress handler                                │    │
│  │     • Extract context updates from WorkflowProgress                 │    │
│  │     • Store in _job_contexts                                        │    │
│  │     • Broadcast to peer managers                                    │    │
│  │                                                                      │    │
│  │ 3.4 Update WorkflowDispatch to include dep context                  │    │
│  │     • Serialize relevant context for dependent workflows            │    │
│  │     • Worker deserializes and uses in execution                     │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│                                                                              │
│  Phase 4: Layer-Based Execution                                             │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │ 4.1 Add _execute_job_layer() method                                 │    │
│  │     • Dispatch all workflows in current layer                       │    │
│  │     • Wait for layer completion                                     │    │
│  │     • Sync context before next layer                                │    │
│  │                                                                      │    │
│  │ 4.2 Add _advance_to_next_layer() method                             │    │
│  │     • Check all layer workflows complete                            │    │
│  │     • Increment _job_current_layer                                  │    │
│  │     • Dispatch next layer if exists                                 │    │
│  │                                                                      │    │
│  │ 4.3 Update workflow completion handling                             │    │
│  │     • Track per-layer completion                                    │    │
│  │     • Trigger next layer when current completes                     │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│                                                                              │
│  Phase 5: Worker Integration                                                 │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │ 5.1 Update WorkflowDispatch message                                 │    │
│  │     • Add dependency_context field (serialized Context)            │    │
│  │     • Add vus_per_thread field (calculated VUs)                    │    │
│  │                                                                      │    │
│  │ 5.2 Update WorkflowProgress message                                 │    │
│  │     • Add context_updates field (for Provide hooks)                │    │
│  │     • Include Lamport timestamps                                    │    │
│  │                                                                      │    │
│  │ 5.3 Worker uses context in execution                                │    │
│  │     • Deserialize dependency context                                │    │
│  │     • Make available to Use hooks                                   │    │
│  │     • Serialize Provide hook results                                │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

## Final Results Flow

This section documents how workflow results, context, and errors flow back through the system after execution completes.

### Overview

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                      FINAL RESULTS FLOW OVERVIEW                             │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  Worker                  Manager                  Gate                Client │
│    │                        │                       │                    │   │
│    │ Execute workflow       │                       │                    │   │
│    │                        │                       │                    │   │
│    │ ① WorkflowFinalResult  │                       │                    │   │
│    │   (results, context,   │                       │                    │   │
│    │    error)              │                       │                    │   │
│    │───────────────────────►│                       │                    │   │
│    │                        │                       │                    │   │
│    │                        │ Store context         │                    │   │
│    │                        │ Sync to peers         │                    │   │
│    │                        │ Advance layers        │                    │   │
│    │                        │                       │                    │   │
│    │                        │ ② JobFinalResult      │                    │   │
│    │                        │   (per-DC results)    │                    │   │
│    │                        │──────────────────────►│                    │   │
│    │                        │                       │                    │   │
│    │                        │       OR (no gates)   │                    │   │
│    │                        │───────────────────────────────────────────►│   │
│    │                        │                       │                    │   │
│    │                        │                       │ ③ GlobalJobResult  │   │
│    │                        │                       │   (aggregated +    │   │
│    │                        │                       │    per-DC results) │   │
│    │                        │                       │───────────────────►│   │
│    │                        │                       │                    │   │
│                                                                              │
│  Key Principle: Workflow is NOT complete until final result is received     │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Message Types

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                      FINAL RESULT MESSAGE TYPES                              │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  ┌────────────────────────────────────────────────────────────────────────┐ │
│  │ WorkflowFinalResult (Worker → Manager)                                 │ │
│  ├────────────────────────────────────────────────────────────────────────┤ │
│  │                                                                         │ │
│  │  @dataclass                                                             │ │
│  │  class WorkflowFinalResult(Message):                                   │ │
│  │      job_id: str                                                        │ │
│  │      workflow_id: str                                                   │ │
│  │      status: str              # COMPLETED | FAILED                      │ │
│  │      results: bytes           # Cloudpickled WorkflowStats             │ │
│  │      context_updates: bytes   # Cloudpickled context dict              │ │
│  │      error: str | None = None # Error message (no traceback)           │ │
│  │                                                                         │ │
│  │  Note: WorkflowStats already contains:                                 │ │
│  │    • run_id: int              # Execution instance ID                  │ │
│  │    • elapsed: float           # Execution time                         │ │
│  │    • results: List[ResultSet] # Per-step results with stats           │ │
│  │    • metrics: List[MetricsSet]                                         │ │
│  │    • checks: List[CheckSet]                                            │ │
│  │    • aps: float               # Actions per second                     │ │
│  │                                                                         │ │
│  └────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
│  ┌────────────────────────────────────────────────────────────────────────┐ │
│  │ JobFinalResult (Manager → Gate OR Client)                              │ │
│  ├────────────────────────────────────────────────────────────────────────┤ │
│  │                                                                         │ │
│  │  @dataclass                                                             │ │
│  │  class JobFinalResult(Message):                                        │ │
│  │      job_id: str                                                        │ │
│  │      datacenter: str                                                    │ │
│  │      status: str              # COMPLETED | FAILED | PARTIAL           │ │
│  │      workflow_results: list[WorkflowResult]  # Per-workflow results    │ │
│  │      total_completed: int     # Total successful actions               │ │
│  │      total_failed: int        # Total failed actions                   │ │
│  │      errors: list[str]        # All error messages                     │ │
│  │      elapsed_seconds: float   # Max elapsed across workflows           │ │
│  │                                                                         │ │
│  │  @dataclass                                                             │ │
│  │  class WorkflowResult(Message):                                        │ │
│  │      workflow_id: str                                                   │ │
│  │      workflow_name: str                                                 │ │
│  │      status: str              # COMPLETED | FAILED                      │ │
│  │      results: bytes           # Cloudpickled WorkflowStats             │ │
│  │      error: str | None                                                  │ │
│  │                                                                         │ │
│  │  Note: Context is NOT included - gates don't need it                   │ │
│  │                                                                         │ │
│  └────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
│  ┌────────────────────────────────────────────────────────────────────────┐ │
│  │ GlobalJobResult (Gate → Client)                                        │ │
│  ├────────────────────────────────────────────────────────────────────────┤ │
│  │                                                                         │ │
│  │  @dataclass                                                             │ │
│  │  class GlobalJobResult(Message):                                       │ │
│  │      job_id: str                                                        │ │
│  │      status: str              # COMPLETED | FAILED | PARTIAL           │ │
│  │                                                                         │ │
│  │      # Per-datacenter breakdown                                         │ │
│  │      per_datacenter_results: list[JobFinalResult]                      │ │
│  │                                                                         │ │
│  │      # Cross-DC aggregated stats                                        │ │
│  │      aggregated: AggregatedJobStats                                    │ │
│  │                                                                         │ │
│  │      # Summary                                                          │ │
│  │      total_completed: int     # Sum across all DCs                     │ │
│  │      total_failed: int        # Sum across all DCs                     │ │
│  │      successful_datacenters: int                                        │ │
│  │      failed_datacenters: int                                            │ │
│  │      errors: list[str]        # All errors from all DCs                │ │
│  │      elapsed_seconds: float   # Max elapsed across all DCs             │ │
│  │                                                                         │ │
│  │  @dataclass                                                             │ │
│  │  class AggregatedJobStats(Message):                                    │ │
│  │      total_requests: int                                                │ │
│  │      successful_requests: int                                           │ │
│  │      failed_requests: int                                               │ │
│  │      overall_rate: float      # Combined rate (requests/sec)           │ │
│  │      avg_latency_ms: float                                              │ │
│  │      p50_latency_ms: float                                              │ │
│  │      p95_latency_ms: float                                              │ │
│  │      p99_latency_ms: float                                              │ │
│  │                                                                         │ │
│  └────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Step 1: Worker Sends Final Result

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    WORKER FINAL RESULT FLOW                                  │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  Workflow execution completes:                                               │
│                                                                              │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │  (results_run_id, results, context, error, status) =                │    │
│  │      await self._remote_manger.execute_workflow(...)                │    │
│  │                                                                      │    │
│  │  # results: WorkflowStats (has run_id, elapsed, step stats, etc.)  │    │
│  │  # context: Context (updated by Provide hooks)                      │    │
│  │  # error: Exception | None                                          │    │
│  │  # status: CoreWorkflowStatus                                       │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│                                                                              │
│  Worker sends final result:                                                  │
│                                                                              │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │  final_result = WorkflowFinalResult(                                │    │
│  │      job_id=dispatch.job_id,                                        │    │
│  │      workflow_id=dispatch.workflow_id,                              │    │
│  │      status=WorkflowStatus.COMPLETED.value if not error             │    │
│  │             else WorkflowStatus.FAILED.value,                       │    │
│  │      results=cloudpickle.dumps(results),  # WorkflowStats           │    │
│  │      context_updates=cloudpickle.dumps(                             │    │
│  │          context.dict() if context else {}                          │    │
│  │      ),                                                              │    │
│  │      error=str(error) if error else None,                           │    │
│  │  )                                                                   │    │
│  │                                                                      │    │
│  │  await self._send_final_result(final_result)                        │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│                                                                              │
│  Core freeing (always in finally block):                                    │
│                                                                              │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │  finally:                                                            │    │
│  │      self._free_cores(dispatch.workflow_id)  # ← ALWAYS called      │    │
│  │      self._increment_version()                                       │    │
│  │      # ... cleanup tracking dicts                                    │    │
│  │                                                                      │    │
│  │  Cores freed on:                                                     │    │
│  │    ✓ COMPLETED (success)                                            │    │
│  │    ✓ FAILED (error)                                                 │    │
│  │    ✓ CANCELLED (user cancel)                                        │    │
│  │    ✓ Any exception                                                  │    │
│  │                                                                      │    │
│  │  Note: Cores freed AFTER sending final result but REGARDLESS of     │    │
│  │        whether send succeeded. This prevents core leaks.            │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Step 2: Manager Processes Final Result

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    MANAGER FINAL RESULT PROCESSING                           │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │  @tcp.receive()                                                      │    │
│  │  async def workflow_final_result(self, addr, data, clock_time):     │    │
│  │      result = WorkflowFinalResult.load(data)                        │    │
│  │                                                                      │    │
│  │      # ─────────────────────────────────────────────────────────    │    │
│  │      # 1. Handle error case (NO RETRY - just mark as failed)       │    │
│  │      # ─────────────────────────────────────────────────────────    │    │
│  │      if result.error:                                                │    │
│  │          # Mark workflow as FAILED immediately - no retry           │    │
│  │          self._workflow_final_results[result.workflow_id] = result  │    │
│  │          if self._is_job_complete(result.job_id):                   │    │
│  │              await self._send_job_final_result(result.job_id)       │    │
│  │          return                                                      │    │
│  │                                                                      │    │
│  │      # ─────────────────────────────────────────────────────────    │    │
│  │      # 2. Store context for dependent workflows                     │    │
│  │      # ─────────────────────────────────────────────────────────    │    │
│  │      context_updates = cloudpickle.loads(result.context_updates)    │    │
│  │      job_context = self._job_contexts[result.job_id]                │    │
│  │      workflow_name = self._get_workflow_name(result.workflow_id)    │    │
│  │                                                                      │    │
│  │      for key, value in context_updates.items():                     │    │
│  │          await job_context.update(workflow_name, key, value)        │    │
│  │                                                                      │    │
│  │      # ─────────────────────────────────────────────────────────    │    │
│  │      # 3. Sync context to peer managers                             │    │
│  │      # ─────────────────────────────────────────────────────────    │    │
│  │      await self._broadcast_context_update(                          │    │
│  │          result.job_id, workflow_name, context_updates              │    │
│  │      )                                                               │    │
│  │                                                                      │    │
│  │      # ─────────────────────────────────────────────────────────    │    │
│  │      # 4. Store final result                                        │    │
│  │      # ─────────────────────────────────────────────────────────    │    │
│  │      self._workflow_final_results[result.workflow_id] = result      │    │
│  │                                                                      │    │
│  │      # ─────────────────────────────────────────────────────────    │    │
│  │      # 5. Check layer completion → advance                          │    │
│  │      # ─────────────────────────────────────────────────────────    │    │
│  │      if self._is_layer_complete(result.job_id):                     │    │
│  │          await self._advance_to_next_layer(result.job_id)           │    │
│  │                                                                      │    │
│  │      # ─────────────────────────────────────────────────────────    │    │
│  │      # 6. Check job completion → send final result                  │    │
│  │      # ─────────────────────────────────────────────────────────    │    │
│  │      if self._is_job_complete(result.job_id):                       │    │
│  │          await self._send_job_final_result(result.job_id)           │    │
│  │                                                                      │    │
│  │      return b'ok'                                                    │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│                                                                              │
│  Key Principle:                                                              │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │                                                                      │    │
│  │  Workflow is NOT complete until:                                    │    │
│  │    1. Worker sends WorkflowFinalResult                              │    │
│  │    2. Manager receives and processes it                             │    │
│  │    3. Manager stores in _workflow_final_results                     │    │
│  │                                                                      │    │
│  │  Progress updates (WorkflowProgress) are for monitoring only.       │    │
│  │  Final result is required for job completion.                       │    │
│  │                                                                      │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Step 3: Manager Sends Job Result

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    MANAGER SENDS JOB FINAL RESULT                            │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  When all workflows in a job complete (or fail):                            │
│                                                                              │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │  async def _send_job_final_result(self, job_id: str):               │    │
│  │      # Gather all workflow results                                   │    │
│  │      workflow_results = []                                           │    │
│  │      total_completed = 0                                             │    │
│  │      total_failed = 0                                                │    │
│  │      errors = []                                                     │    │
│  │      max_elapsed = 0.0                                               │    │
│  │                                                                      │    │
│  │      for wf_id, wf_result in self._workflow_final_results.items():  │    │
│  │          if wf_result.job_id != job_id:                             │    │
│  │              continue                                                │    │
│  │                                                                      │    │
│  │          stats = cloudpickle.loads(wf_result.results)               │    │
│  │          workflow_results.append(WorkflowResult(                    │    │
│  │              workflow_id=wf_id,                                      │    │
│  │              workflow_name=stats.get("workflow", ""),               │    │
│  │              status=wf_result.status,                                │    │
│  │              results=wf_result.results,  # Keep pickled             │    │
│  │              error=wf_result.error,                                  │    │
│  │          ))                                                          │    │
│  │                                                                      │    │
│  │          total_completed += stats.get("stats", {}).get("succeeded")│    │
│  │          total_failed += stats.get("stats", {}).get("failed", 0)   │    │
│  │          max_elapsed = max(max_elapsed, stats.get("elapsed", 0))   │    │
│  │          if wf_result.error:                                         │    │
│  │              errors.append(wf_result.error)                          │    │
│  │                                                                      │    │
│  │      # Determine job status                                          │    │
│  │      if all(r.status == "completed" for r in workflow_results):    │    │
│  │          status = "completed"                                        │    │
│  │      elif all(r.status == "failed" for r in workflow_results):     │    │
│  │          status = "failed"                                           │    │
│  │      else:                                                           │    │
│  │          status = "partial"                                          │    │
│  │                                                                      │    │
│  │      job_result = JobFinalResult(                                   │    │
│  │          job_id=job_id,                                              │    │
│  │          datacenter=self._node_id.datacenter,                        │    │
│  │          status=status,                                              │    │
│  │          workflow_results=workflow_results,                          │    │
│  │          total_completed=total_completed,                            │    │
│  │          total_failed=total_failed,                                  │    │
│  │          errors=errors,                                              │    │
│  │          elapsed_seconds=max_elapsed,                                │    │
│  │      )                                                               │    │
│  │                                                                      │    │
│  │      # ─────────────────────────────────────────────────────────    │    │
│  │      # Send to Gate OR Client                                       │    │
│  │      # ─────────────────────────────────────────────────────────    │    │
│  │      if self._known_gates:                                           │    │
│  │          await self._send_to_primary_gate(job_result)               │    │
│  │      else:                                                           │    │
│  │          # Direct client mode                                        │    │
│  │          callback = self._job_callbacks.get(job_id)                 │    │
│  │          if callback:                                                │    │
│  │              await self._send_to_client(callback, job_result)       │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│                                                                              │
│  Note: Context is NOT included in JobFinalResult                            │
│  Gates do not need context - it's internal to manager execution             │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Step 4: Gate Aggregates and Sends to Client

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    GATE CROSS-DC AGGREGATION                                 │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  Gate receives JobFinalResult from each datacenter:                         │
│                                                                              │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │  @tcp.receive()                                                      │    │
│  │  async def job_final_result(self, addr, data, clock_time):          │    │
│  │      result = JobFinalResult.load(data)                             │    │
│  │                                                                      │    │
│  │      # Store per-DC result                                          │    │
│  │      self._dc_final_results[result.job_id][result.datacenter] = result│   │
│  │                                                                      │    │
│  │      # Check if all DCs complete                                    │    │
│  │      if self._all_datacenters_complete(result.job_id):              │    │
│  │          await self._send_global_result_to_client(result.job_id)    │    │
│  │                                                                      │    │
│  │      return b'ok'                                                    │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│                                                                              │
│  Aggregation logic:                                                          │
│                                                                              │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │  async def _send_global_result_to_client(self, job_id: str):        │    │
│  │      dc_results = self._dc_final_results[job_id]                    │    │
│  │                                                                      │    │
│  │      # Aggregate stats across DCs                                   │    │
│  │      total_completed = sum(r.total_completed for r in dc_results)   │    │
│  │      total_failed = sum(r.total_failed for r in dc_results)         │    │
│  │      all_errors = [e for r in dc_results for e in r.errors]        │    │
│  │      max_elapsed = max(r.elapsed_seconds for r in dc_results)       │    │
│  │                                                                      │    │
│  │      successful_dcs = sum(1 for r in dc_results if r.status == "completed")│
│  │      failed_dcs = sum(1 for r in dc_results if r.status == "failed")│    │
│  │                                                                      │    │
│  │      # Determine global status                                       │    │
│  │      if failed_dcs == len(dc_results):                              │    │
│  │          status = "failed"                                           │    │
│  │      elif successful_dcs == len(dc_results):                        │    │
│  │          status = "completed"                                        │    │
│  │      else:                                                           │    │
│  │          status = "partial"                                          │    │
│  │                                                                      │    │
│  │      # Build aggregated stats                                        │    │
│  │      aggregated = self._compute_aggregated_stats(dc_results)        │    │
│  │                                                                      │    │
│  │      global_result = GlobalJobResult(                               │    │
│  │          job_id=job_id,                                              │    │
│  │          status=status,                                              │    │
│  │          per_datacenter_results=list(dc_results.values()),          │    │
│  │          aggregated=aggregated,                                      │    │
│  │          total_completed=total_completed,                            │    │
│  │          total_failed=total_failed,                                  │    │
│  │          successful_datacenters=successful_dcs,                      │    │
│  │          failed_datacenters=failed_dcs,                              │    │
│  │          errors=all_errors,                                          │    │
│  │          elapsed_seconds=max_elapsed,                                │    │
│  │      )                                                               │    │
│  │                                                                      │    │
│  │      callback = self._job_callbacks.get(job_id)                     │    │
│  │      if callback:                                                    │    │
│  │          await self._send_to_client(callback, global_result)        │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│                                                                              │
│  Client receives:                                                            │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │                                                                      │    │
│  │  GlobalJobResult:                                                    │    │
│  │  ├── status: "completed" | "failed" | "partial"                     │    │
│  │  ├── per_datacenter_results: [                                      │    │
│  │  │     JobFinalResult(datacenter="us-east-1", ...),                 │    │
│  │  │     JobFinalResult(datacenter="eu-west-1", ...),                 │    │
│  │  │   ]                                                               │    │
│  │  ├── aggregated: AggregatedJobStats(                                │    │
│  │  │     total_requests=50000,                                        │    │
│  │  │     successful_requests=49500,                                   │    │
│  │  │     overall_rate=5000.0,  # Combined across DCs                 │    │
│  │  │     avg_latency_ms=45.2,                                         │    │
│  │  │     p99_latency_ms=210.5,                                        │    │
│  │  │   )                                                               │    │
│  │  ├── errors: ["Workflow X failed: connection timeout", ...]        │    │
│  │  └── elapsed_seconds: 10.5                                          │    │
│  │                                                                      │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Error Handling Flow

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                         ERROR HANDLING FLOW                                  │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │                                                                      │    │
│  │  Worker: Workflow fails with error                                  │    │
│  │           │                                                          │    │
│  │           ▼                                                          │    │
│  │  WorkflowFinalResult(status="failed", error="...", results=...)    │    │
│  │           │                                                          │    │
│  │           ▼                                                          │    │
│  │  ─────────────────────────────────────────────────────────────────  │    │
│  │                                                                      │    │
│  │  Manager: Receives error result                                     │    │
│  │           │                                                          │    │
│  │           │  NO RETRY on workflow errors:                           │    │
│  │           │                                                          │    │
│  │           ├─► Mark workflow as FAILED immediately                   │    │
│  │           ├─► Store error in _workflow_final_results                │    │
│  │           └─► Check job completion                                  │    │
│  │               │                                                      │    │
│  │               ▼                                                      │    │
│  │  ─────────────────────────────────────────────────────────────────  │    │
│  │                                                                      │    │
│  │  Job complete with errors:                                          │    │
│  │           │                                                          │    │
│  │           ├───► Gates present?                                      │    │
│  │           │         │                                                │    │
│  │           │    YES: │                                                │    │
│  │           │         └─► Send JobFinalResult(status="failed"|"partial")│   │
│  │           │                to Gate                                   │    │
│  │           │                   │                                      │    │
│  │           │                   ▼                                      │    │
│  │           │             Gate aggregates, sends GlobalJobResult      │    │
│  │           │             to Client with error details                │    │
│  │           │                                                          │    │
│  │           │    NO (direct client mode):                             │    │
│  │           │         └─► Send JobFinalResult(status="failed"|"partial")│   │
│  │           │                directly to Client                       │    │
│  │           │                                                          │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│                                                                              │
│  Status Definitions:                                                         │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │                                                                      │    │
│  │  COMPLETED: All workflows in all DCs succeeded                      │    │
│  │  FAILED:    All workflows in ALL DCs failed (no usable results)    │    │
│  │  PARTIAL:   Some workflows/DCs succeeded, some failed              │    │
│  │             (partial results available)                             │    │
│  │                                                                      │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│                                                                              │
│  Important Distinction - Error vs Worker Failure:                            │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │                                                                      │    │
│  │  WORKFLOW ERROR (workflow returns error result):                    │    │
│  │    • NO RETRY - error is final                                      │    │
│  │    • Workflow marked FAILED immediately                             │    │
│  │    • Error included in final result to client                       │    │
│  │                                                                      │    │
│  │  WORKER FAILURE (SWIM detects worker is DEAD):                      │    │
│  │    • Retry workflow on different worker (see Worker Failure section)│    │
│  │    • Worker excluded from future dispatch for this workflow        │    │
│  │    • If max retries exhausted, then mark FAILED                    │    │
│  │                                                                      │    │
│  │  Rationale:                                                          │    │
│  │    • Worker failure = work never completed (worker crashed)         │    │
│  │    • Workflow error = work completed with error (retrying futile)  │    │
│  │                                                                      │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Complete Final Results State Machine

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    FINAL RESULTS STATE MACHINE                               │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│                        ┌──────────────────┐                                  │
│                        │    DISPATCHED    │                                  │
│                        │                  │                                  │
│                        │ Workflow sent to │                                  │
│                        │ worker           │                                  │
│                        └────────┬─────────┘                                  │
│                                 │                                            │
│                     worker executes                                          │
│                     sends WorkflowFinalResult                                │
│                                 │                                            │
│              ┌──────────────────┼──────────────────┐                         │
│              │                  │                  │                         │
│              ▼                  ▼                  ▼                         │
│    ┌──────────────┐   ┌──────────────┐   ┌──────────────┐                   │
│    │ RESULT_OK    │   │ RESULT_ERROR │   │ NO_RESULT    │                   │
│    │              │   │              │   │ (timeout)    │                   │
│    │ • Store      │   │ • NO RETRY   │   │              │                   │
│    │   results    │   │ • Mark as    │   │ • Treat as   │                   │
│    │ • Store      │   │   FAILED     │   │   failure    │                   │
│    │   context    │   │ • Store error│   │              │                   │
│    └──────┬───────┘   └──────┬───────┘   └──────┬───────┘                   │
│           │                  │                  │                           │
│           │                  │                  │                           │
│           ▼                  ▼                  ▼                           │
│    ┌─────────────────────────────────────────────────────────────────┐      │
│    │                    WORKFLOW COMPLETE                             │      │
│    │                                                                  │      │
│    │  Workflow is marked complete when:                              │      │
│    │  • WorkflowFinalResult received with status=COMPLETED           │      │
│    │  • OR WorkflowFinalResult received with status=FAILED           │      │
│    │  • OR timeout waiting for result (treated as FAILED)            │      │
│    │                                                                  │      │
│    │  NO RETRY on workflow errors - errors are final.                │      │
│    │                                                                  │      │
│    │  Cores freed: In worker's finally block (always)               │      │
│    └─────────────────────────────────────────────────────────────────┘      │
│                          │                                                   │
│                          │ all workflows complete                            │
│                          ▼                                                   │
│    ┌─────────────────────────────────────────────────────────────────┐      │
│    │                      JOB COMPLETE                                │      │
│    │                                                                  │      │
│    │  Manager builds JobFinalResult:                                 │      │
│    │  • Aggregates all workflow results                              │      │
│    │  • Collects all errors                                          │      │
│    │  • Determines status (completed|failed|partial)                 │      │
│    │  • Sends to Gate (or Client if no gates)                        │      │
│    └─────────────────────────────────────────────────────────────────┘      │
│                          │                                                   │
│                          │ Gate receives from all DCs                        │
│                          ▼                                                   │
│    ┌─────────────────────────────────────────────────────────────────┐      │
│    │                    GLOBAL JOB COMPLETE                           │      │
│    │                                                                  │      │
│    │  Gate builds GlobalJobResult:                                   │      │
│    │  • Per-datacenter results (detailed)                            │      │
│    │  • Cross-DC aggregated stats                                    │      │
│    │  • Combined errors list                                         │      │
│    │  • Sends to Client                                              │      │
│    └─────────────────────────────────────────────────────────────────┘      │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Context Flow Summary

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    CONTEXT VS RESULTS FLOW                                   │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │                          CONTEXT                                     │    │
│  │                                                                      │    │
│  │  Purpose: Share state between dependent workflows                   │    │
│  │                                                                      │    │
│  │  Flow:                                                               │    │
│  │    Worker ──context_updates──► Manager                              │    │
│  │                                   │                                  │    │
│  │                     ┌─────────────┼─────────────┐                   │    │
│  │                     ▼             ▼             ▼                   │    │
│  │               Store in     Sync to peer   Include in               │    │
│  │            _job_contexts    managers     dependent                  │    │
│  │                                          workflow                   │    │
│  │                                          dispatch                   │    │
│  │                                                                      │    │
│  │  NOT sent to: Gates, Clients                                        │    │
│  │  Gates don't need context - it's internal execution state          │    │
│  │                                                                      │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│                                                                              │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │                          RESULTS                                     │    │
│  │                                                                      │    │
│  │  Purpose: Report execution stats, errors, metrics                   │    │
│  │                                                                      │    │
│  │  Flow:                                                               │    │
│  │    Worker ──WorkflowStats──► Manager                                │    │
│  │                                 │                                    │    │
│  │                     ┌───────────┴───────────┐                       │    │
│  │                     ▼                       ▼                       │    │
│  │            JobFinalResult           JobFinalResult                  │    │
│  │            (to Gate)                (to Client, no gates)           │    │
│  │                 │                                                    │    │
│  │                 ▼                                                    │    │
│  │          GlobalJobResult                                            │    │
│  │          (to Client)                                                │    │
│  │                                                                      │    │
│  │  Sent to: Gates AND Clients                                         │    │
│  │  Contains: WorkflowStats (stats, metrics, errors, timing)          │    │
│  │                                                                      │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

## Context Consistency Protocol

This section details how context is synchronized across managers to ensure dependent
workflows always see the correct, latest context from their dependencies.

### Workflow Context API

Context enables workflows to share state with their dependents. This is critical for
scenarios where one workflow produces data (e.g., authentication tokens, session IDs)
that subsequent workflows need to consume.

#### Decorators and Type Hints

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                       WORKFLOW CONTEXT API                                   │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  Decorators:                                                                 │
│  ────────────────────────────────────────────────────────────────────────   │
│                                                                              │
│    @state('WorkflowName', ...)                                               │
│      • Marks a method for context interaction                               │
│      • MUST specify target workflow name(s) as string arguments             │
│      • If no args provided → no context flows (nothing to select from)      │
│                                                                              │
│    @depends('WorkflowName', ...)                                             │
│      • Wraps a Workflow class to declare execution dependencies             │
│      • Dependent workflow executes AFTER all specified dependencies         │
│      • Can specify multiple dependencies as separate string arguments       │
│                                                                              │
│  Type Hints (Return Types):                                                  │
│  ────────────────────────────────────────────────────────────────────────   │
│                                                                              │
│    Provide[T]                                                                │
│      • Indicates the method PROVIDES context to specified workflow(s)       │
│      • Return value is stored in context                                    │
│      • Method name becomes the context KEY                                  │
│                                                                              │
│    Use[T]                                                                    │
│      • Indicates the method USES context from specified workflow(s)         │
│      • Keyword argument names must match context keys                       │
│      • Values are injected from context; use default for missing keys       │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

#### Complete Example

```python
from hyperscale import Workflow, depends, state, step
from hyperscale.core.hooks import Provide, Use


class AuthWorkflow(Workflow):
    """First workflow - authenticates and provides token to dependents."""
    vus = 100
    duration = "30s"

    @step()
    async def login(self, url: URL = 'https://api.example.com/login') -> HTTPResponse:
        return await self.client.http.post(url, json={"user": "test"})
    
    @state('DataWorkflow')  # ← Share WITH DataWorkflow
    def auth_token(self) -> Provide[str]:  # ← Method name = context key
        """Provides authentication token to DataWorkflow."""
        return self.login.response.json()['token']


@depends('AuthWorkflow')  # ← Wait for AuthWorkflow to complete first
class DataWorkflow(Workflow):
    """Second workflow - uses token from AuthWorkflow."""
    vus = 100
    duration = "30s"

    @state('AuthWorkflow')  # ← Receive FROM AuthWorkflow
    def get_token(self, auth_token: str | None = None) -> Use[str]:  # ← kwarg matches key
        """Receives authentication token from AuthWorkflow."""
        return auth_token  # Will be injected with the token value

    @step()
    async def fetch_data(self, url: URL = 'https://api.example.com/data') -> HTTPResponse:
        token = self.get_token()  # Access the consumed token
        return await self.client.http.get(
            url, 
            headers={"Authorization": f"Bearer {token}"}
        )
```

#### Context Flow Diagram

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                         CONTEXT FLOW                                         │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  Execution Order (determined by @depends):                                   │
│                                                                              │
│  ┌─────────────────────┐                                                    │
│  │  Layer 0            │                                                    │
│  │  ─────────────────  │                                                    │
│  │  AuthWorkflow runs  │                                                    │
│  │  (no dependencies)  │                                                    │
│  └──────────┬──────────┘                                                    │
│             │                                                                │
│             │  @state('DataWorkflow')                                       │
│             │  def auth_token() -> Provide[str]:                            │
│             │      return 'eyJhbGc...'                                      │
│             │                                                                │
│             ▼                                                                │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │                     CONTEXT STORAGE                                  │    │
│  │  ┌─────────────────────────────────────────────────────────────┐    │    │
│  │  │  context['AuthWorkflow']['auth_token'] = 'eyJhbGc...'       │    │    │
│  │  └─────────────────────────────────────────────────────────────┘    │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│             │                                                                │
│             │  ┌──────────────────────────────────────────┐                 │
│             │  │  DISTRIBUTED: Quorum sync at layer      │                 │
│             │  │  boundary ensures all managers have     │                 │
│             │  │  context before Layer 1 dispatches      │                 │
│             │  └──────────────────────────────────────────┘                 │
│             │                                                                │
│             ▼                                                                │
│  ┌─────────────────────┐                                                    │
│  │  Layer 1            │                                                    │
│  │  ─────────────────  │                                                    │
│  │  DataWorkflow runs  │                                                    │
│  │  @depends('Auth')   │                                                    │
│  └──────────┬──────────┘                                                    │
│             │                                                                │
│             │  @state('AuthWorkflow')                                       │
│             │  def get_token(auth_token=None) -> Use[str]:                  │
│             │                     ▲                                         │
│             │                     │                                         │
│             │          ┌──────────┴──────────┐                              │
│             │          │  Kwarg 'auth_token' │                              │
│             │          │  matches context    │                              │
│             │          │  key 'auth_token'   │                              │
│             │          │  ─────────────────  │                              │
│             │          │  Injected value:    │                              │
│             │          │  'eyJhbGc...'       │                              │
│             │          └─────────────────────┘                              │
│             │                                                                │
│             ▼                                                                │
│       DataWorkflow.get_token() returns 'eyJhbGc...'                         │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

#### Context API Rules Summary

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                       CONTEXT API RULES                                      │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  PROVIDER (sends context):                                                   │
│  ────────────────────────────────────────────────────────────────────────   │
│    @state('TargetWorkflow')      ← Specify WHO receives this context        │
│    def method_name(...):         ← Method name becomes context KEY          │
│        -> Provide[T]             ← Declares providing intent                │
│        return value              ← Return value is stored as context VALUE  │
│                                                                              │
│  CONSUMER (receives context):                                                │
│  ────────────────────────────────────────────────────────────────────────   │
│    @depends('SourceWorkflow')    ← Ensures source runs first (class level) │
│    @state('SourceWorkflow')      ← Specify WHO to receive FROM              │
│    def consume(                                                              │
│        kwarg_name: T | None = None  ← Kwarg name MUST match context key    │
│    ):                                                                        │
│        -> Use[T]                 ← Declares consuming intent                │
│        return kwarg_name         ← Use the injected value                   │
│                                                                              │
│  KEY MATCHING:                                                               │
│  ────────────────────────────────────────────────────────────────────────   │
│    Provider method name  ──────────────►  Consumer kwarg name               │
│    e.g., 'auth_token'    ◄─── MUST MATCH ───►  'auth_token'                 │
│                                                                              │
│  BIDIRECTIONAL CONTRACT:                                                     │
│  ────────────────────────────────────────────────────────────────────────   │
│    • Provider MUST name the target: @state('ConsumerWorkflow')              │
│    • Consumer MUST name the source: @state('ProviderWorkflow')              │
│    • Context only flows when BOTH sides agree on the relationship           │
│    • @state() with NO args = no context flows (no workflow selected)        │
│                                                                              │
│  MULTIPLE TARGETS/SOURCES:                                                   │
│  ────────────────────────────────────────────────────────────────────────   │
│    @state('WorkflowA', 'WorkflowB')  ← Share with multiple workflows        │
│    @depends('WorkflowA', 'WorkflowB') ← Depend on multiple workflows        │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

### The Problem

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    CONTEXT SYNC RACE CONDITION                               │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  Timeline (problematic):                                                     │
│  ───────────────────────────────────────────────────────────────────────    │
│  Manager A                      Manager B                    Worker (on B)   │
│  ───────────────────────────────────────────────────────────────────────    │
│  WorkflowFinalResult                                                         │
│    (context: {auth: token123})                                              │
│      │                                                                       │
│      ├─► Store context locally                                              │
│      │                                                                       │
│      ├─► Broadcast to B ──────────► (in flight...)                          │
│      │                                                                       │
│      ├─► Advance to layer 2                                                 │
│      │                                                                       │
│      ├─► Dispatch DependentWorkflow ──────────────────────► Receives!       │
│      │   to Worker on Manager B                              But context    │
│                                        │                     hasn't arrived │
│                                        ▼                     at Manager B!  │
│                                    Receives context                          │
│                                    (too late!)                               │
│  ───────────────────────────────────────────────────────────────────────    │
│                                                                              │
│  Result: DependentWorkflow executes with STALE or MISSING context!          │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Distributed Consistency Approaches Analyzed

Before choosing our approach, we analyzed how major distributed systems solve this:

```
┌─────────────────────────────────────────────────────────────────────────────┐
│              DISTRIBUTED CONSISTENCY APPROACHES COMPARISON                   │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  1. REDIS SENTINEL / REDIS CLUSTER                                           │
│  ────────────────────────────────────────────────────────────────────────   │
│  Mechanism:                                                                  │
│    • Asynchronous replication from master to replicas                       │
│    • Gossip-based cluster state                                             │
│    • Failover via Sentinel consensus                                        │
│                                                                              │
│  For Context Sync:                                                           │
│    ❌ Async replication means writes can be lost during failover            │
│    ❌ We can't afford lost context updates                                  │
│                                                                              │
│  ─────────────────────────────────────────────────────────────────────────  │
│                                                                              │
│  2. ETCD / RAFT CONSENSUS                                                    │
│  ────────────────────────────────────────────────────────────────────────   │
│  Mechanism:                                                                  │
│    • Strong consistency via Raft log replication                            │
│    • Every write goes through leader                                        │
│    • Leader replicates log entry to majority BEFORE acknowledging           │
│    • Committed = in majority's log                                          │
│                                                                              │
│  For Context Sync:                                                           │
│    ✅ Strong consistency - no lost writes                                   │
│    ✅ We already have leader election                                       │
│    ❌ Every context key update would need consensus (high latency)          │
│    ❌ Log grows unbounded (need compaction)                                 │
│                                                                              │
│  ─────────────────────────────────────────────────────────────────────────  │
│                                                                              │
│  3. COCKROACHDB / SPANNER - HYBRID LOGICAL CLOCKS (HLC)                      │
│  ────────────────────────────────────────────────────────────────────────   │
│  Mechanism:                                                                  │
│    • HLC = max(physical_time, last_hlc) + logical_counter                   │
│    • Combines wall-clock ordering with logical consistency                  │
│    • MVCC: reads at timestamp T see consistent snapshot as of T            │
│    • Spanner uses TrueTime (GPS + atomic clocks) for global ordering        │
│                                                                              │
│  For Context Sync:                                                           │
│    ✅ Global ordering without coordination                                  │
│    ✅ Physical time component aids debugging                                │
│    ✅ Snapshot reads at specific version                                    │
│    ❌ Requires reasonably synchronized clocks (NTP usually sufficient)      │
│                                                                              │
│  ─────────────────────────────────────────────────────────────────────────  │
│                                                                              │
│  4. CASSANDRA - TUNABLE CONSISTENCY WITH LWW                                 │
│  ────────────────────────────────────────────────────────────────────────   │
│  Mechanism:                                                                  │
│    • Write to N replicas, wait for W acknowledgments                        │
│    • Read from R replicas, return highest timestamp                         │
│    • Consistency levels: ONE, QUORUM, ALL                                   │
│    • Last-Write-Wins (LWW) with timestamps for conflict resolution          │
│                                                                              │
│  For Context Sync:                                                           │
│    ✅ Flexible consistency levels                                           │
│    ✅ Quorum writes ensure durability                                       │
│    ✅ LWW handles concurrent writes                                         │
│    ❌ Wall-clock skew can cause "wrong" winner                              │
│                                                                              │
│  ─────────────────────────────────────────────────────────────────────────  │
│                                                                              │
│  5. DYNAMODB / RIAK - VECTOR CLOCKS + APPLICATION RESOLUTION                 │
│  ────────────────────────────────────────────────────────────────────────   │
│  Mechanism:                                                                  │
│    • Vector clock per key tracks causal history                             │
│    • On conflict, ALL versions returned to application                      │
│    • Application decides how to merge                                       │
│    • Anti-entropy (Merkle trees) for background sync                        │
│                                                                              │
│  For Context Sync:                                                           │
│    ✅ Precise causal tracking                                               │
│    ✅ No lost updates (all kept until resolved)                             │
│    ❌ Complex: application must handle conflicts                            │
│    ❌ Vector clock size grows with writers                                  │
│                                                                              │
│  ─────────────────────────────────────────────────────────────────────────  │
│                                                                              │
│  6. CRDTs (CONFLICT-FREE REPLICATED DATA TYPES)                              │
│  ────────────────────────────────────────────────────────────────────────   │
│  Mechanism:                                                                  │
│    • Data structures with mathematically-proven merge functions             │
│    • LWWRegister: Last-writer-wins with timestamp                           │
│    • GCounter: Grow-only counter (sum of per-node counters)                 │
│    • Merge is associative, commutative, idempotent                          │
│                                                                              │
│  For Context Sync:                                                           │
│    ✅ No coordination needed - always merge                                 │
│    ✅ Eventually consistent automatically                                   │
│    ❌ Limited to CRDT-compatible types                                      │
│    ❌ "Eventually" may not be fast enough                                   │
│                                                                              │
│  ─────────────────────────────────────────────────────────────────────────  │
│                                                                              │
│  7. SINGLE-WRITER PATTERN (KAFKA PARTITION LEADER)                           │
│  ────────────────────────────────────────────────────────────────────────   │
│  Mechanism:                                                                  │
│    • Each partition has exactly one leader                                  │
│    • Only leader accepts writes                                             │
│    • Followers replicate from leader                                        │
│    • No conflicts possible (single source of truth)                         │
│                                                                              │
│  For Context Sync:                                                           │
│    ✅ Simplest consistency model                                            │
│    ✅ No conflicts by design                                                │
│    ✅ We already have job leader                                            │
│    ❌ Leader is bottleneck/SPOF for that job                                │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Comparison Matrix

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                         APPROACH COMPARISON MATRIX                           │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  Approach              │ Consistency │ Latency │ Complexity │ Failure       │
│  ──────────────────────┼─────────────┼─────────┼────────────┼──────────────│
│  Async Replication     │ Eventual    │ Low     │ Low        │ May lose     │
│  (Redis)               │             │         │            │ writes       │
│  ──────────────────────┼─────────────┼─────────┼────────────┼──────────────│
│  Raft Log              │ Strong      │ High    │ High       │ Leader       │
│  (etcd)                │ (linear.)   │         │            │ election     │
│  ──────────────────────┼─────────────┼─────────┼────────────┼──────────────│
│  HLC + MVCC            │ Strong      │ Medium  │ Medium     │ Timestamp    │
│  (Spanner)             │             │         │            │ based        │
│  ──────────────────────┼─────────────┼─────────┼────────────┼──────────────│
│  Quorum + LWW          │ Tunable     │ Medium  │ Medium     │ Quorum       │
│  (Cassandra)           │             │         │            │ tolerant     │
│  ──────────────────────┼─────────────┼─────────┼────────────┼──────────────│
│  Vector Clocks         │ Causal      │ Low     │ High       │ App          │
│  (Dynamo)              │             │         │            │ resolves     │
│  ──────────────────────┼─────────────┼─────────┼────────────┼──────────────│
│  CRDTs                 │ Eventual    │ Low     │ Medium     │ Automatic    │
│                        │             │         │            │ merge        │
│  ──────────────────────┼─────────────┼─────────┼────────────┼──────────────│
│  Single-Writer         │ Strong      │ Low     │ Low        │ Leader       │
│                        │             │         │            │ recovery     │
│  ──────────────────────┴─────────────┴─────────┴────────────┴──────────────│
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Chosen Approach: Hybrid Single-Writer + Quorum Replication

We combine the best properties from multiple approaches:

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                     CHOSEN: HYBRID APPROACH                                  │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  From etcd/Raft:                                                             │
│    → Single leader (job leader) is source of truth                          │
│    → Quorum confirmation before advancing                                   │
│                                                                              │
│  From Cassandra:                                                             │
│    → Tunable consistency (QUORUM for context sync)                          │
│    → LWW for any edge-case conflicts                                        │
│                                                                              │
│  From Spanner:                                                               │
│    → Context embedded in dispatch (like snapshot reads)                     │
│    → Version number for stale detection                                     │
│                                                                              │
│  From Kafka:                                                                 │
│    → Single-writer per partition (job)                                      │
│    → No conflicts by construction                                           │
│                                                                              │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  Key Insight: Layers are natural synchronization points.                     │
│  A dependent workflow in layer N+1 can ONLY depend on workflows             │
│  from layers ≤ N. Therefore: sync context at layer boundaries.              │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Protocol Specification

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    CONTEXT CONSISTENCY PROTOCOL                              │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  Principle: Single-Writer + Quorum Replication + Embedded Context            │
│                                                                              │
│  ┌──────────────────────────────────────────────────────────────────────┐   │
│  │                                                                      │   │
│  │  1. Job leader is SINGLE WRITER for job's context                   │   │
│  │     → No conflicts possible (only one writer)                       │   │
│  │     → Simplest consistency model                                    │   │
│  │                                                                      │   │
│  │  2. Workers send results to their manager                           │   │
│  │     → Manager forwards context updates to job leader                │   │
│  │     → Only leader applies updates to authoritative context          │   │
│  │                                                                      │   │
│  │  3. Layer boundaries trigger quorum sync                            │   │
│  │     → Leader creates versioned snapshot                             │   │
│  │     → Leader broadcasts to peers, waits for quorum ack              │   │
│  │     → Peers store snapshot (for failover)                           │   │
│  │                                                                      │   │
│  │  4. Dispatch includes context snapshot                              │   │
│  │     → No extra fetch needed                                         │   │
│  │     → Version number for stale detection                            │   │
│  │                                                                      │   │
│  └──────────────────────────────────────────────────────────────────────┘   │
│                                                                              │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  Manager State (New Fields):                                                 │
│  ┌──────────────────────────────────────────────────────────────────────┐   │
│  │                                                                      │   │
│  │  _job_contexts: Dict[job_id, Context]                               │   │
│  │    # Authoritative context (only job leader writes)                 │   │
│  │                                                                      │   │
│  │  _job_layer_version: Dict[job_id, int]                              │   │
│  │    # Monotonically increasing per job                               │   │
│  │    # Incremented when layer completes and context is synced         │   │
│  │                                                                      │   │
│  │  _job_leaders: Dict[job_id, str]                                    │   │
│  │    # job_id → leader_node_id                                        │   │
│  │    # Set when job is first accepted                                 │   │
│  │                                                                      │   │
│  │  _context_lamport_clock: int                                        │   │
│  │    # For per-key LWW timestamps (edge cases)                        │   │
│  │                                                                      │   │
│  └──────────────────────────────────────────────────────────────────────┘   │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Protocol Flow

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                         PROTOCOL FLOW                                        │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  Step 1: Workflow Completes with Context Updates                            │
│  ────────────────────────────────────────────────────────────────────────   │
│                                                                              │
│  WorkflowFinalResult includes:                                               │
│    context_updates: bytes      # Serialized Dict[key, value]                │
│    context_timestamps: bytes   # Serialized Dict[key, lamport_clock]        │
│                                                                              │
│  ┌──────────────────────────────────────────────────────────────────────┐   │
│  │  # On receiving manager (may or may not be job leader):             │   │
│  │                                                                      │   │
│  │  async def workflow_final_result(self, addr, data, clock_time):     │   │
│  │      result = WorkflowFinalResult.load(data)                        │   │
│  │      job_leader = self._job_leaders[result.job_id]                  │   │
│  │                                                                      │   │
│  │      if self._node_id != job_leader:                                │   │
│  │          # Forward context to job leader                            │   │
│  │          await self._forward_context_to_leader(                     │   │
│  │              result.job_id, result.context_updates,                 │   │
│  │              result.context_timestamps                              │   │
│  │          )                                                          │   │
│  │      else:                                                          │   │
│  │          # We are job leader - apply directly                       │   │
│  │          await self._apply_context_updates(                         │   │
│  │              result.job_id, result.workflow_id,                     │   │
│  │              result.context_updates, result.context_timestamps      │   │
│  │          )                                                          │   │
│  │                                                                      │   │
│  │      # ... rest of result handling                                  │   │
│  │                                                                      │   │
│  └──────────────────────────────────────────────────────────────────────┘   │
│                                                                              │
│  Step 2: Job Leader Applies Context (LWW)                                    │
│  ────────────────────────────────────────────────────────────────────────   │
│                                                                              │
│  ┌──────────────────────────────────────────────────────────────────────┐   │
│  │  async def _apply_context_updates(                                  │   │
│  │      self, job_id, workflow_id, updates_bytes, timestamps_bytes     │   │
│  │  ):                                                                 │   │
│  │      updates = cloudpickle.loads(updates_bytes)                     │   │
│  │      timestamps = cloudpickle.loads(timestamps_bytes)               │   │
│  │      context = self._job_contexts[job_id]                           │   │
│  │      workflow_name = self._get_workflow_name(workflow_id)           │   │
│  │                                                                      │   │
│  │      for key, value in updates.items():                             │   │
│  │          timestamp = timestamps.get(key, self._context_lamport_clock)│   │
│  │          await context.update(                                      │   │
│  │              workflow_name, key, value,                             │   │
│  │              timestamp=timestamp,                                   │   │
│  │              source_node=self._node_id                              │   │
│  │          )                                                          │   │
│  │                                                                      │   │
│  │      self._context_lamport_clock += 1                               │   │
│  │                                                                      │   │
│  └──────────────────────────────────────────────────────────────────────┘   │
│                                                                              │
│  Step 3: Layer Completion Triggers Quorum Sync                               │
│  ────────────────────────────────────────────────────────────────────────   │
│                                                                              │
│  ┌──────────────────────────────────────────────────────────────────────┐   │
│  │  async def _sync_context_and_advance(self, job_id: str):             │   │
│  │      # Only job leader does this                                    │   │
│  │      assert self._job_leaders[job_id] == self._node_id              │   │
│  │                                                                      │   │
│  │      # 1. Increment layer version                                   │   │
│  │      new_version = self._job_layer_version[job_id] + 1              │   │
│  │      self._job_layer_version[job_id] = new_version                  │   │
│  │                                                                      │   │
│  │      # 2. Create context snapshot                                   │   │
│  │      context = self._job_contexts[job_id]                           │   │
│  │      snapshot = ContextLayerSync(                                   │   │
│  │          job_id=job_id,                                             │   │
│  │          layer_version=new_version,                                 │   │
│  │          context_snapshot=cloudpickle.dumps(context.dict()),       │   │
│  │          source_node_id=self._node_id                               │   │
│  │      )                                                              │   │
│  │                                                                      │   │
│  │      # 3. Broadcast to peers and WAIT for quorum                    │   │
│  │      confirmations = await self._broadcast_context_sync(snapshot)   │   │
│  │                                                                      │   │
│  │      if confirmations < self._quorum_size:                          │   │
│  │          raise QuorumTimeoutError("Context sync failed")            │   │
│  │                                                                      │   │
│  │      # 4. ONLY THEN advance to next layer                           │   │
│  │      await self._dispatch_next_layer(job_id)                        │   │
│  │                                                                      │   │
│  └──────────────────────────────────────────────────────────────────────┘   │
│                                                                              │
│  Step 4: Dependent Workflow Dispatch Includes Context                        │
│  ────────────────────────────────────────────────────────────────────────   │
│                                                                              │
│  ┌──────────────────────────────────────────────────────────────────────┐   │
│  │  WorkflowDispatch (updated fields):                                  │   │
│  │    ...                                                               │   │
│  │    context_version: int           # Expected layer version          │   │
│  │    dependency_context: bytes      # Context from dependencies       │   │
│  │                                                                      │   │
│  │  # Extracting just what the workflow needs:                         │   │
│  │  def _extract_dependency_context(self, job_id, workflow_name):      │   │
│  │      dependencies = self._get_workflow_dependencies(workflow_name)   │   │
│  │      context = self._job_contexts[job_id]                           │   │
│  │      relevant = {}                                                   │   │
│  │      for dep_workflow in dependencies:                              │   │
│  │          if dep_workflow in context._context:                       │   │
│  │              relevant[dep_workflow] = context[dep_workflow].dict()  │   │
│  │      return cloudpickle.dumps(relevant)                             │   │
│  │                                                                      │   │
│  └──────────────────────────────────────────────────────────────────────┘   │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

### New Messages

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                         CONTEXT SYNC MESSAGES                                │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  @dataclass                                                                  │
│  class ContextForward(Message):                                              │
│      """Non-leader forwards context updates to job leader"""                │
│      job_id: str                                                             │
│      workflow_id: str                                                        │
│      context_updates: bytes      # Serialized dict                          │
│      context_timestamps: bytes   # Per-key Lamport timestamps               │
│      source_manager: str         # Who received from worker                 │
│                                                                              │
│  @dataclass                                                                  │
│  class ContextLayerSync(Message):                                            │
│      """Job leader broadcasts at layer completion"""                        │
│      job_id: str                                                             │
│      layer_version: int          # Monotonic per job                        │
│      context_snapshot: bytes     # Full context as of this layer            │
│      source_node_id: str         # Job leader's node ID                     │
│                                                                              │
│  @dataclass                                                                  │
│  class ContextLayerSyncAck(Message):                                         │
│      """Peer confirms receipt of context sync"""                            │
│      job_id: str                                                             │
│      layer_version: int                                                      │
│      applied: bool               # True if applied, False if stale          │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Conflict Resolution (Edge Cases)

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    LWW CONFLICT RESOLUTION                                   │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  Already implemented in WorkflowContext.set():                               │
│    • If new_timestamp > existing_timestamp: accept                          │
│    • If new_timestamp <= existing_timestamp: reject (stale)                 │
│                                                                              │
│  Enhanced for tie-breaking (same timestamp):                                 │
│  ┌──────────────────────────────────────────────────────────────────────┐   │
│  │                                                                      │   │
│  │  async def set(self, key, value, timestamp, source_node=None):       │   │
│  │      async with self._write_lock:                                    │   │
│  │          existing_ts = self._timestamps.get(key)                     │   │
│  │          existing_src = self._sources.get(key)                       │   │
│  │                                                                      │   │
│  │          should_update = (                                           │   │
│  │              existing_ts is None or                                  │   │
│  │              timestamp > existing_ts or                              │   │
│  │              (timestamp == existing_ts and                           │   │
│  │               source_node and existing_src and                       │   │
│  │               source_node > existing_src)  # Tiebreaker             │   │
│  │          )                                                           │   │
│  │                                                                      │   │
│  │          if should_update:                                           │   │
│  │              self._context[key] = value                              │   │
│  │              self._timestamps[key] = timestamp                       │   │
│  │              self._sources[key] = source_node                        │   │
│  │                                                                      │   │
│  └──────────────────────────────────────────────────────────────────────┘   │
│                                                                              │
│  Note: With single-writer (job leader), conflicts should not occur.         │
│  LWW is defensive programming for edge cases (leader failover, etc.)        │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Correctness Guarantees

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                       CORRECTNESS GUARANTEES                                 │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  1. ORDERING                                                                 │
│     Layer N+1 workflows NEVER execute before layer N context is             │
│     synced to quorum.                                                        │
│                                                                              │
│  2. CONSISTENCY                                                              │
│     Single writer (job leader) means no conflicts. LWW with                 │
│     timestamps handles edge cases (failover).                               │
│                                                                              │
│  3. DURABILITY                                                               │
│     Quorum confirmation means majority has context before advancing.        │
│     If leader fails, another manager has the snapshot.                      │
│                                                                              │
│  4. NO EXTRA FETCHES                                                         │
│     Context is embedded in WorkflowDispatch. Worker has everything          │
│     it needs immediately.                                                    │
│                                                                              │
│  5. VERSION VERIFICATION                                                     │
│     context_version in dispatch allows worker to detect stale               │
│     dispatches (e.g., from a lagging manager).                              │
│                                                                              │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  Corrected Timeline:                                                         │
│  ───────────────────────────────────────────────────────────────────────    │
│  Job Leader (A)              Manager B                                       │
│  ───────────────────────────────────────────────────────────────────────    │
│  WorkflowFinalResult                                                         │
│    (context: {auth: token123})                                              │
│      │                                                                       │
│      ├─► Store context locally                                              │
│      │                                                                       │
│      ├─► Layer complete!                                                    │
│      │                                                                       │
│      ├─► Broadcast ContextLayerSync ──────► Receives, stores                │
│      │                                          │                            │
│      │   ◄──────────────────────────────────── Sends ack                    │
│      │                                                                       │
│      ├─► Quorum reached ✓                                                   │
│      │                                                                       │
│      ├─► NOW dispatch layer 2 ────────────► Receives dispatch               │
│      │   (includes context_version=2,       (has correct context!)          │
│      │    dependency_context={auth: ...})                                   │
│  ───────────────────────────────────────────────────────────────────────    │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Drawbacks and Mitigations

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                              DRAWBACKS                                       │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  1. LEADER BOTTLENECK                                                        │
│     ────────────────────────────────────────────────────────────────────    │
│     • All context updates funnel through job leader                         │
│     • Leader does more work than peers                                      │
│                                                                              │
│     Mitigation: Layer batching reduces frequency. One leader per JOB,       │
│                 not per cluster - load distributed across jobs.             │
│                                                                              │
│  2. LEADER FAILURE RECOVERY                                                  │
│     ────────────────────────────────────────────────────────────────────    │
│     • If leader fails mid-layer, context updates in flight may be lost     │
│     • New leader must recover from last quorum-synced snapshot              │
│                                                                              │
│     Mitigation: Layer snapshots are quorum-replicated. Worst case:          │
│                 re-execute current layer (idempotent workflows help).       │
│                                                                              │
│  3. QUORUM UNAVAILABILITY                                                    │
│     ────────────────────────────────────────────────────────────────────    │
│     • If < quorum managers available, can't advance layers                  │
│     • Job blocks waiting for quorum                                         │
│                                                                              │
│     Mitigation: Circuit breaker + configurable timeout. Return partial      │
│                 results or fail job with clear error.                       │
│                                                                              │
│  4. INCREASED MESSAGE SIZE                                                   │
│     ────────────────────────────────────────────────────────────────────    │
│     • Context embedded in every WorkflowDispatch                            │
│     • Large contexts = larger messages                                      │
│                                                                              │
│     Mitigation: Only include dependencies' context, not full context.       │
│                 Compress large contexts.                                     │
│                                                                              │
│  5. NOT SUITABLE FOR FINE-GRAINED UPDATES                                    │
│     ────────────────────────────────────────────────────────────────────    │
│     • Designed for layer-boundary sync                                      │
│     • High-frequency mid-workflow updates would be slow                     │
│                                                                              │
│     Mitigation: Context is for workflow outputs, not streaming data.        │
│                 Use separate mechanism for real-time data if needed.        │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Integration with Existing Architecture

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    DATACENTER ROUTING COMPATIBILITY                          │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  Impact Analysis:                                                            │
│                                                                              │
│  ┌───────────────────────────────────────────────────────────────────────┐  │
│  │ Component              │ Impact      │ Notes                         │  │
│  ├────────────────────────┼─────────────┼───────────────────────────────┤  │
│  │ Gate → Manager submit  │ None        │ Context sync is internal      │  │
│  ├────────────────────────┼─────────────┼───────────────────────────────┤  │
│  │ DC health routing      │ Integrates  │ Quorum issues = degraded DC   │  │
│  ├────────────────────────┼─────────────┼───────────────────────────────┤  │
│  │ Manager → Worker       │ Larger msgs │ Context embedded in dispatch  │  │
│  ├────────────────────────┼─────────────┼───────────────────────────────┤  │
│  │ Worker → Manager       │ Extra hop   │ Non-leader forwards to leader │  │
│  ├────────────────────────┼─────────────┼───────────────────────────────┤  │
│  │ Cross-DC dependencies  │ N/A         │ Not supported (each DC indep) │  │
│  ├────────────────────────┼─────────────┼───────────────────────────────┤  │
│  │ Fencing tokens         │ Synergistic │ Both provide staleness detect │  │
│  ├────────────────────────┼─────────────┼───────────────────────────────┤  │
│  │ Progress deduplication │ Minor fix   │ Use layer_version as key      │  │
│  └────────────────────────┴─────────────┴───────────────────────────────┘  │
│                                                                              │
│  Limitation: Cross-DC Context Sync                                           │
│  ────────────────────────────────────────────────────────────────────────   │
│  NOT SUPPORTED: Workflows in DC-1 depending on context from DC-2            │
│  Current design: Each DC runs full job independently                        │
│  If needed: Gate becomes cross-DC coordinator (significant change)          │
│                                                                              │
│  Two Types of Leaders (Clarification):                                       │
│  ────────────────────────────────────────────────────────────────────────   │
│  CLUSTER LEADER: One per manager cluster, handles cluster ops (SWIM)        │
│  JOB LEADER: One per job per DC, handles that job's context                 │
│                                                                              │
│  These are different roles - a follower manager can be job leader:          │
│    Manager A: Cluster Leader, Job Leader for Job-1, Job-3                   │
│    Manager B: Follower,       Job Leader for Job-2                          │
│    Manager C: Follower,       Job Leader for Job-4, Job-5                   │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

## Gate Per-Job Leadership Architecture

This section documents the distributed job ownership model for gates, enabling horizontal scaling and fault tolerance without single-leader bottlenecks.

### Overview

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    GATE PER-JOB LEADERSHIP MODEL                            │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  PROBLEM: Single cluster-leader model bottlenecks at high job volumes       │
│  SOLUTION: Each job has its own leader gate, distributed via consistent hash│
│                                                                              │
│  ┌──────────┐   ┌──────────┐   ┌──────────┐   ┌──────────┐                 │
│  │  Gate-1  │   │  Gate-2  │   │  Gate-3  │   │  Gate-4  │                 │
│  │ [0-25%]  │   │ [25-50%] │   │ [50-75%] │   │ [75-100%]│                 │
│  └────┬─────┘   └────┬─────┘   └────┬─────┘   └────┬─────┘                 │
│       │              │              │              │                        │
│       │    Job-abc ──┴──────────────│              │                        │
│       │    (owner: Gate-2)          │              │                        │
│       │                             │              │                        │
│       └── Job-xyz ──────────────────┴──────────────│                        │
│           (owner: Gate-3)                          │                        │
│                                                    │                        │
│                                                    └── Job-123              │
│                                                        (owner: Gate-4)      │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Architecture Components

The architecture consists of five key components that work together:

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                      COMPONENT SUMMARY                                       │
├───────────────────────┬─────────────────────────────────────────────────────┤
│  Component            │  Status         │  Description                      │
├───────────────────────┼─────────────────┼───────────────────────────────────┤
│  1. Consistent Hashing│  IMPLEMENTED    │  Foundation for job distribution  │
│  2. Lease-Based Owner │  IMPLEMENTED    │  Job ownership with TTL           │
│  3. Direct DC Routing │  IMPLEMENTED    │  DC managers send to job leader   │
│  4. Client Reconnect  │  IMPLEMENTED    │  Client computes job owner        │
│  5. Fencing Tokens    │  IMPLEMENTED    │  Stale update protection          │
└───────────────────────┴─────────────────┴───────────────────────────────────┘
```

---

### Component 1: Consistent Hashing Ring

**Status: IMPLEMENTED**

**Decision**: Sophisticated approach - Use consistent hashing to deterministically map jobs to gates.

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    CONSISTENT HASHING RING                                   │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  How It Works:                                                               │
│  ─────────────────────────────────────────────────────────────────────────  │
│  1. Each gate is assigned a position on a virtual ring (0 to 2^32-1)        │
│  2. Jobs are hashed to a position on the same ring                          │
│  3. Job owner = first gate clockwise from job's hash position               │
│  4. Backup = next gate clockwise (for failover)                             │
│                                                                              │
│  Ring Visualization:                                                         │
│                          0                                                   │
│                          │                                                   │
│                    ┌─────┼─────┐                                             │
│                   /      │      \                                            │
│                 Gate-1   │       Gate-2                                      │
│                /         │          \                                        │
│              /           │            \                                      │
│  270° ─────┼─────────────┼─────────────┼───── 90°                           │
│              \           │            /                                      │
│                \         │          /                                        │
│                 Gate-4   │     Gate-3                                        │
│                   \      │      /                                            │
│                    └─────┼─────┘                                             │
│                          │                                                   │
│                         180                                                  │
│                                                                              │
│  Example:                                                                    │
│  ─────────────────────────────────────────────────────────────────────────  │
│  hash("job-abc") = 135° → Owner: Gate-2 (at 90°), Backup: Gate-3 (at 180°) │
│  hash("job-xyz") = 315° → Owner: Gate-1 (at 0°),  Backup: Gate-2 (at 90°)  │
│                                                                              │
│  Benefits:                                                                   │
│  ─────────────────────────────────────────────────────────────────────────  │
│  • Adding/removing gate only affects ~1/N of jobs                           │
│  • Deterministic - any node can compute ownership without coordination      │
│  • Client can compute owner directly (no queries needed)                    │
│  • Natural load balancing across gates                                       │
│                                                                              │
│  Data Structures:                                                            │
│  ─────────────────────────────────────────────────────────────────────────  │
│                                                                              │
│  class ConsistentHashRing:                                                   │
│      """Consistent hash ring for gate job distribution"""                   │
│                                                                              │
│      def __init__(self, virtual_nodes: int = 150):                          │
│          self._ring: dict[int, str] = {}    # hash → node_id               │
│          self._sorted_keys: list[int] = []  # sorted hash positions         │
│          self._virtual_nodes = virtual_nodes                                 │
│                                                                              │
│      def add_node(self, node_id: str) -> None:                              │
│          """Add a gate to the ring with virtual nodes"""                    │
│          for i in range(self._virtual_nodes):                                │
│              key = hash(f"{node_id}:{i}") % (2**32)                         │
│              self._ring[key] = node_id                                       │
│          self._sorted_keys = sorted(self._ring.keys())                       │
│                                                                              │
│      def remove_node(self, node_id: str) -> None:                           │
│          """Remove a gate from the ring"""                                  │
│          self._ring = {k: v for k, v in self._ring.items()                  │
│                       if v != node_id}                                       │
│          self._sorted_keys = sorted(self._ring.keys())                       │
│                                                                              │
│      def get_node(self, key: str) -> str:                                   │
│          """Get the owner gate for a job_id"""                              │
│          if not self._ring:                                                  │
│              raise NoGatesAvailable()                                        │
│          hash_val = hash(key) % (2**32)                                      │
│          idx = bisect.bisect(self._sorted_keys, hash_val)                    │
│          if idx == len(self._sorted_keys):                                   │
│              idx = 0                                                         │
│          return self._ring[self._sorted_keys[idx]]                           │
│                                                                              │
│      def get_nodes(self, key: str, count: int = 2) -> list[str]:            │
│          """Get owner and N-1 backup gates for a job_id"""                  │
│          nodes = []                                                          │
│          hash_val = hash(key) % (2**32)                                      │
│          idx = bisect.bisect(self._sorted_keys, hash_val)                    │
│          while len(nodes) < count and len(nodes) < len(set(self._ring)):    │
│              if idx >= len(self._sorted_keys):                               │
│                  idx = 0                                                     │
│              node = self._ring[self._sorted_keys[idx]]                       │
│              if node not in nodes:                                           │
│                  nodes.append(node)                                          │
│              idx += 1                                                        │
│          return nodes                                                        │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

### Component 2: Lease-Based Job Ownership

**Status: IMPLEMENTED**

**Decision**: Sophisticated approach - Jobs have leases with TTL that must be renewed.

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    LEASE-BASED JOB OWNERSHIP                                 │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  Why Leases:                                                                 │
│  ─────────────────────────────────────────────────────────────────────────  │
│  • Consistent hash determines INITIAL owner                                 │
│  • Lease confirms ACTIVE ownership                                          │
│  • If owner fails, lease expires and backup can claim                       │
│  • Prevents split-brain: only one lease holder at a time                    │
│                                                                              │
│  Lease Lifecycle:                                                            │
│  ─────────────────────────────────────────────────────────────────────────  │
│                                                                              │
│  ┌─────────────┐     ┌─────────────┐     ┌─────────────┐                    │
│  │   CLAIMED   │────▶│   ACTIVE    │────▶│   EXPIRED   │                    │
│  │             │     │             │     │             │                    │
│  │ fence_token │     │ renewing... │     │ backup can  │                    │
│  │ assigned    │     │             │     │ claim       │                    │
│  └─────────────┘     └──────┬──────┘     └─────────────┘                    │
│         ▲                   │                   │                            │
│         │                   │ renewal           │ backup claims              │
│         │                   ▼                   ▼                            │
│         │            ┌─────────────┐     ┌─────────────┐                    │
│         │            │   ACTIVE    │     │   CLAIMED   │                    │
│         │            │  (renewed)  │     │  (new owner)│                    │
│         │            └─────────────┘     │ fence+1     │                    │
│         │                                └─────────────┘                    │
│         │                                       │                            │
│         └───────────────────────────────────────┘                            │
│                        (cycle continues)                                     │
│                                                                              │
│  Lease State:                                                                │
│  ─────────────────────────────────────────────────────────────────────────  │
│                                                                              │
│  @dataclass(slots=True)                                                      │
│  class GateJobLease:                                                         │
│      """Lease for job ownership"""                                          │
│      job_id: str                                                             │
│      owner_node_id: str           # Current lease holder                    │
│      fence_token: int             # Monotonic, increments on ownership change│
│      lease_acquired: float        # time.monotonic() when acquired          │
│      lease_duration: float = 30.0 # TTL in seconds                          │
│      backup_node_id: str | None = None  # Next in consistent hash ring      │
│                                                                              │
│      @property                                                               │
│      def is_expired(self) -> bool:                                          │
│          return time.monotonic() > self.lease_acquired + self.lease_duration│
│                                                                              │
│      def renew(self) -> None:                                                │
│          self.lease_acquired = time.monotonic()                              │
│                                                                              │
│  Lease Operations:                                                           │
│  ─────────────────────────────────────────────────────────────────────────  │
│                                                                              │
│  async def claim_job_lease(self, job_id: str) -> GateJobLease:              │
│      """Claim ownership of a job (on first submission)"""                   │
│      nodes = self._hash_ring.get_nodes(job_id, count=2)                     │
│      owner = nodes[0]                                                        │
│      backup = nodes[1] if len(nodes) > 1 else None                          │
│                                                                              │
│      lease = GateJobLease(                                                   │
│          job_id=job_id,                                                      │
│          owner_node_id=owner,                                                │
│          fence_token=1,                                                      │
│          lease_acquired=time.monotonic(),                                    │
│          backup_node_id=backup,                                              │
│      )                                                                       │
│      self._job_leases[job_id] = lease                                        │
│      return lease                                                            │
│                                                                              │
│  async def claim_expired_lease(self, job_id: str) -> GateJobLease | None:   │
│      """Backup claims an expired lease"""                                   │
│      lease = self._job_leases.get(job_id)                                   │
│      if not lease or not lease.is_expired:                                   │
│          return None                                                         │
│      if lease.backup_node_id != self._node_id.full:                         │
│          return None  # Not the backup                                       │
│                                                                              │
│      # Claim with incremented fence token                                    │
│      new_backup = self._hash_ring.get_nodes(job_id, count=3)[2:]            │
│      new_lease = GateJobLease(                                               │
│          job_id=job_id,                                                      │
│          owner_node_id=self._node_id.full,                                   │
│          fence_token=lease.fence_token + 1,                                  │
│          lease_acquired=time.monotonic(),                                    │
│          backup_node_id=new_backup[0] if new_backup else None,              │
│      )                                                                       │
│      self._job_leases[job_id] = new_lease                                    │
│      return new_lease                                                        │
│                                                                              │
│  Lease Renewal Loop:                                                         │
│  ─────────────────────────────────────────────────────────────────────────  │
│                                                                              │
│  async def _lease_renewal_loop(self):                                        │
│      """Background task to renew leases for owned jobs"""                   │
│      while self._running:                                                    │
│          for job_id, lease in list(self._job_leases.items()):               │
│              if lease.owner_node_id == self._node_id.full:                   │
│                  if not lease.is_expired:                                    │
│                      lease.renew()                                           │
│          await asyncio.sleep(lease.lease_duration / 3)  # Renew at 1/3 TTL  │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

### Component 3: Direct DC-to-Job-Leader Result Routing

**Status: IMPLEMENTED**

**Decision**: Sophisticated approach - DC managers send results directly to job leader gate.

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    DIRECT RESULT ROUTING                                     │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  Why Direct Routing:                                                         │
│  ─────────────────────────────────────────────────────────────────────────  │
│  • No intermediate hops = lower latency                                     │
│  • Job leader gate aggregates results directly                              │
│  • Less load on cluster leader gate                                         │
│                                                                              │
│  Flow Diagram:                                                               │
│  ─────────────────────────────────────────────────────────────────────────  │
│                                                                              │
│                    ┌─────────────────────────────────────┐                   │
│                    │         Gate Cluster                │                   │
│                    │  ┌──────┐  ┌──────┐  ┌──────┐      │                   │
│                    │  │Gate-1│  │Gate-2│  │Gate-3│      │                   │
│                    │  │      │  │ (job │  │      │      │                   │
│                    │  │      │  │leader)│  │      │      │                   │
│                    │  └──────┘  └───▲──┘  └──────┘      │                   │
│                    └────────────────┼───────────────────┘                   │
│                                     │                                        │
│         ┌───────────────────────────┼───────────────────────────┐           │
│         │                           │                           │           │
│         │ JobFinalResult            │ JobFinalResult            │           │
│         │                           │                           │           │
│  ┌──────┴──────┐             ┌──────┴──────┐             ┌──────┴──────┐   │
│  │   DC-ALPHA  │             │   DC-BETA   │             │   DC-GAMMA  │   │
│  │   Manager   │             │   Manager   │             │   Manager   │   │
│  │   Cluster   │             │   Cluster   │             │   Cluster   │   │
│  └─────────────┘             └─────────────┘             └─────────────┘   │
│                                                                              │
│  Manager-Side Implementation:                                                │
│  ─────────────────────────────────────────────────────────────────────────  │
│                                                                              │
│  # Managers need to know job → gate owner mapping                           │
│  # This is embedded in the job dispatch from gate                           │
│                                                                              │
│  async def send_job_final_result(self, job_id: str, result: JobFinalResult):│
│      # Get job leader gate from stored job info                             │
│      job_info = self._job_info[job_id]                                       │
│      job_leader_gate = job_info.origin_gate  # Stored when job dispatched   │
│                                                                              │
│      # Send directly to job leader gate                                      │
│      await self.send_tcp(                                                    │
│          job_leader_gate,                                                    │
│          "job_final_result",                                                 │
│          result.dump(),                                                      │
│      )                                                                       │
│                                                                              │
│  Gate-Side Implementation:                                                   │
│  ─────────────────────────────────────────────────────────────────────────  │
│                                                                              │
│  async def job_final_result(self, addr, data, clock_time):                  │
│      result = JobFinalResult.load(data)                                      │
│      lease = self._job_leases.get(result.job_id)                            │
│                                                                              │
│      # Verify we're the owner (fence token check)                           │
│      if not self._owns_job(result.job_id, result.fence_token):              │
│          # Ring changed or lease transferred                                 │
│          actual_owner = self._hash_ring.get_node(result.job_id)             │
│          await self.forward_result(actual_owner, result)                     │
│          return b'forwarded'                                                 │
│                                                                              │
│      # Aggregate with other DC results                                       │
│      await self._aggregate_dc_result(result)                                 │
│                                                                              │
│  Edge Case - Ring Changed:                                                   │
│  ─────────────────────────────────────────────────────────────────────────  │
│                                                                              │
│  If gate added/removed while job running:                                    │
│  1. DC manager sends to old owner (from stored job_info)                    │
│  2. Old owner detects "I don't own this" via hash ring                      │
│  3. Old owner forwards to new owner                                          │
│  4. New owner processes (fence token prevents duplicates)                    │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

### Component 4: Client Reconnection

**Status: IMPLEMENTED**

**Decision**: Sophisticated approach - Clients compute job owner deterministically.

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    CLIENT RECONNECTION                                       │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  Why Client Computes Owner:                                                  │
│  ─────────────────────────────────────────────────────────────────────────  │
│  • After disconnect, client knows exactly where to reconnect                │
│  • No need to query gates for "who owns my job?"                            │
│  • Client maintains same hash ring as gates                                  │
│                                                                              │
│  Client State:                                                               │
│  ─────────────────────────────────────────────────────────────────────────  │
│                                                                              │
│  class HyperscaleClient:                                                     │
│      def __init__(self, gate_addrs: list[tuple[str, int]]):                 │
│          self._gate_addrs = gate_addrs                                       │
│          self._hash_ring = ConsistentHashRing()                              │
│          self._job_callbacks: dict[str, asyncio.Future] = {}                │
│                                                                              │
│          # Initialize ring with known gates                                  │
│          for host, port in gate_addrs:                                       │
│              self._hash_ring.add_node(f"{host}:{port}")                      │
│                                                                              │
│      async def submit_job(self, job: Job) -> JobAck:                        │
│          # Compute owner from job_id                                         │
│          owner = self._hash_ring.get_node(job.job_id)                        │
│          host, port = owner.split(":")                                       │
│                                                                              │
│          # Submit to owner                                                   │
│          return await self.send_tcp(                                         │
│              (host, int(port)),                                              │
│              "job_submission",                                               │
│              job.dump(),                                                     │
│          )                                                                   │
│                                                                              │
│  Reconnection Logic:                                                         │
│  ─────────────────────────────────────────────────────────────────────────  │
│                                                                              │
│  async def reconnect_to_job(self, job_id: str, max_retries: int = 3):       │
│      """Reconnect to job after disconnect"""                                │
│      for attempt in range(max_retries):                                      │
│          owner = self._hash_ring.get_node(job_id)                            │
│          host, port = owner.split(":")                                       │
│                                                                              │
│          try:                                                                │
│              response = await self.send_tcp(                                 │
│                  (host, int(port)),                                          │
│                  "register_callback",                                        │
│                  RegisterCallback(job_id=job_id).dump(),                     │
│              )                                                               │
│              if response.success:                                            │
│                  return True                                                 │
│          except (ConnectionError, TimeoutError):                             │
│              pass                                                            │
│                                                                              │
│          # Gate might have failed, wait for lease transfer                   │
│          await asyncio.sleep(LEASE_DURATION / 2)                             │
│                                                                              │
│      raise ReconnectFailed(job_id)                                           │
│                                                                              │
│  Ring Update Protocol:                                                       │
│  ─────────────────────────────────────────────────────────────────────────  │
│                                                                              │
│  Clients receive ring updates via push notifications:                        │
│                                                                              │
│  async def handle_ring_update(self, update: RingUpdate):                    │
│      """Gate cluster sends ring updates to clients"""                       │
│      if update.type == "add":                                                │
│          self._hash_ring.add_node(update.node_id)                            │
│      elif update.type == "remove":                                           │
│          self._hash_ring.remove_node(update.node_id)                         │
│                                                                              │
│  Timeline (Reconnect After Gate Failure):                                    │
│  ─────────────────────────────────────────────────────────────────────────  │
│                                                                              │
│  t=0    Client connected to Gate-2 for job-abc                               │
│  t=5    Gate-2 crashes                                                       │
│  t=5    Client detects disconnect                                            │
│  t=6    Client computes owner: hash("job-abc") → Gate-2 (still in ring)     │
│  t=6    Client tries Gate-2, fails                                           │
│  t=6    Client waits LEASE_DURATION/2 = 15s                                  │
│  t=21   Client retries: Gate-3 now owns (lease transferred)                 │
│  t=21   Client connects to Gate-3, registers callback                       │
│  t=21   Client receives remaining updates ✓                                  │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

### Component 5: Fencing Tokens

**Status: IMPLEMENTED**

**Decision**: Simple approach - Monotonic fence tokens reject stale operations.

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    FENCING TOKENS                                            │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  Why Fencing Tokens:                                                         │
│  ─────────────────────────────────────────────────────────────────────────  │
│  • Prevent stale updates from old owner after lease transfer                │
│  • Simple, proven pattern (used in ZooKeeper, etcd, etc.)                   │
│  • No consensus needed - just monotonic comparison                          │
│                                                                              │
│  How It Works:                                                               │
│  ─────────────────────────────────────────────────────────────────────────  │
│                                                                              │
│  1. Job created with fence_token = 1                                         │
│  2. Each ownership transfer increments fence_token                           │
│  3. All operations include fence_token                                       │
│  4. Receiver rejects if received_token < current_token                       │
│                                                                              │
│  Fence Token in Messages:                                                    │
│  ─────────────────────────────────────────────────────────────────────────  │
│                                                                              │
│  # All job-related messages include fence token                              │
│                                                                              │
│  @dataclass(slots=True)                                                      │
│  class JobDispatch(Message):                                                 │
│      job_id: str                                                             │
│      fence_token: int  # ← Must match current owner's token                 │
│      workflows: list[bytes]                                                  │
│      # ...                                                                   │
│                                                                              │
│  @dataclass(slots=True)                                                      │
│  class JobFinalResult(Message):                                              │
│      job_id: str                                                             │
│      fence_token: int  # ← Proves result is from valid ownership period     │
│      datacenter: str                                                         │
│      # ...                                                                   │
│                                                                              │
│  @dataclass(slots=True)                                                      │
│  class JobStatusPush(Message):                                               │
│      job_id: str                                                             │
│      fence_token: int  # ← Client can detect ownership changes              │
│      # ...                                                                   │
│                                                                              │
│  Validation Logic:                                                           │
│  ─────────────────────────────────────────────────────────────────────────  │
│                                                                              │
│  def validate_fence_token(self, job_id: str, received_token: int) -> bool:  │
│      """Reject operations with stale fence tokens"""                        │
│      lease = self._job_leases.get(job_id)                                    │
│      if not lease:                                                           │
│          return False  # Unknown job                                         │
│      if received_token < lease.fence_token:                                  │
│          return False  # Stale token from old owner                         │
│      if received_token > lease.fence_token:                                  │
│          # Future token - might be from new owner we don't know yet         │
│          # Accept and update our lease info                                  │
│          self._update_lease_from_newer_token(job_id, received_token)         │
│      return True                                                             │
│                                                                              │
│  Scenario: Stale Update Rejected                                             │
│  ─────────────────────────────────────────────────────────────────────────  │
│                                                                              │
│  t=0    Gate-2 owns job-abc (fence=1)                                        │
│  t=1    Gate-2 dispatches to DC-ALPHA (fence=1)                             │
│  t=2    Gate-2 crashes                                                       │
│  t=5    Gate-3 claims lease (fence=2)                                        │
│  t=10   DC-ALPHA returns result (fence=1)  ← STALE!                         │
│  t=10   Gate-3 rejects: received_token(1) < current(2)                       │
│  t=11   DC-ALPHA retries with updated fence from Gate-3                     │
│                                                                              │
│  Scenario: Split-Brain Prevention                                            │
│  ─────────────────────────────────────────────────────────────────────────  │
│                                                                              │
│  t=0    Gate-2 owns job-abc (fence=1)                                        │
│  t=1    Network partition: Gate-2 isolated from Gate-3                      │
│  t=2    Gate-2 thinks it still owns job (lease not expired locally)         │
│  t=2    Gate-3 claims lease (fence=2) - sees Gate-2 as dead                 │
│  t=3    Gate-2 sends update (fence=1)                                        │
│  t=3    Receiver rejects: fence=1 < current=2                                │
│  t=4    Gate-2 learns it's not owner anymore, stops processing              │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

### Component Interactions

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    COMPONENT SYNERGIES                                       │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  ┌─────────────────┐                                                         │
│  │ Consistent Hash │─────────────────────────────────────┐                  │
│  │    (Foundation) │                                     │                  │
│  └────────┬────────┘                                     │                  │
│           │ determines initial                           │                  │
│           │ owner & backup                               │                  │
│           ▼                                              ▼                  │
│  ┌─────────────────┐                            ┌─────────────────┐         │
│  │  Lease-Based    │                            │ Client Reconnect│         │
│  │  Ownership      │                            │ (computes owner)│         │
│  └────────┬────────┘                            └────────┬────────┘         │
│           │ fence token                                  │ queries          │
│           │ assigned                                     │ job owner        │
│           ▼                                              │                  │
│  ┌─────────────────┐                                     │                  │
│  │ Fencing Tokens  │◀────────────────────────────────────┘                  │
│  │ (prevents stale)│                                                         │
│  └────────┬────────┘                                                         │
│           │ validates                                                        │
│           │ operations                                                       │
│           ▼                                                                  │
│  ┌─────────────────┐                                                         │
│  │ Direct DC Route │                                                         │
│  │ (low latency)   │                                                         │
│  └─────────────────┘                                                         │
│                                                                              │
│  Data Flow:                                                                  │
│  ─────────────────────────────────────────────────────────────────────────  │
│                                                                              │
│  1. Client submits job → hash_ring.get_node(job_id) → Gate-X               │
│  2. Gate-X claims lease → fence_token=1                                     │
│  3. Gate-X dispatches to DCs → includes fence_token                         │
│  4. DCs complete → send results to Gate-X (job leader)                      │
│  5. Gate-X aggregates, sends to client                                       │
│                                                                              │
│  Failure Handling:                                                           │
│  ─────────────────────────────────────────────────────────────────────────  │
│                                                                              │
│  • Gate-X fails → lease expires → Gate-Y (backup) claims → fence+1         │
│  • Stale results from DCs (fence=1) rejected by Gate-Y (fence=2)           │
│  • Client reconnects to Gate-Y (computed via hash ring)                     │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

### Testing Approach

All tests follow this pattern:

```python
# examples/servers/test_<feature>.py

async def main():
    # 1. Setup cluster with appropriate logging
    LoggingConfig.directory = os.getcwd()
    
    # 2. Start nodes in order: gates → managers → workers
    gate = GateServer(...)
    await gate.start()
    await asyncio.sleep(3)  # Wait for leader election
    
    manager = ManagerServer(..., gate_addrs=[...])
    await manager.start()
    await asyncio.sleep(3)  # Wait for registration
    
    worker = WorkerServer(..., seed_managers=[...])
    await worker.start()
    await asyncio.sleep(2)  # Wait for registration
    
    # 3. Run test scenario
    client = HyperscaleClient(gate_tcp_addrs=[...])
    await client.start()
    job_id = await client.submit_job(...)
    result = await client.wait_for_completion(job_id)
    
    # 4. Validate results
    assert result.status == "completed"
    
    # 5. Cleanup (in reverse order, with timeouts)
    await client.stop()
    await worker.stop()  # Note: workers use stop(), not graceful_shutdown()
    await manager.graceful_shutdown()
    await gate.graceful_shutdown()

if __name__ == "__main__":
    asyncio.run(main())
```

**Debug Workflow**:
1. Let user test with `timeout 180 python examples/servers/test_<name>.py 2>&1 | tail -100`
2. Watch for warnings/exceptions
3. Kill test if error found
4. Fix the issue
5. Commit with descriptive message
6. Push to branch
7. Repeat until test passes

---

### Key Files Reference

| File | Purpose |
|------|---------|
| `hyperscale/distributed_rewrite/nodes/gate.py` | Gate node - job dispatch, results aggregation |
| `hyperscale/distributed_rewrite/nodes/manager.py` | Manager node - workflow dispatch, worker tracking |
| `hyperscale/distributed_rewrite/nodes/worker.py` | Worker node - workflow execution |
| `hyperscale/distributed_rewrite/nodes/client.py` | Client API for job submission |
| `hyperscale/distributed_rewrite/models/distributed.py` | All message types (dataclasses) |
| `hyperscale/distributed_rewrite/swim/health_aware_server.py` | Base server with SWIM protocol |
| `hyperscale/distributed_rewrite/swim/health/federated_health_monitor.py` | Cross-cluster health monitoring |
| `hyperscale/distributed_rewrite/env/env.py` | Configuration via environment variables |
| `hyperscale/core/hooks/hook.py` | Hook types including `HookType.TEST` |
| `hyperscale/core/jobs/workers/provisioner.py` | Priority-based core allocation |
| `hyperscale/reporting/results.py` | Results merging and aggregation |

---

---

## Implemented Feature Documentation

This section documents features that have been implemented, including their architecture, configuration, and usage patterns.

### Terminal UI Architecture

The Terminal UI provides real-time visual feedback during test execution with workflow progress, metrics, and statistics.

#### Architecture Overview

```
┌─────────────────────────────────────────────────────────────────┐
│                    Terminal UI Architecture                      │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  ┌──────────────────────────────────────────────────────────┐  │
│  │                  HyperscaleInterface                      │  │
│  │                                                           │  │
│  │  • Coordinates UI components                              │  │
│  │  • Cycles through active workflows                        │  │
│  │  • Handles updates from InterfaceUpdatesController        │  │
│  └────────────────────────┬─────────────────────────────────┘  │
│                           │                                     │
│                           ▼                                     │
│  ┌──────────────────────────────────────────────────────────┐  │
│  │                      Terminal                             │  │
│  │                                                           │  │
│  │  • Raw terminal control (ANSI escape sequences)          │  │
│  │  • Manages Canvas layout                                  │  │
│  │  • Handles refresh rate and rendering                     │  │
│  └────────────────────────┬─────────────────────────────────┘  │
│                           │                                     │
│                           ▼                                     │
│  ┌──────────────────────────────────────────────────────────┐  │
│  │                       Canvas                              │  │
│  │                                                           │  │
│  │  • Contains Sections arranged in rows                     │  │
│  │  • Handles resize and layout calculations                 │  │
│  │  • Manages padding (horizontal/vertical)                  │  │
│  └────────────────────────┬─────────────────────────────────┘  │
│                           │                                     │
│                           ▼                                     │
│  ┌──────────────────────────────────────────────────────────┐  │
│  │                      Sections                             │  │
│  │                                                           │  │
│  │  • Group related components                               │  │
│  │  • Support auto-width and fixed-width modes              │  │
│  │  • Handle component visibility toggling                   │  │
│  └────────────────────────┬─────────────────────────────────┘  │
│                           │                                     │
│                           ▼                                     │
│  ┌──────────────────────────────────────────────────────────┐  │
│  │                     Components                            │  │
│  │                                                           │  │
│  │  • Header: ASCII art title with gradient colors           │  │
│  │  • ProgressBar: Animated progress with fill/background    │  │
│  │  • Spinner: Multiple animation styles (dots, bars, etc.)  │  │
│  │  • Counter: Numeric display with formatting               │  │
│  │  • TotalRate: Requests/second over entire run             │  │
│  │  • WindowedRate: Recent requests/second (sliding window)  │  │
│  │  • ScatterPlot: Plotille-based latency visualization      │  │
│  │  • Table: Tabulated statistics display                    │  │
│  │  • Text/MultilineText: Status messages                    │  │
│  │  • Timer: Elapsed time display                            │  │
│  │  • StatusBar/AnimatedStatusBar: Status indicators         │  │
│  └──────────────────────────────────────────────────────────┘  │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

#### Component Hierarchy

```python
# Main interface entry point
interface = HyperscaleInterface(updates_controller)
interface.initialize(workflows, terminal_mode="full")
await interface.run()

# Terminal modes:
# - "full": Complete TUI with all components
# - "ci": Simplified output for CI environments
# - "none": No UI output (headless)
```

#### Key Files

| File | Purpose |
|------|---------|
| `hyperscale/ui/__init__.py` | Main exports (HyperscaleInterface, InterfaceUpdatesController) |
| `hyperscale/ui/hyperscale_interface.py` | Interface orchestration, workflow cycling |
| `hyperscale/ui/interface_updates_controller.py` | Async update queue management |
| `hyperscale/ui/components/terminal/terminal.py` | Raw terminal control |
| `hyperscale/ui/components/terminal/canvas.py` | Layout engine |
| `hyperscale/ui/components/terminal/section.py` | Section container |
| `hyperscale/ui/styling/` | Colors, attributes, stylization |

#### Update Flow

```
┌─────────────────────────────────────────────────────────────────┐
│                      UI Update Flow                              │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  Worker Progress  ──►  RemoteGraphManager  ──►  Updates Queue  │
│       │                      │                       │          │
│       │                      │                       ▼          │
│       │               ┌──────┴──────┐    InterfaceUpdatesController
│       │               │             │               │           │
│       │               ▼             ▼               ▼           │
│       │        Stats Update   Progress Update   Workflow List   │
│       │               │             │               │           │
│       │               └──────┬──────┘               │           │
│       │                      │                      │           │
│       │                      ▼                      ▼           │
│       │              HyperscaleInterface._run() loop            │
│       │                      │                                  │
│       │                      ▼                                  │
│       │              Set active components for                  │
│       │              current workflow                           │
│       │                      │                                  │
│       │                      ▼                                  │
│       │              Terminal.trigger_render()                  │
│       │                      │                                  │
│       └──────────────────────┴──────────────────────────────────│
│                                                                 │
│  Refresh rate: Configurable via _interval (default ~30fps)     │
│  Workflow cycling: update_interval (default 3 seconds)         │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

---

### Reporting Architecture

Hyperscale supports exporting test results to numerous backends for analysis and visualization.

#### Supported Backends

| Category | Backends |
|----------|----------|
| **Time Series** | InfluxDB, TimescaleDB, AWS Timestream, Prometheus, Graphite |
| **Cloud Storage** | S3, Google Cloud Storage, BigQuery, BigTable |
| **Databases** | PostgreSQL, MySQL, SQLite, MongoDB, Cassandra, CosmosDB, Redis |
| **Monitoring** | Datadog, NewRelic, Cloudwatch, Honeycomb, Netdata |
| **Metrics** | StatsD, DogStatsD, Telegraf, Telegraf-StatsD |
| **Message Queue** | Kafka |
| **File Formats** | JSON, CSV, XML |
| **Serverless** | AWS Lambda |
| **Custom** | CustomReporter (user-defined) |

#### Architecture

```
┌─────────────────────────────────────────────────────────────────┐
│                    Reporting Architecture                        │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  ┌──────────────────────────────────────────────────────────┐  │
│  │                      Reporter[T]                          │  │
│  │                                                           │  │
│  │  • Generic reporter with backend type parameter           │  │
│  │  • Factory pattern for backend instantiation              │  │
│  │  • Unified submit() interface                             │  │
│  └────────────────────────┬─────────────────────────────────┘  │
│                           │                                     │
│                           ▼                                     │
│  ┌──────────────────────────────────────────────────────────┐  │
│  │                    Backend Config                         │  │
│  │                                                           │  │
│  │  • PostgresConfig, InfluxDBConfig, S3Config, etc.        │  │
│  │  • Connection parameters                                  │  │
│  │  • Batching and retry settings                            │  │
│  └────────────────────────┬─────────────────────────────────┘  │
│                           │                                     │
│                           ▼                                     │
│  ┌──────────────────────────────────────────────────────────┐  │
│  │                  Metrics/Results                          │  │
│  │                                                           │  │
│  │  • WorkflowMetric: Per-workflow statistics                │  │
│  │  • WorkflowMetricSet: Collection of workflow metrics      │  │
│  │  • StepMetricSet: Per-step breakdown                      │  │
│  │  • ResultSet: Final aggregated results                    │  │
│  │  • MetricsSet: Timing and throughput metrics              │  │
│  │  • CheckSet: Validation check results                     │  │
│  └──────────────────────────────────────────────────────────┘  │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

#### Usage Example

```python
from hyperscale.reporting import Reporter, PostgresConfig, ReporterTypes

# Configure backend
config = PostgresConfig(
    host="localhost",
    port=5432,
    database="hyperscale_results",
    username="user",
    password="password",
)

# Create reporter
reporter = Reporter[PostgresConfig](
    reporter_type=ReporterTypes.Postgres,
    config=config,
)

# Submit results
await reporter.connect()
await reporter.submit(workflow_metrics)
await reporter.close()
```

#### Key Files

| File | Purpose |
|------|---------|
| `hyperscale/reporting/reporter.py` | Generic Reporter class, backend factory |
| `hyperscale/reporting/results.py` | Result aggregation and merging |
| `hyperscale/reporting/common/types.py` | ReporterTypes enum |
| `hyperscale/reporting/common/results_types.py` | Metric data classes |
| `hyperscale/reporting/<backend>/` | Per-backend implementation |

---

### Local Execution Mode

Local mode enables single-machine testing without distributed infrastructure.

#### Architecture

```
┌─────────────────────────────────────────────────────────────────┐
│                    Local Execution Mode                          │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  ┌──────────────────────────────────────────────────────────┐  │
│  │                     LocalRunner                           │  │
│  │                                                           │  │
│  │  • Entry point for local test execution                   │  │
│  │  • Manages worker subprocess pool                         │  │
│  │  • Coordinates UI and results collection                  │  │
│  └────────────────────────┬─────────────────────────────────┘  │
│                           │                                     │
│            ┌──────────────┼──────────────┐                     │
│            │              │              │                      │
│            ▼              ▼              ▼                      │
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐            │
│  │LocalServer  │  │LocalServer  │  │LocalServer  │  ...       │
│  │Pool Worker 1│  │Pool Worker 2│  │Pool Worker N│            │
│  └──────┬──────┘  └──────┬──────┘  └──────┬──────┘            │
│         │                │                │                     │
│         └────────────────┼────────────────┘                     │
│                          │                                      │
│                          ▼                                      │
│  ┌──────────────────────────────────────────────────────────┐  │
│  │                  RemoteGraphManager                       │  │
│  │                                                           │  │
│  │  • Manages workflow dispatch to workers                   │  │
│  │  • Collects results and progress                          │  │
│  │  • Feeds InterfaceUpdatesController                       │  │
│  └──────────────────────────────────────────────────────────┘  │
│                                                                 │
│  Worker Count: Auto-detected via psutil.cpu_count(logical=False)│
│  Communication: In-process TCP (localhost bindings)             │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

#### Usage

```python
from hyperscale.core.jobs.runner.local_runner import LocalRunner
from hyperscale.core.graph import Workflow

# Create runner
runner = LocalRunner(
    host="localhost",
    port=8080,
    workers=4,  # Optional, defaults to CPU cores
)

# Define workflows
workflows = [
    (["tag1"], MyWorkflow()),
]

# Execute
await runner.run(
    test_name="my_test",
    workflows=workflows,
    terminal_mode="full",  # "full", "ci", or "none"
    timeout="5m",
)
```

#### Key Files

| File | Purpose |
|------|---------|
| `hyperscale/core/jobs/runner/local_runner.py` | LocalRunner entry point |
| `hyperscale/core/jobs/runner/local_server_pool.py` | Worker subprocess pool |
| `hyperscale/core/jobs/graphs/remote_graph_manager.py` | Workflow dispatch |

---

### Rate Limiting Implementation (AD-24)

Rate limiting prevents any single client from overwhelming the system while adapting behavior based on system health.

#### Architecture

```
┌─────────────────────────────────────────────────────────────────┐
│                   Rate Limiting Architecture                     │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  ┌──────────────────────────────────────────────────────────┐  │
│  │              HybridOverloadDetector (AD-18)               │  │
│  │                                                           │  │
│  │  Provides health state: HEALTHY / BUSY / STRESSED /       │  │
│  │  OVERLOADED based on latency, CPU, memory signals         │  │
│  └────────────────────────┬─────────────────────────────────┘  │
│                           │                                     │
│                           ▼                                     │
│  ┌──────────────────────────────────────────────────────────┐  │
│  │                 AdaptiveRateLimiter                       │  │
│  │                                                           │  │
│  │  Health-gated rate limiting:                              │  │
│  │  • HEALTHY: Per-operation limits apply                    │  │
│  │  • BUSY: LOW priority shed + per-operation limits         │  │
│  │  • STRESSED: Per-client fair-share limiting               │  │
│  │  • OVERLOADED: Only CRITICAL requests pass                │  │
│  └────────────────────────┬─────────────────────────────────┘  │
│                           │                                     │
│            ┌──────────────┴──────────────┐                     │
│            │                             │                      │
│            ▼                             ▼                      │
│  ┌─────────────────────┐      ┌─────────────────────┐         │
│  │ SlidingWindowCounter│      │ Per-Client Stress   │         │
│  │                     │      │ Counters            │         │
│  │ Per-operation limits│      │                     │         │
│  │ (100 req/10s for    │      │ Fair-share limits   │         │
│  │ job_submit, etc.)   │      │ when stressed       │         │
│  └─────────────────────┘      └─────────────────────┘         │
│                                                                 │
│  ┌──────────────────────────────────────────────────────────┐  │
│  │                   Request Priority                        │  │
│  │                                                           │  │
│  │  CRITICAL (0): Health checks, cancellation, final results│  │
│  │  HIGH (1): Job submission, workflow dispatch             │  │
│  │  NORMAL (2): Progress updates, stats queries             │  │
│  │  LOW (3): Debug requests, non-essential sync             │  │
│  └──────────────────────────────────────────────────────────┘  │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

#### SlidingWindowCounter

The SlidingWindowCounter provides deterministic rate limiting without the edge cases of token bucket algorithms:

```python
effective_count = current_window_count + previous_window_count * (1 - window_progress)
```

Example:
- Window size: 60 seconds
- Previous window: 100 requests
- Current window: 30 requests
- 15 seconds into current window (25% progress)
- Effective count = 30 + 100 * 0.75 = 105

#### Configuration

```python
# Environment variables for rate limiting
RATE_LIMIT_DEFAULT_BUCKET_SIZE: int = 100
RATE_LIMIT_DEFAULT_REFILL_RATE: float = 10.0
RATE_LIMIT_CLIENT_IDLE_TIMEOUT: float = 300.0
RATE_LIMIT_CLEANUP_INTERVAL: float = 60.0
RATE_LIMIT_MAX_RETRIES: int = 3
RATE_LIMIT_MAX_TOTAL_WAIT: float = 60.0
RATE_LIMIT_BACKOFF_MULTIPLIER: float = 1.5
```

#### Per-Operation Limits

| Operation | Max Requests | Window (seconds) |
|-----------|--------------|------------------|
| stats_update | 500 | 10.0 |
| heartbeat | 200 | 10.0 |
| progress_update | 300 | 10.0 |
| job_submit | 50 | 10.0 |
| job_status | 100 | 10.0 |
| workflow_dispatch | 100 | 10.0 |
| cancel | 20 | 10.0 |
| reconnect | 10 | 10.0 |

#### Client-Side Cooperation

The `CooperativeRateLimiter` enables clients to respect server rate limits:

```python
limiter = CooperativeRateLimiter()

# Before sending request
await limiter.wait_if_needed("job_submit")

# After receiving 429 response
if response.status == 429:
    retry_after = float(response.headers.get("Retry-After", 1.0))
    limiter.handle_rate_limit("job_submit", retry_after)
```

#### Key Files

| File | Purpose |
|------|---------|
| `hyperscale/distributed_rewrite/reliability/rate_limiting.py` | All rate limiting components |
| `hyperscale/distributed_rewrite/reliability/overload.py` | HybridOverloadDetector |
| `hyperscale/distributed_rewrite/reliability/load_shedding.py` | RequestPriority enum |

---

### Three-Signal Health Detection (AD-19)

The three-signal health model provides nuanced health tracking beyond simple alive/dead status.

#### The Three Signals

```
┌─────────────────────────────────────────────────────────────────┐
│                   Three-Signal Health Model                      │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  ┌─────────────────────────────────────────────────────────────┐
│  │ Signal 1: LIVENESS                                          │
│  │                                                             │
│  │ "Is the node alive and responsive?"                         │
│  │                                                             │
│  │ • UDP ping/ack from SWIM protocol                           │
│  │ • Timeout: LIVENESS_PROBE_TIMEOUT (1.0s)                    │
│  │ • Period: LIVENESS_PROBE_PERIOD (10.0s)                     │
│  │ • Failure threshold: LIVENESS_PROBE_FAILURE_THRESHOLD (3)   │
│  └─────────────────────────────────────────────────────────────┘
│                                                                 │
│  ┌─────────────────────────────────────────────────────────────┐
│  │ Signal 2: READINESS                                         │
│  │                                                             │
│  │ "Can the node accept new work?"                             │
│  │                                                             │
│  │ • Capacity check (available cores/slots)                    │
│  │ • Overload state from HybridOverloadDetector                │
│  │ • Not accepting if: at capacity, overloaded, draining       │
│  │ • Timeout: READINESS_PROBE_TIMEOUT (2.0s)                   │
│  └─────────────────────────────────────────────────────────────┘
│                                                                 │
│  ┌─────────────────────────────────────────────────────────────┐
│  │ Signal 3: PROGRESS                                          │
│  │                                                             │
│  │ "Is the node making forward progress?"                      │
│  │                                                             │
│  │ States:                                                     │
│  │ • IDLE: No active work, but healthy                         │
│  │ • PROGRESSING: Completing work (throughput > 0)             │
│  │ • STALLED: Active work but no recent completions            │
│  │ • STUCK: Extended period without progress                   │
│  └─────────────────────────────────────────────────────────────┘
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

#### Routing Decisions

The three signals combine to produce routing decisions:

| Liveness | Readiness | Progress | Decision |
|----------|-----------|----------|----------|
| ✓ | ✓ | PROGRESSING/IDLE | **ROUTE** - Send work |
| ✓ | ✗ | Any | **HOLD** - Don't send new work |
| ✓ | ✓ | STALLED | **INVESTIGATE** - Probe further |
| ✓ | Any | STUCK | **DRAIN** - Complete existing, no new |
| ✗ | Any | Any | **EVICT** - Node is dead |

#### Health State Protocol

```python
class HealthSignals(Protocol):
    """Protocol defining the three-signal health interface."""

    @property
    def liveness(self) -> bool:
        """Is the node alive and responsive?"""
        ...

    @property
    def readiness(self) -> bool:
        """Can the node accept work?"""
        ...

    @property
    def progress_state(self) -> ProgressState:
        """Is the node making progress?"""
        ...

    def get_routing_decision(self) -> RoutingDecision:
        """Get routing decision based on combined signals."""
        ...
```

#### Correlation Detection

The NodeHealthTracker prevents cascade evictions when multiple nodes fail simultaneously (likely network issue):

```python
tracker = NodeHealthTracker[WorkerHealthState]()

# Check if we should evict (with correlation detection)
evict_decision = tracker.should_evict("worker-1")
if evict_decision.should_evict:
    if evict_decision.correlated_failures:
        # Investigate network issue, don't evict
        pass
    else:
        # Safe to evict
        pass
```

#### Configuration

```python
# Health probe settings
LIVENESS_PROBE_TIMEOUT: float = 1.0
LIVENESS_PROBE_PERIOD: float = 10.0
LIVENESS_PROBE_FAILURE_THRESHOLD: int = 3
LIVENESS_PROBE_SUCCESS_THRESHOLD: int = 1

READINESS_PROBE_TIMEOUT: float = 2.0
READINESS_PROBE_PERIOD: float = 10.0
READINESS_PROBE_FAILURE_THRESHOLD: int = 3
READINESS_PROBE_SUCCESS_THRESHOLD: int = 1

STARTUP_PROBE_TIMEOUT: float = 5.0
STARTUP_PROBE_PERIOD: float = 5.0
STARTUP_PROBE_FAILURE_THRESHOLD: int = 30  # Allow slow startups (150s)
STARTUP_PROBE_SUCCESS_THRESHOLD: int = 1
```

#### SWIM Piggyback

Health signals are piggybacked on SWIM protocol messages for efficiency:

```python
@dataclass
class HealthPiggyback:
    node_id: str
    node_type: str  # "worker" | "manager" | "gate"
    is_alive: bool = True
    accepting_work: bool = True
    capacity: int = 0
    throughput: float = 0.0
    expected_throughput: float = 0.0
    overload_state: str = "healthy"
    timestamp: float = field(default_factory=time.monotonic)
```

#### Key Files

| File | Purpose |
|------|---------|
| `hyperscale/distributed_rewrite/health/tracker.py` | NodeHealthTracker, HealthSignals protocol |
| `hyperscale/distributed_rewrite/health/worker_health.py` | WorkerHealthState implementation |
| `hyperscale/distributed_rewrite/health/worker_health_manager.py` | Manager-side health tracking |

---

### Adaptive Healthcheck Extensions (AD-26)

Allows workers to request deadline extensions for long-running operations with graceful exhaustion handling.

#### Extension Grant Formula

Extensions use logarithmic decay to prevent indefinite delays:

```
grant = max(min_grant, base_deadline / 2^(extension_count + 1))
```

| Extension # | Formula | Grant (base=30s) | Cumulative |
|-------------|---------|------------------|------------|
| 1 | 30 / 2^1 | 15.0s | 15.0s |
| 2 | 30 / 2^2 | 7.5s | 22.5s |
| 3 | 30 / 2^3 | 3.75s | 26.25s |
| 4 | 30 / 2^4 | 1.875s | 28.125s |
| 5 | 30 / 2^5 | 1.0s (min) | 29.125s |
| 6+ | — | denied | — |

#### Graceful Exhaustion

When extensions run out, the system provides warning and grace period:

```
┌─────────────────────────────────────────────────────────────────┐
│                Graceful Exhaustion Timeline                      │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  Extension 1  Extension 2  Extension 3  Extension 4  Extension 5│
│      │            │            │            │            │      │
│      ▼            ▼            ▼            ▼            ▼      │
│  ┌──────┐    ┌──────┐    ┌──────┐    ┌──────┐    ┌──────┐     │
│  │ 15s  │    │ 7.5s │    │3.75s │    │1.875s│    │ 1s   │     │
│  │grant │    │grant │    │grant │    │grant │    │grant │     │
│  └──────┘    └──────┘    └──────┘    └──────┘    └──┬───┘     │
│                                                      │          │
│                                           ┌──────────▼────────┐│
│                                           │  WARNING SENT     ││
│                                           │  (remaining <= 1) ││
│                                           └──────────┬────────┘│
│                                                      │          │
│                                                      ▼          │
│                                           ┌─────────────────┐  │
│                                           │  EXHAUSTED      │  │
│                                           │                 │  │
│                                           │  Grace Period   │  │
│                                           │  (10s default)  │  │
│                                           │                 │  │
│                                           │  Worker can:    │  │
│                                           │  • Checkpoint   │  │
│                                           │  • Save state   │  │
│                                           │  • Clean up     │  │
│                                           └────────┬────────┘  │
│                                                    │            │
│                                                    ▼            │
│                                           ┌─────────────────┐  │
│                                           │  EVICTION       │  │
│                                           │  (after grace)  │  │
│                                           └─────────────────┘  │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

#### Extension Tracker State

```python
@dataclass(slots=True)
class ExtensionTracker:
    worker_id: str
    base_deadline: float = 30.0
    min_grant: float = 1.0
    max_extensions: int = 5
    warning_threshold: int = 1      # Extensions remaining to trigger warning
    grace_period: float = 10.0      # Seconds after exhaustion before kill

    extension_count: int = 0
    last_progress: float = 0.0
    total_extended: float = 0.0
    last_extension_time: float = field(default_factory=time.monotonic)
    exhaustion_time: float | None = None
    warning_sent: bool = False

    def request_extension(
        self,
        reason: str,
        current_progress: float,
    ) -> tuple[bool, float, str | None, bool]:
        """
        Returns: (granted, extension_seconds, denial_reason, is_warning)
        """
        ...

    @property
    def is_exhausted(self) -> bool: ...

    @property
    def is_in_grace_period(self) -> bool: ...

    @property
    def grace_period_remaining(self) -> float: ...

    @property
    def should_evict(self) -> bool:
        """True if exhausted AND grace period expired."""
        ...
```

#### Extension Response Fields

```python
@dataclass
class HealthcheckExtensionResponse:
    granted: bool
    extension_seconds: float
    new_deadline: float
    remaining_extensions: int
    denial_reason: str | None = None
    is_exhaustion_warning: bool = False    # True if about to exhaust
    grace_period_remaining: float = 0.0    # Seconds remaining after exhaustion
    in_grace_period: bool = False          # True if exhausted but within grace
```

#### Configuration

```python
# Environment variables
EXTENSION_BASE_DEADLINE: float = 30.0
EXTENSION_MIN_GRANT: float = 1.0
EXTENSION_MAX_EXTENSIONS: int = 5
EXTENSION_EVICTION_THRESHOLD: int = 3
EXTENSION_EXHAUSTION_WARNING_THRESHOLD: int = 1
EXTENSION_EXHAUSTION_GRACE_PERIOD: float = 10.0
```

#### Key Files

| File | Purpose |
|------|---------|
| `hyperscale/distributed_rewrite/health/extension_tracker.py` | ExtensionTracker, ExtensionTrackerConfig |
| `hyperscale/distributed_rewrite/health/worker_health_manager.py` | WorkerHealthManager integration |
| `hyperscale/distributed_rewrite/models/distributed.py` | HealthcheckExtensionRequest/Response |

---

### Zombie Job Prevention & Detection

Multiple mechanisms work together to detect and prevent zombie jobs (jobs that appear running but are actually stuck or orphaned).

#### Detection Mechanisms

```
┌─────────────────────────────────────────────────────────────────┐
│                   Zombie Detection Mechanisms                    │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  1. TIMEOUT DETECTION                                           │
│     ├─ Per-workflow timeout (user-configured)                   │
│     ├─ Checked during progress updates                          │
│     └─ Triggers workflow failure and cleanup                    │
│                                                                 │
│  2. SWIM DEAD DETECTION                                         │
│     ├─ SWIM protocol detects unresponsive workers              │
│     ├─ States: alive → suspect → dead                          │
│     ├─ Dead workers trigger workflow reassignment              │
│     └─ Reap interval: MANAGER_DEAD_WORKER_REAP_INTERVAL (15m)  │
│                                                                 │
│  3. PROGRESS HEALTH (AD-19)                                     │
│     ├─ Three-signal model tracks progress state                │
│     ├─ States: IDLE → PROGRESSING → STALLED → STUCK            │
│     ├─ STUCK triggers investigation and potential eviction     │
│     └─ Correlation detection prevents cascade evictions        │
│                                                                 │
│  4. LEASE EXPIRY                                                │
│     ├─ Gates hold time-limited leases for jobs                 │
│     ├─ Lease duration: configurable per-job                    │
│     ├─ Expired leases allow other gates to take over           │
│     └─ Prevents single-gate failures from blocking jobs        │
│                                                                 │
│  5. ORPHAN WORKFLOW SCANNER (New)                               │
│     ├─ Manager periodically queries workers for active workflows│
│     ├─ Compares against manager's workflow assignments          │
│     ├─ Marks orphaned workflows as failed                       │
│     ├─ Interval: ORPHAN_SCAN_INTERVAL (120s)                    │
│     └─ Worker timeout: ORPHAN_SCAN_WORKER_TIMEOUT (5s)          │
│                                                                 │
│  6. EXTENSION EXHAUSTION (AD-26)                                │
│     ├─ Workers have limited extension requests                  │
│     ├─ Exhaustion triggers warning, then grace period           │
│     ├─ Grace period expiry triggers eviction                    │
│     └─ Prevents infinitely-extending stuck workflows            │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

#### Prevention Mechanisms

```
┌─────────────────────────────────────────────────────────────────┐
│                  Zombie Prevention Mechanisms                    │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  1. FENCE TOKENS                                                │
│     ├─ Monotonically increasing token per job                  │
│     ├─ Prevents stale updates from old job executions          │
│     ├─ Gates reject results with outdated fence tokens         │
│     └─ Incremented on: retry, failover, reassignment           │
│                                                                 │
│  2. VERSIONED CLOCK                                             │
│     ├─ Per-entity Lamport timestamps                           │
│     ├─ All state updates include clock version                 │
│     ├─ Rejects updates with older clock values                 │
│     └─ Ensures consistent ordering across DCs                  │
│                                                                 │
│  3. CANCELLATION POLLING                                        │
│     ├─ Workers poll manager for job cancellation status        │
│     ├─ Interval: WORKER_CANCELLATION_POLL_INTERVAL (5s)        │
│     ├─ Catches cancellations even if push notification fails   │
│     └─ Self-termination on discovering cancelled state         │
│                                                                 │
│  4. QUORUM CONFIRMATION                                         │
│     ├─ Critical state changes require manager quorum           │
│     ├─ Prevents split-brain scenarios                          │
│     └─ Failed quorum blocks state transition                   │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

#### Orphan Workflow Scanner

The orphan scanner runs periodically on managers to detect workflows that:
- Are tracked by the manager but not running on any worker
- Are running on workers but not tracked by the manager

```python
async def _orphan_workflow_scan_loop(self) -> None:
    """Background loop that scans for orphaned workflows."""
    while not self._shutdown_event.is_set():
        try:
            await asyncio.sleep(self._orphan_scan_interval)

            # Get all known workflow IDs from manager state
            known_workflow_ids = set(self._workflow_assignments.keys())

            # Query each worker for active workflows
            worker_workflows: dict[str, set[str]] = {}
            for worker_id, registration in self._workers.items():
                active_ids = await self._query_worker_workflows(
                    worker_id,
                    registration.address,
                )
                worker_workflows[worker_id] = active_ids

            # Find orphans: known to manager but not on any worker
            all_worker_workflows = set()
            for workflows in worker_workflows.values():
                all_worker_workflows.update(workflows)

            orphaned = known_workflow_ids - all_worker_workflows

            # Mark orphaned workflows as failed
            for workflow_id in orphaned:
                await self._mark_workflow_failed(
                    workflow_id,
                    "Orphaned - not found on any worker",
                )
```

#### Configuration

```python
# Dead node reaping
MANAGER_DEAD_WORKER_REAP_INTERVAL: float = 900.0  # 15 minutes
MANAGER_DEAD_PEER_REAP_INTERVAL: float = 900.0
MANAGER_DEAD_GATE_REAP_INTERVAL: float = 900.0
WORKER_DEAD_MANAGER_REAP_INTERVAL: float = 900.0

# Job cleanup
COMPLETED_JOB_MAX_AGE: float = 300.0  # 5 minutes
FAILED_JOB_MAX_AGE: float = 3600.0    # 1 hour
JOB_CLEANUP_INTERVAL: float = 60.0

# Orphan scanning
ORPHAN_SCAN_INTERVAL: float = 120.0       # 2 minutes
ORPHAN_SCAN_WORKER_TIMEOUT: float = 5.0

# Cancellation polling
WORKER_CANCELLATION_POLL_INTERVAL: float = 5.0
```

---

### Per-Workflow Result Streaming

Results are streamed from workers to managers to gates to clients as workflows complete, rather than waiting for entire jobs to finish.

#### Streaming Flow

```
┌─────────────────────────────────────────────────────────────────┐
│               Per-Workflow Result Streaming                      │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  Worker                Manager               Gate        Client │
│    │                     │                    │            │    │
│    │─ WorkflowResult ───►│                    │            │    │
│    │  (wf-001 complete)  │                    │            │    │
│    │                     │─ WorkflowResult ──►│            │    │
│    │                     │  (aggregated)      │            │    │
│    │                     │                    │─ Stream ──►│    │
│    │                     │                    │  Result    │    │
│    │                     │                    │            │    │
│    │─ WorkflowResult ───►│                    │            │    │
│    │  (wf-002 complete)  │                    │            │    │
│    │                     │─ WorkflowResult ──►│            │    │
│    │                     │                    │─ Stream ──►│    │
│    │                     │                    │            │    │
│    │                     │                    │            │    │
│    │  [All workflows complete]                │            │    │
│    │                     │                    │            │    │
│    │                     │─ JobComplete ─────►│            │    │
│    │                     │                    │─ Final ───►│    │
│    │                     │                    │  Summary   │    │
│                                                                 │
│  Benefits:                                                      │
│  • Real-time progress visibility                                │
│  • Early failure detection                                      │
│  • Lower latency for time-sensitive results                     │
│  • Memory efficiency (results processed incrementally)          │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

#### Client API

```python
client = HyperscaleClient(gate_tcp_addrs=[...])
await client.start()

# Submit job
job_id = await client.submit_job(submission)

# Stream results as they arrive
async for workflow_result in client.stream_workflow_results(job_id):
    print(f"Workflow {workflow_result.workflow_id}: {workflow_result.status}")
    # Process individual workflow results...

# Or wait for all results
final_result = await client.wait_for_completion(job_id)
```

---

### Time Alignment for Cross-DC Aggregation

When aggregating results across datacenters, clock skew must be handled to produce accurate timing metrics.

#### Clock Synchronization

```
┌─────────────────────────────────────────────────────────────────┐
│              Cross-DC Time Alignment                             │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  Problem: Different DCs have different wall-clock times         │
│                                                                 │
│  DC-West (PDT)        DC-East (EDT)        DC-EU (CET)         │
│  10:00:00.000         13:00:00.050         19:00:00.120        │
│       │                    │                    │               │
│       │  Clock skew: 50ms  │  Clock skew: 70ms  │              │
│       │                    │                    │               │
│                                                                 │
│  Solution: Versioned Clock with Lamport timestamps              │
│                                                                 │
│  ┌──────────────────────────────────────────────────────────┐  │
│  │ VersionedClock                                            │  │
│  │                                                           │  │
│  │ • Logical clock increments on each event                  │  │
│  │ • Merged with received clock on message receipt           │  │
│  │ • Provides total ordering without wall-clock dependency   │  │
│  │                                                           │  │
│  │ clock_value = max(local_clock, received_clock) + 1        │  │
│  └──────────────────────────────────────────────────────────┘  │
│                                                                 │
│  For latency metrics:                                           │
│  ┌──────────────────────────────────────────────────────────┐  │
│  │ Monotonic Time Basis                                      │  │
│  │                                                           │  │
│  │ • All timing within a node uses time.monotonic()          │  │
│  │ • Cross-node timing uses relative deltas                  │  │
│  │ • Aggregation preserves statistical properties            │  │
│  │   (min, max, mean, percentiles all computed from deltas)  │  │
│  └──────────────────────────────────────────────────────────┘  │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

---

### Datacenter List Query

Clients can query gates for the list of registered datacenters.

#### API

```python
# Client-side
client = HyperscaleClient(gate_tcp_addrs=[...])
await client.start()

# Query available datacenters
datacenters = await client.get_datacenters()
# Returns: ["us-west-1", "us-east-1", "eu-west-1", ...]

# Submit job to specific datacenters
submission = JobSubmission(
    workflows=[...],
    target_datacenters=["us-west-1", "us-east-1"],
)
```

#### Message Types

```python
@dataclass
class DatacenterListRequest:
    """Request to list available datacenters."""
    request_id: str = field(default_factory=lambda: str(uuid.uuid4()))

@dataclass
class DatacenterListResponse:
    """Response containing available datacenters."""
    request_id: str
    datacenters: list[str]
    timestamp: float = field(default_factory=time.time)
```

#### Handler (Gate)

```python
@tcp.receive()
async def datacenter_list(self, addr, data, clock_time):
    """Handle datacenter list query from client."""
    request = DatacenterListRequest.load(data)

    # Collect datacenter IDs from known managers
    datacenter_ids = list(self._datacenter_status.keys())

    response = DatacenterListResponse(
        request_id=request.request_id,
        datacenters=datacenter_ids,
    )

    return response.dump()
```

---

### Known Issues to Investigate

---

### Commands for Quick Resume

```bash
# Run all existing tests
python examples/servers/test_single_worker.py
python examples/servers/test_workflow_end_to_end.py  
python examples/servers/test_workflow_stats_push.py
python examples/servers/test_gate_results_aggregation.py

# Check for regressions
cd /home/ada/Projects/hyperscale
git status
git log --oneline -10

# Current branch
git branch --show-current  # AL-distributed-wip
```

---

## License

See the main project LICENSE file.


---

## Worker → Manager Progress Update Architecture

### Overview

Workers collect progress updates from their local workflow execution (via `RemoteGraphManager`) and send them to the job leader Manager. This system is designed to be:

1. **Lossless** - Every progress update is captured (no dropped samples)
2. **Backpressure-aware** - Respects Manager overload signals
3. **Lifecycle-immediate** - Status transitions (STARTED, COMPLETED, FAILED) are sent immediately
4. **Rate-controlled** - Regular progress updates are batched to avoid Manager spam

### Architecture

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    WORKER PROGRESS UPDATE FLOW                               │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  Local Workflow Execution (Subprocess Pool)                                  │
│  ┌──────────────────────────────────────────────────────────────────────┐   │
│  │  RemoteGraphController (subprocess)                                   │   │
│  │                                                                       │   │
│  │  ┌─────────────────┐    ┌─────────────────┐                          │   │
│  │  │ push_workflow_  │    │ aggregate_      │                          │   │
│  │  │ status_update   │───►│ status_updates  │                          │   │
│  │  │ (0.1s schedule) │    │ (0.05s schedule)│                          │   │
│  │  └─────────────────┘    └────────┬────────┘                          │   │
│  │                                  │                                    │   │
│  │                     completion_state.status_update_queue              │   │
│  │                                  │                                    │   │
│  └──────────────────────────────────┼───────────────────────────────────┘   │
│                                     │                                        │
│  Worker (Main Process)              │                                        │
│  ┌──────────────────────────────────┼───────────────────────────────────┐   │
│  │                                  ▼                                    │   │
│  │  ┌─────────────────────────────────────────────────────────────────┐ │   │
│  │  │               RemoteGraphManager (Leader Process)               │ │   │
│  │  │                                                                 │ │   │
│  │  │  ┌───────────────────────┐    ┌──────────────────────────────┐ │ │   │
│  │  │  │ _wait_for_workflow_   │    │ get_availability()           │ │ │   │
│  │  │  │ completion loop       │    │ (sync, non-blocking)         │ │ │   │
│  │  │  │                       │    │                              │ │ │   │
│  │  │  │ • Poll status queue   │    │ Returns: (assigned,          │ │ │   │
│  │  │  │ • Update stats        │    │           completed,         │ │ │   │
│  │  │  │ • Call callback       │    │           available)         │ │ │   │
│  │  │  └───────────┬───────────┘    └──────────────────────────────┘ │ │   │
│  │  │              │                                                  │ │   │
│  │  └──────────────┼──────────────────────────────────────────────────┘ │   │
│  │                 │                                                     │   │
│  │                 ▼                                                     │   │
│  │  ┌─────────────────────────────────────────────────────────────────┐ │   │
│  │  │               _monitor_workflow_progress()                       │ │   │
│  │  │                                                                  │ │   │
│  │  │  • Convert WorkflowStatusUpdate → WorkflowProgress              │ │   │
│  │  │  • Add core allocation info from CoreAllocator                  │ │   │
│  │  │  • Add CPU/memory metrics                                       │ │   │
│  │  │  • Call _send_progress_update() [BUFFER]                        │ │   │
│  │  │                                                                  │ │   │
│  │  └───────────────────────────────┬─────────────────────────────────┘ │   │
│  │                                  │                                    │   │
│  │          ┌───────────────────────┴───────────────────────┐           │   │
│  │          │                                               │           │   │
│  │          ▼                                               ▼           │   │
│  │  ┌───────────────────────┐                 ┌────────────────────────┐│   │
│  │  │ _progress_buffer      │                 │ _transition_workflow_  ││   │
│  │  │ (dict: workflow_id →  │                 │ status()               ││   │
│  │  │  latest progress)     │                 │                        ││   │
│  │  │                       │                 │ For: STARTED,          ││   │
│  │  │ Latest-wins: only     │                 │      COMPLETED,        ││   │
│  │  │ most recent per       │                 │      FAILED            ││   │
│  │  │ workflow kept         │                 │                        ││   │
│  │  └───────────┬───────────┘                 │ → Immediate send       ││   │
│  │              │                             │   (bypass buffer)      ││   │
│  │              │                             └───────────┬────────────┘│   │
│  │              ▼                                         │             │   │
│  │  ┌───────────────────────┐                             │             │   │
│  │  │ _progress_flush_loop  │                             │             │   │
│  │  │ (background task)     │                             │             │   │
│  │  │                       │                             │             │   │
│  │  │ • Sleep for interval  │                             │             │   │
│  │  │   (50ms default)      │                             │             │   │
│  │  │ • Check backpressure  │                             │             │   │
│  │  │ • Clear buffer        │                             │             │   │
│  │  │ • Send to job leader  │                             │             │   │
│  │  └───────────┬───────────┘                             │             │   │
│  │              │                                         │             │   │
│  │              └─────────────────────┬───────────────────┘             │   │
│  │                                    │                                  │   │
│  └────────────────────────────────────┼─────────────────────────────────┘   │
│                                       │                                      │
│                                       ▼                                      │
│                     ┌─────────────────────────────────────┐                 │
│                     │   _send_progress_to_job_leader()    │                 │
│                     │                                     │                 │
│                     │ Routes to the Manager that          │                 │
│                     │ dispatched this workflow (not       │                 │
│                     │ necessarily primary manager)        │                 │
│                     │                                     │                 │
│                     │ Handles:                            │                 │
│                     │ • Job leader discovery              │                 │
│                     │ • Failover to new leader            │                 │
│                     │ • Circuit breaker per manager       │                 │
│                     └──────────────────┬──────────────────┘                 │
│                                        │                                     │
└────────────────────────────────────────┼─────────────────────────────────────┘
                                         │
                                         ▼
                              ┌─────────────────────┐
                              │   Manager (TCP)     │
                              │                     │
                              │ workflow_progress() │
                              │ handler             │
                              └─────────────────────┘
```

### Key Components

#### 1. RemoteGraphManager State Tracking

The `RemoteGraphManager` maintains core availability as simple state (not a queue):

```python
class RemoteGraphManager:
    def __init__(self, ...):
        # Latest core availability state (assigned, completed, available)
        # Updated atomically - readers get current value immediately
        self._latest_availability: tuple[int, int, int] = (0, 0, 0)

    def get_availability(self) -> tuple[int, int, int]:
        """
        Get the current core availability state.

        Returns (assigned, completed, available) tuple.
        This is NON-BLOCKING and returns immediately.
        """
        return self._latest_availability

    def _update_available_cores(self, assigned: int, completed: int):
        """Update state atomically and notify if cores freed."""
        available = self._threads - max(assigned - completed, 0)
        self._latest_availability = (assigned, completed, available)

        # Instant callback if cores became available
        if self._on_cores_available and available > 0:
            self._on_cores_available(available)
```

**Why state-based, not queue-based?**
- Progress updates are cumulative (totals, not deltas)
- We only care about the *current* state, not history
- Queue-based `await queue.get()` blocked when empty, causing 5+ second delays
- State-based reads are instant and non-blocking

#### 2. Progress Buffer (Latest-Wins)

The Worker maintains a simple buffer that keeps only the latest progress per workflow:

```python
class WorkerServer:
    def __init__(self, ...):
        self._progress_buffer: dict[str, WorkflowProgress] = {}
        self._progress_buffer_lock = asyncio.Lock()
        self._progress_flush_interval: float = env.WORKER_PROGRESS_FLUSH_INTERVAL  # 50ms

    async def _send_progress_update(self, progress: WorkflowProgress) -> None:
        """
        Buffer a progress update for batched sending.

        Instead of sending immediately, updates are collected in a buffer
        and flushed periodically by _progress_flush_loop.
        """
        async with self._progress_buffer_lock:
            # Latest-wins: only keep most recent per workflow
            self._progress_buffer[progress.workflow_id] = progress
```

**Why latest-wins?**
- Progress is cumulative (`completed_count` is total, not delta)
- Old samples are superseded by newer ones
- No need for complex aggregation
- Memory bounded: O(active_workflows)

#### 3. Flush Loop (Backpressure-Aware)

```python
async def _progress_flush_loop(self) -> None:
    """Background loop that flushes buffered progress to manager."""
    while self._running:
        # Respect backpressure signals from managers
        effective_interval = self._get_effective_flush_interval()
        await asyncio.sleep(effective_interval)

        # Drop updates under heavy backpressure
        if self._get_max_backpressure_level() >= BackpressureLevel.REJECT:
            async with self._progress_buffer_lock:
                self._progress_buffer.clear()
            continue

        # Get and clear buffer atomically
        async with self._progress_buffer_lock:
            if not self._progress_buffer:
                continue
            updates_to_send = dict(self._progress_buffer)
            self._progress_buffer.clear()

        # Send to job leaders
        if self._healthy_manager_ids:
            for workflow_id, progress in updates_to_send.items():
                await self._send_progress_to_job_leader(progress)

def _get_effective_flush_interval(self) -> float:
    """Increase interval when managers signal backpressure."""
    base = self._progress_flush_interval  # 50ms
    if self._backpressure_delay_ms > 0:
        return base + (self._backpressure_delay_ms / 1000.0)
    return base
```

#### 4. Lifecycle Events (Immediate Send)

Status transitions bypass the buffer for immediate visibility:

```python
async def _transition_workflow_status(
    self,
    progress: WorkflowProgress,
    new_status: WorkflowStatus,
    start_time: float | None = None,
) -> None:
    """
    Transition workflow to a new status with IMMEDIATE send.

    This is the ONLY method that should change workflow status.
    Lifecycle events (STARTED, COMPLETED, FAILED) are always sent
    immediately to ensure visibility even for short workflows.
    """
    progress.status = new_status.value
    progress.timestamp = time.monotonic()
    progress.collected_at = time.time()

    if start_time is not None:
        progress.elapsed_seconds = time.monotonic() - start_time

    # Always send lifecycle transitions immediately (bypass buffer)
    if self._healthy_manager_ids:
        await self._send_progress_update_direct(progress)
```

### Job Leader Routing

Progress updates are routed to the Manager that dispatched the workflow:

```python
async def _send_progress_to_job_leader(
    self,
    progress: WorkflowProgress,
) -> bool:
    """
    Send progress to the job leader for this workflow.

    Routes to the manager that dispatched (job leader).
    Handles failover if job leader becomes unhealthy.
    """
    workflow_id = progress.workflow_id
    job_leader_addr = self._workflow_job_leader.get(workflow_id)

    # Try job leader first
    if job_leader_addr:
        success = await self._try_send_progress_to_addr(progress, job_leader_addr)
        if success:
            return True

        # Job leader failed - need to find new leader
        # Query any healthy manager for the current leader

    # Fallback: query healthy managers for job leader
    for manager_id in list(self._healthy_manager_ids):
        manager_info = self._known_managers.get(manager_id)
        if manager_info:
            success = await self._try_send_progress_to_addr(
                progress,
                (manager_info.host, manager_info.tcp_port)
            )
            if success:
                # Ack includes current job leader address - update routing
                return True

    return False
```

### Configuration

Environment variables in `Env`:

```python
# Worker progress update configuration
WORKER_PROGRESS_UPDATE_INTERVAL: float = 0.1    # How often to poll status queue (100ms)
WORKER_PROGRESS_FLUSH_INTERVAL: float = 0.05    # How often to flush buffer (50ms)

# Backpressure (AD-23)
# Managers can signal workers to slow down progress updates
# by including BackpressureSignal in progress acks
```

### Flow Comparison: Before vs After

**Before (Inline Rate-Limiting):**
```
[status update] → [rate limit check] → [send if time passed]
                          ↓
                   (DROP if too soon)
```
- Updates could be dropped
- No backpressure awareness
- Competed with flush loop

**After (Buffer + Flush):**
```
[status update] → [_progress_buffer] → [flush loop] → [send]
                   (latest-wins)       (controlled)
```
- No updates dropped (latest kept)
- Backpressure-aware
- Single unified mechanism
- Lifecycle events bypass for immediacy

### Integration with Windowed Stats

This Worker → Manager flow feeds into the Manager's `WindowedStatsCollector`:

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    END-TO-END PROGRESS FLOW                                  │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  ┌────────────┐    ┌────────────┐                                           │
│  │  Worker 1  │    │  Worker 2  │                                           │
│  │            │    │            │                                           │
│  │ [buffer]   │    │ [buffer]   │     Worker → Manager                      │
│  │ [flush]    │    │ [flush]    │     (This section)                        │
│  └─────┬──────┘    └─────┬──────┘                                           │
│        │                 │                                                   │
│        │ WorkflowProgress│                                                   │
│        │ (50ms batched)  │                                                   │
│        │                 │                                                   │
│        └────────┬────────┘                                                   │
│                 ▼                                                            │
│  ┌─────────────────────────────────────────────────────────────────────────┐│
│  │                    MANAGER                                               ││
│  │                                                                          ││
│  │  workflow_progress() ──► WindowedStatsCollector                          ││
│  │         │                    │                                           ││
│  │         │                    │ (time-bucketed windows)                   ││
│  │         │                    │ (drift tolerance)                         ││
│  │         │                    │ (aggregation)                             ││
│  │         │                    ▼                                           ││
│  │         │              [flush closed windows]                            ││
│  │         │                    │                                           ││
│  └─────────┼────────────────────┼───────────────────────────────────────────┘│
│            │                    │                                            │
│            │                    │ WindowedStatsPush                          │
│            │                    │ (50ms aggregated)                          │
│            ▼                    ▼                                            │
│   ┌─────────────────┐    ┌─────────────────┐                                │
│   │ Job tracking    │    │ Client/Gate     │     Manager → Client           │
│   │ (internal)      │    │ (streaming)     │     (Next section)             │
│   └─────────────────┘    └─────────────────┘                                │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

## Time-Windowed Streaming Stats System

### Overview

The streaming stats system provides real-time progress updates from workers to clients while:
1. **Correlating stats across workers by time** - Stats from different workers within the same time window are aggregated together
2. **Preventing client spam** - One aggregated push per window interval instead of per-worker updates
3. **Bounding memory usage** - Windows are cleared after each push cycle
4. **Supporting hierarchical aggregation** - Manager aggregates for direct clients; Gate aggregates across DCs

### Architecture

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    TIME-WINDOWED STATS FLOW                                 │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  Workers (rapid updates ~1s)                                                │
│  ┌─────────┐  ┌─────────┐  ┌─────────┐  ┌─────────┐                        │
│  │Worker 1 │  │Worker 2 │  │Worker 3 │  │Worker N │                        │
│  │ t=0.1s  │  │ t=0.15s │  │ t=0.12s │  │ t=0.18s │  ← collected_at        │
│  └────┬────┘  └────┬────┘  └────┬────┘  └────┬────┘    (Unix timestamp)    │
│       │            │            │            │                              │
│       └────────────┴─────┬──────┴────────────┘                              │
│                          ▼                                                  │
│  ┌───────────────────────────────────────────────────────────────────────┐  │
│  │                    MANAGER - WindowedStatsCollector                   │  │
│  ├───────────────────────────────────────────────────────────────────────┤  │
│  │                                                                       │  │
│  │  Time Windows (100ms buckets):                                        │  │
│  │  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐                   │  │
│  │  │ Window T=0  │  │ Window T=1  │  │ Window T=2  │  ...              │  │
│  │  │ [0ms-100ms) │  │[100ms-200ms)│  │[200ms-300ms)│                   │  │
│  │  │             │  │             │  │             │                   │  │
│  │  │ Worker1 ──┐ │  │ Worker2 ──┐ │  │ Worker1 ──┐ │                   │  │
│  │  │ Worker3 ──┼─│  │ Worker4 ──┼─│  │ Worker2 ──┼─│                   │  │
│  │  │ Worker2 ──┘ │  │ Worker1 ──┘ │  │ Worker3 ──┘ │                   │  │
│  │  └──────┬──────┘  └──────┬──────┘  └──────┬──────┘                   │  │
│  │         │                │                │                           │  │
│  │         ▼                ▼                ▼                           │  │
│  │    [aggregate]      [aggregate]      [aggregate]                      │  │
│  │         │                │                │                           │  │
│  │         └────────────────┼────────────────┘                           │  │
│  │                          │                                            │  │
│  │  Flush Timer (100ms)     │                                            │  │
│  │  ────────────────────────┼──────────────────────────────              │  │
│  │                          ▼                                            │  │
│  │              ┌───────────────────────┐                                │  │
│  │              │  Closed windows only  │                                │  │
│  │              │  (T < current - drift)│                                │  │
│  │              └───────────┬───────────┘                                │  │
│  │                          │                                            │  │
│  └──────────────────────────┼────────────────────────────────────────────┘  │
│                             │                                               │
│          ┌──────────────────┴──────────────────┐                           │
│          │                                     │                           │
│          ▼                                     ▼                           │
│  ┌───────────────────┐               ┌─────────────────────┐               │
│  │  Direct Client    │               │       Gate          │               │
│  │  (aggregated)     │               │   (unaggregated)    │               │
│  │                   │               │                     │               │
│  │  WindowedStatsPush│               │  WindowedStatsPush  │               │
│  │  - window_start   │               │  - window_start     │               │
│  │  - window_end     │               │  - window_end       │               │
│  │  - aggregated:    │               │  - per_worker:      │               │
│  │    completed,     │               │    [{worker_id,     │               │
│  │    failed,        │               │      completed,     │               │
│  │    rate,          │               │      failed, ...}]  │               │
│  │    step_stats     │               │                     │               │
│  └───────────────────┘               └──────────┬──────────┘               │
│                                                 │                          │
│                                                 ▼                          │
│                                      ┌─────────────────────┐               │
│                                      │  Gate Aggregation   │               │
│                                      │  (same windowing)   │               │
│                                      │                     │               │
│                                      │  Correlates windows │               │
│                                      │  across DCs         │               │
│                                      └──────────┬──────────┘               │
│                                                 │                          │
│                                                 ▼                          │
│                                      ┌─────────────────────┐               │
│                                      │      Client         │               │
│                                      │   (aggregated)      │               │
│                                      └─────────────────────┘               │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Time Window Bucketing

Stats are bucketed by their `collected_at` Unix timestamp into discrete windows:

```python
WINDOW_SIZE_MS = 100  # 100ms windows
DRIFT_TOLERANCE_MS = 50  # Allow 50ms clock drift between workers

def get_window_bucket(collected_at: float) -> int:
    """Convert Unix timestamp to window bucket number."""
    return int(collected_at * 1000 / WINDOW_SIZE_MS)

def is_window_closed(bucket: int, now: float) -> bool:
    """Check if a window can be flushed (all expected stats have arrived)."""
    window_end_ms = (bucket + 1) * WINDOW_SIZE_MS
    current_ms = now * 1000
    # Window is closed when current time exceeds window_end + drift tolerance
    return current_ms > window_end_ms + DRIFT_TOLERANCE_MS
```

### WindowedStatsCollector Class

Located at `hyperscale/distributed_rewrite/jobs/windowed_stats_collector.py`:

```python
@dataclass
class WindowBucket:
    """Stats collected within a single time window."""
    window_start: float  # Unix timestamp of window start
    window_end: float    # Unix timestamp of window end
    job_id: str
    workflow_id: str
    worker_stats: dict[str, WorkflowProgress]  # worker_id -> progress
    created_at: float    # When this bucket was created (for cleanup)

class WindowedStatsCollector:
    """
    Collects workflow progress updates into time-correlated windows.
    
    Thread-safe for concurrent progress updates from multiple workers.
    """
    
    def __init__(
        self,
        window_size_ms: float = 100.0,
        drift_tolerance_ms: float = 50.0,
        max_window_age_ms: float = 5000.0,  # Cleanup windows older than 5s
    ):
        self._window_size_ms = window_size_ms
        self._drift_tolerance_ms = drift_tolerance_ms
        self._max_window_age_ms = max_window_age_ms
        
        # Buckets indexed by (job_id, workflow_id, bucket_number)
        self._buckets: dict[tuple[str, str, int], WindowBucket] = {}
        self._lock = asyncio.Lock()
    
    async def add_progress(
        self,
        worker_id: str,
        progress: WorkflowProgress,
    ) -> None:
        """Add a progress update to the appropriate time window."""
        bucket_num = self._get_bucket_number(progress.collected_at)
        key = (progress.job_id, progress.workflow_id, bucket_num)
        
        async with self._lock:
            if key not in self._buckets:
                self._buckets[key] = WindowBucket(
                    window_start=bucket_num * self._window_size_ms / 1000,
                    window_end=(bucket_num + 1) * self._window_size_ms / 1000,
                    job_id=progress.job_id,
                    workflow_id=progress.workflow_id,
                    worker_stats={},
                    created_at=time.time(),
                )
            
            self._buckets[key].worker_stats[worker_id] = progress
    
    async def flush_closed_windows(
        self,
        aggregate: bool = True,
    ) -> list[WindowedStatsPush]:
        """
        Flush all closed windows and return them for pushing.
        
        Args:
            aggregate: If True, aggregate stats within window.
                      If False, return per-worker stats (for Gate forwarding).
        
        Returns:
            List of WindowedStatsPush messages ready for client/gate.
        """
        now = time.time()
        results = []
        keys_to_remove = []
        
        async with self._lock:
            for key, bucket in self._buckets.items():
                _, _, bucket_num = key
                
                if self._is_window_closed(bucket_num, now):
                    if aggregate:
                        push = self._aggregate_bucket(bucket)
                    else:
                        push = self._unaggregated_bucket(bucket)
                    results.append(push)
                    keys_to_remove.append(key)
                
                # Also cleanup very old windows (missed or stuck)
                elif (now - bucket.created_at) * 1000 > self._max_window_age_ms:
                    keys_to_remove.append(key)
            
            for key in keys_to_remove:
                del self._buckets[key]
        
        return results
    
    def _aggregate_bucket(self, bucket: WindowBucket) -> WindowedStatsPush:
        """Aggregate all worker stats in a bucket into single stats."""
        total_completed = 0
        total_failed = 0
        total_rate = 0.0
        step_stats_by_name: dict[str, StepStats] = {}
        
        for progress in bucket.worker_stats.values():
            total_completed += progress.completed_count
            total_failed += progress.failed_count
            total_rate += progress.rate_per_second
            
            for step in progress.step_stats:
                if step.step_name in step_stats_by_name:
                    existing = step_stats_by_name[step.step_name]
                    step_stats_by_name[step.step_name] = StepStats(
                        step_name=step.step_name,
                        completed_count=existing.completed_count + step.completed_count,
                        failed_count=existing.failed_count + step.failed_count,
                        total_count=existing.total_count + step.total_count,
                    )
                else:
                    step_stats_by_name[step.step_name] = step
        
        return WindowedStatsPush(
            job_id=bucket.job_id,
            workflow_id=bucket.workflow_id,
            window_start=bucket.window_start,
            window_end=bucket.window_end,
            completed_count=total_completed,
            failed_count=total_failed,
            rate_per_second=total_rate,
            step_stats=list(step_stats_by_name.values()),
            worker_count=len(bucket.worker_stats),
            is_aggregated=True,
        )
```

### Message Types

```python
@dataclass(slots=True)
class WindowedStatsPush(Message):
    """
    Time-windowed stats push to client or gate.
    
    When is_aggregated=True (for clients):
        - Contains aggregated stats across all workers in window
        - step_stats are merged by step name
        
    When is_aggregated=False (for gates):
        - per_worker_stats contains individual worker progress
        - Gate performs its own aggregation across DCs
    """
    job_id: str
    workflow_id: str
    workflow_name: str = ""
    window_start: float = 0.0  # Unix timestamp
    window_end: float = 0.0    # Unix timestamp
    
    # Aggregated stats (when is_aggregated=True)
    completed_count: int = 0
    failed_count: int = 0
    rate_per_second: float = 0.0
    step_stats: list[StepStats] = field(default_factory=list)
    worker_count: int = 0
    
    # Per-worker stats (when is_aggregated=False, for gate forwarding)
    per_worker_stats: list[WorkerWindowStats] = field(default_factory=list)
    
    is_aggregated: bool = True
    datacenter: str = ""  # Set by manager when forwarding to gate


@dataclass(slots=True)
class WorkerWindowStats(Message):
    """Individual worker stats within a time window."""
    worker_id: str
    completed_count: int = 0
    failed_count: int = 0
    rate_per_second: float = 0.0
    step_stats: list[StepStats] = field(default_factory=list)
```

### Manager Integration

The Manager integrates the WindowedStatsCollector into its workflow progress handling:

```python
class ManagerServer:
    def __init__(self, ...):
        ...
        # Windowed stats for streaming to clients
        self._windowed_stats = WindowedStatsCollector(
            window_size_ms=env.STATS_WINDOW_SIZE_MS,      # Default: 100ms
            drift_tolerance_ms=env.STATS_DRIFT_TOLERANCE_MS,  # Default: 50ms
        )
        
    async def workflow_progress(self, addr, data, clock_time):
        """Handle workflow progress update from worker."""
        progress = WorkflowProgress.load(data)
        
        # Add to windowed collector for streaming
        worker_id = self._resolve_worker_id_from_addr(addr)
        await self._windowed_stats.add_progress(worker_id, progress)
        
        # ... existing progress handling ...
    
    async def _windowed_stats_push_loop(self):
        """Background loop to flush and push windowed stats."""
        interval = self._env.STATS_PUSH_INTERVAL  # Default: 100ms
        
        while self._running:
            await asyncio.sleep(interval / 1000)
            
            # Determine if we're pushing to clients or gates
            has_gates = bool(self._gate_addrs or self._known_gates)
            
            # Flush closed windows
            pushes = await self._windowed_stats.flush_closed_windows(
                aggregate=not has_gates  # Aggregate for clients, not for gates
            )
            
            if not pushes:
                continue
            
            if has_gates:
                # Forward unaggregated to gates
                for push in pushes:
                    push.datacenter = self._node_id.datacenter
                    await self._forward_stats_to_gates(push)
            else:
                # Push aggregated to clients
                for push in pushes:
                    await self._push_stats_to_client(push)
```

### Gate Integration

Gates receive unaggregated windowed stats from managers and perform cross-DC aggregation:

```python
class GateServer:
    def __init__(self, ...):
        ...
        # Collect stats from all DCs for cross-DC aggregation
        self._dc_windowed_stats: dict[str, WindowedStatsCollector] = {}
        
    @tcp.receive()
    async def windowed_stats_push(self, addr, data, clock_time):
        """Receive windowed stats from a manager."""
        push = WindowedStatsPush.load(data)
        
        # Store in per-DC collector
        dc_id = push.datacenter
        if dc_id not in self._dc_windowed_stats:
            self._dc_windowed_stats[dc_id] = WindowedStatsCollector()
        
        # Re-add each worker's stats to preserve window alignment
        for worker_stats in push.per_worker_stats:
            # Create a synthetic progress for the collector
            progress = WorkflowProgress(
                job_id=push.job_id,
                workflow_id=push.workflow_id,
                collected_at=push.window_start,  # Use window start for alignment
                completed_count=worker_stats.completed_count,
                ...
            )
            await self._dc_windowed_stats[dc_id].add_progress(
                f"{dc_id}:{worker_stats.worker_id}",
                progress,
            )
        
        return b'ok'
    
    async def _gate_windowed_stats_push_loop(self):
        """Aggregate across DCs and push to clients."""
        interval = self._env.STATS_PUSH_INTERVAL
        
        while self._running:
            await asyncio.sleep(interval / 1000)
            
            # Collect and aggregate from all DCs
            all_pushes: dict[tuple[str, str, float], list[WindowedStatsPush]] = {}
            
            for dc_id, collector in self._dc_windowed_stats.items():
                pushes = await collector.flush_closed_windows(aggregate=True)
                for push in pushes:
                    key = (push.job_id, push.workflow_id, push.window_start)
                    if key not in all_pushes:
                        all_pushes[key] = []
                    all_pushes[key].append(push)
            
            # Aggregate same-window stats across DCs
            for key, dc_pushes in all_pushes.items():
                aggregated = self._aggregate_dc_pushes(dc_pushes)
                await self._push_stats_to_client(aggregated)
```

### Client Integration

The client receives windowed stats via a new `on_progress_update` callback:

```python
class HyperscaleClient:
    async def submit_job(
        self,
        workflows: list[type],
        ...
        on_status_update: Callable[[JobStatusPush], None] | None = None,
        on_progress_update: Callable[[WindowedStatsPush], None] | None = None,  # NEW
        on_workflow_result: Callable[[WorkflowResultPush], None] | None = None,
        ...
    ) -> str:
        """
        Submit a job for execution.
        
        Args:
            ...
            on_status_update: Callback for job status changes (started, completed, failed)
            on_progress_update: Callback for streaming progress stats (time-windowed)
            on_workflow_result: Callback for workflow completion results
        """
        ...
        if on_progress_update:
            self._progress_callbacks[job_id] = on_progress_update
    
    @tcp.receive()
    async def windowed_stats_push(self, addr, data, clock_time):
        """Handle windowed stats push from manager/gate."""
        push = WindowedStatsPush.load(data)
        
        callback = self._progress_callbacks.get(push.job_id)
        if callback:
            try:
                callback(push)
            except Exception:
                pass
        
        return b'ok'
```

### Client Rate Limiting (Stats Updates Only)

The client applies rate limiting specifically to `windowed_stats_push` to prevent overwhelming the callback:

```python
class HyperscaleClient:
    def __init__(self, ...):
        ...
        # Rate limit for progress updates (stats streaming)
        self._progress_rate_limit = RateLimiter(
            max_per_second=env.CLIENT_PROGRESS_RATE_LIMIT,  # Default: 20/sec
            burst=env.CLIENT_PROGRESS_BURST,  # Default: 5
        )
    
    @tcp.receive()
    async def windowed_stats_push(self, addr, data, clock_time):
        """Handle windowed stats push with rate limiting."""
        # Apply rate limiting - drop if over limit
        if not self._progress_rate_limit.try_acquire():
            return b'rate_limited'
        
        push = WindowedStatsPush.load(data)
        
        callback = self._progress_callbacks.get(push.job_id)
        if callback:
            try:
                callback(push)
            except Exception:
                pass
        
        return b'ok'
```

### Configuration

New environment variables in `Env`:

```python
# Stats windowing
STATS_WINDOW_SIZE_MS: float = 100.0        # Window bucket size
STATS_DRIFT_TOLERANCE_MS: float = 50.0     # Clock drift tolerance
STATS_PUSH_INTERVAL: float = 100.0         # How often to flush windows (ms)

# Client rate limiting (progress updates only)
CLIENT_PROGRESS_RATE_LIMIT: float = 20.0   # Max progress callbacks per second
CLIENT_PROGRESS_BURST: int = 5             # Burst allowance
```

### Memory Management

Windows are automatically cleaned up:

1. **On flush**: Closed windows are removed after being pushed
2. **Age-based cleanup**: Windows older than `max_window_age_ms` (default 5s) are dropped
3. **Job completion**: All windows for a job are cleared when job completes

```python
async def cleanup_job_windows(self, job_id: str) -> None:
    """Remove all windows for a completed job."""
    async with self._lock:
        keys_to_remove = [
            key for key in self._buckets.keys()
            if key[0] == job_id
        ]
        for key in keys_to_remove:
            del self._buckets[key]
```

### Sequence Diagram

```
Worker1     Worker2     Manager           Gate           Client
   │           │           │                │               │
   │──progress─▶│          │                │               │
   │  t=0.12s  │──progress─▶                │               │
   │           │  t=0.15s  │                │               │
   │           │           │                │               │
   │           │      [bucket 0: W1, W2]    │               │
   │           │           │                │               │
   │           │      (100ms flush timer)   │               │
   │           │           │                │               │
   │           │      [window closed]       │               │
   │           │           │                │               │
   │           │           │──(unaggregated)─▶              │
   │           │           │  WindowedStats  │              │
   │           │           │                │              │
   │           │           │                │──(aggregated)─▶
   │           │           │                │ WindowedStats │
   │           │           │                │               │
   │           │           │                │     [callback]│
```

---

## Bootstrap & Service Discovery

### Design Goals

The bootstrap system must satisfy these requirements:

1. **Environment Agnostic**: Works identically on bare metal, VMs, containers, and Kubernetes
2. **No External Dependencies**: No etcd, Consul, Zookeeper, or other coordination services
3. **Fast Convergence**: New nodes join the cluster in sub-second time under normal conditions
4. **Churn Resilient**: Handles frequent node restarts, rolling deployments, and autoscaling
5. **Robust Under Failure**: Continues operating when some seeds are unavailable
6. **Simple Configuration**: Minimal config required - just seed addresses or DNS name

### Architecture Decision

**Decision**: Hybrid DNS + Static Seeds with Parallel Probing

After evaluating multiple approaches, we chose a hybrid strategy that:
- Accepts static seed addresses (bare metal friendly)
- Optionally accepts DNS names for dynamic discovery (Kubernetes friendly)
- Probes all candidates in parallel with short timeouts
- Succeeds on first response (any live peer is sufficient)
- Hands off to SWIM gossip once joined

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                         BOOTSTRAP ARCHITECTURE                               │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  ┌──────────────┐     ┌──────────────┐     ┌──────────────┐                │
│  │   Static     │     │     DNS      │     │   Health     │                │
│  │   Seeds      │     │   Resolver   │     │    Cache     │                │
│  │              │     │              │     │              │                │
│  │ 10.0.1.5:9000│     │ managers.svc │     │ Recently     │                │
│  │ 10.0.1.6:9000│     │ → [IP1, IP2] │     │ alive peers  │                │
│  └──────┬───────┘     └──────┬───────┘     └──────┬───────┘                │
│         │                    │                    │                         │
│         └────────────────────┼────────────────────┘                         │
│                              │                                              │
│                              ▼                                              │
│                    ┌─────────────────┐                                      │
│                    │   Candidate     │                                      │
│                    │   Aggregator    │                                      │
│                    │                 │                                      │
│                    │ Dedup + Merge   │                                      │
│                    └────────┬────────┘                                      │
│                             │                                               │
│                             ▼                                               │
│         ┌───────────────────────────────────────────┐                       │
│         │           PARALLEL PROBER                  │                       │
│         │                                            │                       │
│         │  ┌─────┐  ┌─────┐  ┌─────┐  ┌─────┐      │                       │
│         │  │Probe│  │Probe│  │Probe│  │Probe│      │                       │
│         │  │ #1  │  │ #2  │  │ #3  │  │ #4  │ ...  │                       │
│         │  └──┬──┘  └──┬──┘  └──┬──┘  └──┬──┘      │                       │
│         │     │        │        │        │          │                       │
│         │     └────────┴────┬───┴────────┘          │                       │
│         │                   │                       │                       │
│         │            First Success                  │                       │
│         │            (cancel rest)                  │                       │
│         └───────────────────┬───────────────────────┘                       │
│                             │                                               │
│                             ▼                                               │
│                    ┌─────────────────┐                                      │
│                    │  SWIM Cluster   │                                      │
│                    │     Join        │                                      │
│                    │                 │                                      │
│                    │ Gossip takes    │                                      │
│                    │ over from here  │                                      │
│                    └─────────────────┘                                      │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Discovery Approaches Evaluated

| Approach | Pros | Cons | Verdict |
|----------|------|------|---------|
| **Static Seeds** | Simple, predictable, works everywhere | Requires config updates when seeds change | ✅ Use as primary |
| **DNS-Based** | Dynamic, K8s-native via headless services | TTL caching, stale records | ✅ Use as supplement |
| **Multicast/Broadcast** | Zero config, auto-discovery | Blocked by cloud providers, no cross-subnet | ❌ Rejected |
| **External Service (etcd/Consul)** | Feature-rich, proven | External dependency, operational burden | ❌ Rejected |
| **Shared Storage** | Works with NFS/S3 | Latency, complexity, another dependency | ❌ Rejected |
| **Port Scanning** | No config needed | Slow, looks malicious, security alerts | ❌ Rejected |

### Chosen Solution: DNS + Seeds with Parallel Probing

The key insight: **bootstrap is a one-time operation per node startup**. Once joined, SWIM handles all membership changes. We only need to find *one* live peer to join through.

#### Why This Works Under Churn

```
Timeline showing node C crashing and replacement C' joining:
─────────────────────────────────────────────────────────────────────────────
t=0     Cluster healthy: [A, B, C, D, E] all running
t=1     Pod C crashes, orchestrator starts replacement C'
t=2     DNS still returns C's old IP (TTL not expired)
t=3     New node F tries to join, resolves [A, B, C_old, D, E]
t=4     F probes ALL in parallel with 500ms timeout
t=5     A responds first (50ms) → F joins via A, cancels other probes
t=6     C_old probe times out (ignored, F already joined)
t=7     DNS updates, now returns [A, B, C', D, E]
t=8     C' bootstrap probes, joins via any live peer
t=9     SWIM gossip propagates C' membership to all nodes
─────────────────────────────────────────────────────────────────────────────

Key points:
- Parallel probing means one dead node doesn't block join
- 500ms timeout prevents long waits for unreachable hosts
- First responder wins - we don't wait for all probes
- SWIM handles ongoing membership after initial join
```

### Bootstrap Protocol

#### State Machine

```
                              ┌─────────────┐
                              │   INITIAL   │
                              └──────┬──────┘
                                     │
                              resolve candidates
                                     │
                                     ▼
                              ┌─────────────┐
                     ┌───────▶│  RESOLVING  │◀───────┐
                     │        └──────┬──────┘        │
                     │               │               │
                     │        candidates ready       │
                     │               │               │
                     │               ▼               │
                     │        ┌─────────────┐        │
                     │        │   PROBING   │        │
                     │        └──────┬──────┘        │
                     │               │               │
                     │     ┌─────────┴─────────┐     │
                     │     │                   │     │
                     │  success             all fail │
                     │     │                   │     │
                     │     ▼                   ▼     │
                     │ ┌────────┐      ┌───────────┐ │
                     │ │ JOINED │      │  BACKOFF  │─┘
                     │ └────────┘      └───────────┘
                     │                       │
                     │                  max retries
                     │                       │
                     │                       ▼
                     │               ┌──────────────┐
                     └───────────────│    FAILED    │
                                     └──────────────┘
```

#### Sequence Diagram: Successful Join

```
    New Node              Seed A             Seed B (dead)         Seed C
        │                    │                    │                    │
        │──── resolve() ────▶│                    │                    │
        │◀─── [A, B, C] ─────│                    │                    │
        │                    │                    │                    │
        ├─────── PING ──────▶│                    │                    │
        ├─────── PING ───────┼───────────────────▶│                    │
        ├─────── PING ───────┼────────────────────┼───────────────────▶│
        │                    │                    │                    │
        │◀────── PONG ───────│                    │     (timeout)      │
        │                    │              (500ms)│                    │
        │    [cancel B, C probes]                 │                    │
        │                    │                    │                    │
        │───── JOIN_REQ ────▶│                    │                    │
        │◀──── JOIN_ACK ─────│                    │                    │
        │                    │                    │                    │
        │   [SWIM gossip begins]                  │                    │
        │◀───── GOSSIP ──────│                    │                    │
        │                    │                    │                    │
     JOINED               ACTIVE              DEAD                  ACTIVE
```

#### Sequence Diagram: All Seeds Down, Retry with Backoff

```
    New Node              Seed A (down)      Seed B (down)      Seed C (down)
        │                      │                  │                  │
        │──── resolve() ──────▶│                  │                  │
        │◀─── [A, B, C] ───────│                  │                  │
        │                      │                  │                  │
        ├─────── PING ────────▶│                  │                  │
        ├─────── PING ─────────┼─────────────────▶│                  │
        ├─────── PING ─────────┼──────────────────┼─────────────────▶│
        │                      │                  │                  │
        │                (500ms timeout)    (500ms timeout)   (500ms timeout)
        │                      │                  │                  │
        │   [all probes failed]│                  │                  │
        │                      │                  │                  │
        │   [backoff: 500ms]   │                  │                  │
        │        ...           │                  │                  │
        │                      │                  │                  │
        │──── resolve() ──────▶│                  │                  │
        │◀─── [A, B, C] ───────│ (A comes back up)│                  │
        │                      │                  │                  │
        ├─────── PING ────────▶│                  │                  │
        │◀────── PONG ─────────│                  │                  │
        │                      │                  │                  │
        │───── JOIN_REQ ──────▶│                  │                  │
        │◀──── JOIN_ACK ───────│                  │                  │
        │                      │                  │                  │
     JOINED                 ACTIVE             DOWN                DOWN
```

### DNS Resolution

#### Architecture

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                           DNS RESOLVER                                       │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  ┌─────────────────┐                                                        │
│  │  DNSConfig      │                                                        │
│  │                 │                                                        │
│  │  - name: str    │     ┌─────────────────────────────────────────┐       │
│  │  - port: int    │────▶│            AsyncDNSResolver              │       │
│  │  - timeout: 2.0 │     │                                          │       │
│  │  - cache_ttl: 5 │     │  ┌──────────────────────────────────┐   │       │
│  └─────────────────┘     │  │       Resolution Cache           │   │       │
│                          │  │                                   │   │       │
│                          │  │  name → (addresses, expiry_time) │   │       │
│                          │  └──────────────────────────────────┘   │       │
│                          │                                          │       │
│                          │  resolve(name) → list[PeerAddress]      │       │
│                          │                                          │       │
│                          │  Uses asyncio.get_event_loop()          │       │
│                          │       .getaddrinfo() for non-blocking   │       │
│                          └─────────────────────────────────────────┘       │
│                                                                              │
│  Resolution Flow:                                                           │
│  ┌────────┐    ┌─────────┐    ┌─────────┐    ┌──────────────┐             │
│  │ Check  │───▶│ Cache   │───▶│ Return  │    │              │             │
│  │ Cache  │    │ Valid?  │yes │ Cached  │    │   Resolve    │             │
│  └────────┘    └────┬────┘    └─────────┘    │   via DNS    │             │
│                     │ no                      │              │             │
│                     └────────────────────────▶│ getaddrinfo  │             │
│                                               └──────┬───────┘             │
│                                                      │                      │
│                                               ┌──────▼───────┐             │
│                                               │ Update Cache │             │
│                                               │ + Return     │             │
│                                               └──────────────┘             │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

#### DNS TTL Considerations

```
Problem: DNS caching returns stale IPs for crashed pods

┌──────────────────────────────────────────────────────────────────────────┐
│                                                                           │
│  Time    DNS Response         Actual Cluster       Issue                 │
│  ────    ────────────         ──────────────       ─────                 │
│  t=0     [A, B, C]            [A, B, C]            None                  │
│  t=1     [A, B, C]            [A, B, C']           C crashed, C' started │
│  t=2     [A, B, C] (cached)   [A, B, C']           Stale C in DNS        │
│  t=3     [A, B, C'] (updated) [A, B, C']           Resolved              │
│                                                                           │
└──────────────────────────────────────────────────────────────────────────┘

Solution: Parallel probing with short timeouts

- Probe ALL resolved addresses simultaneously
- Use 500ms timeout (not TCP default 30s)
- Dead IPs timeout while live ones respond
- First responder wins, cancel the rest
- Stale DNS entries cause 500ms delay, not blocking failure
```

### Peer Probing

#### Parallel Probe Strategy

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                        PARALLEL PROBE EXECUTION                              │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  Input: candidates = [(10.0.1.5, 9000), (10.0.1.6, 9000), (10.0.1.7, 9000)] │
│  Timeout: 500ms per probe                                                   │
│  Max concurrent: 10 (configurable)                                          │
│                                                                              │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │                                                                      │   │
│  │  t=0ms    ┌──────┐   ┌──────┐   ┌──────┐                           │   │
│  │           │Probe │   │Probe │   │Probe │   All start simultaneously│   │
│  │           │ :5   │   │ :6   │   │ :7   │                           │   │
│  │           └──┬───┘   └──┬───┘   └──┬───┘                           │   │
│  │              │          │          │                                │   │
│  │  t=50ms      │          │          │                                │   │
│  │              ▼          │          │                                │   │
│  │           ┌──────┐      │          │   :5 responds first!          │   │
│  │           │ PONG │      │          │                                │   │
│  │           └──────┘      │          │                                │   │
│  │              │          │          │                                │   │
│  │              │     ┌────┴────┐ ┌───┴───┐                           │   │
│  │              │     │ CANCEL  │ │CANCEL │   Cancel remaining probes │   │
│  │              │     └─────────┘ └───────┘                           │   │
│  │              │                                                      │   │
│  │              ▼                                                      │   │
│  │        Return (10.0.1.5, 9000)                                     │   │
│  │                                                                      │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                              │
│  Worst case (all dead):                                                     │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │                                                                      │   │
│  │  t=0ms    ┌──────┐   ┌──────┐   ┌──────┐                           │   │
│  │           │Probe │   │Probe │   │Probe │                           │   │
│  │           │ :5   │   │ :6   │   │ :7   │                           │   │
│  │           └──┬───┘   └──┬───┘   └──┬───┘                           │   │
│  │              │          │          │                                │   │
│  │  t=500ms     ▼          ▼          ▼    All timeout together       │   │
│  │          TIMEOUT    TIMEOUT    TIMEOUT                              │   │
│  │                                                                      │   │
│  │        Return None (trigger backoff + retry)                        │   │
│  │                                                                      │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

#### Probe Protocol

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                           PROBE WIRE PROTOCOL                                │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  Request (PING):                                                            │
│  ┌────────────────────────────────────────────────────────────────────┐    │
│  │  0                   1                   2                   3     │    │
│  │  0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1  │    │
│  │ +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+  │    │
│  │ |     'P'       |     'I'       |     'N'       |     'G'       |  │    │
│  │ +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+  │    │
│  └────────────────────────────────────────────────────────────────────┘    │
│                                                                              │
│  Response (PONG):                                                           │
│  ┌────────────────────────────────────────────────────────────────────┐    │
│  │  0                   1                   2                   3     │    │
│  │  0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1  │    │
│  │ +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+  │    │
│  │ |     'P'       |     'O'       |     'N'       |     'G'       |  │    │
│  │ +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+  │    │
│  └────────────────────────────────────────────────────────────────────┘    │
│                                                                              │
│  Simple 4-byte exchange:                                                    │
│  - Fast to send/receive                                                     │
│  - Easy to validate                                                         │
│  - No serialization overhead                                                │
│  - Works with any TCP implementation                                        │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Health-Aware Peer Cache

To accelerate subsequent bootstrap attempts (e.g., after network blip), we cache recently-responsive peers:

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                         HEALTH-AWARE PEER CACHE                              │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │                        PeerHealthCache                               │   │
│  │                                                                      │   │
│  │  ┌────────────────────────────────────────────────────────────┐    │   │
│  │  │  (host, port)  │  last_seen   │  success_count  │  state   │    │   │
│  │  ├────────────────┼──────────────┼─────────────────┼──────────┤    │   │
│  │  │  10.0.1.5:9000 │  1704067200  │       47        │  HEALTHY │    │   │
│  │  │  10.0.1.6:9000 │  1704067180  │       12        │  HEALTHY │    │   │
│  │  │  10.0.1.7:9000 │  1704066000  │        0        │  EXPIRED │    │   │
│  │  └────────────────┴──────────────┴─────────────────┴──────────┘    │   │
│  │                                                                      │   │
│  │  Methods:                                                            │   │
│  │  - record_success(addr): Update last_seen, increment count          │   │
│  │  - record_failure(addr): Decrement count, mark stale if zero        │   │
│  │  - get_healthy_peers(): Return peers seen within TTL                │   │
│  │  - evict_expired(): Remove entries older than cache_ttl             │   │
│  │                                                                      │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                              │
│  Usage in Candidate Aggregation:                                            │
│                                                                              │
│     1. Get candidates from DNS/seeds                                        │
│     2. Get healthy peers from cache                                         │
│     3. Prioritize: cached healthy → DNS/seeds → all others                 │
│     4. Probe in priority order (still parallel, but start with likely-live) │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Failure Scenarios

#### Scenario Matrix

| Scenario | Behavior | Recovery Time |
|----------|----------|---------------|
| 1 of N seeds down | Parallel probe, others respond | < 100ms |
| All seeds down temporarily | Backoff + retry until one recovers | backoff intervals |
| DNS returns stale IPs | Stale IPs timeout, live ones respond | + 500ms worst case |
| Network partition (split brain) | Nodes join different partitions | Requires SWIM partition healing |
| Total cluster failure | Retry indefinitely with backoff | Until first node recovers |
| DNS completely unavailable | Fall back to static seeds | Immediate if seeds configured |

#### Backoff Strategy

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                        EXPONENTIAL BACKOFF                                   │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  Attempt    Base Delay    Jitter (0-25%)    Actual Delay    Cumulative     │
│  ───────    ──────────    ──────────────    ────────────    ──────────     │
│     1         500ms          0-125ms        500-625ms        ~560ms        │
│     2        1000ms          0-250ms       1000-1250ms       ~1.7s         │
│     3        2000ms          0-500ms       2000-2500ms       ~3.9s         │
│     4        4000ms         0-1000ms       4000-5000ms       ~8.4s         │
│     5        8000ms         0-2000ms       8000-10000ms     ~17.4s         │
│     6       15000ms         0-3750ms      15000-18750ms     ~34.3s         │
│    ...        ...             ...             ...             ...           │
│    N       15000ms (cap)   0-3750ms      15000-18750ms       ...           │
│                                                                              │
│  Configuration:                                                             │
│  - initial_backoff: 500ms                                                   │
│  - max_backoff: 15000ms (15 seconds)                                        │
│  - backoff_multiplier: 2.0                                                  │
│  - jitter_factor: 0.25 (25% randomization)                                  │
│                                                                              │
│  Why jitter?                                                                │
│  - Prevents thundering herd when multiple nodes retry simultaneously        │
│  - Spreads load on recovering seeds                                         │
│  - Reduces contention during cluster-wide restarts                          │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Configuration

#### BootstrapConfig

```python
@dataclass(slots=True)
class BootstrapConfig:
    """Configuration for cluster bootstrap."""
    
    # Static seed addresses (tried first)
    seeds: list[str] = field(default_factory=list)
    
    # DNS name for dynamic discovery (optional, supplements seeds)
    dns_name: str | None = None
    
    # Default port when not specified in address
    default_port: int = 9000
    
    # Probe timeout per candidate (short to enable fast failure detection)
    probe_timeout: float = 0.5  # 500ms
    
    # Maximum concurrent probes (prevent socket exhaustion)
    max_concurrent_probes: int = 10
    
    # Backoff configuration
    initial_backoff: float = 0.5      # 500ms
    max_backoff: float = 15.0         # 15 seconds
    backoff_multiplier: float = 2.0
    jitter_factor: float = 0.25       # 25% randomization
    
    # DNS resolution timeout
    dns_timeout: float = 2.0
    
    # Health cache TTL (how long to remember responsive peers)
    health_cache_ttl: float = 60.0    # 1 minute
```

#### Environment-Specific Examples

```yaml
# Bare Metal / Static IPs
bootstrap:
  seeds:
    - "10.0.1.5:9000"
    - "10.0.1.6:9000"
    - "10.0.1.7:9000"

# Kubernetes (Headless Service)
bootstrap:
  dns_name: "managers.hyperscale.svc.cluster.local"
  default_port: 9000

# Hybrid (DNS primary, static fallback)
bootstrap:
  dns_name: "managers.prod.internal"
  seeds:
    - "10.0.1.5:9000"  # Fallback if DNS fails
  default_port: 9000
```

### Bootstrap Module Structure

```
hyperscale/distributed_rewrite/bootstrap/
├── __init__.py                 # Public exports
├── bootstrap.py                # Main Bootstrapper class
├── dns/
│   ├── __init__.py
│   ├── resolver.py             # AsyncDNSResolver
│   └── models/
│       ├── __init__.py
│       ├── dns_config.py       # DNSConfig dataclass
│       └── dns_result.py       # DNSResult dataclass
├── probing/
│   ├── __init__.py
│   ├── parallel_prober.py      # ParallelProber class
│   └── models/
│       ├── __init__.py
│       ├── probe_config.py     # ProbeConfig dataclass
│       └── probe_result.py     # ProbeResult dataclass
├── cache/
│   ├── __init__.py
│   ├── peer_health_cache.py    # PeerHealthCache class
│   └── models/
│       ├── __init__.py
│       └── peer_entry.py       # PeerCacheEntry dataclass
└── models/
    ├── __init__.py
    ├── bootstrap_config.py     # BootstrapConfig dataclass
    ├── bootstrap_result.py     # BootstrapResult dataclass
    ├── bootstrap_state.py      # BootstrapState enum
    └── peer_address.py         # PeerAddress dataclass
```

### Example Implementations

#### Integration with ManagerServer

```python
class ManagerServer(HealthAwareServer):
    def __init__(
        self,
        host: str,
        tcp_port: int,
        udp_port: int,
        env: Env,
        dc_id: str = "default",
        # New: Bootstrap configuration (replaces seed_managers)
        bootstrap_config: BootstrapConfig | None = None,
        # Legacy: Still supported for backwards compatibility
        seed_managers: list[tuple[str, int]] | None = None,
        ...
    ):
        ...
        
        # Initialize bootstrapper
        if bootstrap_config:
            self._bootstrapper = Bootstrapper(bootstrap_config)
        elif seed_managers:
            # Legacy: Convert seed_managers to BootstrapConfig
            self._bootstrapper = Bootstrapper(
                BootstrapConfig(
                    seeds=[f"{host}:{port}" for host, port in seed_managers]
                )
            )
        else:
            self._bootstrapper = None
    
    async def start(self) -> None:
        await self.start_server(init_context=self.env.get_swim_init_context())
        
        # Bootstrap: discover peers before joining cluster
        if self._bootstrapper:
            bootstrap_result = await self._bootstrapper.bootstrap()
            
            if bootstrap_result.success:
                # Join cluster via discovered peer
                await self.join_cluster(bootstrap_result.peer.to_udp_addr())
                
                # Register with the peer to get full cluster topology
                await self._register_with_peer(bootstrap_result.peer.to_tcp_addr())
        
        # Continue with normal startup...
        await self._task_runner.run(self.start_probe_cycle)
        ...
```

#### Integration with WorkerServer

```python
class WorkerServer(HealthAwareServer):
    def __init__(
        self,
        host: str,
        tcp_port: int,
        udp_port: int,
        env: Env,
        dc_id: str = "default",
        # New: Bootstrap configuration
        bootstrap_config: BootstrapConfig | None = None,
        # Legacy: Still supported
        seed_managers: list[tuple[str, int]] | None = None,
    ):
        ...
        
        # Workers bootstrap to find managers
        if bootstrap_config:
            self._bootstrapper = Bootstrapper(bootstrap_config)
        elif seed_managers:
            self._bootstrapper = Bootstrapper(
                BootstrapConfig(
                    seeds=[f"{host}:{port}" for host, port in seed_managers]
                )
            )
        else:
            self._bootstrapper = None
    
    async def start(self, timeout: float | None = None) -> None:
        await self.start_server(init_context=self.env.get_swim_init_context())
        
        # Bootstrap: find at least one manager
        if self._bootstrapper:
            result = await self._bootstrapper.bootstrap()
            
            if result.success:
                # Register with discovered manager
                success = await self._register_with_manager(result.peer.to_tcp_addr())
                
                if success:
                    # Manager returns full topology in registration response
                    # _known_managers populated by _register_with_manager
                    pass
            else:
                raise RuntimeError(f"Failed to bootstrap: {result.error}")
        
        # Join SWIM cluster with all known managers
        for manager in self._known_managers.values():
            await self.join_cluster((manager.udp_host, manager.udp_port))
        
        # Continue with normal startup...
```

#### Integration with GateServer

```python
class GateServer(HealthAwareServer):
    def __init__(
        self,
        host: str,
        tcp_port: int,
        udp_port: int,
        env: Env,
        dc_id: str = "global",
        # New: Per-role bootstrap configs
        gate_bootstrap: BootstrapConfig | None = None,
        manager_bootstrap: dict[str, BootstrapConfig] | None = None,  # dc_id -> config
        # Legacy
        gate_peers: list[tuple[str, int]] | None = None,
        datacenter_managers: dict[str, list[tuple[str, int]]] | None = None,
        ...
    ):
        ...
        
        # Gate peer discovery
        if gate_bootstrap:
            self._gate_bootstrapper = Bootstrapper(gate_bootstrap)
        elif gate_peers:
            self._gate_bootstrapper = Bootstrapper(
                BootstrapConfig(
                    seeds=[f"{h}:{p}" for h, p in gate_peers]
                )
            )
        else:
            self._gate_bootstrapper = None
        
        # Per-datacenter manager discovery
        self._dc_bootstrappers: dict[str, Bootstrapper] = {}
        if manager_bootstrap:
            for dc_id, config in manager_bootstrap.items():
                self._dc_bootstrappers[dc_id] = Bootstrapper(config)
        elif datacenter_managers:
            for dc_id, addrs in datacenter_managers.items():
                self._dc_bootstrappers[dc_id] = Bootstrapper(
                    BootstrapConfig(
                        seeds=[f"{h}:{p}" for h, p in addrs]
                    )
                )
    
    async def start(self) -> None:
        await self.start_server(init_context=self.env.get_swim_init_context())
        
        # Bootstrap gate cluster
        if self._gate_bootstrapper:
            result = await self._gate_bootstrapper.bootstrap()
            if result.success:
                await self.join_cluster(result.peer.to_udp_addr())
        
        # Bootstrap per-datacenter manager connections
        for dc_id, bootstrapper in self._dc_bootstrappers.items():
            result = await bootstrapper.bootstrap()
            if result.success:
                # Store discovered manager for this DC
                self._dc_primary_managers[dc_id] = result.peer.to_tcp_addr()
        
        # Continue with normal startup...
```

#### Bootstrapper Core Implementation

```python
class Bootstrapper:
    """
    Discovers and connects to cluster peers.
    
    Combines DNS resolution, static seeds, and health caching
    to find live peers quickly. Uses parallel probing with short
    timeouts for fast convergence even when some candidates are dead.
    """
    
    def __init__(self, config: BootstrapConfig):
        self._config = config
        self._dns_resolver = AsyncDNSResolver(
            timeout=config.dns_timeout,
            cache_ttl=config.health_cache_ttl,
        )
        self._prober = ParallelProber(
            timeout=config.probe_timeout,
            max_concurrent=config.max_concurrent_probes,
        )
        self._health_cache = PeerHealthCache(ttl=config.health_cache_ttl)
        self._state = BootstrapState.INITIAL
    
    async def bootstrap(self) -> BootstrapResult:
        """
        Discover and connect to a live peer.
        
        Returns BootstrapResult with the first responsive peer,
        or an error if all candidates fail after retries.
        """
        backoff = self._config.initial_backoff
        
        while True:
            self._state = BootstrapState.RESOLVING
            candidates = await self._resolve_candidates()
            
            if not candidates:
                self._state = BootstrapState.BACKOFF
                await self._sleep_with_jitter(backoff)
                backoff = min(backoff * self._config.backoff_multiplier, 
                             self._config.max_backoff)
                continue
            
            self._state = BootstrapState.PROBING
            result = await self._prober.probe_first_success(candidates)
            
            if result.success:
                self._state = BootstrapState.JOINED
                self._health_cache.record_success(result.peer)
                return BootstrapResult(success=True, peer=result.peer)
            
            # All probes failed - backoff and retry
            self._state = BootstrapState.BACKOFF
            await self._sleep_with_jitter(backoff)
            backoff = min(backoff * self._config.backoff_multiplier,
                         self._config.max_backoff)
    
    async def _resolve_candidates(self) -> list[PeerAddress]:
        """Aggregate candidates from all sources."""
        candidates: list[PeerAddress] = []
        seen: set[tuple[str, int]] = set()
        
        # Priority 1: Recently healthy peers from cache
        for peer in self._health_cache.get_healthy_peers():
            key = (peer.host, peer.port)
            if key not in seen:
                candidates.append(peer)
                seen.add(key)
        
        # Priority 2: Static seeds
        for seed in self._config.seeds:
            peer = PeerAddress.parse(seed, self._config.default_port)
            key = (peer.host, peer.port)
            if key not in seen:
                candidates.append(peer)
                seen.add(key)
        
        # Priority 3: DNS resolution
        if self._config.dns_name:
            dns_peers = await self._dns_resolver.resolve(
                self._config.dns_name,
                self._config.default_port,
            )
            for peer in dns_peers:
                key = (peer.host, peer.port)
                if key not in seen:
                    candidates.append(peer)
                    seen.add(key)
        
        return candidates
    
    async def _sleep_with_jitter(self, base_delay: float) -> None:
        """Sleep with randomized jitter to prevent thundering herd."""
        jitter = base_delay * self._config.jitter_factor * random.random()
        await asyncio.sleep(base_delay + jitter)
```

---

### AD-33: Federated Health Monitoring for Cross-DC Coordination

**Problem**: Gates need to monitor health of remote datacenter manager clusters to make routing decisions. The existing SWIM protocol is designed for intra-cluster membership with low-latency assumptions (1-10ms RTT), but cross-DC links have high latency (50-300ms RTT) and don't need full membership semantics.

**Solution**: FederatedHealthMonitor - a separate health monitoring layer that uses SWIM-style probe/ack but without gossip or membership.

---

## Part 1: Architecture Overview

```
┌─────────────────────────────────────────────────────────────────┐
│                         GATE CLUSTER                            │
│  ┌─────────┐    ┌─────────┐    ┌─────────┐                     │
│  │  Gate   │←──→│  Gate   │←──→│  Gate   │  ← SWIM membership  │
│  │(leader) │    │         │    │         │    between gates    │
│  └────┬────┘    └─────────┘    └─────────┘                     │
│       │                                                         │
│       │ FederatedHealthMonitor                                  │
│       │ (xprobe/xack)                                           │
│       ▼                                                         │
├─────────────────────────────────────────────────────────────────┤
│       │              │              │                           │
│  ┌────┴────┐    ┌────┴────┐    ┌────┴────┐                     │
│  │ DC-East │    │ DC-West │    │DC-Europe│   ← Remote DCs      │
│  │ Leader  │    │ Leader  │    │ Leader  │                     │
│  └─────────┘    └─────────┘    └─────────┘                     │
│       ↑              ↑              ↑                           │
│       │              │              │                           │
│     SWIM           SWIM           SWIM       ← Each DC has its  │
│   (managers)     (managers)     (managers)    own SWIM cluster  │
└─────────────────────────────────────────────────────────────────┘
```

**Key Distinction**: FederatedHealthMonitor is NOT cluster membership - it's health monitoring using probe/ack.

---

## Part 2: Comparison with SWIM

| Aspect | SWIM (Intra-cluster) | FederatedHealthMonitor (Cross-cluster) |
|--------|---------------------|---------------------------------------|
| **Scope** | Nodes within single DC cluster | Gates → DC leader managers across DCs |
| **Protocol** | Full SWIM (ping, ping-req, suspect, dead) | Simple probe/ack only (`xprobe`/`xack`) |
| **Gossip** | Yes - membership and state propagation | No - just health checking |
| **Latency tolerance** | Low (local network, 1-10ms) | High (global network, 50-300ms) |
| **Suspicion timeout** | Short (1.5-8 seconds) | Long (30 seconds default) |
| **Purpose** | Cluster membership and failure detection | Cross-DC routing decisions |
| **Incarnation** | Shared cluster incarnation | Separate external incarnation per DC |

---

## Part 3: Protocol Messages

**CrossClusterProbe (xprobe)**: Sent from gates to DC leader managers.

```python
@dataclass(slots=True)
class CrossClusterProbe(Message):
    source_cluster_id: str  # Gate cluster ID
    source_node_id: str     # Sending gate's node ID
    source_addr: tuple[str, int]  # For response routing
```

**CrossClusterAck (xack)**: Response from DC leader with aggregate health.

```python
@dataclass(slots=True)
class CrossClusterAck(Message):
    # Identity
    datacenter: str
    node_id: str
    incarnation: int  # External incarnation (separate from SWIM)

    # Leadership
    is_leader: bool
    leader_term: int

    # Cluster health (aggregate)
    cluster_size: int       # Total managers in DC
    healthy_managers: int   # Managers responding to SWIM

    # Worker capacity
    worker_count: int
    healthy_workers: int
    total_cores: int
    available_cores: int

    # Workload
    active_jobs: int
    active_workflows: int

    # Self-reported health
    dc_health: str  # "HEALTHY", "DEGRADED", "BUSY", "UNHEALTHY"
    health_reason: str = ""
```

---

## Part 4: State Machine

**DCReachability States**:

```
                    ┌─────────────┐
                    │ UNREACHABLE │ ◄── Initial state
                    └──────┬──────┘
                           │ First successful ack
                           ▼
                    ┌─────────────┐
         ┌─────────►│  REACHABLE  │◄──────────────┐
         │          └──────┬──────┘               │
         │                 │ consecutive_failures │
         │                 │ >= max_failures      │
         │                 ▼                      │
         │          ┌─────────────┐               │
         │          │  SUSPECTED  │───────────────┘
         │          └──────┬──────┘  ack received
         │                 │ suspicion_timeout
         │                 │ expired
         │                 ▼
         │          ┌─────────────┐
         └──────────│ UNREACHABLE │
    leader change   └─────────────┘
```

---

## Part 5: Configuration

**Environment Variables (env.py)**:

```python
# Federated Health Monitor Settings (Gate -> DC Leader probing)
# Tuned for high-latency, globally distributed links
FEDERATED_PROBE_INTERVAL: StrictFloat = 2.0      # Seconds between probes to each DC
FEDERATED_PROBE_TIMEOUT: StrictFloat = 5.0       # Timeout for single probe (high for cross-DC)
FEDERATED_SUSPICION_TIMEOUT: StrictFloat = 30.0  # Time before suspected -> unreachable
FEDERATED_MAX_CONSECUTIVE_FAILURES: StrictInt = 5  # Failures before marking suspected
```

**Timing Rationale**:

| Setting | Value | Rationale |
|---------|-------|-----------|
| `FEDERATED_PROBE_INTERVAL` | 2s | Reduce cross-DC traffic while maintaining freshness |
| `FEDERATED_PROBE_TIMEOUT` | 5s | Accommodate 100-300ms RTT + processing time |
| `FEDERATED_SUSPICION_TIMEOUT` | 30s | Tolerate transient network issues |
| `FEDERATED_MAX_CONSECUTIVE_FAILURES` | 5 | ~10 seconds of failures before suspected |

---

## Part 6: Integration with Cross-DC Correlation

FederatedHealthMonitor feeds into the Cross-DC Correlation system (Phase 7) to prevent cascade evictions:

```python
# Latency callback for correlation detection
def _on_dc_latency(self, datacenter: str, latency_ms: float) -> None:
    """Called with RTT for each successful probe."""
    # Used by CrossDCCorrelationDetector to identify network issues
    # High latency across multiple DCs suggests network problem, not DC failure
    self._correlation_detector.record_latency(datacenter, latency_ms)

# Health change callback
def _on_dc_health_change(self, datacenter: str, new_health: str) -> None:
    """Called when DC reachability or health changes."""
    if new_health in ("SUSPECTED", "UNREACHABLE"):
        # Check if multiple DCs failing simultaneously = network partition
        correlation = self._correlation_detector.check_correlation()
        if correlation.level >= CorrelationLevel.MEDIUM:
            # Delay eviction - likely network issue, not actual DC failures
            pass
```

---

## Part 7: Usage in Gate

```python
class Gate:
    def __init__(self, ...):
        # SWIM for gate-to-gate membership
        self._swim_server = HealthAwareServer(...)

        # FederatedHealthMonitor for cross-DC health
        fed_config = env.get_federated_health_config()
        self._dc_health_monitor = FederatedHealthMonitor(
            probe_interval=fed_config['probe_interval'],
            probe_timeout=fed_config['probe_timeout'],
            suspicion_timeout=fed_config['suspicion_timeout'],
            max_consecutive_failures=fed_config['max_consecutive_failures'],
        )

    async def _route_job(self, job: Job) -> str:
        """Route job to best DC."""
        healthy_dcs = self._dc_health_monitor.get_healthy_datacenters()
        if not healthy_dcs:
            raise NoHealthyDatacentersError()

        # Select based on capacity from xack
        return self._select_best_dc(healthy_dcs)
```

---

## Part 8: Key Design Decisions

1. **No Gossip**: Cross-DC gossip would add latency and complexity. DC leaders already have aggregate health from their local SWIM cluster.

2. **Separate Incarnation**: Each DC tracks its own external incarnation, independent of internal SWIM incarnations. This prevents cross-cluster incarnation conflicts.

3. **Aggregate Health**: DC leaders report aggregate cluster health (healthy managers, available cores) rather than individual node states. This reduces message size and provides the information gates actually need.

4. **Leader-Only Probing**: Gates probe DC leaders, not all managers. Leaders have authoritative cluster state and can respond with aggregate health.

5. **High Latency Tolerance**: Default timeouts (5s probe, 30s suspicion) are 5-10x higher than SWIM defaults, appropriate for global networks.

---

## Part 9: Files

| File | Purpose |
|------|---------|
| `swim/health/federated_health_monitor.py` | FederatedHealthMonitor, CrossClusterProbe, CrossClusterAck |
| `nodes/gate.py` | Integration with gate routing |
| `env/env.py` | Configuration settings |
| `datacenters/cross_dc_correlation.py` | Integration with correlation detection |

---

---

# AD-33: Workflow State Machine for Complete Lifecycle Management

## Overview

A comprehensive state machine that governs the **entire workflow lifecycle**, from initial queuing through completion, failure, cancellation, and retry. This replaces ad-hoc status checks with a formal state machine that enforces valid transitions, prevents race conditions, and provides clear semantics for all workflow operations.

**Problem**: Current workflow status management is fragmented:
- Status stored in multiple places (`WorkflowProgress.status`, `sub_workflows`, pending queues)
- No validation of state transitions (can accidentally dispatch a failed workflow)
- Race conditions during worker failure (can retry before dependents cancelled)
- Unclear semantics (is workflow "failed and waiting" or "failed and ready to retry"?)
- Difficult debugging (no state history, hard to trace what happened)

**Solution**: Single state machine that:
- ✅ Enforces valid state transitions
- ✅ Prevents all race conditions
- ✅ Provides clear semantics for every operation
- ✅ Tracks state history for debugging
- ✅ Guarantees idempotency
- ✅ Works with WorkflowDispatcher's dependency-aware dispatch

---

## Part 1: Complete State Diagram

```
                    ┌──────────────────────────────────────┐
                    │                                      │
                    ▼                                      │
              ┌─────────┐                                  │
         ┌───►│ PENDING │◄──────────────────┐             │
         │    └─────────┘                    │             │
         │         │                         │             │
         │         │ dispatch               │             │
         │         ▼                         │             │
         │    ┌──────────┐                  │             │
         │    │DISPATCHED│                  │             │
         │    └──────────┘                  │             │
         │         │                         │             │
         │         │ worker ack              │             │
         │         ▼                         │             │
         │    ┌─────────┐                   │             │
         │    │ RUNNING │                   │             │
         │    └─────────┘                   │             │
         │         │                         │             │
         │         ├──success────────────────┼────────────►│ COMPLETED
         │         │                         │             │  (terminal)
         │         ├──timeout/error──────────┼────────────►│ FAILED
         │         │                         │             │  (terminal if max retries)
         │         └──cancel request─────────┼────────────►│ CANCELLED
         │                                    │             │  (terminal)
         │                                    │
         │                                    │
   retry │    ┌────────────────┐             │
   after │    │     FAILED     │             │
   deps  │    └────────────────┘             │
  cancel │           │                        │
         │           │ find dependents        │
         │           ▼                        │
         │    ┌────────────────┐             │
         │    │FAILED_CANCELING│─────────────┤ (cancel dependents)
         │    │   _DEPENDENTS  │             │
         │    └────────────────┘             │
         │           │                        │
         │           │ dependents cancelled   │
         │           ▼                        │
         │    ┌────────────────┐             │
         └────┤ FAILED_READY   │             │
              │  _FOR_RETRY    │             │
              └────────────────┘             │
                                              │
              ┌──────────────┐               │
              │  CANCELLING  │───────────────┤ (cancel request)
              └──────────────┘               │
                     │                        │
                     └────────────────────────┘ CANCELLED
```

---

## Part 2: State Definitions

### Normal Execution Path

| State | Description | Valid Transitions | Duration |
|-------|-------------|-------------------|----------|
| **PENDING** | In WorkflowDispatcher queue, waiting for worker with capacity | DISPATCHED, CANCELLING, FAILED | Seconds to minutes (depends on queue depth) |
| **DISPATCHED** | Dispatch message sent to worker, awaiting acknowledgment | RUNNING, CANCELLING, FAILED | Milliseconds (network RTT) |
| **RUNNING** | Worker executing workflow | COMPLETED, FAILED, CANCELLING | Seconds to minutes (workflow duration) |
| **COMPLETED** | Workflow finished successfully | *(none - terminal)* | Forever (until job cleanup) |

### Failure & Retry Path

| State | Description | Valid Transitions | Duration |
|-------|-------------|-------------------|----------|
| **FAILED** | Worker died, timeout, or execution error | FAILED_CANCELING_DEPENDENTS, CANCELLED | Milliseconds (transition is fast) |
| **FAILED_CANCELING_DEPENDENTS** | Cancelling workflows that depend on this failed workflow | FAILED_READY_FOR_RETRY | Seconds (depends on # of dependents) |
| **FAILED_READY_FOR_RETRY** | All dependents cancelled, safe to retry | PENDING | Milliseconds (re-queued immediately) |

**Rationale for Three-State Failure Path**:
1. **FAILED**: Immediate transition when failure detected. Prevents dispatch while we cancel dependents.
2. **FAILED_CANCELING_DEPENDENTS**: Explicit state while cancelling dependents. Prevents retry before dependents cleared.
3. **FAILED_READY_FOR_RETRY**: Explicit "ready" state. State machine enforces we can only reach PENDING from here.

### Cancellation Path

| State | Description | Valid Transitions | Duration |
|-------|-------------|-------------------|----------|
| **CANCELLING** | Cancel request sent, awaiting worker confirmation | CANCELLED | Milliseconds to seconds (worker response time) |
| **CANCELLED** | Cancellation confirmed | *(none - terminal)* | Forever (until job cleanup) |

### Additional States

| State | Description | Valid Transitions | Duration |
|-------|-------------|-------------------|----------|
| **AGGREGATED** | Results aggregated (multi-core workflows only) | *(none - terminal)* | Forever (until job cleanup) |

---

## Part 3: Valid State Transitions

```python
class WorkflowState(Enum):
    """
    Complete workflow lifecycle states (AD-33).
    
    State machine ensures workflows can only transition through valid paths,
    preventing race conditions and maintaining system invariants.
    """
    # Normal execution path
    PENDING = "pending"
    DISPATCHED = "dispatched"
    RUNNING = "running"
    COMPLETED = "completed"
    
    # Failure & retry path
    FAILED = "failed"
    FAILED_CANCELING_DEPENDENTS = "failed_canceling_deps"
    FAILED_READY_FOR_RETRY = "failed_ready"
    
    # Cancellation path
    CANCELLING = "cancelling"
    CANCELLED = "cancelled"
    
    # Additional states
    AGGREGATED = "aggregated"


VALID_TRANSITIONS: dict[WorkflowState, set[WorkflowState]] = {
    WorkflowState.PENDING: {
        WorkflowState.DISPATCHED,     # Normal: selected worker, sending dispatch
        WorkflowState.CANCELLING,     # Cancel requested before dispatch
        WorkflowState.FAILED,         # Worker died during dispatch selection
    },
    
    WorkflowState.DISPATCHED: {
        WorkflowState.RUNNING,        # Worker acked, started execution
        WorkflowState.CANCELLING,     # Cancel requested after dispatch
        WorkflowState.FAILED,         # Worker died before ack
    },
    
    WorkflowState.RUNNING: {
        WorkflowState.COMPLETED,      # Execution succeeded
        WorkflowState.FAILED,         # Worker died, timeout, or execution error
        WorkflowState.CANCELLING,     # Cancel requested during execution
        WorkflowState.AGGREGATED,     # Multi-core workflow aggregation
    },
    
    WorkflowState.FAILED: {
        WorkflowState.FAILED_CANCELING_DEPENDENTS,  # Start cancelling dependents
        WorkflowState.CANCELLED,      # Job-level cancel supersedes retry
    },
    
    WorkflowState.FAILED_CANCELING_DEPENDENTS: {
        WorkflowState.FAILED_READY_FOR_RETRY,  # All dependents cancelled
    },
    
    WorkflowState.FAILED_READY_FOR_RETRY: {
        WorkflowState.PENDING,        # Re-queued for retry
    },
    
    WorkflowState.CANCELLING: {
        WorkflowState.CANCELLED,      # Cancellation confirmed
    },
    
    # Terminal states - no outbound transitions
    WorkflowState.COMPLETED: set(),
    WorkflowState.CANCELLED: set(),
    WorkflowState.AGGREGATED: set(),
}
```

**Transition Validation**:
- Every state transition is validated before execution
- Invalid transitions are logged and rejected
- Prevents impossible states (e.g., COMPLETED → PENDING)

---

## Part 4: State Machine Implementation

```python
@dataclass
class StateTransition:
    """Record of a state transition for observability."""
    from_state: WorkflowState
    to_state: WorkflowState
    timestamp: float
    reason: str  # Why transition occurred


class WorkflowStateMachine:
    """
    Manages workflow state transitions with validation (AD-33).
    
    Ensures workflows can only transition through valid paths,
    preventing race conditions and maintaining system invariants.
    """
    
    def __init__(self):
        # Current state per workflow
        self._states: dict[str, WorkflowState] = {}
        
        # State transition history (for debugging)
        self._state_history: dict[str, list[StateTransition]] = {}
        
        # Lock for atomic state transitions
        self._lock = asyncio.Lock()
    
    async def transition(
        self,
        workflow_id: str,
        to_state: WorkflowState,
        reason: str = ""
    ) -> bool:
        """
        Attempt to transition workflow to new state.
        
        Args:
            workflow_id: Workflow to transition
            to_state: Target state
            reason: Human-readable reason for transition
        
        Returns:
            True if transition succeeded, False if invalid
        """
        async with self._lock:
            current_state = self._states.get(workflow_id, WorkflowState.PENDING)
            
            # Validate transition
            valid_next_states = VALID_TRANSITIONS.get(current_state, set())
            if to_state not in valid_next_states:
                await self._log_invalid_transition(
                    workflow_id, current_state, to_state, reason
                )
                return False
            
            # Record transition
            self._states[workflow_id] = to_state
            
            # Record in history
            if workflow_id not in self._state_history:
                self._state_history[workflow_id] = []
            
            self._state_history[workflow_id].append(StateTransition(
                from_state=current_state,
                to_state=to_state,
                timestamp=time.monotonic(),
                reason=reason
            ))
            
            await self._log_transition(workflow_id, current_state, to_state, reason)
            return True
    
    def get_state(self, workflow_id: str) -> WorkflowState:
        """Get current state of workflow."""
        return self._states.get(workflow_id, WorkflowState.PENDING)
    
    def is_in_state(self, workflow_id: str, *states: WorkflowState) -> bool:
        """Check if workflow is in any of the given states."""
        return self.get_state(workflow_id) in states
    
    def get_history(self, workflow_id: str) -> list[StateTransition]:
        """Get complete state history for debugging."""
        return self._state_history.get(workflow_id, [])
    
    def cleanup_workflow(self, workflow_id: str) -> None:
        """Remove workflow from tracking (job cleanup)."""
        self._states.pop(workflow_id, None)
        self._state_history.pop(workflow_id, None)
```

---

## Part 5: Worker Failure Handling with State Machine

### Problem Statement

When a worker fails:
1. ❌ Current: Immediately retries failed workflows
2. ❌ Doesn't cancel dependent workflows
3. ❌ Can violate dependency order
4. ❌ Race condition: dependent workflows might start before parent retries

### Solution: State-Driven Failure Recovery

```python
async def _handle_worker_failure(self, worker_node_id: str) -> None:
    """
    Handle worker becoming unavailable (AD-33 state machine).
    
    Flow:
    1. Identify workflows in RUNNING/DISPATCHED states on failed worker
    2. Transition to FAILED
    3. For each failed workflow, find ALL dependents
    4. Cancel dependents (removes from pending queue, cancels on workers)
    5. Transition FAILED → FAILED_CANCELING_DEPENDENTS
    6. Wait for dependent cancellation confirmation
    7. Transition FAILED_CANCELING_DEPENDENTS → FAILED_READY_FOR_RETRY
    8. Re-queue failed workflow + dependents in dependency order
    9. Transition FAILED_READY_FOR_RETRY → PENDING
    """
    # Step 1: Find all workflows on this worker
    failed_workflow_ids: list[tuple[str, str]] = []  # (job_id, workflow_id)
    
    for job in self._job_manager.iter_jobs():
        for sub_wf in job.sub_workflows.values():
            workflow_id = str(sub_wf.token)
            
            # Check if on failed worker and in active state
            if sub_wf.worker_id == worker_node_id:
                current_state = self._workflow_states.get_state(workflow_id)
                if current_state in {WorkflowState.DISPATCHED, WorkflowState.RUNNING}:
                    failed_workflow_ids.append((job.job_id, workflow_id))
    
    if not failed_workflow_ids:
        return
    
    await self._udp_logger.log(ServerInfo(
        message=f"Worker {worker_node_id} failed, handling {len(failed_workflow_ids)} workflows",
        node_host=self._host,
        node_port=self._tcp_port,
        node_id=self._node_id.short,
    ))
    
    # Step 2: Transition all failed workflows: (DISPATCHED|RUNNING) → FAILED
    for job_id, workflow_id in failed_workflow_ids:
        success = await self._workflow_states.transition(
            workflow_id,
            WorkflowState.FAILED,
            reason=f"worker {worker_node_id} died"
        )
        if not success:
            await self._udp_logger.log(ServerWarning(
                message=f"Failed to transition {workflow_id} to FAILED state",
                node_host=self._host,
                node_port=self._tcp_port,
                node_id=self._node_id.short,
            ))
    
    # Step 3-7: For each failed workflow, cancel dependents and prepare for retry
    all_workflows_to_retry: list[tuple[str, str]] = []  # (job_id, workflow_id)
    
    for job_id, workflow_id in failed_workflow_ids:
        # Find all workflows that depend on this one
        dependent_workflow_ids = self._find_dependent_workflows(job_id, workflow_id)
        
        # Transition: FAILED → FAILED_CANCELING_DEPENDENTS
        await self._workflow_states.transition(
            workflow_id,
            WorkflowState.FAILED_CANCELING_DEPENDENTS,
            reason=f"cancelling {len(dependent_workflow_ids)} dependents"
        )
        
        # Cancel dependent workflows
        if dependent_workflow_ids:
            await self._cancel_dependent_workflows_for_failure(
                job_id,
                dependent_workflow_ids
            )
        
        # Transition: FAILED_CANCELING_DEPENDENTS → FAILED_READY_FOR_RETRY
        await self._workflow_states.transition(
            workflow_id,
            WorkflowState.FAILED_READY_FOR_RETRY,
            reason="dependents cancelled, ready for retry"
        )
        
        # Collect for retry
        all_workflows_to_retry.append((job_id, workflow_id))
        all_workflows_to_retry.extend((job_id, dep_id) for dep_id in dependent_workflow_ids)
    
    # Step 8-9: Re-queue in dependency order
    await self._requeue_workflows_in_dependency_order(all_workflows_to_retry)


async def _cancel_dependent_workflows_for_failure(
    self,
    job_id: str,
    dependent_workflow_ids: list[str]
) -> None:
    """
    Cancel dependent workflows after parent failed.
    
    1. Remove pending dependents from WorkflowDispatcher
    2. Cancel running dependents on workers
    3. Transition dependents to CANCELLED
    """
    # Remove from pending queue
    if self._workflow_dispatcher:
        removed_pending = await self._workflow_dispatcher.cancel_pending_workflows_by_ids(
            job_id,
            dependent_workflow_ids
        )
        
        # Transition removed pending workflows to CANCELLED
        for wf_id in removed_pending:
            await self._workflow_states.transition(
                wf_id,
                WorkflowState.CANCELLED,
                reason="parent workflow failed"
            )
    
    # Cancel running dependents on workers
    job = self._job_manager.get_job_by_id(job_id)
    if not job:
        return
    
    for dep_id in dependent_workflow_ids:
        # Skip if already cancelled (was pending)
        if self._workflow_states.is_in_state(dep_id, WorkflowState.CANCELLED):
            continue
        
        # Find the sub-workflow
        sub_wf = None
        for sw in job.sub_workflows.values():
            if str(sw.token) == dep_id:
                sub_wf = sw
                break
        
        if not sub_wf:
            continue
        
        # If running on a worker, cancel it
        if sub_wf.worker_id and self._workflow_states.is_in_state(dep_id, WorkflowState.RUNNING):
            worker_addr = self._get_worker_tcp_addr(sub_wf.worker_id)
            if worker_addr:
                try:
                    # Transition to CANCELLING
                    await self._workflow_states.transition(
                        dep_id,
                        WorkflowState.CANCELLING,
                        reason="parent workflow failed"
                    )
                    
                    # Send cancel request to worker
                    cancel_req = WorkflowCancelRequest(
                        job_id=job_id,
                        workflow_id=dep_id,
                        requester_id="manager_failure_handler",
                        timestamp=time.monotonic(),
                    )
                    response, _ = await self.send_tcp(
                        worker_addr,
                        "cancel_workflow",
                        cancel_req.dump(),
                        timeout=5.0,
                    )
                    
                    # Verify cancellation
                    if isinstance(response, bytes):
                        wf_response = WorkflowCancelResponse.load(response)
                        if wf_response.success:
                            # Transition to CANCELLED
                            await self._workflow_states.transition(
                                dep_id,
                                WorkflowState.CANCELLED,
                                reason="worker confirmed cancellation"
                            )
                
                except Exception as e:
                    await self._udp_logger.log(ServerError(
                        message=f"Failed to cancel dependent workflow {dep_id}: {e}",
                        node_host=self._host,
                        node_port=self._tcp_port,
                        node_id=self._node_id.short,
                    ))


async def _requeue_workflows_in_dependency_order(
    self,
    workflows_to_retry: list[tuple[str, str]]
) -> None:
    """
    Re-queue failed workflows in dependency order.
    
    Workflows are added back to WorkflowDispatcher's pending queue,
    preserving dependency metadata. WorkflowDispatcher's existing
    dispatch loop handles dependency-aware dispatch.
    
    Args:
        workflows_to_retry: List of (job_id, workflow_id) tuples
    """
    # Group by job
    workflows_by_job: dict[str, list[str]] = {}
    for job_id, workflow_id in workflows_to_retry:
        if job_id not in workflows_by_job:
            workflows_by_job[job_id] = []
        workflows_by_job[job_id].append(workflow_id)
    
    # Process each job
    for job_id, workflow_ids in workflows_by_job.items():
        job = self._job_manager.get_job_by_id(job_id)
        if not job:
            continue
        
        # Get dependency graph for this job
        workflow_deps = self._build_dependency_graph(job)
        
        # Topological sort to get correct order
        ordered_workflows = self._topological_sort(workflow_ids, workflow_deps)
        
        # Add back to WorkflowDispatcher in dependency order
        for workflow_id in ordered_workflows:
            # Find original dispatch data
            sub_wf = None
            for sw in job.sub_workflows.values():
                if str(sw.token) == workflow_id:
                    sub_wf = sw
                    break
            
            if not sub_wf:
                continue
            
            # Get original dispatch bytes from retry tracking
            retry_info = self._workflow_retries.get(workflow_id)
            if not retry_info or not retry_info[1]:
                continue
            
            dispatch_bytes = retry_info[1]
            
            # Add to WorkflowDispatcher
            if self._workflow_dispatcher:
                await self._workflow_dispatcher.add_pending_workflow(
                    job_id=job_id,
                    workflow_id=workflow_id,
                    dispatch_bytes=dispatch_bytes,
                    dependencies=getattr(sub_wf, 'dependencies', []),
                )
            
            # Transition: FAILED_READY_FOR_RETRY → PENDING
            await self._workflow_states.transition(
                workflow_id,
                WorkflowState.PENDING,
                reason="re-queued after failure"
            )
        
        await self._udp_logger.log(ServerInfo(
            message=f"Re-queued {len(ordered_workflows)} workflows for job {job_id} in dependency order",
            node_host=self._host,
            node_port=self._tcp_port,
            node_id=self._node_id.short,
        ))


def _build_dependency_graph(self, job) -> dict[str, list[str]]:
    """Build workflow ID → dependencies map."""
    deps = {}
    for sub_wf in job.sub_workflows.values():
        workflow_id = str(sub_wf.token)
        deps[workflow_id] = getattr(sub_wf, 'dependencies', [])
    return deps


def _topological_sort(
    self,
    workflow_ids: list[str],
    deps: dict[str, list[str]]
) -> list[str]:
    """
    Topological sort of workflows to preserve dependency order.
    
    Returns workflows in order such that dependencies come before dependents.
    """
    # Build adjacency list (reverse: who depends on me)
    dependents = {wf_id: [] for wf_id in workflow_ids}
    in_degree = {wf_id: 0 for wf_id in workflow_ids}
    
    for wf_id in workflow_ids:
        for dep in deps.get(wf_id, []):
            if dep in workflow_ids:  # Only consider workflows in our set
                dependents[dep].append(wf_id)
                in_degree[wf_id] += 1
    
    # Kahn's algorithm
    queue = [wf_id for wf_id in workflow_ids if in_degree[wf_id] == 0]
    result = []
    
    while queue:
        wf_id = queue.pop(0)
        result.append(wf_id)
        
        for dependent in dependents[wf_id]:
            in_degree[dependent] -= 1
            if in_degree[dependent] == 0:
                queue.append(dependent)
    
    # If result doesn't contain all workflows, there's a cycle
    # (shouldn't happen with valid dependency graphs)
    if len(result) != len(workflow_ids):
        # Fall back to original order
        return workflow_ids
    
    return result
```

---

## Part 6: Integration with Other Operations

### Dispatch

```python
async def _dispatch_workflow_to_worker(
    self,
    workflow_id: str,
    worker_id: str,
    dispatch: WorkflowDispatch
) -> bool:
    """Dispatch workflow with state machine transitions."""
    
    # Validate we're in PENDING state
    if not self._workflow_states.is_in_state(workflow_id, WorkflowState.PENDING):
        await self._udp_logger.log(ServerError(
            message=f"Cannot dispatch {workflow_id} - not in PENDING state",
            ...
        ))
        return False
    
    # Transition: PENDING → DISPATCHED
    await self._workflow_states.transition(
        workflow_id,
        WorkflowState.DISPATCHED,
        reason=f"dispatching to worker {worker_id}"
    )
    
    try:
        # Send dispatch
        response, _ = await self.send_tcp(worker_addr, "workflow_dispatch", ...)
        
        if response and isinstance(response, bytes):
            ack = WorkflowDispatchAck.load(response)
            if ack.accepted:
                # Transition: DISPATCHED → RUNNING
                await self._workflow_states.transition(
                    workflow_id,
                    WorkflowState.RUNNING,
                    reason="worker acknowledged"
                )
                return True
        
        # Worker rejected or no response
        await self._workflow_states.transition(
            workflow_id,
            WorkflowState.FAILED,
            reason="worker rejected dispatch"
        )
        return False
    
    except Exception as e:
        # Dispatch failed
        await self._workflow_states.transition(
            workflow_id,
            WorkflowState.FAILED,
            reason=f"dispatch exception: {e}"
        )
        return False
```

### Completion

```python
async def receive_workflow_result(
    self,
    addr: tuple[str, int],
    data: bytes,
    clock_time: int
):
    """Handle workflow completion with state transition."""
    result = WorkflowFinalResult.load(data)
    
    # Validate state
    if not self._workflow_states.is_in_state(
        result.workflow_id,
        WorkflowState.RUNNING
    ):
        # Workflow not in RUNNING state - may have been cancelled
        return
    
    # Transition: RUNNING → COMPLETED
    await self._workflow_states.transition(
        result.workflow_id,
        WorkflowState.COMPLETED,
        reason="worker reported success"
    )
    
    # ... rest of completion logic ...
```

### Cancellation

```python
async def receive_cancel_job(
    self,
    addr: tuple[str, int],
    data: bytes,
    clock_time: int
):
    """Cancel job with state transitions."""
    # ... parse request, validate job ...
    
    for sub_wf in job.sub_workflows.values():
        workflow_id = str(sub_wf.token)
        current_state = self._workflow_states.get_state(workflow_id)
        
        if current_state == WorkflowState.PENDING:
            # Remove from queue directly
            if self._workflow_dispatcher:
                await self._workflow_dispatcher.cancel_pending_workflows_by_ids(
                    job_id, [workflow_id]
                )
            
            # Transition: PENDING → CANCELLED
            await self._workflow_states.transition(
                workflow_id,
                WorkflowState.CANCELLED,
                reason="job cancelled while pending"
            )
        
        elif current_state in {WorkflowState.DISPATCHED, WorkflowState.RUNNING}:
            # Transition: (DISPATCHED|RUNNING) → CANCELLING
            await self._workflow_states.transition(
                workflow_id,
                WorkflowState.CANCELLING,
                reason="job cancel request"
            )
            
            # Send cancel to worker
            # ... send WorkflowCancelRequest ...
            
            # When worker confirms:
            # Transition: CANCELLING → CANCELLED
            await self._workflow_states.transition(
                workflow_id,
                WorkflowState.CANCELLED,
                reason="worker confirmed cancellation"
            )
```

---

## Part 7: Benefits

### 1. Race Condition Prevention

**Before**:
```python
# Race: workflow might be dispatched during this check
if workflow.status == "pending":
    remove_from_queue()
    # ❌ Another thread might dispatch it here!
    mark_as_cancelled()
```

**After**:
```python
# State machine prevents invalid transitions
if self._workflow_states.is_in_state(wf_id, WorkflowState.PENDING):
    await self._workflow_states.transition(wf_id, WorkflowState.CANCELLING, ...)
    # ✅ No one can transition to DISPATCHED now - invalid transition!
    remove_from_queue()
```

### 2. Clear Failure Semantics

**Before**:
```python
# Unclear: is it safe to retry?
if workflow.status == "failed":
    retry_workflow()  # ❌ What about dependents?
```

**After**:
```python
# Can only retry from FAILED_READY_FOR_RETRY state
if self._workflow_states.is_in_state(wf_id, WorkflowState.FAILED_READY_FOR_RETRY):
    # ✅ Guaranteed that dependents are cancelled
    retry_workflow()
```

### 3. Debugging with State History

```python
# Get complete state history
history = self._workflow_states.get_history(workflow_id)

# Output:
# 0.0s:   PENDING → DISPATCHED (dispatching to worker-1)
# 0.1s:   DISPATCHED → RUNNING (worker acknowledged)
# 5.0s:   RUNNING → FAILED (worker worker-1 died)
# 5.0s:   FAILED → FAILED_CANCELING_DEPENDENTS (cancelling 3 dependents)
# 6.2s:   FAILED_CANCELING_DEPENDENTS → FAILED_READY_FOR_RETRY (dependents cancelled)
# 6.2s:   FAILED_READY_FOR_RETRY → PENDING (re-queued after failure)
# 6.5s:   PENDING → DISPATCHED (dispatching to worker-2)
# 6.6s:   DISPATCHED → RUNNING (worker acknowledged)
# 10.0s:  RUNNING → COMPLETED (worker reported success)
```

### 4. Idempotency

```python
# If worker failure handler runs twice
async def _handle_worker_failure(worker_id):
    for wf_id in workflows_on_worker:
        current = self._workflow_states.get_state(wf_id)
        
        # Check if already handled
        if current in {
            WorkflowState.FAILED,
            WorkflowState.FAILED_CANCELING_DEPENDENTS,
            WorkflowState.FAILED_READY_FOR_RETRY,
            WorkflowState.PENDING  # Already re-queued
        }:
            # ✅ Already processing or done - skip
            continue
        
        # Only process if in valid starting state
        if current in {WorkflowState.DISPATCHED, WorkflowState.RUNNING}:
            # Handle failure...
```

---

## Part 8: State Persistence

### In-Memory State

```python
class Manager:
    def __init__(self, ...):
        # State machine instance
        self._workflow_states = WorkflowStateMachine()
        
        # Other tracking...
```

### State Synchronization with WorkflowProgress

```python
# WorkflowProgress.status remains for external API compatibility
# But internally, state machine is authoritative

def _sync_workflow_status(self, workflow_id: str):
    """Sync state machine state to WorkflowProgress.status."""
    state = self._workflow_states.get_state(workflow_id)
    
    # Map state machine state to WorkflowStatus
    status_map = {
        WorkflowState.PENDING: WorkflowStatus.PENDING,
        WorkflowState.DISPATCHED: WorkflowStatus.PENDING,  # Not yet running
        WorkflowState.RUNNING: WorkflowStatus.RUNNING,
        WorkflowState.COMPLETED: WorkflowStatus.COMPLETED,
        WorkflowState.FAILED: WorkflowStatus.FAILED,
        WorkflowState.FAILED_CANCELING_DEPENDENTS: WorkflowStatus.FAILED,
        WorkflowState.FAILED_READY_FOR_RETRY: WorkflowStatus.PENDING,  # Ready to retry
        WorkflowState.CANCELLING: WorkflowStatus.CANCELLED,  # Cancelling counts as cancelled
        WorkflowState.CANCELLED: WorkflowStatus.CANCELLED,
        WorkflowState.AGGREGATED: WorkflowStatus.AGGREGATED,
    }
    
    # Update WorkflowProgress.status
    # ... sync logic ...
```

---

## Part 9: Configuration

**No new environment variables** - state machine is always enabled.

**Logging Configuration**:
```python
WORKFLOW_STATE_TRANSITION_LOG_LEVEL: str = "DEBUG"  # TRACE, DEBUG, INFO, WARNING
```

---

## Part 10: Observability

### Logging Models

```python
@dataclass
class WorkflowStateTransition(ServerDebug):
    """Logged on every state transition."""
    workflow_id: str
    job_id: str
    from_state: str
    to_state: str
    reason: str
    transition_duration_ms: float  # Time in previous state


@dataclass
class InvalidStateTransition(ServerWarning):
    """Logged when invalid transition attempted."""
    workflow_id: str
    current_state: str
    attempted_state: str
    reason: str


@dataclass
class WorkflowStateStats(ServerInfo):
    """Periodic stats about workflow states."""
    pending_count: int
    dispatched_count: int
    running_count: int
    completed_count: int
    failed_count: int
    failed_canceling_deps_count: int
    failed_ready_for_retry_count: int
    cancelling_count: int
    cancelled_count: int
```

### Metrics

Track per-state counts:
```python
workflow_state_count{state="pending"} 150
workflow_state_count{state="dispatched"} 20
workflow_state_count{state="running"} 300
workflow_state_count{state="failed"} 5
workflow_state_count{state="failed_canceling_deps"} 2
workflow_state_count{state="failed_ready_for_retry"} 0
```

Track transition counts:
```python
workflow_state_transitions_total{from="running",to="completed"} 1500
workflow_state_transitions_total{from="running",to="failed"} 10
workflow_state_transitions_total{from="failed",to="failed_canceling_deps"} 10
workflow_state_transitions_total{from="failed_ready_for_retry",to="pending"} 8
```

---

## Part 11: Files

| File | Purpose |
|------|---------|
| `distributed_rewrite/workflow/state_machine.py` | WorkflowStateMachine, WorkflowState enum, transition validation |
| `nodes/manager.py` | Integration with Manager, _handle_worker_failure rewrite |
| `jobs/workflow_dispatcher.py` | State-aware dispatch (only dispatch PENDING workflows) |
| `models/distributed.py` | StateTransition model |

---

## Part 12: Migration Strategy

**Phase 1**: Add state machine alongside existing status tracking
- State machine tracks state
- Existing `WorkflowProgress.status` still used
- Sync state machine → status after each transition

**Phase 2**: Migrate operations one at a time
- Start with dispatch (add state transitions)
- Then completion
- Then cancellation
- Then failure handling

**Phase 3**: Make state machine authoritative
- Remove direct status assignments
- Always go through state machine
- Keep `WorkflowProgress.status` for API compatibility

**Phase 4**: Cleanup
- Remove redundant status tracking
- State machine is single source of truth

---

## Summary

AD-33 introduces a **complete workflow lifecycle state machine** that:

✅ **Enforces valid transitions** - prevents impossible states  
✅ **Prevents race conditions** - atomic state changes with locking  
✅ **Clear failure semantics** - explicit states for each failure stage  
✅ **Dependency-aware retry** - workflows only retry after dependents cancelled  
✅ **Complete observability** - state history for every workflow  
✅ **Idempotent operations** - safe to call failure handler multiple times  
✅ **Works with WorkflowDispatcher** - reuses existing dependency-aware dispatch  

This is the **most robust and correct** approach to workflow lifecycle management.

---

# AD-34: Adaptive Job Timeout with Multi-DC Coordination

## Overview

Jobs need timeout protection to prevent resource leaks when workers are alive but workflows are stuck. The challenge: **the same job may execute in multiple datacenters simultaneously**, requiring coordinated timeout detection and cancellation.

AD-34 provides an **adaptive timeout architecture** that:
- Auto-detects deployment topology (single-DC vs multi-DC)
- Uses **local authority** for single-DC (manager decides)
- Uses **gate coordination** for multi-DC (gate decides globally)
- Handles leader failures, network partitions, and race conditions
- Detects both "overall timeout" and "workflows stuck but worker alive"

---

## Problem Statement

### Timeout Scenarios

1. **Overall Job Timeout**: Job exceeds `timeout_seconds` from submission
2. **Stuck Workflows**: Worker alive but workflows making no progress
3. **Multi-DC Consistency**: In multi-DC, if DC-A times out, DC-B/C should be cancelled
4. **Worker vs Workflow Failure**: Worker heartbeat OK, but workflow stuck

### Challenges

1. **Multi-DC Coordination**: How does DC-A timeout trigger cancellation in DC-B/C?
2. **Topology Flexibility**: System must work in both single-DC and multi-DC
3. **Fault Tolerance**: Leader failures, gate failures, network partitions
4. **Race Conditions**: Job completes while timeout is being declared
5. **State Recovery**: New leader must resume timeout tracking

---

## Part 1: Architecture Overview

### Deployment Topologies

```
┌─────────────────────────────────────────────────────────────────┐
│                     Single-DC Deployment                        │
└─────────────────────────────────────────────────────────────────┘

Client → Manager Leader → Workers
              ↓
         (Local Authority)
         Directly marks job
         as timed out


┌─────────────────────────────────────────────────────────────────┐
│                     Multi-DC Deployment                         │
└─────────────────────────────────────────────────────────────────┘

                    Client
                      ↓
                    Gate (Global Authority)
                      ↓
        ┌─────────────┼─────────────┐
        ↓             ↓             ↓
      DC-A          DC-B          DC-C
    Manager       Manager       Manager
    (Reports)     (Reports)     (Reports)
        ↓             ↓             ↓
    Workers       Workers       Workers

Gate receives timeout reports from each DC
Gate declares global timeout
Gate cancels job in ALL DCs
```

### Auto-Detection Pattern

**Strategy selected per-job based on JobSubmission:**

```python
if job_submission.gate_addr is not None:
    # Multi-DC: Gate submitted job
    strategy = GateCoordinatedTimeout(manager)
else:
    # Single-DC: Client submitted directly
    strategy = LocalAuthorityTimeout(manager)
```

No configuration needed! System adapts automatically.

---

## Part 2: Core Components

### Timeout Tracking State (Persistent)

```python
@dataclass
class TimeoutTrackingState:
    """
    Timeout tracking state persisted in JobInfo.

    Survives leader transfers via state sync - new leader
    inherits this state and resumes timeout tracking.
    """
    strategy_type: str  # "local_authority" | "gate_coordinated"
    gate_addr: tuple[str, int] | None  # Where to report (multi-DC only)

    # Timestamps (absolute, monotonic)
    started_at: float  # When job started (never changes)
    last_progress_at: float  # Last workflow progress
    last_report_at: float  # Last progress report to gate (multi-DC only)

    # Timeout configuration
    timeout_seconds: float
    stuck_threshold: float = 120.0  # No progress threshold (2 minutes)

    # State flags (idempotency)
    locally_timed_out: bool = False  # Manager reported timeout to gate
    globally_timed_out: bool = False  # Gate declared global timeout
    timeout_reason: str = ""

    # Fencing (prevent stale decisions)
    timeout_fence_token: int = 0  # Incremented on leader transfer
```

**Key Design Points:**

1. **Stored in JobInfo**: Survives leader failures (transferred via state sync)
2. **Absolute Timestamps**: `started_at` never changes, enables timeout calculation after leader transfer
3. **Idempotency Flags**: `locally_timed_out` prevents duplicate timeout reports
4. **Fence Tokens**: Prevent stale timeout decisions after leader transfer

### Timeout Strategy Interface

```python
class TimeoutStrategy(ABC):
    """Base timeout strategy with state recovery."""

    @abstractmethod
    async def start_tracking(
        self,
        job_id: str,
        timeout_seconds: float,
        gate_addr: tuple[str, int] | None = None
    ) -> None:
        """Start tracking on job submission."""
        pass

    @abstractmethod
    async def resume_tracking(self, job_id: str) -> None:
        """
        Resume tracking after leader transfer.

        CRITICAL: New leader calls this to continue timeout tracking.
        Reconstructs strategy state from JobInfo.timeout_tracking.
        """
        pass

    @abstractmethod
    async def report_progress(self, job_id: str, progress_type: str) -> None:
        """Record workflow progress event."""
        pass

    @abstractmethod
    async def check_timeout(self, job_id: str) -> tuple[bool, str]:
        """
        Check if job timed out.

        Returns (is_timed_out, reason).
        Idempotent - safe to call multiple times.
        """
        pass

    @abstractmethod
    async def handle_global_timeout(
        self,
        job_id: str,
        reason: str,
        fence_token: int
    ) -> bool:
        """
        Handle global timeout decision from gate.

        Returns True if accepted, False if rejected (stale).
        """
        pass
```

---

## Part 3: Strategy 1 - Local Authority (Single-DC)

### Overview

**When**: No gate involved (direct client → manager submission)
**Authority**: Manager leader has full timeout authority
**Behavior**: Manager directly marks job as timed out

### Implementation

```python
class LocalAuthorityTimeout(TimeoutStrategy):
    """
    Manager has full authority (single-DC deployment).

    Fault Tolerance:
    - State in JobInfo.timeout_tracking (survives leader transfer)
    - New leader calls resume_tracking() to continue
    - Idempotent timeout marking (won't double-timeout)
    """

    def __init__(self, manager: 'ManagerServer'):
        self._manager = manager

    async def start_tracking(
        self,
        job_id: str,
        timeout_seconds: float,
        gate_addr: tuple[str, int] | None = None
    ) -> None:
        """Initialize timeout tracking state in JobInfo."""
        job = self._manager._job_manager.get_job_by_id(job_id)
        if not job:
            return

        async with job.lock:
            now = time.monotonic()
            job.timeout_tracking = TimeoutTrackingState(
                strategy_type="local_authority",
                gate_addr=None,
                started_at=now,
                last_progress_at=now,
                last_report_at=now,
                timeout_seconds=timeout_seconds,
                timeout_fence_token=0
            )

    async def resume_tracking(self, job_id: str) -> None:
        """
        Resume after leader transfer.

        State already in JobInfo - just increment fence token.
        """
        job = self._manager._job_manager.get_job_by_id(job_id)
        if not job or not job.timeout_tracking:
            await self._manager._udp_logger.log(ServerWarning(
                message=f"Cannot resume timeout tracking for {job_id} - no state",
                node_host=self._manager._host,
                node_port=self._manager._tcp_port,
                node_id=self._manager._node_id.short,
            ))
            return

        # Increment fence token (prevents stale operations)
        async with job.lock:
            job.timeout_tracking.timeout_fence_token += 1

    async def report_progress(self, job_id: str, progress_type: str) -> None:
        """Update last_progress_at timestamp."""
        job = self._manager._job_manager.get_job_by_id(job_id)
        if not job or not job.timeout_tracking:
            return

        async with job.lock:
            job.timeout_tracking.last_progress_at = time.monotonic()

    async def check_timeout(self, job_id: str) -> tuple[bool, str]:
        """
        Check for timeout. Idempotent - safe to call repeatedly.

        Only times out once (checked via locally_timed_out flag).
        """
        job = self._manager._job_manager.get_job_by_id(job_id)
        if not job or not job.timeout_tracking:
            return False, ""

        # Idempotent: already timed out
        if job.timeout_tracking.locally_timed_out:
            return False, ""

        # Check terminal state
        if job.status in {JobStatus.COMPLETED.value, JobStatus.FAILED.value}:
            return False, ""

        now = time.monotonic()
        tracking = job.timeout_tracking

        # Check overall timeout
        elapsed = now - tracking.started_at
        if elapsed > tracking.timeout_seconds:
            async with job.lock:
                tracking.locally_timed_out = True
                tracking.timeout_reason = (
                    f"Job timeout exceeded ({elapsed:.1f}s > "
                    f"{tracking.timeout_seconds:.1f}s)"
                )

            await self._manager._timeout_job(job_id, tracking.timeout_reason)
            return True, tracking.timeout_reason

        # Check for stuck (no progress)
        time_since_progress = now - tracking.last_progress_at
        if time_since_progress > tracking.stuck_threshold:
            async with job.lock:
                tracking.locally_timed_out = True
                tracking.timeout_reason = (
                    f"Job stuck (no progress for {time_since_progress:.1f}s)"
                )

            await self._manager._timeout_job(job_id, tracking.timeout_reason)
            return True, tracking.timeout_reason

        return False, ""

    async def handle_global_timeout(
        self,
        job_id: str,
        reason: str,
        fence_token: int
    ) -> bool:
        """Not applicable for local authority."""
        return False
```

### State Diagram - Local Authority

```
Job Submitted
     ↓
TimeoutTrackingState created
  started_at = now
  locally_timed_out = False
     ↓
╔═══════════════════════════════════╗
║    Periodic Timeout Checks         ║
║    (every 30s, leader only)        ║
╚═══════════════════════════════════╝
     ↓
┌─────────────────────────────────┐
│ Check 1: Overall Timeout        │
│ elapsed > timeout_seconds?      │
└─────────────────────────────────┘
     ↓ YES                    ↓ NO
  Mark timed out           Continue
  Call _timeout_job()         ↓
                        ┌─────────────────────────────────┐
                        │ Check 2: Stuck Detection        │
                        │ (now - last_progress_at) > 120s?│
                        └─────────────────────────────────┘
                             ↓ YES              ↓ NO
                          Mark stuck         Keep tracking
                          Call _timeout_job()   ↓
                                            Resume loop

Leader Failure → New Leader → resume_tracking() → Continue from same state
```

---

## Part 4: Strategy 2 - Gate Coordinated (Multi-DC)

### Overview

**When**: Gate submitted job (`gate_addr` in JobSubmission)
**Authority**: Gate has global timeout authority
**Manager Role**: Detect local timeouts, report to gate
**Gate Role**: Collect reports from all DCs, declare global timeout, broadcast cancellation

### Implementation - Manager Side

```python
class GateCoordinatedTimeout(TimeoutStrategy):
    """
    Gate has authority (multi-DC deployment).

    Manager:
    - Detects DC-local timeouts/stuck state
    - Reports to gate (not mark job failed locally)
    - Sends periodic progress reports
    - Waits for gate's global decision

    Fault Tolerance:
    - Progress reports are periodic (loss tolerated)
    - Timeout reports are persistent until ACK'd
    - Fallback to local timeout if gate unreachable for 5+ minutes
    """

    def __init__(self, manager: 'ManagerServer'):
        self._manager = manager
        self._pending_reports: dict[str, list[Message]] = {}
        self._report_lock = asyncio.Lock()

    async def start_tracking(
        self,
        job_id: str,
        timeout_seconds: float,
        gate_addr: tuple[str, int] | None = None
    ) -> None:
        """Initialize gate-coordinated tracking."""
        if not gate_addr:
            raise ValueError("Gate address required for gate-coordinated timeout")

        job = self._manager._job_manager.get_job_by_id(job_id)
        if not job:
            return

        async with job.lock:
            now = time.monotonic()
            job.timeout_tracking = TimeoutTrackingState(
                strategy_type="gate_coordinated",
                gate_addr=gate_addr,
                started_at=now,
                last_progress_at=now,
                last_report_at=now,
                timeout_seconds=timeout_seconds,
                timeout_fence_token=0
            )

    async def resume_tracking(self, job_id: str) -> None:
        """Resume after leader transfer - notify gate."""
        job = self._manager._job_manager.get_job_by_id(job_id)
        if not job or not job.timeout_tracking:
            return

        async with job.lock:
            job.timeout_tracking.timeout_fence_token += 1
            fence_token = job.timeout_tracking.timeout_fence_token

        # Send leadership transfer notification to gate
        await self._send_leader_transfer_report(job_id, fence_token)

    async def report_progress(self, job_id: str, progress_type: str) -> None:
        """Update progress timestamp."""
        job = self._manager._job_manager.get_job_by_id(job_id)
        if not job or not job.timeout_tracking:
            return

        async with job.lock:
            job.timeout_tracking.last_progress_at = time.monotonic()

    async def check_timeout(self, job_id: str) -> tuple[bool, str]:
        """
        Check DC-local timeout and report to gate.

        Does NOT mark job failed locally - waits for gate decision.
        Fallback: if can't reach gate for 5+ minutes, timeout locally.
        """
        job = self._manager._job_manager.get_job_by_id(job_id)
        if not job or not job.timeout_tracking:
            return False, ""

        tracking = job.timeout_tracking

        # Already reported, waiting for gate decision
        if tracking.locally_timed_out:
            # Fallback: gate unresponsive for 5+ minutes
            if not tracking.globally_timed_out:
                time_since_report = time.monotonic() - tracking.last_report_at
                if time_since_report > 300.0:  # 5 minutes
                    await self._manager._udp_logger.log(ServerWarning(
                        message=f"Gate unresponsive for {time_since_report:.0f}s, "
                                f"timing out job {job_id} locally",
                        node_host=self._manager._host,
                        node_port=self._manager._tcp_port,
                        node_id=self._manager._node_id.short,
                    ))
                    await self._manager._timeout_job(
                        job_id,
                        "Gate unresponsive, local timeout fallback"
                    )
                    return True, "gate_unresponsive_fallback"

            return False, ""

        # Check terminal state
        if job.status in {JobStatus.COMPLETED.value, JobStatus.FAILED.value}:
            return False, ""

        now = time.monotonic()

        # Send periodic progress reports
        if now - tracking.last_report_at > 10.0:
            await self._send_progress_report(job_id)
            async with job.lock:
                tracking.last_report_at = now

        # Check for DC-local timeout
        elapsed = now - tracking.started_at
        if elapsed > tracking.timeout_seconds:
            reason = (
                f"DC-local timeout ({elapsed:.1f}s > "
                f"{tracking.timeout_seconds:.1f}s)"
            )
            await self._send_timeout_report(job_id, reason)

            async with job.lock:
                tracking.locally_timed_out = True
                tracking.timeout_reason = reason
                tracking.last_report_at = now

            return True, reason

        # Check for stuck
        time_since_progress = now - tracking.last_progress_at
        if time_since_progress > tracking.stuck_threshold:
            reason = f"DC-local stuck (no progress for {time_since_progress:.1f}s)"
            await self._send_timeout_report(job_id, reason)

            async with job.lock:
                tracking.locally_timed_out = True
                tracking.timeout_reason = reason
                tracking.last_report_at = now

            return True, reason

        return False, ""

    async def handle_global_timeout(
        self,
        job_id: str,
        reason: str,
        fence_token: int
    ) -> bool:
        """
        Handle global timeout from gate.

        Validates fence token to reject stale decisions.
        """
        job = self._manager._job_manager.get_job_by_id(job_id)
        if not job or not job.timeout_tracking:
            return False

        # Fence token validation (prevent stale decisions)
        if fence_token < job.timeout_tracking.timeout_fence_token:
            await self._manager._udp_logger.log(ServerWarning(
                message=f"Rejected stale global timeout for {job_id} "
                        f"(fence {fence_token} < {job.timeout_tracking.timeout_fence_token})",
                node_host=self._manager._host,
                node_port=self._manager._tcp_port,
                node_id=self._manager._node_id.short,
            ))
            return False

        # Check if already terminal
        if job.status in {JobStatus.COMPLETED.value, JobStatus.FAILED.value}:
            # Send correction to gate
            await self._send_status_correction(job_id, job.status)
            return False

        # Accept gate's decision
        async with job.lock:
            job.timeout_tracking.globally_timed_out = True
            job.timeout_tracking.timeout_reason = reason

        await self._manager._timeout_job(job_id, f"Global timeout: {reason}")
        return True

    async def _send_progress_report(self, job_id: str) -> None:
        """Send progress to gate (best-effort, loss tolerated)."""
        job = self._manager._job_manager.get_job_by_id(job_id)
        if not job or not job.timeout_tracking:
            return

        report = JobProgressReport(
            job_id=job_id,
            datacenter=self._manager._datacenter,
            manager_id=self._manager._node_id.short,
            workflows_total=job.workflows_total,
            workflows_completed=job.workflows_completed,
            workflows_failed=job.workflows_failed,
            has_recent_progress=(
                time.monotonic() - job.timeout_tracking.last_progress_at < 10.0
            ),
            timestamp=time.monotonic(),
            fence_token=job.timeout_tracking.timeout_fence_token
        )

        try:
            await self._manager.send_tcp(
                job.timeout_tracking.gate_addr,
                "job_progress_report",
                report.dump()
            )
        except Exception as e:
            # Progress report failure is non-critical
            await self._manager._udp_logger.log(ServerDebug(
                message=f"Failed to send progress report for {job_id}: {e}",
                node_host=self._manager._host,
                node_port=self._manager._tcp_port,
                node_id=self._manager._node_id.short,
            ))

    async def _send_timeout_report(self, job_id: str, reason: str) -> None:
        """Send timeout report to gate (persistent until ACK'd)."""
        job = self._manager._job_manager.get_job_by_id(job_id)
        if not job or not job.timeout_tracking:
            return

        report = JobTimeoutReport(
            job_id=job_id,
            datacenter=self._manager._datacenter,
            manager_id=self._manager._node_id.short,
            reason=reason,
            elapsed_seconds=time.monotonic() - job.timeout_tracking.started_at,
            fence_token=job.timeout_tracking.timeout_fence_token
        )

        # Store for retry
        async with self._report_lock:
            if job_id not in self._pending_reports:
                self._pending_reports[job_id] = []
            self._pending_reports[job_id].append(report)

        try:
            await self._manager.send_tcp(
                job.timeout_tracking.gate_addr,
                "job_timeout_report",
                report.dump()
            )
            # Success - remove from pending
            async with self._report_lock:
                self._pending_reports.pop(job_id, None)
        except Exception as e:
            await self._manager._udp_logger.log(ServerWarning(
                message=f"Failed to send timeout report for {job_id}: {e} (will retry)",
                node_host=self._manager._host,
                node_port=self._manager._tcp_port,
                node_id=self._manager._node_id.short,
            ))
```

### State Diagram - Gate Coordinated (Manager)

```
Job Submitted (with gate_addr)
     ↓
TimeoutTrackingState created
  strategy = "gate_coordinated"
  gate_addr = <gate>
     ↓
╔═══════════════════════════════════╗
║  Periodic Checks (every 30s)      ║
╚═══════════════════════════════════╝
     ↓
Send Progress Report (every 10s)
     ↓ (best-effort)
   Gate
     ↓
Check DC-Local Timeout
     ↓ TIMEOUT DETECTED
Send Timeout Report to Gate
  locally_timed_out = True
     ↓
╔═══════════════════════════════════╗
║    Wait for Gate Decision          ║
║  (or 5min fallback timeout)       ║
╚═══════════════════════════════════╝
     ↓
  ┌──────────────┬──────────────┐
  ↓              ↓              ↓
Gate            Gate         5min passed
Says            Unresponsive  No response
Timeout                       ↓
  ↓                          Local
Mark                         Fallback
globally_timed_out           Timeout
  ↓                            ↓
_timeout_job()           _timeout_job()
```

---

## Part 5: Gate Global Timeout Coordination

### Gate Job Tracker

```python
@dataclass
class GateJobTrackingInfo:
    """Gate's view of a job across all DCs."""
    job_id: str
    submitted_at: float  # Global start time
    timeout_seconds: float
    target_datacenters: list[str]  # Which DCs running this job

    # Per-DC state
    dc_status: dict[str, str]  # dc_name -> "running" | "completed" | "timed_out"
    dc_last_progress: dict[str, float]  # dc_name -> last progress timestamp
    dc_manager_addrs: dict[str, tuple[str, int]]  # dc_name -> manager addr

    # Global timeout decision
    globally_timed_out: bool = False
    timeout_reason: str = ""
    timeout_fence_token: int = 0  # Gate's fence token for this decision


class GateJobTracker:
    """Track jobs across all DCs (Gate-side)."""

    def __init__(self, gate: 'GateServer'):
        self._gate = gate
        self._tracked_jobs: dict[str, GateJobTrackingInfo] = {}
        self._lock = asyncio.Lock()

    async def start_tracking_job(
        self,
        job_id: str,
        timeout_seconds: float,
        target_dcs: list[str]
    ) -> None:
        """Start tracking when job is submitted."""
        async with self._lock:
            self._tracked_jobs[job_id] = GateJobTrackingInfo(
                job_id=job_id,
                submitted_at=time.monotonic(),
                timeout_seconds=timeout_seconds,
                target_datacenters=target_dcs,
                dc_status={dc: "running" for dc in target_dcs},
                dc_last_progress={dc: time.monotonic() for dc in target_dcs},
                dc_manager_addrs={},
                timeout_fence_token=0
            )

    async def record_progress(self, report: JobProgressReport) -> None:
        """Record progress from a DC."""
        async with self._lock:
            info = self._tracked_jobs.get(report.job_id)
            if not info:
                return

            info.dc_last_progress[report.datacenter] = report.timestamp
            info.dc_manager_addrs[report.datacenter] = (
                report.manager_host,
                report.manager_port
            )

            if report.workflows_completed == report.workflows_total:
                info.dc_status[report.datacenter] = "completed"

    async def record_timeout(self, report: JobTimeoutReport) -> None:
        """Record timeout from a DC."""
        async with self._lock:
            info = self._tracked_jobs.get(report.job_id)
            if not info:
                return

            info.dc_status[report.datacenter] = "timed_out"
            info.dc_manager_addrs[report.datacenter] = (
                report.manager_host,
                report.manager_port
            )

    async def check_global_timeouts(self) -> list[tuple[str, str]]:
        """
        Check for global timeouts.

        Returns list of (job_id, reason) for timed-out jobs.
        """
        timed_out_jobs = []
        now = time.monotonic()

        async with self._lock:
            for info in list(self._tracked_jobs.values()):
                if info.globally_timed_out:
                    continue

                # Check 1: Global timeout exceeded
                elapsed = now - info.submitted_at
                if elapsed > info.timeout_seconds:
                    info.globally_timed_out = True
                    info.timeout_reason = (
                        f"Global timeout exceeded ({elapsed:.1f}s > "
                        f"{info.timeout_seconds:.1f}s)"
                    )
                    info.timeout_fence_token += 1
                    timed_out_jobs.append((info.job_id, info.timeout_reason))
                    continue

                # Check 2: Any DC reported timeout
                timed_out_dcs = [
                    dc for dc, status in info.dc_status.items()
                    if status == "timed_out"
                ]

                if timed_out_dcs:
                    info.globally_timed_out = True
                    info.timeout_reason = (
                        f"DC timeout: {', '.join(timed_out_dcs)}"
                    )
                    info.timeout_fence_token += 1
                    timed_out_jobs.append((info.job_id, info.timeout_reason))
                    continue

                # Check 3: All DCs stuck (no progress for 3+ minutes)
                stuck_dcs = [
                    dc for dc, last_progress in info.dc_last_progress.items()
                    if now - last_progress > 180.0
                ]

                if stuck_dcs and len(stuck_dcs) == len(info.target_datacenters):
                    info.globally_timed_out = True
                    info.timeout_reason = f"All DCs stuck: {', '.join(stuck_dcs)}"
                    info.timeout_fence_token += 1
                    timed_out_jobs.append((info.job_id, info.timeout_reason))

        return timed_out_jobs

    def get_job(self, job_id: str) -> GateJobTrackingInfo | None:
        """Get tracking info for a job."""
        return self._tracked_jobs.get(job_id)
```

### Gate Global Timeout Loop

```python
# In GateServer
async def _global_timeout_loop(self) -> None:
    """Check for global timeouts and coordinate cancellation."""
    while not self._shutdown:
        await asyncio.sleep(15.0)  # Gate checks more frequently

        timed_out_jobs = await self._job_tracker.check_global_timeouts()

        for job_id, reason in timed_out_jobs:
            await self._declare_and_broadcast_timeout(job_id, reason)

async def _declare_and_broadcast_timeout(self, job_id: str, reason: str) -> None:
    """Declare job globally timed out and cancel in ALL DCs."""
    tracking_info = self._job_tracker.get_job(job_id)
    if not tracking_info:
        return

    await self._logger.log(ServerInfo(
        message=f"Job {job_id} globally timed out: {reason}",
        node_host=self._host,
        node_port=self._tcp_port,
        node_id=self._node_id.short,
    ))

    # Send cancellation to ALL target DCs
    timeout_msg = JobGlobalTimeout(
        job_id=job_id,
        reason=reason,
        timed_out_at=time.monotonic(),
        fence_token=tracking_info.timeout_fence_token
    )

    for dc_name in tracking_info.target_datacenters:
        manager_addr = tracking_info.dc_manager_addrs.get(dc_name)
        if manager_addr and tracking_info.dc_status.get(dc_name) not in {
            "completed", "timed_out", "failed"
        }:
            try:
                await self.send_tcp(
                    manager_addr,
                    "job_global_timeout",
                    timeout_msg.dump()
                )
            except Exception as e:
                await self._logger.log(ServerWarning(
                    message=f"Failed to send global timeout to {dc_name}: {e}",
                    node_host=self._host,
                    node_port=self._tcp_port,
                    node_id=self._node_id.short,
                ))
```

### State Diagram - Gate Global Coordinator

```
Job Submitted to Multiple DCs
     ↓
GateJobTrackingInfo created
  dc_status = {A: "running", B: "running", C: "running"}
     ↓
╔═══════════════════════════════════╗
║   Receive Reports from DCs         ║
║   - Progress (every 10s)           ║
║   - Timeout (when detected)        ║
╚═══════════════════════════════════╝
     ↓
Update dc_last_progress[dc]
Update dc_status[dc]
     ↓
╔═══════════════════════════════════╗
║  Periodic Global Timeout Check     ║
║      (every 15s)                   ║
╚═══════════════════════════════════╝
     ↓
Check 3 Conditions:
  1. Global timeout exceeded?
  2. Any DC reported timeout?
  3. All DCs stuck (no progress 3+ min)?
     ↓ ANY TRUE
Declare Global Timeout
  globally_timed_out = True
  timeout_fence_token++
     ↓
Broadcast JobGlobalTimeout to ALL DCs
     ↓
   DC-A         DC-B         DC-C
     ↓           ↓            ↓
 Cancel      Cancel       Cancel
  Job         Job          Job
```

---

## Part 6: Manager Integration

### Auto-Selection and State Recovery

```python
class ManagerServer:
    def __init__(self, ...):
        # Per-job timeout strategies
        self._job_timeout_strategies: dict[str, TimeoutStrategy] = {}

    async def receive_submit_job(self, addr, data, clock_time):
        """Handle job submission."""
        submission = JobSubmission.load(data)

        # Auto-select strategy based on topology
        strategy = await self._select_timeout_strategy(submission)

        # ... existing job submission logic ...

        # Start timeout tracking
        await strategy.start_tracking(
            job_id=submission.job_id,
            timeout_seconds=submission.timeout_seconds,
            gate_addr=getattr(submission, 'gate_addr', None)
        )

        self._job_timeout_strategies[submission.job_id] = strategy

    async def _select_timeout_strategy(
        self,
        submission: JobSubmission
    ) -> TimeoutStrategy:
        """
        Auto-detect deployment topology and select strategy.

        Detection:
        - If submission has gate_addr → Multi-DC (GateCoordinatedTimeout)
        - If no gate_addr → Single-DC (LocalAuthorityTimeout)
        """
        if hasattr(submission, 'gate_addr') and submission.gate_addr:
            return GateCoordinatedTimeout(self)
        else:
            return LocalAuthorityTimeout(self)

    async def _on_leadership_acquired(self, job_id: str) -> None:
        """
        Called when this manager becomes leader for a job.

        CRITICAL: Must resume timeout tracking.
        """
        job = self._job_manager.get_job_by_id(job_id)
        if not job or not job.timeout_tracking:
            return

        # Resume timeout tracking with appropriate strategy
        strategy = await self._get_or_create_timeout_strategy(job)
        await strategy.resume_tracking(job_id)

        self._job_timeout_strategies[job_id] = strategy

    async def _get_or_create_timeout_strategy(
        self,
        job: JobInfo
    ) -> TimeoutStrategy:
        """Get strategy for job (resume if exists)."""
        if not job.timeout_tracking:
            return LocalAuthorityTimeout(self)

        if job.timeout_tracking.strategy_type == "gate_coordinated":
            return GateCoordinatedTimeout(self)
        else:
            return LocalAuthorityTimeout(self)

    async def _unified_timeout_loop(self) -> None:
        """Unified timeout loop for both single-DC and multi-DC."""
        while not self._shutdown:
            await asyncio.sleep(30.0)

            if self._state != ManagerState.ACTIVE:
                continue

            for job in self._job_manager.iter_jobs():
                # Only leader checks
                if job.leader_node_id != self._node_id.short:
                    continue

                # Get or resume strategy
                if job.job_id not in self._job_timeout_strategies:
                    strategy = await self._get_or_create_timeout_strategy(job)
                    await strategy.resume_tracking(job.job_id)
                    self._job_timeout_strategies[job.job_id] = strategy
                else:
                    strategy = self._job_timeout_strategies[job.job_id]

                # Check timeout
                try:
                    is_timed_out, reason = await strategy.check_timeout(job.job_id)
                    if is_timed_out:
                        await self._udp_logger.log(ServerInfo(
                            message=f"Job {job.job_id} timed out: {reason}",
                            node_host=self._host,
                            node_port=self._tcp_port,
                            node_id=self._node_id.short,
                        ))
                except Exception as e:
                    await self._udp_logger.log(ServerError(
                        message=f"Timeout check failed for {job.job_id}: {e}",
                        node_host=self._host,
                        node_port=self._tcp_port,
                        node_id=self._node_id.short,
                    ))
```

### Progress Reporting Integration

```python
# Integrate with WorkflowStateMachine from AD-33
async def _on_workflow_state_transition(
    self,
    job_id: str,
    workflow_id: str,
    from_state: WorkflowState,
    to_state: WorkflowState
) -> None:
    """Called when workflow transitions state."""
    # Report progress to timeout strategy
    strategy = self._job_timeout_strategies.get(job_id)
    if strategy:
        await strategy.report_progress(job_id, f"workflow_{to_state.value}")
```

### Handling Global Timeout from Gate

```python
async def receive_job_global_timeout(self, addr, data, clock_time):
    """
    Receive global timeout decision from gate.

    Gate has declared job timed out - cancel it locally.
    """
    timeout_msg = JobGlobalTimeout.load(data)

    strategy = self._job_timeout_strategies.get(timeout_msg.job_id)
    if not strategy:
        return

    # Delegate to strategy (handles fence token validation)
    accepted = await strategy.handle_global_timeout(
        timeout_msg.job_id,
        timeout_msg.reason,
        timeout_msg.fence_token
    )

    if accepted:
        # Clean up tracking
        self._job_timeout_strategies.pop(timeout_msg.job_id, None)
```

---

## Part 7: Protocol Messages

### JobProgressReport

```python
@dataclass
class JobProgressReport(Message):
    """Manager → Gate: Periodic progress report."""
    job_id: str
    datacenter: str
    manager_id: str
    manager_host: str  # For gate to send replies
    manager_port: int
    workflows_total: int
    workflows_completed: int
    workflows_failed: int
    has_recent_progress: bool  # Any workflow progressed in last 10s
    timestamp: float
    fence_token: int  # Manager's fence token
```

### JobTimeoutReport

```python
@dataclass
class JobTimeoutReport(Message):
    """Manager → Gate: DC-local timeout detected."""
    job_id: str
    datacenter: str
    manager_id: str
    manager_host: str
    manager_port: int
    reason: str  # "timeout" | "stuck"
    elapsed_seconds: float
    fence_token: int
```

### JobGlobalTimeout

```python
@dataclass
class JobGlobalTimeout(Message):
    """Gate → Manager: Global timeout declared."""
    job_id: str
    reason: str  # Why gate timed out the job
    timed_out_at: float  # Gate's timestamp
    fence_token: int  # Gate's fence token for this decision
```

### JobLeaderTransfer

```python
@dataclass
class JobLeaderTransfer(Message):
    """Manager → Gate: Notify gate of leader change."""
    job_id: str
    datacenter: str
    new_leader_id: str
    fence_token: int  # New leader's fence token
```

### JobSubmission Enhancement

```python
@dataclass
class JobSubmission(Message):
    # ... existing fields ...

    # Multi-DC coordination (optional, None for single-DC)
    gate_addr: tuple[str, int] | None = None
    target_datacenters: list[str] = field(default_factory=list)
```

---

## Part 8: Fault Tolerance Scenarios

### Scenario 1: Manager Leader Failure

```
Timeline:
T0: Leader-A tracking job timeout (started_at = 100.0)
T1: Leader-A fails
T2: Leader-B elected
T3: Leader-B receives job via state sync
T4: Leader-B calls resume_tracking()
     - Increments fence_token (1 → 2)
     - Continues from started_at = 100.0 (preserved!)
T5: Leader-B continues timeout checking

Result: Timeout tracking continues seamlessly
```

**Key**: `started_at` in TimeoutTrackingState is absolute, preserved across transfers.

### Scenario 2: Gate Failure (Multi-DC)

```
Timeline:
T0: Gate tracking job across DC-A, DC-B, DC-C
T1: Gate fails
T2: Managers continue sending reports (stored in pending_reports)
T3: Gate restarts/replaced
T4: Managers resend pending timeout reports
T5: New gate reconstructs state from reports
T6: Gate declares global timeout

Fallback:
If gate down for 5+ minutes:
  - Managers timeout jobs locally (fallback)
  - Each DC independently marks job failed
```

**Key**: Managers have fallback to local timeout if gate unreachable.

### Scenario 3: Timeout Detected, Job Completes (Race)

```
Timeline:
T0: Manager detects timeout, sends JobTimeoutReport to gate
T1: Job completes on worker before gate receives report
T2: Manager sends JobCompletionReport to gate
T3: Gate receives both messages

Gate Resolution:
- Use timestamp ordering:
  if timeout_report.timestamp < completion.timestamp:
      declare_timeout()  # Timeout happened first
  else:
      accept_completion()  # Completion happened first

Manager Side:
- When receive_job_global_timeout() called:
  - Check if job already COMPLETED/FAILED
  - If yes, send JobStatusCorrection to gate
  - Gate reconciles
```

**Key**: Timestamps + status corrections resolve races.

### Scenario 4: Stale Global Timeout (After Leader Transfer)

```
Timeline:
T0: Leader-A (fence_token=1) reports timeout to gate
T1: Leader-A fails
T2: Leader-B takes over (fence_token=2)
T3: Gate sends JobGlobalTimeout(fence_token=1) [stale!]
T4: Leader-B receives message
     - Validates: 1 < 2 (stale)
     - Rejects message
     - Sends status correction to gate

Result: Stale timeout rejected, gate updates state
```

**Key**: Fence tokens prevent stale decisions.

### Scenario 5: Network Partition Isolates DC from Gate

```
Timeline:
T0: DC-A partitioned from gate
T1: DC-A continues local timeout detection
T2: DC-A stores pending timeout reports (can't reach gate)
T3: Gate sees no progress reports from DC-A for 3+ minutes
T4: Gate declares global timeout (assumes DC-A stuck)
T5: Gate sends JobGlobalTimeout to DC-B, DC-C (cancels them)
T6: Partition heals
T7: DC-A receives JobGlobalTimeout
T8: DC-A cancels job (or already done via fallback)

Fallback:
If partition lasts 5+ minutes:
  - DC-A times out job locally
  - When partition heals, sends status correction
```

**Key**: Gate assumes stuck if no reports, DCs have fallback.

---

## Part 9: Complete Workflow Integration

### Progress Tracking with AD-33 State Machine

```python
# Enhance WorkflowStateMachine to track progress
class WorkflowStateMachine:
    def __init__(self, ...):
        self._last_progress: dict[str, float] = {}  # workflow_id → timestamp
        self._progress_callbacks: list[Callable] = []

    def register_progress_callback(
        self,
        callback: Callable[[str, WorkflowState], Awaitable[None]]
    ) -> None:
        """Register callback for state transitions (progress events)."""
        self._progress_callbacks.append(callback)

    async def transition(
        self,
        workflow_id: str,
        to_state: WorkflowState,
        reason: str = ""
    ) -> bool:
        """Transition with progress tracking."""
        success = await self._transition_impl(workflow_id, to_state, reason)

        if success:
            # Record progress
            self._last_progress[workflow_id] = time.monotonic()

            # Notify progress callbacks (timeout strategies)
            for callback in self._progress_callbacks:
                try:
                    await callback(workflow_id, to_state)
                except Exception:
                    pass  # Don't let callback errors break transition

        return success

    def get_time_since_progress(self, workflow_id: str) -> float:
        """Get seconds since workflow last made progress."""
        last_time = self._last_progress.get(workflow_id, 0.0)
        if last_time == 0.0:
            return 0.0
        return time.monotonic() - last_time

    def get_stuck_workflows(self, threshold_seconds: float) -> list[str]:
        """Find workflows with no progress for threshold_seconds."""
        now = time.monotonic()
        stuck = []
        for wf_id, last_time in self._last_progress.items():
            if now - last_time > threshold_seconds:
                stuck.append(wf_id)
        return stuck


# Manager connects timeout strategy to state machine
async def _setup_timeout_progress_tracking(self, job_id: str) -> None:
    """Connect state machine progress events to timeout strategy."""
    if not self._workflow_lifecycle_states:
        return

    strategy = self._job_timeout_strategies.get(job_id)
    if not strategy:
        return

    async def on_progress(workflow_id: str, state: WorkflowState) -> None:
        # Find job for this workflow
        for job in self._job_manager.iter_jobs():
            if any(str(wf.token) == workflow_id for wf in job.workflows.values()):
                await strategy.report_progress(job.job_id, f"workflow_{state.value}")
                break

    self._workflow_lifecycle_states.register_progress_callback(on_progress)
```

---

## Part 10: Observability

### Metrics

```python
# Timeout detection metrics
job_timeout_checks_total{strategy="local_authority|gate_coordinated"} 1000
job_timeouts_detected_total{reason="overall|stuck"} 50
job_timeout_reports_sent_total{datacenter="us-east"} 30
job_timeout_reports_failed_total{datacenter="us-east"} 2

# Gate coordination metrics
gate_global_timeouts_declared_total{reason="dc_timeout|all_stuck|overall"} 20
gate_dc_progress_reports_received_total{datacenter="us-east"} 5000
gate_dc_timeout_reports_received_total{datacenter="us-east"} 10

# Fence token metrics
timeout_fence_token_rejections_total{reason="stale_global_timeout"} 5
timeout_leader_transfers_total{job_id="..."} 3
```

### Logs

```python
# Manager logs
ServerInfo: "Job abc123 timed out: Job timeout exceeded (310.5s > 300.0s)"
ServerWarning: "Gate unresponsive for 302s, timing out job abc123 locally"
ServerWarning: "Rejected stale global timeout for abc123 (fence 1 < 2)"
ServerDebug: "Resumed timeout tracking for abc123 (fence=2)"

# Gate logs
ServerInfo: "Job abc123 globally timed out: DC timeout: us-east, eu-west"
ServerWarning: "Failed to send global timeout to us-east: Connection refused"
```

---

## Part 11: Benefits

### Adaptability

✅ **Single deployment, dual behavior** - Same code, auto-detects topology
✅ **Per-job strategy** - Different jobs can use different strategies
✅ **No configuration** - Detection via `gate_addr` in JobSubmission

### Fault Tolerance

✅ **Leader failure recovery** - State in JobInfo, survives transfers
✅ **Gate failure handling** - Fallback to local timeout after 5 minutes
✅ **Network partition resilience** - Managers continue independently
✅ **Idempotent operations** - Safe to call check_timeout() repeatedly

### Correctness

✅ **Fence tokens** - Prevent stale decisions after leader transfer
✅ **Race condition handling** - Timestamps + status corrections
✅ **Progress detection** - Distinguishes stuck from slow
✅ **Multi-DC consistency** - Gate ensures all DCs cancelled together

### Observability

✅ **Complete state tracking** - TimeoutTrackingState captures everything
✅ **Detailed logging** - Every timeout decision logged with reason
✅ **Metrics** - Track detection, reports, rejections

---

## Part 12: Files

| File | Purpose |
|------|---------|
| `distributed_rewrite/jobs/timeout_strategy.py` | TimeoutStrategy interface, LocalAuthorityTimeout, GateCoordinatedTimeout |
| `distributed_rewrite/models/jobs.py` | TimeoutTrackingState dataclass added to JobInfo |
| `distributed_rewrite/models/distributed.py` | JobProgressReport, JobTimeoutReport, JobGlobalTimeout, JobLeaderTransfer messages |
| `nodes/manager.py` | Strategy selection, unified timeout loop, leader transfer handling |
| `nodes/gate.py` | GateJobTracker, global timeout loop, broadcast coordination |
| `distributed_rewrite/workflow/state_machine.py` | Progress tracking integration (from AD-33) |

---

## Part 13: Migration Strategy

**Phase 1**: Implement LocalAuthorityTimeout only (single-DC)
- Add TimeoutTrackingState to JobInfo
- Implement unified_timeout_loop in Manager
- Test with single-DC deployments

**Phase 2**: Add gate_addr to JobSubmission
- Gates populate gate_addr when submitting jobs
- Managers check for gate_addr (falls back to local if missing)
- No behavior change yet (still uses local timeout)

**Phase 3**: Implement GateCoordinatedTimeout
- Add progress/timeout reporting to gate
- Implement GateJobTracker and global timeout loop
- Enable gate_addr-based strategy selection

**Phase 4**: Integration with AD-33
- Connect WorkflowStateMachine progress events
- Timeout strategies receive workflow state transitions
- Complete stuck workflow detection

---

## Summary

AD-34 introduces **adaptive job timeout with multi-DC coordination** that:

✅ **Auto-detects topology** - Uses local authority (single-DC) or gate coordination (multi-DC)
✅ **Robust to failures** - Leader transfers, gate failures, network partitions
✅ **Race condition safe** - Fence tokens, timestamps, status corrections
✅ **Detects stuck workflows** - Progress tracking via AD-33 state machine
✅ **Global consistency** - Gate ensures timeout cancels job in ALL DCs
✅ **Fallback protection** - Managers timeout locally if gate unreachable (5 min)
✅ **Zero configuration** - Strategy chosen per-job based on `gate_addr`
✅ **State recovery** - Timeout state persists in JobInfo, survives leader transfers

This architecture ensures jobs never leak resources, even when workers are alive but workflows are stuck, across both single-datacenter and multi-datacenter deployments.

---

## Part 14: Integration with AD-26 (Healthcheck Extensions)

### The Problem

**Worker extension requests (AD-26) and job timeouts (AD-34) must cooperate**. Currently, they operate independently, creating several critical issues:

#### Issue 1: Extension-Timeout Race Condition

```
Timeline:
T0:   Job starts (timeout_seconds = 300s)
T50:  Worker executing long workflow, requests extension (+15s granted)
T100: Worker requests 2nd extension (+7.5s granted)
T150: Worker requests 3rd extension (+3.75s granted)
T300: Job timeout fires! ❌

Problem:
- Worker has 26.25s of legitimately granted extensions remaining
- Worker is making progress (each extension required progress)
- Job timeout doesn't account for extensions
- Job killed prematurely despite legitimate work
```

#### Issue 2: Multi-DC Extension Coordination

```
Multi-DC Scenario:
DC-A: Worker-1 granted 3 extensions (total_extended = 26.25s)
DC-B: Worker-2 granted 1 extension (total_extended = 15s)
DC-C: Worker-3 granted 0 extensions (stuck, denied)

Gate receives:
- DC-A: JobProgressReport (has_recent_progress = True, extensions_granted = 26.25s)
- DC-B: JobProgressReport (has_recent_progress = True, extensions_granted = 15s)
- DC-C: JobTimeoutReport (reason = "stuck", extensions_granted = 0s)

Gate must decide:
- Should it declare global timeout?
- DC-C is stuck, but DC-A and DC-B are making progress with extensions
- Should gate account for DC-A/B's extended deadlines?
```

#### Issue 3: Progress Tracking Mismatch

```
AD-34 tracks progress: WorkflowStateMachine state transitions
AD-26 grants extensions: Worker-reported progress metric

These are DIFFERENT:
- Worker progress: "I've completed 50% of this workflow" (incremental)
- Workflow progress: State transition PENDING → DISPATCHED → RUNNING → COMPLETED (discrete)

Scenario:
- Worker executing long workflow (e.g., 5-minute test)
- Worker at 50% completion (deserves extension based on progress)
- No workflow state transition in last 2 minutes (looks stuck to AD-34)
- AD-34 declares timeout despite legitimate progress
```

### The Solution: Extension-Aware Timeout Tracking

#### Enhanced TimeoutTrackingState

```python
@dataclass
class TimeoutTrackingState:
    """Timeout tracking state with extension awareness."""
    strategy_type: str
    gate_addr: tuple[str, int] | None

    # Timestamps
    started_at: float
    last_progress_at: float
    last_report_at: float

    # Timeout configuration
    timeout_seconds: float
    stuck_threshold: float = 120.0

    # Extension tracking (NEW)
    total_extensions_granted: float = 0.0  # Total seconds granted to ALL workers
    max_worker_extension: float = 0.0      # Largest extension granted to any worker
    last_extension_at: float = 0.0         # When last extension was granted
    active_workers_with_extensions: set[str] = field(default_factory=set)

    # State flags
    locally_timed_out: bool = False
    globally_timed_out: bool = False
    timeout_reason: str = ""

    # Fencing
    timeout_fence_token: int = 0
```

**Key Design:**
- `total_extensions_granted`: Sum of ALL extensions granted to workers executing this job
- `max_worker_extension`: Largest single extension granted (for timeout calculation)
- `active_workers_with_extensions`: Track which workers have active extensions
- Extensions are **additive to timeout_seconds**, not replacements

#### Extension Notification Protocol

```python
@dataclass
class WorkerExtensionGranted(Message):
    """
    Manager → Timeout Strategy: Worker extension granted (internal).

    When manager grants a worker extension (AD-26), it must notify
    the job timeout strategy so the job timeout is adjusted accordingly.
    """
    job_id: str
    worker_id: str
    extension_seconds: float
    total_worker_extensions: float  # Total extensions for this worker
    worker_progress: float          # Progress metric that justified extension
    timestamp: float
```

#### Updated Progress Reporting (Multi-DC)

```python
@dataclass
class JobProgressReport(Message):
    """Manager → Gate: Periodic progress report."""
    job_id: str
    datacenter: str
    manager_id: str
    manager_host: str
    manager_port: int
    workflows_total: int
    workflows_completed: int
    workflows_failed: int
    has_recent_progress: bool
    timestamp: float
    fence_token: int

    # Extension tracking (NEW)
    total_extensions_granted: float = 0.0  # Total extensions granted to workers
    max_worker_extension: float = 0.0      # Largest extension granted
    workers_with_extensions: int = 0       # Count of workers with active extensions
```

### Updated Timeout Strategies

#### LocalAuthorityTimeout with Extensions

```python
class LocalAuthorityTimeout(TimeoutStrategy):
    async def record_worker_extension(
        self,
        job_id: str,
        worker_id: str,
        extension_seconds: float,
        worker_progress: float
    ) -> None:
        """
        Record that a worker was granted an extension.

        This adjusts the job's effective timeout to account for
        legitimate long-running work.
        """
        job = self._manager._job_manager.get_job_by_id(job_id)
        if not job or not job.timeout_tracking:
            return

        async with job.lock:
            tracking = job.timeout_tracking

            # Update extension tracking
            tracking.total_extensions_granted += extension_seconds
            tracking.max_worker_extension = max(
                tracking.max_worker_extension,
                extension_seconds
            )
            tracking.last_extension_at = time.monotonic()
            tracking.active_workers_with_extensions.add(worker_id)

            # Extension = progress! Update last_progress_at
            tracking.last_progress_at = time.monotonic()

        await self._manager._udp_logger.log(ServerDebug(
            message=f"Job {job_id} timeout extended by {extension_seconds:.1f}s "
                    f"(worker {worker_id} progress={worker_progress:.2f})",
            node_host=self._manager._host,
            node_port=self._manager._tcp_port,
            node_id=self._manager._node_id.short,
        ))

    async def check_timeout(self, job_id: str) -> tuple[bool, str]:
        """Check timeout with extension awareness."""
        job = self._manager._job_manager.get_job_by_id(job_id)
        if not job or not job.timeout_tracking:
            return False, ""

        if job.timeout_tracking.locally_timed_out:
            return False, ""

        if job.status in {JobStatus.COMPLETED.value, JobStatus.FAILED.value}:
            return False, ""

        now = time.monotonic()
        tracking = job.timeout_tracking

        # Calculate effective timeout with extensions
        effective_timeout = tracking.timeout_seconds + tracking.total_extensions_granted

        # Check overall timeout (with extensions)
        elapsed = now - tracking.started_at
        if elapsed > effective_timeout:
            async with job.lock:
                tracking.locally_timed_out = True
                tracking.timeout_reason = (
                    f"Job timeout exceeded ({elapsed:.1f}s > {effective_timeout:.1f}s, "
                    f"base={tracking.timeout_seconds:.1f}s + "
                    f"extensions={tracking.total_extensions_granted:.1f}s)"
                )

            await self._manager._timeout_job(job_id, tracking.timeout_reason)
            return True, tracking.timeout_reason

        # Check for stuck (no progress AND no recent extensions)
        time_since_progress = now - tracking.last_progress_at
        time_since_extension = now - tracking.last_extension_at if tracking.last_extension_at > 0 else float('inf')

        # If extensions granted recently, not stuck
        if time_since_extension < tracking.stuck_threshold:
            return False, ""

        # Otherwise check progress-based stuck detection
        if time_since_progress > tracking.stuck_threshold:
            async with job.lock:
                tracking.locally_timed_out = True
                tracking.timeout_reason = (
                    f"Job stuck (no progress for {time_since_progress:.1f}s, "
                    f"no extensions for {time_since_extension:.1f}s)"
                )

            await self._manager._timeout_job(job_id, tracking.timeout_reason)
            return True, tracking.timeout_reason

        return False, ""
```

**Key Changes:**
1. **Additive Extensions**: `effective_timeout = base + total_extensions`
2. **Extension = Progress**: Granting extension updates `last_progress_at`
3. **Recent Extension Check**: Not stuck if extension granted within `stuck_threshold`

#### GateCoordinatedTimeout with Extensions

```python
class GateCoordinatedTimeout(TimeoutStrategy):
    async def record_worker_extension(
        self,
        job_id: str,
        worker_id: str,
        extension_seconds: float,
        worker_progress: float
    ) -> None:
        """Record extension and notify gate."""
        # Update local tracking (same as LocalAuthorityTimeout)
        job = self._manager._job_manager.get_job_by_id(job_id)
        if not job or not job.timeout_tracking:
            return

        async with job.lock:
            tracking = job.timeout_tracking
            tracking.total_extensions_granted += extension_seconds
            tracking.max_worker_extension = max(
                tracking.max_worker_extension,
                extension_seconds
            )
            tracking.last_extension_at = time.monotonic()
            tracking.last_progress_at = time.monotonic()
            tracking.active_workers_with_extensions.add(worker_id)

        # Gate will learn about extensions via next JobProgressReport
        # (which includes total_extensions_granted field)

    async def _send_progress_report(self, job_id: str) -> None:
        """Send progress with extension info."""
        job = self._manager._job_manager.get_job_by_id(job_id)
        if not job or not job.timeout_tracking:
            return

        report = JobProgressReport(
            job_id=job_id,
            datacenter=self._manager._datacenter,
            manager_id=self._manager._node_id.short,
            manager_host=self._manager._host,
            manager_port=self._manager._tcp_port,
            workflows_total=job.workflows_total,
            workflows_completed=job.workflows_completed,
            workflows_failed=job.workflows_failed,
            has_recent_progress=(
                time.monotonic() - job.timeout_tracking.last_progress_at < 10.0
            ),
            timestamp=time.monotonic(),
            fence_token=job.timeout_tracking.timeout_fence_token,
            # Extension info (NEW)
            total_extensions_granted=job.timeout_tracking.total_extensions_granted,
            max_worker_extension=job.timeout_tracking.max_worker_extension,
            workers_with_extensions=len(job.timeout_tracking.active_workers_with_extensions),
        )

        try:
            await self._manager.send_tcp(
                job.timeout_tracking.gate_addr,
                "job_progress_report",
                report.dump()
            )
        except Exception as e:
            await self._manager._udp_logger.log(ServerDebug(
                message=f"Failed to send progress report for {job_id}: {e}",
                node_host=self._manager._host,
                node_port=self._manager._tcp_port,
                node_id=self._manager._node_id.short,
            ))
```

### Gate Extension-Aware Timeout Coordination

```python
class GateJobTrackingInfo:
    """Gate's view with extension tracking."""
    job_id: str
    submitted_at: float
    timeout_seconds: float
    target_datacenters: list[str]

    # Per-DC state
    dc_status: dict[str, str]
    dc_last_progress: dict[str, float]
    dc_manager_addrs: dict[str, tuple[str, int]]

    # Per-DC extension tracking (NEW)
    dc_total_extensions: dict[str, float] = field(default_factory=dict)
    dc_max_extension: dict[str, float] = field(default_factory=dict)
    dc_workers_with_extensions: dict[str, int] = field(default_factory=dict)

    # Global timeout decision
    globally_timed_out: bool = False
    timeout_reason: str = ""
    timeout_fence_token: int = 0


class GateJobTracker:
    async def record_progress(self, report: JobProgressReport) -> None:
        """Record progress with extension info."""
        async with self._lock:
            info = self._tracked_jobs.get(report.job_id)
            if not info:
                return

            # Update progress
            info.dc_last_progress[report.datacenter] = report.timestamp
            info.dc_manager_addrs[report.datacenter] = (
                report.manager_host,
                report.manager_port
            )

            # Update extension tracking
            info.dc_total_extensions[report.datacenter] = report.total_extensions_granted
            info.dc_max_extension[report.datacenter] = report.max_worker_extension
            info.dc_workers_with_extensions[report.datacenter] = report.workers_with_extensions

            if report.workflows_completed == report.workflows_total:
                info.dc_status[report.datacenter] = "completed"

    async def check_global_timeouts(self) -> list[tuple[str, str]]:
        """Check timeouts with extension awareness."""
        timed_out_jobs = []
        now = time.monotonic()

        async with self._lock:
            for info in list(self._tracked_jobs.values()):
                if info.globally_timed_out:
                    continue

                # Calculate global effective timeout
                # Use MAX extension across all DCs (most lenient)
                max_dc_extension = max(
                    info.dc_total_extensions.values(),
                    default=0.0
                )
                effective_timeout = info.timeout_seconds + max_dc_extension

                # Check 1: Global timeout exceeded (with extensions)
                elapsed = now - info.submitted_at
                if elapsed > effective_timeout:
                    info.globally_timed_out = True
                    info.timeout_reason = (
                        f"Global timeout exceeded ({elapsed:.1f}s > {effective_timeout:.1f}s, "
                        f"base={info.timeout_seconds:.1f}s + max_extension={max_dc_extension:.1f}s)"
                    )
                    info.timeout_fence_token += 1
                    timed_out_jobs.append((info.job_id, info.timeout_reason))
                    continue

                # Check 2: Any DC reported timeout WITHOUT extensions
                # If DC has extensions, it's legitimately taking longer
                timed_out_dcs = [
                    dc for dc, status in info.dc_status.items()
                    if status == "timed_out" and info.dc_total_extensions.get(dc, 0.0) == 0.0
                ]

                if timed_out_dcs:
                    info.globally_timed_out = True
                    info.timeout_reason = f"DC timeout (no extensions): {', '.join(timed_out_dcs)}"
                    info.timeout_fence_token += 1
                    timed_out_jobs.append((info.job_id, info.timeout_reason))
                    continue

                # Check 3: All DCs stuck (no progress AND no extensions for 3+ min)
                stuck_dcs = []
                for dc in info.target_datacenters:
                    last_progress = info.dc_last_progress.get(dc, info.submitted_at)
                    time_since_progress = now - last_progress

                    # Get last extension time for this DC
                    # (Gate doesn't track this directly, use progress report frequency)
                    has_recent_extensions = info.dc_workers_with_extensions.get(dc, 0) > 0

                    # Stuck if: no progress for 3+ min AND no workers have extensions
                    if time_since_progress > 180.0 and not has_recent_extensions:
                        stuck_dcs.append(dc)

                if stuck_dcs and len(stuck_dcs) == len(info.target_datacenters):
                    info.globally_timed_out = True
                    info.timeout_reason = f"All DCs stuck: {', '.join(stuck_dcs)}"
                    info.timeout_fence_token += 1
                    timed_out_jobs.append((info.job_id, info.timeout_reason))

        return timed_out_jobs
```

**Key Gate Logic:**
1. **Global Effective Timeout** = `base_timeout + MAX(dc_extensions)`
2. **Extension-Aware Stuck Detection**: DC not stuck if workers have active extensions
3. **Timeout Without Extensions**: Only timeout DCs that haven't been granted extensions

### Manager Integration

```python
# In ManagerServer.request_extension()
async def request_extension(
    self,
    addr: tuple[str, int],
    data: bytes,
    clock_time: int,
):
    """Handle extension request with timeout coordination."""
    try:
        request = HealthcheckExtensionRequest.load(data)

        # ... existing validation ...

        response = self._worker_health_manager.handle_extension_request(
            request=request,
            current_deadline=current_deadline,
        )

        # Update deadline if granted
        if response.granted:
            self._worker_deadlines[request.worker_id] = response.new_deadline

            # NEW: Notify job timeout strategy about extension
            await self._notify_timeout_strategies_of_extension(
                worker_id=request.worker_id,
                extension_seconds=response.extension_seconds,
                worker_progress=request.current_progress,
            )

            await self._udp_logger.log(ServerInfo(...))

        return response.dump()

    except Exception as e:
        await self.handle_exception(e, "request_extension")


async def _notify_timeout_strategies_of_extension(
    self,
    worker_id: str,
    extension_seconds: float,
    worker_progress: float,
) -> None:
    """
    Notify all job timeout strategies that a worker received an extension.

    This ensures job timeouts are adjusted to account for legitimate
    long-running work.
    """
    # Find all jobs this worker is executing
    affected_jobs = []
    for job in self._job_manager.iter_jobs():
        # Check if this worker is executing workflows for this job
        for workflow_info in job.workflows.values():
            if workflow_info.assigned_worker_id == worker_id:
                affected_jobs.append(job.job_id)
                break

    # Notify timeout strategy for each affected job
    for job_id in affected_jobs:
        strategy = self._job_timeout_strategies.get(job_id)
        if strategy:
            await strategy.record_worker_extension(
                job_id=job_id,
                worker_id=worker_id,
                extension_seconds=extension_seconds,
                worker_progress=worker_progress,
            )
```

### Benefits of Integration

✅ **No Premature Timeouts**: Job timeout extended when workers receive legitimate extensions
✅ **Multi-DC Coordination**: Gate accounts for DC-specific extensions when declaring global timeout
✅ **Progress Recognition**: Extension grant = progress signal (updates `last_progress_at`)
✅ **Stuck Detection**: Not stuck if extensions granted recently, even without state transitions
✅ **Observability**: Extension info included in progress reports to gate
✅ **Backward Compatible**: Jobs without extensions work exactly as before

### Updated State Diagram

```
Job Timeline with Extensions:

T0:     Job starts (timeout = 300s)
T50:    Worker-1 requests extension (+15s granted)
        → total_extensions = 15s
        → effective_timeout = 315s
        → last_progress_at updated
T100:   Worker-2 requests extension (+7.5s granted)
        → total_extensions = 22.5s
        → effective_timeout = 322.5s
        → last_progress_at updated
T322:   Check timeout:
        elapsed = 322s
        effective_timeout = 322.5s
        Result: NOT timed out (within extended deadline)
T330:   Check timeout:
        elapsed = 330s
        effective_timeout = 322.5s
        Result: TIMED OUT (exceeded even with extensions)
```

### Fault Tolerance with Extensions

**Scenario: Leader transfer with pending extensions**

```
T0: Leader-A tracking job (started_at = 100, timeout = 300)
T50: Leader-A grants Worker-1 extension (+15s)
     → total_extensions = 15s stored in JobInfo.timeout_tracking
T60: Leader-A fails
T65: Leader-B elected, receives job via state sync
T70: Leader-B calls resume_tracking()
     → Reads total_extensions = 15s from JobInfo
     → Continues with effective_timeout = 315s
     → No extension lost!
```

**Key**: Extensions stored in `TimeoutTrackingState` which is part of `JobInfo`, so they survive leader transfers.

---

## Summary of AD-26 Integration

AD-34 now cooperates with AD-26 healthcheck extensions:

✅ **Extension-Aware Timeout**: `effective_timeout = base_timeout + total_extensions_granted`
✅ **Extension = Progress**: Granting extension updates `last_progress_at` (not stuck)
✅ **Multi-DC Extension Tracking**: Gate uses `MAX(dc_extensions)` for global timeout
✅ **Extension Notification**: Manager notifies timeout strategies when extensions granted
✅ **State Persistence**: Extension data in `TimeoutTrackingState`, survives leader transfers
✅ **Progress Reporting**: Extension info included in `JobProgressReport` to gate
✅ **Gate Coordination**: Gate distinguishes "timed out" from "legitimately taking longer"

This ensures workers executing long-running workflows with legitimate extensions are not prematurely killed by job timeouts.

---

## Part 15: Timeout Cleanup and Lifecycle Management

### The Problem: Zombie Timeouts

**Timeout tracking must be cleaned up** when jobs/workflows terminate to prevent:
1. **Memory leaks**: Timeout state persists after job completion
2. **Zombie timeouts**: Timeout fires for already-completed/cancelled jobs
3. **Stale extension tracking**: Extension data remains after worker failure
4. **Resource exhaustion**: Timeout strategies accumulate indefinitely

### Cleanup Triggers

Timeout tracking must be cleaned up on:

1. **Job Completion** (successful)
2. **Job Failure** (execution error)
3. **Job Cancellation** (user/gate requested)
4. **Job Timeout** (self-triggered)
5. **Worker Failure** (all workflows on worker)
6. **Manager Cleanup** (periodic cleanup of old jobs)

### Enhanced TimeoutStrategy Interface

```python
class TimeoutStrategy(ABC):
    """Base timeout strategy with lifecycle management."""

    @abstractmethod
    async def start_tracking(
        self,
        job_id: str,
        timeout_seconds: float,
        gate_addr: tuple[str, int] | None = None
    ) -> None:
        """Start tracking on job submission."""
        pass

    @abstractmethod
    async def stop_tracking(self, job_id: str, reason: str) -> None:
        """
        Stop tracking timeout for a job.

        Called when job reaches terminal state (completed, failed, cancelled, timed out).
        Must be idempotent - safe to call multiple times.

        Args:
            job_id: Job to stop tracking
            reason: Why tracking stopped (e.g., "completed", "cancelled", "timed_out")
        """
        pass

    @abstractmethod
    async def cleanup_worker_extensions(self, job_id: str, worker_id: str) -> None:
        """
        Clean up extension tracking for a failed/removed worker.

        Called when worker dies or is removed from job.
        Removes worker from active_workers_with_extensions.

        Args:
            job_id: Job ID
            worker_id: Worker to remove from extension tracking
        """
        pass

    # ... existing methods ...
```

### LocalAuthorityTimeout Cleanup

```python
class LocalAuthorityTimeout(TimeoutStrategy):
    async def stop_tracking(self, job_id: str, reason: str) -> None:
        """
        Stop timeout tracking for job.

        Idempotent - safe to call multiple times.
        """
        job = self._manager._job_manager.get_job_by_id(job_id)
        if not job or not job.timeout_tracking:
            return

        async with job.lock:
            # Mark as stopped to prevent further timeout checks
            job.timeout_tracking.locally_timed_out = True
            job.timeout_tracking.timeout_reason = f"Tracking stopped: {reason}"

        await self._manager._udp_logger.log(ServerDebug(
            message=f"Stopped timeout tracking for job {job_id}: {reason}",
            node_host=self._manager._host,
            node_port=self._manager._tcp_port,
            node_id=self._manager._node_id.short,
        ))

    async def cleanup_worker_extensions(self, job_id: str, worker_id: str) -> None:
        """Remove failed worker from extension tracking."""
        job = self._manager._job_manager.get_job_by_id(job_id)
        if not job or not job.timeout_tracking:
            return

        async with job.lock:
            job.timeout_tracking.active_workers_with_extensions.discard(worker_id)

        await self._manager._udp_logger.log(ServerDebug(
            message=f"Cleaned up extensions for worker {worker_id} in job {job_id}",
            node_host=self._manager._host,
            node_port=self._manager._tcp_port,
            node_id=self._manager._node_id.short,
        ))
```

### GateCoordinatedTimeout Cleanup

```python
class GateCoordinatedTimeout(TimeoutStrategy):
    async def stop_tracking(self, job_id: str, reason: str) -> None:
        """
        Stop tracking and notify gate.

        Sends final status update to gate so gate can clean up tracking.
        """
        job = self._manager._job_manager.get_job_by_id(job_id)
        if not job or not job.timeout_tracking:
            return

        async with job.lock:
            job.timeout_tracking.locally_timed_out = True
            job.timeout_tracking.timeout_reason = f"Tracking stopped: {reason}"

        # Send final status to gate
        if job.timeout_tracking.gate_addr:
            await self._send_final_status(job_id, reason)

        await self._manager._udp_logger.log(ServerDebug(
            message=f"Stopped timeout tracking for job {job_id}: {reason}",
            node_host=self._manager._host,
            node_port=self._manager._tcp_port,
            node_id=self._manager._node_id.short,
        ))

    async def cleanup_worker_extensions(self, job_id: str, worker_id: str) -> None:
        """Remove failed worker and send update to gate."""
        job = self._manager._job_manager.get_job_by_id(job_id)
        if not job or not job.timeout_tracking:
            return

        async with job.lock:
            job.timeout_tracking.active_workers_with_extensions.discard(worker_id)

        # Next progress report will reflect updated worker count

        await self._manager._udp_logger.log(ServerDebug(
            message=f"Cleaned up extensions for worker {worker_id} in job {job_id}",
            node_host=self._manager._host,
            node_port=self._manager._tcp_port,
            node_id=self._manager._node_id.short,
        ))

    async def _send_final_status(self, job_id: str, reason: str) -> None:
        """Send final status to gate for cleanup."""
        job = self._manager._job_manager.get_job_by_id(job_id)
        if not job or not job.timeout_tracking:
            return

        # Map reason to status
        status_map = {
            "completed": JobStatus.COMPLETED.value,
            "failed": JobStatus.FAILED.value,
            "cancelled": JobStatus.CANCELLED.value,
            "timed_out": JobStatus.TIMEOUT.value,
        }
        status = status_map.get(reason, JobStatus.FAILED.value)

        final_report = JobFinalStatus(
            job_id=job_id,
            datacenter=self._manager._datacenter,
            manager_id=self._manager._node_id.short,
            status=status,
            timestamp=time.monotonic(),
            fence_token=job.timeout_tracking.timeout_fence_token,
        )

        try:
            await self._manager.send_tcp(
                job.timeout_tracking.gate_addr,
                "job_final_status",
                final_report.dump()
            )
        except Exception as e:
            # Best-effort cleanup notification
            await self._manager._udp_logger.log(ServerDebug(
                message=f"Failed to send final status for {job_id}: {e}",
                node_host=self._manager._host,
                node_port=self._manager._tcp_port,
                node_id=self._manager._node_id.short,
            ))
```

### Manager Integration - Cleanup Hooks

```python
class ManagerServer:
    async def receive_cancel_job(
        self,
        addr: tuple[str, int],
        data: bytes,
        clock_time: int,
    ):
        """Handle job cancellation with timeout cleanup."""
        try:
            request = JobCancelRequest.load(data)

            # ... existing cancellation logic ...

            # NEW: Stop timeout tracking
            strategy = self._job_timeout_strategies.get(request.job_id)
            if strategy:
                await strategy.stop_tracking(request.job_id, "cancelled")
                self._job_timeout_strategies.pop(request.job_id, None)

            # ... existing response logic ...

        except Exception as e:
            await self.handle_exception(e, "receive_cancel_job")

    async def _handle_job_completion(self, job_id: str) -> None:
        """
        Handle job completion.

        Called when all workflows complete successfully.
        """
        # ... existing completion logic ...

        # Stop timeout tracking
        strategy = self._job_timeout_strategies.get(job_id)
        if strategy:
            await strategy.stop_tracking(job_id, "completed")
            self._job_timeout_strategies.pop(job_id, None)

    async def _handle_job_failure(self, job_id: str, reason: str) -> None:
        """
        Handle job failure.

        Called when job fails due to execution error.
        """
        # ... existing failure logic ...

        # Stop timeout tracking
        strategy = self._job_timeout_strategies.get(job_id)
        if strategy:
            await strategy.stop_tracking(job_id, "failed")
            self._job_timeout_strategies.pop(job_id, None)

    async def _timeout_job(self, job_id: str, reason: str) -> None:
        """
        Time out a job.

        NEW method - called by timeout strategies when timeout detected.
        """
        job = self._job_manager.get_job_by_id(job_id)
        if not job:
            return

        # Mark job as timed out
        async with job.lock:
            job.status = JobStatus.TIMEOUT.value

        # Cancel all workflows
        await self._cancel_all_workflows_for_job(job_id, reason="timeout")

        # Stop timeout tracking (idempotent)
        strategy = self._job_timeout_strategies.get(job_id)
        if strategy:
            await strategy.stop_tracking(job_id, "timed_out")
            self._job_timeout_strategies.pop(job_id, None)

        # Notify callback (gate or client)
        if job.callback_addr:
            await self._send_job_timeout_notification(job_id, reason)

        await self._udp_logger.log(ServerWarning(
            message=f"Job {job_id} timed out: {reason}",
            node_host=self._host,
            node_port=self._tcp_port,
            node_id=self._node_id.short,
        ))

    async def _handle_worker_failure(self, worker_id: str) -> None:
        """
        Handle worker failure.

        Clean up extension tracking for all jobs using this worker.
        """
        # ... existing worker failure logic ...

        # Clean up extension tracking
        for job in self._job_manager.iter_jobs():
            strategy = self._job_timeout_strategies.get(job.job_id)
            if strategy:
                # Check if this worker was executing workflows for this job
                has_workflows = any(
                    wf_info.assigned_worker_id == worker_id
                    for wf_info in job.workflows.values()
                )
                if has_workflows:
                    await strategy.cleanup_worker_extensions(job.job_id, worker_id)

    def _cleanup_job(self, job_id: str) -> None:
        """
        Clean up all state associated with a job.

        Called by periodic cleanup loop for old jobs.
        """
        # NEW: Clean up timeout strategy
        strategy = self._job_timeout_strategies.pop(job_id, None)
        if strategy:
            # Fire-and-forget stop_tracking
            self._task_runner.run(strategy.stop_tracking, job_id, "cleanup")

        # ... existing cleanup logic ...

        self._task_runner.run(self._job_manager.complete_job, job_id)
        self._job_leaders.pop(job_id, None)
        # ... rest of cleanup ...
```

### Gate Cleanup Integration

```python
class GateJobTracker:
    async def handle_final_status(self, report: JobFinalStatus) -> None:
        """
        Handle final status from manager (cleanup trigger).

        Removes job from tracking when it reaches terminal state.
        """
        async with self._lock:
            info = self._tracked_jobs.get(report.job_id)
            if not info:
                return

            # Update DC status
            info.dc_status[report.datacenter] = report.status

            # Check if all DCs have reached terminal state
            all_terminal = all(
                status in {
                    JobStatus.COMPLETED.value,
                    JobStatus.FAILED.value,
                    JobStatus.CANCELLED.value,
                    JobStatus.TIMEOUT.value,
                }
                for status in info.dc_status.values()
            )

            if all_terminal:
                # Clean up tracking
                self._tracked_jobs.pop(report.job_id, None)

                await self._gate._logger.log(ServerDebug(
                    message=f"Cleaned up timeout tracking for job {report.job_id}",
                    node_host=self._gate._host,
                    node_port=self._gate._tcp_port,
                    node_id=self._gate._node_id.short,
                ))


class GateServer:
    async def receive_job_final_status(
        self,
        addr: tuple[str, int],
        data: bytes,
        clock_time: int,
    ):
        """Receive final status from manager for cleanup."""
        try:
            report = JobFinalStatus.load(data)
            await self._job_tracker.handle_final_status(report)
        except Exception as e:
            await self.handle_exception(e, "receive_job_final_status")
```

### New Protocol Message

```python
@dataclass
class JobFinalStatus(Message):
    """
    Manager → Gate: Final job status for cleanup.

    Sent when job reaches terminal state (completed/failed/cancelled/timed out).
    Gate uses this to clean up timeout tracking for the job.
    """
    job_id: str
    datacenter: str
    manager_id: str
    status: str  # JobStatus.COMPLETED/FAILED/CANCELLED/TIMEOUT
    timestamp: float
    fence_token: int
```

### Cleanup State Diagram

```
Job Lifecycle with Cleanup:

                    ┌─────────────────┐
                    │  Job Submitted  │
                    └────────┬────────┘
                             ↓
                    ┌─────────────────┐
                    │ start_tracking()│
                    │  (Strategy)     │
                    └────────┬────────┘
                             ↓
                ┌────────────┴────────────┐
                │                         │
                ↓                         ↓
        ┌──────────────┐         ┌──────────────┐
        │   Running    │         │  Cancelled   │
        └──────┬───────┘         └──────┬───────┘
               │                        │
        ┌──────┴──────┐                 │
        ↓             ↓                 ↓
   ┌─────────┐  ┌──────────┐    ┌──────────────┐
   │Completed│  │ Failed   │    │  Timed Out   │
   └────┬────┘  └────┬─────┘    └──────┬───────┘
        │            │                  │
        └────────────┴──────────────────┘
                     ↓
            ┌─────────────────┐
            │ stop_tracking() │
            │   (Strategy)    │
            └────────┬────────┘
                     ↓
            ┌─────────────────┐
            │ Strategy removed│
            │ from tracking   │
            └─────────────────┘
                     ↓
            ┌─────────────────┐
            │  _cleanup_job() │
            │ (periodic loop) │
            └─────────────────┘
```

### Cleanup Guarantees

✅ **Idempotent Cleanup**: `stop_tracking()` safe to call multiple times
✅ **No Zombie Timeouts**: Strategy removed immediately when job terminal
✅ **Extension Cleanup**: Worker extensions removed on worker failure
✅ **Memory Safety**: Timeout state cleaned up with job
✅ **Multi-DC Sync**: Gate cleans up when ALL DCs report terminal state
✅ **Graceful Degradation**: Cleanup failures logged but don't block job completion

### Edge Cases Handled

#### Race: Job completes while timeout check running

```python
async def check_timeout(self, job_id: str) -> tuple[bool, str]:
    """Check with terminal state protection."""
    job = self._manager._job_manager.get_job_by_id(job_id)
    if not job or not job.timeout_tracking:
        return False, ""

    # Check terminal state FIRST (race protection)
    if job.status in {
        JobStatus.COMPLETED.value,
        JobStatus.FAILED.value,
        JobStatus.CANCELLED.value,
        JobStatus.TIMEOUT.value,
    }:
        return False, ""  # Don't timeout terminal jobs

    # ... rest of timeout check ...
```

#### Race: Worker fails while extension granted

```python
async def _handle_worker_failure(self, worker_id: str) -> None:
    """Worker failure with extension cleanup."""
    # Remove worker from ALL job extension tracking
    for job in self._job_manager.iter_jobs():
        strategy = self._job_timeout_strategies.get(job.job_id)
        if strategy:
            await strategy.cleanup_worker_extensions(job.job_id, worker_id)

    # If job has no more workers, may need to timeout
    # (handled by regular timeout check loop)
```

#### Double cleanup: Job cancelled then cleaned up

```python
async def stop_tracking(self, job_id: str, reason: str) -> None:
    """Idempotent cleanup."""
    job = self._manager._job_manager.get_job_by_id(job_id)
    if not job or not job.timeout_tracking:
        return  # Already cleaned up

    # Safe to mark multiple times
    async with job.lock:
        job.timeout_tracking.locally_timed_out = True
```

### Observability for Cleanup

```python
# Cleanup metrics
timeout_tracking_stopped_total{reason="completed|failed|cancelled|timed_out|cleanup"} 100
timeout_strategies_active_count 50  # Current active strategies
worker_extensions_cleaned_total{reason="worker_failure"} 10

# Cleanup logs
ServerDebug: "Stopped timeout tracking for job abc123: completed"
ServerDebug: "Cleaned up extensions for worker worker-1 in job abc123"
ServerDebug: "Cleaned up timeout tracking for job abc123 (all DCs terminal)"
```

---

## Summary: Lifecycle Management

AD-34 timeout tracking now includes comprehensive lifecycle management:

✅ **Start Tracking**: `start_tracking()` called on job submission
✅ **Stop Tracking**: `stop_tracking()` called on job completion/failure/cancellation/timeout
✅ **Extension Cleanup**: `cleanup_worker_extensions()` called on worker failure
✅ **Periodic Cleanup**: `_cleanup_job()` removes stale timeout strategies
✅ **Idempotent Operations**: Safe to call cleanup multiple times
✅ **Race Protection**: Terminal state checked before timeout
✅ **Multi-DC Sync**: Gate cleans up when all DCs report final status
✅ **Memory Safety**: No timeout tracking leaks

**Critical Rule**: Timeout strategies MUST be removed from `_job_timeout_strategies` when job reaches terminal state to prevent zombie timeouts and memory leaks.

# AD-35: Vivaldi Network Coordinates with Role-Aware Failure Detection

**Status**: Proposed
**Related**: AD-29 (Peer Confirmation), AD-30 (Hierarchical Failure Detection), AD-33 (Federated Health Monitoring)

---

## Problem Statement

The current failure detection system has three critical gaps for globally-distributed, multi-tier architectures:

### 1. **Geographic Latency Blindness**
Gates detecting managers across datacenters use **static timeouts** that don't account for network distance:
- Same-region manager (10ms RTT): 30s timeout is too conservative
- Cross-continent manager (150ms RTT): 30s timeout causes false positives
- Intercontinental manager (300ms RTT): 30s timeout is dangerously aggressive

**Result**: False positives from geographic latency variance, or overly conservative timeouts that delay failure detection.

### 2. **Role-Agnostic Confirmation Strategy**
All peers are treated identically during unconfirmed peer cleanup (AD-29):
- **Gates** (cross-DC, high-latency): Need proactive confirmation with retries
- **Managers** (moderate load): Need load-aware confirmation
- **Workers** (extreme load): Probing stressed workers adds MORE load

**Result**: Either we're too aggressive (removing legitimate slow peers) or too passive (accumulating memory from dead peers).

### 3. **No Network Topology Learning**
The system cannot learn or adapt to actual network conditions:
- Static datacenter configuration required
- No adaptation to route changes, CDN shifts, or network degradation
- Cannot predict RTT to peers without direct measurement

**Result**: Manual tuning required for each deployment topology, and no automatic adaptation to changing conditions.

---

## Solution: Vivaldi Coordinates + Role-Aware Detection + Lifecycle States

Combine three architectural improvements:

1. **Vivaldi Network Coordinates**: Learn network topology and predict RTT
2. **Role-Aware Confirmation Strategies**: Tailor timeout/confirmation logic to peer role (Gate/Manager/Worker)
3. **UNCONFIRMED Lifecycle State**: Explicit state for unconfirmed peers (from AD-29 analysis)

---

## Part 1: Vivaldi Network Coordinates

### What is Vivaldi?

Vivaldi is a **decentralized network coordinate system** where each node maintains a position in a virtual coordinate space. The distance between two nodes in this space approximates their network RTT.

**Key Properties**:
- ✅ **Decentralized**: Each node calculates its own coordinates independently
- ✅ **Adaptive**: Coordinates converge as network conditions change
- ✅ **Predictive**: Estimate RTT to nodes without direct measurement
- ✅ **Low overhead**: Coordinates are small (~50 bytes) and piggyback on existing messages

### How It Works

Each node maintains a **VivaldiCoordinate**:
```python
@dataclass
class VivaldiCoordinate:
    position: list[float]  # N-dimensional coordinate (typically 4D)
    height: float          # Models asymmetric routes
    error: float          # Prediction confidence (lower = better)
```

**Update Algorithm** (simplified):
1. Node A sends ping to Node B with A's coordinate
2. Node B responds with ack, B's coordinate, and measured RTT
3. Node A updates its position to reduce prediction error:
   ```
   predicted_rtt = distance(A.coord, B.coord)
   error = measured_rtt - predicted_rtt
   A.position += delta * error * unit_vector(B.coord → A.coord)
   ```

**Convergence**: Typically 10-20 measurement rounds (~10-20 seconds with 1s probe interval).

### Integration with SWIM

Vivaldi coordinates **piggyback on existing SWIM messages** with zero additional probes:

```python
# Ping message (already exists in SWIM)
{
    "type": "ping",
    "from": ("10.0.1.5", 8000),
    "seq": 42,
    "vivaldi_coord": {  # NEW: Add coordinate (50 bytes)
        "position": [1.2, -0.5, 3.1, 0.8],
        "height": 0.3,
        "error": 0.15,
    },
}

# Ack message (already exists in SWIM)
{
    "type": "ack",
    "from": ("10.0.2.7", 8000),
    "seq": 42,
    "rtt_ms": 145.3,  # Measured RTT
    "vivaldi_coord": {  # NEW: Add coordinate (50 bytes)
        "position": [5.1, 2.3, -1.2, 0.4],
        "height": 0.5,
        "error": 0.22,
    },
}
```

**Total overhead**: ~50-80 bytes per message (negligible compared to existing SWIM gossip).

---

## Part 2: Role-Aware Failure Detection

### Peer Roles

Classify peers into three roles based on their position in the architecture:

```python
class PeerRole(Enum):
    GATE = "gate"          # Cross-datacenter coordinators
    MANAGER = "manager"    # Datacenter-local job orchestrators
    WORKER = "worker"      # Load test generators (extreme load)
```

**Role Detection**:
- **Explicit**: Role gossiped in membership messages
- **Implicit**: Inferred from port range, hostname pattern, or configuration

### Role-Specific Confirmation Strategies

Each role has a tailored strategy for handling unconfirmed peers:

```python
@dataclass
class RoleBasedConfirmationStrategy:
    passive_timeout: float             # Base timeout before action
    enable_proactive_confirmation: bool # Whether to actively probe
    confirmation_attempts: int         # Number of retries
    attempt_interval: float           # Delay between retries
    latency_aware: bool               # Use Vivaldi for timeout adjustment
    use_vivaldi: bool                 # Enable Vivaldi coordinate system
    load_multiplier_max: float        # Max timeout multiplier under load
```

**Strategies by Role**:

| Role | Passive Timeout | Proactive Confirmation | Vivaldi | Load Multiplier | Rationale |
|------|----------------|------------------------|---------|-----------------|-----------|
| **Gate** | 120s | ✅ Yes (5 attempts) | ✅ Yes | 3x | Cross-DC, high-latency, need high confidence |
| **Manager** | 90s | ✅ Yes (3 attempts) | ✅ Yes | 5x | Moderate load, mission-critical |
| **Worker** | 180s | ❌ No | ❌ No | 10x | Extreme load, passive only (don't add more load) |

### Adaptive Timeout Calculation

For **Gates and Managers** (using Vivaldi):
```python
def get_adaptive_timeout(peer: NodeAddress, base_timeout: float) -> float:
    # Estimate RTT using Vivaldi coordinates
    estimated_rtt = vivaldi.estimate_rtt(peer)

    # Reference RTT (same-datacenter baseline)
    reference_rtt = 10.0  # ms

    # Latency multiplier
    latency_multiplier = min(10.0, max(1.0, estimated_rtt / reference_rtt))

    # Load multiplier (from LHM - existing system)
    load_multiplier = get_lhm_multiplier()

    # Confidence adjustment (higher error → more conservative)
    confidence_adjustment = 1.0 + (vivaldi.get_error() / 10.0)

    # Combined adaptive timeout
    return base_timeout * latency_multiplier * load_multiplier * confidence_adjustment
```

**Example**:
```python
# Base timeout: 5 seconds
# Gate in US-East detecting managers:

Manager in US-East:     estimated_rtt=5ms   → timeout = 5s × 1.0 × 1.0 × 1.05 = 5.25s
Manager in US-West:     estimated_rtt=50ms  → timeout = 5s × 5.0 × 1.0 × 1.08 = 27s
Manager in EU:          estimated_rtt=100ms → timeout = 5s × 10.0 × 1.2 × 1.12 = 67s
Manager in Asia:        estimated_rtt=200ms → timeout = 5s × 10.0 × 1.5 × 1.15 = 86s
                                                       (capped at max)
```

---

## Part 3: UNCONFIRMED Lifecycle State

### Current Problem (from AD-29)

Peers discovered via gossip are immediately marked `ALIVE`, but AD-29 prevents suspecting unconfirmed peers. This creates ambiguity:
- Is an unconfirmed peer "alive but not yet confirmed" or "dead but never joined"?
- How long do we wait before cleanup?

### Solution: Explicit UNCONFIRMED State

Add a new lifecycle state to the incarnation tracker:

```python
class NodeLifecycleState(Enum):
    UNCONFIRMED = b"UNCONFIRMED"  # Discovered but never confirmed
    ALIVE = b"ALIVE"               # Confirmed and healthy
    SUSPECT = b"SUSPECT"           # Suspected of failure
    DEAD = b"DEAD"                 # Confirmed dead
```

### State Transition Diagram

```
       [Gossip Discovery]
              ↓
         UNCONFIRMED ──────[role-aware timeout]──────→ [Removed from membership]
              ↓                                         (not marked DEAD)
      [First successful bidirectional
       communication: ping/ack]
              ↓
            ALIVE ──────[probe timeout]──────→ SUSPECT ──────[suspicion timeout]──────→ DEAD
              ↑                                    ↓
              └──────────[refutation]──────────────┘
```

**Key Transitions**:
1. **Discovery → UNCONFIRMED**: Peer added via gossip, no confirmation yet
2. **UNCONFIRMED → ALIVE**: First successful ping/ack (bidirectional confirmation)
3. **UNCONFIRMED → Removed**: Role-aware timeout expires without confirmation
4. **ALIVE → SUSPECT → DEAD**: Existing SWIM failure detection (unchanged)

---

## Part 4: Combined Architecture

### Component Diagram

```
┌──────────────────────────────────────────────────────────────────────────┐
│                         HealthAwareServer                                │
├──────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│  ┌─────────────────────────────────────────────────────────────────┐    │
│  │              VivaldiCoordinateSystem                            │    │
│  │  - Maintains own coordinate in virtual space                    │    │
│  │  - Updates coordinate on each ping/ack RTT measurement          │    │
│  │  - Estimates RTT to peers using coordinate distance             │    │
│  │  - Gossips coordinate in SWIM messages (50 byte overhead)       │    │
│  └────────────────────┬────────────────────────────────────────────┘    │
│                       │                                                  │
│                       ▼                                                  │
│  ┌─────────────────────────────────────────────────────────────────┐    │
│  │         RoleAwareConfirmationManager                            │    │
│  │  - Classifies peers by role (Gate/Manager/Worker)               │    │
│  │  - Applies role-specific confirmation strategies                │    │
│  │  - Combines Vivaldi RTT + LHM load + confidence                 │    │
│  │  - Proactively confirms Gates/Managers, passive for Workers     │    │
│  └────────────────────┬────────────────────────────────────────────┘    │
│                       │                                                  │
│                       ▼                                                  │
│  ┌─────────────────────────────────────────────────────────────────┐    │
│  │            IncarnationTracker (Enhanced)                        │    │
│  │  - Tracks node lifecycle: UNCONFIRMED → ALIVE → SUSPECT → DEAD  │    │
│  │  - New: UNCONFIRMED state for unconfirmed peers                 │    │
│  │  - Enforces AD-29: Only ALIVE peers can transition to SUSPECT   │    │
│  └─────────────────────────────────────────────────────────────────┘    │
│                                                                          │
└──────────────────────────────────────────────────────────────────────────┘
```

### Workflow: Peer Discovery to Confirmation

```
1. Gate discovers Manager via gossip
   ├─> IncarnationTracker: Mark as UNCONFIRMED
   ├─> VivaldiCoordinateSystem: No coordinate yet (use conservative default)
   └─> RoleAwareConfirmationManager: Start passive timeout (120s for Gate role)

2. Gate sends SWIM ping to Manager
   ├─> Include Gate's Vivaldi coordinate in ping message
   └─> Measure RTT start time

3. Manager responds with ack
   ├─> Include Manager's Vivaldi coordinate in ack
   └─> Gate measures RTT: 145ms

4. Gate processes ack
   ├─> VivaldiCoordinateSystem.update_coordinate(manager, manager_coord, 145ms)
   │   ├─> Update Gate's position to minimize prediction error
   │   └─> Store Manager's coordinate for future distance calculations
   │
   ├─> IncarnationTracker: Transition Manager from UNCONFIRMED → ALIVE
   │   └─> Manager is now confirmed (successful bidirectional communication)
   │
   └─> RoleAwareConfirmationManager: Cancel passive timeout timer
       └─> Manager is confirmed, no cleanup needed

5. Future suspicion timeouts for this Manager
   ├─> VivaldiCoordinateSystem.estimate_rtt(manager) → 145ms (from coordinates)
   ├─> Calculate adaptive timeout: base × latency_multiplier × lhm × confidence
   └─> Use adaptive timeout for suspicion (e.g., 67s instead of 5s)
```

### Workflow: Unconfirmed Peer Cleanup

```
1. Gate discovers Manager via gossip (Manager never joins)
   ├─> IncarnationTracker: Mark as UNCONFIRMED
   └─> RoleAwareConfirmationManager: Start passive timeout (120s)

2. 60 seconds elapse, no confirmation
   └─> RoleAwareConfirmationManager: Check strategy for MANAGER role
       ├─> enable_proactive_confirmation = True
       ├─> confirmation_attempts = 3
       └─> Schedule proactive confirmation attempts

3. Attempt 1: Send ping for confirmation
   ├─> Wait 5 seconds for ack
   └─> No response

4. Attempt 2: Send ping for confirmation (5s later)
   ├─> Wait 5 seconds for ack
   └─> No response

5. Attempt 3: Send ping for confirmation (5s later)
   ├─> Wait 5 seconds for ack
   └─> No response

6. All attempts exhausted (135s total elapsed)
   ├─> RoleAwareConfirmationManager: Remove Manager from membership
   ├─> IncarnationTracker: Remove node (NOT marked as DEAD)
   ├─> Metrics: Increment "unconfirmed_peers_removed_manager"
   └─> Audit: Record UNCONFIRMED_PEER_REMOVED event
```

---

## Part 5: Benefits

### For Gates (Cross-Datacenter Detection)

**Before** (Static Timeouts):
```
Gate → Manager (US-East, 10ms):   30s timeout → Too conservative
Gate → Manager (US-West, 50ms):   30s timeout → Reasonable
Gate → Manager (EU, 150ms):       30s timeout → Too aggressive (false positives)
Gate → Manager (Asia, 300ms):     30s timeout → Very aggressive (many false positives)
```

**After** (Vivaldi + Role-Aware):
```
Gate → Manager (US-East, 10ms):   5s timeout   → Fast detection, no false positives
Gate → Manager (US-West, 50ms):   27s timeout  → Latency-adjusted
Gate → Manager (EU, 150ms):       67s timeout  → Accounts for cross-Atlantic latency
Gate → Manager (Asia, 300ms):     86s timeout  → Conservative for intercontinental
```

**Improvements**:
- ✅ **6x faster detection** for nearby peers
- ✅ **Zero false positives** from geographic latency
- ✅ **Automatic adaptation** to network topology changes

### For Managers (High Update Load)

**Before** (Static Timeouts + LHM):
```
Manager → Manager (under load):  30s × 2.5 LHM = 75s timeout
```

**After** (Vivaldi + LHM + Role-Aware):
```
Manager → Manager (same DC, under load):  5s × 1.0 latency × 2.5 LHM × 1.1 confidence = 13.75s

Benefits:
- Vivaldi detects same-DC peers (low latency) → Use tighter base timeout
- LHM scales for load spikes (existing mechanism preserved)
- Confidence adjustment prevents premature detection during convergence
```

**Improvements**:
- ✅ **5.4x faster detection** when both peers healthy
- ✅ **Graceful degradation** under load via LHM
- ✅ **No spurious failures** during Vivaldi convergence

### For Workers (Extreme Load)

**Before**:
```
Manager → Worker:  Proactive confirmation attempts add load to stressed worker
```

**After** (Passive-Only Strategy):
```
Manager → Worker:  180s passive timeout, no probing
                   Under extreme load: 180s × 10 LHM = 1800s (30 minutes)

Benefits:
- Workers never receive proactive confirmation probes
- Very high timeout tolerates multi-minute busy periods
- Workers are expendable (can be removed without suspicion/DEAD marking)
```

**Improvements**:
- ✅ **Zero additional load** on stressed workers
- ✅ **30-minute tolerance** for extreme load test scenarios
- ✅ **Clean removal** without protocol violations

---

## Part 6: Dual-Purpose Vivaldi (Failure Detection + Routing)

Vivaldi coordinates serve **two purposes** in the architecture:

### 1. Failure Detection (This AD)
- Adaptive timeouts for cross-datacenter suspicion
- Reduces false positives from geographic latency

### 2. Job Routing (Future: AD-36)
Gates can use Vivaldi to route jobs to optimal datacenters:

```python
class GateJobRouter:
    def select_datacenter_for_job(self, job_id: str) -> str:
        """
        Select datacenter using Vivaldi distance + health + load.
        """
        candidates = []

        for dc_name, dc_leader_addr in self.datacenter_leaders.items():
            # Filter unhealthy DCs
            if not self.is_datacenter_healthy(dc_name):
                continue

            # Estimate RTT to DC leader using Vivaldi
            estimated_rtt = self.vivaldi.estimate_rtt(dc_leader_addr)

            # Get DC load from gossip (LHM)
            dc_load = self.get_datacenter_load(dc_name)

            # Score = RTT × load (lower is better)
            # Balances "close and fast" with "not overloaded"
            score = estimated_rtt * dc_load

            candidates.append((dc_name, score))

        # Return DC with best score
        candidates.sort(key=lambda x: x[1])
        return candidates[0][0] if candidates else None
```

**Result**: Jobs routed to **closest available datacenter** based on learned network topology, not static configuration.

---

## Part 7: Implementation Phases

### Phase 1: Vivaldi Coordinate System (Standalone)
- ✅ Implement VivaldiCoordinateSystem class
- ✅ Integrate with SWIM ping/ack for RTT measurement
- ✅ Add coordinate to gossip messages (~50 byte overhead)
- ✅ Test coordinate convergence (10-20 rounds)

### Phase 2: UNCONFIRMED Lifecycle State
- ✅ Add UNCONFIRMED to NodeLifecycleState enum
- ✅ Update IncarnationTracker to support UNCONFIRMED → ALIVE transition
- ✅ Mark new peers as UNCONFIRMED on discovery
- ✅ Transition to ALIVE on first successful bidirectional communication

### Phase 3: Role-Aware Confirmation Strategies
- ✅ Implement PeerRole classification
- ✅ Define RoleBasedConfirmationStrategy per role
- ✅ Implement role-specific cleanup logic:
  - Gates: Proactive confirmation with 5 retries
  - Managers: Proactive confirmation with 3 retries
  - Workers: Passive removal only (no probes)

### Phase 4: Integration and Adaptive Timeouts
- ✅ Integrate Vivaldi RTT estimates with suspicion timeouts
- ✅ Combine Vivaldi latency multiplier + LHM load multiplier + confidence adjustment
- ✅ Update HierarchicalFailureDetector to accept adaptive timeouts
- ✅ Add metrics and observability

### Phase 5: Job Routing (Future - AD-36)
- ⏳ Implement GateJobRouter using Vivaldi distance
- ⏳ Add DC health + load balancing
- ⏳ Test cross-datacenter job routing

---

## Part 8: Tradeoffs and Limitations

### Tradeoffs

| Aspect | Benefit | Cost |
|--------|---------|------|
| **Vivaldi Overhead** | Adaptive timeouts, topology learning | 50-80 bytes per message |
| **Coordinate Convergence** | Accurate RTT prediction | 10-20 seconds initial convergence |
| **Role Classification** | Tailored strategies per role | Requires role detection logic |
| **UNCONFIRMED State** | Explicit lifecycle, clear semantics | Additional state to manage |
| **Proactive Confirmation** | Fewer false removals for Gates/Managers | Additional network probes |

### Limitations

1. **Vivaldi Accuracy**: Triangle inequality violations in real networks can reduce accuracy
   - **Mitigation**: Use height component to model asymmetric routes
   - **Impact**: ~10-20% RTT prediction error acceptable for timeout adjustment

2. **Role Detection**: Requires correct role classification
   - **Mitigation**: Multiple detection methods (explicit gossip, port range, config)
   - **Impact**: Misclassified role uses suboptimal strategy (still safe, just not optimal)

3. **Memory Overhead**: Storing coordinates for all peers
   - **Mitigation**: 4D coordinate = 40 bytes per peer (negligible)
   - **Impact**: For 1000 peers: 40KB total (insignificant)

4. **Cold Start**: New nodes have high error initially
   - **Mitigation**: Confidence adjustment makes timeouts more conservative during convergence
   - **Impact**: Slightly slower detection for first 10-20 seconds, then converges

---

## Part 9: Metrics and Observability

### New Metrics

```python
# Vivaldi metrics
vivaldi_coordinate_updates          # Counter: Coordinate update events
vivaldi_prediction_error            # Histogram: |predicted_rtt - measured_rtt|
vivaldi_convergence_time            # Histogram: Time to converge (error < threshold)

# Role-aware confirmation metrics
unconfirmed_peers_removed_gate      # Counter: Gates removed due to no confirmation
unconfirmed_peers_removed_manager   # Counter: Managers removed due to no confirmation
unconfirmed_peers_removed_worker    # Counter: Workers removed due to no confirmation
confirmation_attempts_total         # Counter: Proactive confirmation attempts
confirmation_attempts_success       # Counter: Successful late confirmations

# Lifecycle state metrics
peers_unconfirmed                   # Gauge: Peers currently in UNCONFIRMED state
peers_alive                         # Gauge: Peers currently in ALIVE state
peers_suspect                       # Gauge: Peers currently in SUSPECT state
peers_dead                          # Gauge: Peers currently in DEAD state
transitions_unconfirmed_to_alive    # Counter: UNCONFIRMED → ALIVE transitions
transitions_unconfirmed_to_removed  # Counter: UNCONFIRMED → Removed transitions

# Adaptive timeout metrics
adaptive_timeout_applied            # Histogram: Final adaptive timeout values
latency_multiplier                  # Histogram: Vivaldi latency multiplier
load_multiplier                     # Histogram: LHM load multiplier
confidence_adjustment               # Histogram: Vivaldi confidence adjustment
```

### Debug Endpoints

```python
# GET /debug/vivaldi/coordinate
{
    "position": [1.2, -0.5, 3.1, 0.8],
    "height": 0.3,
    "error": 0.15,
    "peer_count": 47,
    "convergence_status": "converged"
}

# GET /debug/vivaldi/peers
[
    {
        "peer": "10.0.1.5:8000",
        "estimated_rtt_ms": 145.3,
        "measured_rtt_samples": [143.1, 147.2, 145.5],
        "prediction_error_ms": 2.8,
        "adaptive_timeout_s": 67.2
    },
    ...
]

# GET /debug/peers/unconfirmed
[
    {
        "peer": "10.0.2.7:8000",
        "role": "manager",
        "discovered_at": "2026-01-10T10:23:45Z",
        "age_seconds": 47.3,
        "passive_timeout_remaining": 72.7,
        "confirmation_attempts": 1,
        "next_attempt_in": 5.0
    },
    ...
]
```

---

## Part 10: Success Criteria

This AD is successful when:

1. ✅ **Zero false positives from geographic latency**
   - Measured: `suspicions_started{reason="timeout"}` for cross-DC peers
   - Target: <1% false positive rate

2. ✅ **Faster detection for nearby peers**
   - Measured: Time from failure to detection for same-DC peers
   - Target: <10s (currently ~30s)

3. ✅ **No additional load on workers**
   - Measured: `confirmation_attempts_total{role="worker"}` = 0
   - Target: Zero proactive probes to workers

4. ✅ **Vivaldi convergence**
   - Measured: `vivaldi_prediction_error` < 20% of measured RTT
   - Target: Converges within 20 seconds of node start

5. ✅ **Clean unconfirmed peer removal**
   - Measured: `peers_unconfirmed` gauge remains bounded
   - Target: No unbounded growth over time

6. ✅ **Dual-purpose utility**
   - Measured: Vivaldi used for both failure detection AND job routing
   - Target: Single coordinate system serves both use cases

---

## Part 11: Related Work

### Vivaldi in Production Systems

1. **Serf/Consul (HashiCorp)**:
   - Uses Vivaldi for network tomography
   - Helps route RPC requests through nearby nodes
   - Documented: https://github.com/hashicorp/serf/blob/master/docs/internals/coordinates.html.markdown

2. **Cassandra**:
   - Uses Vivaldi-like coordinates for replica placement
   - Dynamic snitch adapts routing based on measured latency

3. **Research**:
   - Original Vivaldi paper: "Vivaldi: A Decentralized Network Coordinate System" (Dabek et al., SIGCOMM 2004)
   - 98% accuracy for predicting RTT in PlanetLab experiments

### Role-Aware Failure Detection

Inspired by:
- **Google Chubby**: Different timeout strategies for different client types
- **ZooKeeper**: Session timeout negotiation based on client capabilities
- **etcd**: Adaptive timeouts based on observed client latency

---

## Part 5: Confidence-Aware RTT Estimation (Routing-Safe)

Vivaldi estimates must be used **conservatively** for routing and failure detection. The robust approach is to use an
**upper-confidence-bound (UCB)** RTT that incorporates coordinate error and staleness.

### Coordinate Quality

```python
def coordinate_quality(sample_count: int, error_ms: float, staleness_s: float) -> float:
    sample_quality = min(1.0, sample_count / MIN_SAMPLES_FOR_ROUTING)
    error_quality = min(1.0, ERROR_GOOD_MS / max(error_ms, 1.0))
    staleness_quality = 1.0 if staleness_s <= COORD_TTL_S else COORD_TTL_S / staleness_s
    return max(0.0, min(1.0, sample_quality * error_quality * staleness_quality))
```

### RTT UCB Formula

```python
def estimate_rtt_ucb_ms(local, remote) -> float:
    if local is None or remote is None:
        rtt_hat_ms = RTT_DEFAULT_MS
        sigma_ms = SIGMA_DEFAULT_MS
    else:
        rtt_hat_ms = vivaldi_distance(local, remote)
        sigma_ms = clamp(local.error_ms + remote.error_ms, SIGMA_MIN_MS, SIGMA_MAX_MS)

    return clamp(rtt_hat_ms + K_SIGMA * sigma_ms, RTT_MIN_MS, RTT_MAX_MS)
```

**Robustness rules**:
- Missing or low-quality coordinates **never exclude** a peer/DC.
- Use conservative defaults until coordinates converge.
- Always cap RTT estimates to avoid score blowups.

---

## Part 6: Timing Diagram (Ping/Ack, Confirmation, and Cleanup)

```
Time →

Gate                     Manager
 |---- gossip --------->|  (UNCONFIRMED)
 |---- ping + coord ---->|
 |<--- ack + coord + RTT |
 |  update coord         |
 |  confirm peer         |
 |  cancel timeout       |
 |                       |
 |---- periodic ping ---->|
 |<--- ack --------------|
 |  adaptive timeout      |
 |  suspicion timer tuned |

Unconfirmed path:
 |---- gossip --------->|  (UNCONFIRMED)
 |---- ping + coord ---->|
 |  (no ack)             |
 |---- retry (role-based)|
 |  (no ack)             |
 |-- timeout expires --> remove from membership
```

---

## Part 7: AD-17/AD-36 Integration Invariants

The AD-17 fallback chain is the safety backbone. Vivaldi inputs must **never override** the health buckets.

**Invariant rules**:
1. **Bucket-first ordering**: HEALTHY > BUSY > DEGRADED (UNHEALTHY excluded)
2. **Vivaldi only ranks within a chosen bucket**
3. **Confidence-aware RTT** is used for ranking and timeouts (UCB)
4. **Hysteresis** required to prevent routing churn (see AD-36)

---

## Part 8: Routing-Safe Inputs and Defaults

**Inputs used by AD-35/AD-36**:
- Vivaldi coordinate: position, height, error, sample_count, updated_at
- LHM load multiplier and recent probe health
- Peer role (Gate/Manager/Worker)
- Coordinate staleness (seconds since update)

**Defaults when missing**:
- RTT defaults to conservative `RTT_DEFAULT_MS`
- Error defaults to `SIGMA_DEFAULT_MS`
- Quality defaults to 0 (no penalty removal until samples arrive)

---

## Part 9: Hysteresis and Coordinate Quality Gates

To avoid routing churn and false positives, the system must:

- Enter **Coordinate-Unaware Mode** if local coordinate quality is below thresholds
- Apply **hold-down** windows for routing decisions
- Require **minimum improvement** before switching primary DCs
- Use **cooldowns** after dispatch failure to a DC

These mechanisms are mandatory for robustness under high load and WAN variability.

---

## Part 10: Failure-Detection Timing Diagram (Role-Aware)

```
Time →

Gate (role-aware)          Manager (role-aware)
 |-- ping (coord) -------->|
 |<-- ack (coord + RTT) ----|
 |-- adaptive timeout ------|
 |-- proactive confirm (N) ->|
 |-- role-aware cleanup -----|
```

Workers skip proactive confirmation and rely on passive timeouts only.

---

## Part 11: Observability

**Metrics**:
- `vivaldi_coord_quality{peer}`
- `vivaldi_rtt_ucb_ms{peer}`
- `peer_confirmation_attempts_total{role}`
- `unconfirmed_cleanup_total{role,reason}`
- `adaptive_timeout_seconds{role}`

**Logs**:
- `RoleConfirmationAttempt` with role, attempts, outcome
- `PeerConfirmed` with RTT, error, samples
- `PeerUnconfirmedCleanup` with reason and elapsed

---

## Part 12: Alternatives Considered

### Alternative 1: Static Per-Datacenter Timeouts

**Approach**: Configure different timeouts for each datacenter pair manually.

**Pros**:
- ✅ Simpler implementation
- ✅ No coordinate system needed

**Cons**:
- ❌ Requires manual configuration for every datacenter pair (O(n²))
- ❌ Cannot adapt to network changes
- ❌ No learning of actual topology
- ❌ Doesn't help with job routing

**Verdict**: Rejected - doesn't scale, no adaptation.

### Alternative 2: Exponential Backoff for All Timeouts

**Approach**: Start with short timeout, double on each false positive.

**Pros**:
- ✅ Simple to implement
- ✅ Eventually converges to safe timeout

**Cons**:
- ❌ Many false positives during convergence
- ❌ Per-peer state required
- ❌ Doesn't distinguish legitimate slowness from failure
- ❌ No topology learning

**Verdict**: Rejected - too many false positives during learning phase.

### Alternative 3: Ping-Based Latency Measurement Only (No Vivaldi)

**Approach**: Measure RTT during pings, adjust timeouts based on measured RTT.

**Pros**:
- ✅ Simpler than Vivaldi
- ✅ Direct measurement is accurate

**Cons**:
- ❌ Cannot predict RTT to nodes you haven't measured yet
- ❌ No benefit for job routing (need to probe all candidates)
- ❌ Slower convergence (need N measurements for N peers)

**Verdict**: Rejected - Vivaldi provides prediction without measurement, crucial for routing.

### Alternative 4: Vivaldi Only (No Role-Aware Logic)

**Approach**: Use Vivaldi for all peers uniformly.

**Pros**:
- ✅ Simpler than role-aware logic
- ✅ Handles latency variance

**Cons**:
- ❌ Still probes stressed workers (adds load)
- ❌ Doesn't account for role-specific needs
- ❌ Workers don't benefit from Vivaldi (same-DC as manager)

**Verdict**: Rejected - role-aware logic is critical for worker protection.

---

## Conclusion

**AD-35 combines three orthogonal improvements** that together provide a robust, adaptive, globally-aware failure detection system:

1. **Vivaldi Coordinates**: Learn network topology, predict RTT, eliminate geographic false positives
2. **Role-Aware Strategies**: Tailor confirmation logic to peer role (Gate/Manager/Worker)
3. **UNCONFIRMED State**: Explicit lifecycle for unconfirmed peers, clean semantics

**Result**: A failure detection system that is:
- ✅ **Adaptive** to real network conditions
- ✅ **Role-aware** for optimal per-tier behavior
- ✅ **Dual-purpose** for both detection and routing
- ✅ **Production-proven** algorithms (Vivaldi used in Serf, Consul, Cassandra)
- ✅ **AD-29 compliant** (only confirmed peers can be suspected)

This architecture provides the foundation for globally-distributed, multi-tier failure detection at scale.
---

### AD-36: Vivaldi-Based Cross-Datacenter Job Routing

**Status**: Proposed
**Related**: AD-35 (Vivaldi Coordinates), AD-33 (Federated Health Monitoring), AD-16 (Datacenter Health Classification)

---

## Problem Statement

Gates need to route jobs to the optimal datacenter while respecting safety and stability constraints:

### Current Challenges

1. **Static Routing Rules**: Manual configuration of datacenter priorities
   - Requires O(n²) configuration for n datacenters
   - Cannot adapt to network changes (route shifts, CDN changes, degradation)
   - No learning of actual topology

2. **No Latency Awareness**: All datacenters treated equally
   - May route to distant datacenter while nearby datacenter is available
   - User jobs experience higher latency than necessary
   - Inefficient use of network capacity

3. **Binary Health Decisions**: Datacenter is either "healthy" or "unhealthy"
   - Ignores partial degradation (e.g., 80% capacity available)
   - Ignores load imbalance (one DC overloaded, another idle)
   - All-or-nothing routing decisions

4. **No Multi-Factor Optimization**: Cannot balance competing factors
   - Closest datacenter may be overloaded
   - Healthiest datacenter may be far away
   - No principled way to trade off latency vs. load vs. health

---

## Solution: Vivaldi-Based Multi-Factor Routing

AD-36 extends AD-17 by using AD-35's confidence-aware RTT estimation to rank candidates **within** health buckets.
This keeps safety monotonic while improving latency and load efficiency.

### Design Goals

1. **Monotonic safety**: Never route to a worse health bucket because it is closer
2. **Confidence-aware latency**: Use RTT UCB, not raw RTT
3. **Graceful bootstrapping**: Missing coordinates never exclude a DC
4. **Low churn**: Hysteresis prevents routing oscillations
5. **Deterministic fallback**: Clear, ordered fallback chain

---

## Part 1: Routing Inputs

**Per-datacenter inputs**:
- Health bucket: HEALTHY / BUSY / DEGRADED (AD-16)
- Capacity: available_cores, total_cores
- Load signals: queue_depth, LHM multiplier, circuit-breaker pressure
- Vivaldi: leader coordinate, error, sample_count, updated_at

**Per-manager inputs** (within a DC):
- Circuit state (OPEN/HALF/closed)
- Manager health and capacity
- Vivaldi RTT to manager

---

## Part 2: Candidate Filtering

**DC hard excludes**:
- `UNHEALTHY` status
- No registered managers
- All managers circuit-open

**DC soft demotions**:
- Stale health → treat as DEGRADED (do not exclude)
- Missing coordinates → keep, but apply conservative RTT defaults

**Manager hard excludes**:
- Circuit breaker OPEN
- Heartbeat stale beyond TTL

---

## Part 3: Bucket Selection (AD-17 Preserved)

```
primary_bucket = first_non_empty([HEALTHY, BUSY, DEGRADED])
```

- Only candidates in `primary_bucket` are eligible for primary selection.
- Lower buckets are **fallback only**.
- Health ordering is never violated by RTT scoring.

---

## Part 4: Authoritative Scoring Function

### Step 1: RTT UCB (from AD-35)

```
rtt_ucb_ms = estimate_rtt_ucb_ms(local_coord, dc_leader_coord)
```

### Step 2: Load Factor (monotonic, capped)

```python
util = 1.0 - clamp01(available_cores / max(total_cores, 1))
queue = queue_depth / (queue_depth + QUEUE_SMOOTHING)
cb = open_managers / max(total_managers, 1)

load_factor = 1.0 + A_UTIL * util + A_QUEUE * queue + A_CB * cb
load_factor = min(load_factor, LOAD_FACTOR_MAX)
```

### Step 3: Coordinate Quality Penalty

```python
quality = coordinate_quality(sample_count, error_ms, staleness_s)
quality_penalty = 1.0 + A_QUALITY * (1.0 - quality)
quality_penalty = min(quality_penalty, QUALITY_PENALTY_MAX)
```

### Final Score

```python
score = rtt_ucb_ms * load_factor * quality_penalty
```

**Preferred DCs** (if provided) apply a bounded multiplier **within the primary bucket only**:

```python
if dc in preferred:
    score *= PREFERENCE_MULT
```

---

## Part 5: Hysteresis and Stickiness

Routing decisions must be stable to avoid oscillation:

1. **Hold-down**: keep current primary for `HOLD_DOWN_S` unless it becomes excluded
2. **Switch threshold**: only switch if new best improves by `IMPROVEMENT_RATIO`
3. **Forced switch** if:
   - current DC drops bucket
   - current DC is excluded
   - score degrades by `DEGRADE_RATIO` for `DEGRADE_CONFIRM_S`
4. **Cooldown after failover**: add a temporary penalty to recently failed DCs

### State Diagram

```
[Selected]
   │ hold-down
   │
   ├─(forced switch)───────────────► [Switch]
   │                                  │
   ├─(improvement >= threshold)────► [Switch]
   │                                  │
   └─(no change)────────────────────► [Selected]

[Switch] ──► [Cooldown] ──(cooldown expires)──► [Selected]
```

---

## Part 6: Bootstrapping and Convergence

When coordinates are missing or immature:

- Enter **Coordinate-Unaware Mode**
- Rank by capacity, then queue depth, then circuit pressure
- Exit when:
  - `sample_count >= MIN_SAMPLES_FOR_ROUTING` and
  - `error_ms <= ERROR_MAX_FOR_ROUTING`

This prevents early-stage noise from destabilizing routing.

---

## Part 7: Fallback Chain Construction

1. Select `primary_dcs` from `primary_bucket` in score order (with hysteresis)
2. Add remaining DCs from `primary_bucket` as fallback
3. Append next buckets in order (BUSY, then DEGRADED), each sorted by score

This yields a deterministic fallback chain that preserves AD-17 semantics.

---

## Part 8: Manager Selection Within a Datacenter

Managers are ranked similarly (within a DC):

- Exclude circuit-open or stale managers
- Score by RTT UCB + manager load + quality penalty
- Apply per-job stickiness: reuse the manager that already accepted the job in this DC

---

## Part 9: Routing Decision Flow

```
┌──────────────────────────────────────────────────────────────┐
│ Gate receives job                                            │
├──────────────────────────────────────────────────────────────┤
│ 1) Filter DCs (exclude UNHEALTHY)                            │
│ 2) Bucket by health (AD-17)                                  │
│ 3) Score within primary bucket (RTT UCB × load × quality)    │
│ 4) Apply hysteresis/stickiness                               │
│ 5) Select primary_dcs and fallback_dcs                        │
└──────────────────────────────────────────────────────────────┘
```

---

## Part 10: Timing Diagram (Dispatch + Fallback)

```
Time →

Gate               DC-A Manager          DC-B Manager
 |-- dispatch A -->|
 |<-- reject -------|
 |-- fallback B ------------------------->|
 |<-- accept --------------------------------|
 |-- record leader ------------------------>|
```

---

## Part 11: Observability

**Metrics**:
- `routing_decisions_total{bucket,reason}`
- `routing_score{dc_id}`
- `routing_score_component{dc_id,component="rtt_ucb|load|quality"}`
- `routing_switch_total{reason}`
- `routing_hold_down_blocks_total`
- `routing_fallback_used_total{from_dc,to_dc}`

**Logs**:
- `RoutingDecision` with candidate list and score components
- `RoutingSwitch` with old/new DC and improvement ratio
- `RoutingCooldown` when a DC fails dispatch

---

## Part 12: Success Criteria

1. **Latency Reduction**: 50% lower median RTT than random routing
2. **Load Distribution**: load variation coefficient < 0.3
3. **Failover Speed**: < 10 seconds from DC failure to routing around it
4. **Stability**: switch rate < 1% of routing decisions
5. **Zero Configuration**: no static priority lists required

---

## Conclusion

AD-36 uses AD-35's conservative RTT UCB and AD-17's health ordering to route jobs safely and efficiently.
The combination is robust against noisy coordinates, high load, and WAN variability, while avoiding routing churn.

---

### AD-37: Explicit Backpressure Policy (Gate → Manager → Worker)

**Decision**: Make backpressure explicit for high-volume stats/progress updates, while preserving AD-22/AD-32
bounded execution and priority load shedding as the global safety net for all traffic.

**Rationale**:
- Workers are CPU/memory bound and emit frequent stats; explicit backpressure prevents stats from starving control.
- Control-plane messages (SWIM, cancellation, leadership transfer) are CRITICAL and never shed by AD-32.
- Global load shedding still protects the system under overload without slowing critical paths.

**Compatibility**:
- AD-37 extends AD-23 (stats/progress backpressure) and does not override AD-20 cancellation guarantees.
- AD-37 does not change AD-17/AD-36 routing decisions; it only shapes update traffic.

**Message Classes**:
| Class | Examples | Policy |
|------|----------|--------|
| CONTROL | SWIM probes/acks, cancellation, leadership transfer | Never backpressured (CRITICAL) |
| DISPATCH | Job submission, workflow dispatch, state sync | Shed under overload, bounded by priority |
| DATA | Workflow progress, stats updates | Explicit backpressure + batching |
| TELEMETRY | Debug stats, detailed metrics | Shed first under overload |

**Backpressure Levels (StatsBuffer)**:
- `NONE` (<70% hot tier fill): accept all
- `THROTTLE` (70–85%): increase worker flush interval
- `BATCH` (85–95%): accept batched updates only
- `REJECT` (>95%): drop non-critical updates

**Flow Diagram**:
```
Worker Progress  ──► Manager WorkflowProgress handler
        │                 │
        │                 ├─ StatsBuffer.record(rate)
        │                 ├─ BackpressureLevel derived
        │                 └─ WorkflowProgressAck(backpressure_*)
        │                              │
        └────────── ack ◄──────────────┘
                 │
                 ├─ _handle_backpressure_signal()
                 ├─ _get_max_backpressure_level()
                 └─ _progress_flush_loop() throttles/batches/drops
```

**State Diagram (Worker Flush)**:
```
[NO_BACKPRESSURE]
   | (level >= THROTTLE)
   v
[THROTTLED] --(level >= BATCH)--> [BATCH_ONLY]
   ^  (level < THROTTLE)            | (level >= REJECT)
   |                                v
   +---------------------------- [REJECT]
```

**Timing Diagram (Progress Flush)**:
```
T0: Worker collects progress
T0+Δ: Manager acks with backpressure_level
T0+Δ+ε: Worker updates per-manager signal
T0+interval: Flush loop checks max signal
  - NONE: flush immediately
  - THROTTLE: add delay
  - BATCH: aggregate buffer, flush less often
  - REJECT: drop non-critical updates
```

**Implementation**:
- Manager emits `BackpressureSignal` in `WorkflowProgressAck` based on `StatsBuffer` fill ratio.
- Worker consumes ack and throttles progress flush loop using max backpressure across managers.
- Gate uses load shedding for job submission and respects manager backpressure for forwarded updates.

**References**:
- `hyperscale/distributed_rewrite/reliability/backpressure.py:7`
- `hyperscale/distributed_rewrite/nodes/manager.py:6066`
- `hyperscale/distributed_rewrite/nodes/worker.py:3320`
- `hyperscale/distributed_rewrite/server/protocol/in_flight_tracker.py:1`

---

### AD-38: Global Job Ledger with Per-Node Write-Ahead Logging

**Decision**: Implement a tiered durability architecture combining per-node Write-Ahead Logs (WAL) with a globally replicated Job Ledger for cross-datacenter job coordination, with operation-specific durability levels and separate control/data planes.

**Related**: AD-20 (Cancellation), AD-33 (Federated Health Monitoring), AD-35 (Vivaldi Coordinates), AD-36 (Cross-DC Routing), AD-37 (Backpressure)

**Rationale**:
- Gates assign jobs to datacenters worldwide; job state must survive node, rack, and region failures.
- Per-node WAL provides sub-millisecond local durability for immediate crash recovery.
- Global ledger provides cross-region consistency and authoritative job state.
- Event sourcing enables audit trail, conflict detection, and temporal queries.
- Hybrid Logical Clocks provide causal ordering without requiring synchronized clocks.
- **Workers are under heavy CPU/memory load during tests and MUST NOT participate in any consensus path.**
- **Different operations have different durability requirements; one-size-fits-all is inefficient.**
- **Stats/metrics streaming requires high throughput, not strong consistency (Data Plane).**

**Operational Model**:

Hyperscale operates with three distinct node types with different responsibilities:

| Node Type | Role | Consensus Participation | Durability Responsibility |
|-----------|------|------------------------|---------------------------|
| **Gates** | Job submission, monitoring, cross-DC coordination | GLOBAL (full participant) | Job lifecycle (create/cancel/complete) |
| **Managers** | Workflow dispatch, worker health, DC coordination | REGIONAL (within DC only) | Workflow lifecycle, aggregated stats |
| **Workers** | Execute load tests (high CPU/memory) | NONE (fire-and-forget) | None - reports upward to manager |

**Critical Design Constraint**: Workers running load tests may be slow to respond (100ms+ for acks). They MUST NOT be in any consensus or acknowledgment path. Managers are the "durability boundary" within each datacenter.

**Architecture Overview**:

```
┌─────────────────────────────────────────────────────────────────────────┐
│                    TIER 1: Global Job Ledger (Gates Only)                │
│                    ─────────────────────────────────────                 │
│   Participants: Gates (global consensus)                                 │
│   Operations: Job create, cancel, complete, timeout                      │
│   Durability: Survives region failure                                    │
│   Latency: 50-300ms                                                      │
└─────────────────────────────────────────────────────────────────────────┘
                                    ▲
                                    │ Async replication (Causal+ consistency)
                                    │ Circuit breakers for cross-DC failures
                                    │
┌─────────────────────────────────────────────────────────────────────────┐
│                    TIER 2: Regional Consensus (Gates + Managers)         │
│                    ────────────────────────────────────────              │
│   Participants: Gates and Managers within datacenter                     │
│   Operations: Workflow dispatch, workflow complete, job acceptance       │
│   Durability: Survives node failure within DC                           │
│   Latency: 2-10ms                                                        │
└─────────────────────────────────────────────────────────────────────────┘
                                    ▲
                                    │ Sync replication within DC
                                    │
┌───────────────────────────────────────────────────────────────────────────┐
│                    TIER 3: Per-Node WAL (Gates + Managers Only)           │
│                    ───────────────────────────────────────────            │
│                                                                           │
│   ┌─────────────┐     ┌─────────────┐     ┌─────────────┐                │
│   │  Gate WAL   │     │ Manager WAL │     │ Manager WAL │                │
│   │  (job ops)  │     │(workflow ops)│    │(workflow ops)│               │
│   └─────────────┘     └─────────────┘     └─────────────┘                │
│                                                                           │
│   Durability: Survives process crash (<1ms)                              │
└───────────────────────────────────────────────────────────────────────────┘
                                    ▲
                                    │ Fire-and-forget + Acknowledgment Windows
                                    │ (NO consensus participation)
                                    │
┌───────────────────────────────────────────────────────────────────────────┐
│                    WORKERS (No Durability Responsibility)                 │
│                    ──────────────────────────────────────                 │
│                                                                           │
│   ┌─────────────┐     ┌─────────────┐     ┌─────────────┐                │
│   │  Worker-1   │     │  Worker-2   │     │  Worker-N   │                │
│   │ (executing) │     │ (executing) │     │ (executing) │                │
│   │ High CPU/Mem│     │ High CPU/Mem│     │ High CPU/Mem│                │
│   └─────────────┘     └─────────────┘     └─────────────┘                │
│                                                                           │
│   Reports: Progress updates (fire-and-forget to Manager)                 │
│   Health: Manager detects failures via health checks, NOT consensus      │
│   Recovery: Manager reschedules workflows without global coordination    │
└───────────────────────────────────────────────────────────────────────────┘
```

**Separate Control Plane vs Data Plane**:

```
┌─────────────────────────────────────────────────────────────────────────┐
│                           CONTROL PLANE                                  │
│                    (Reliable, Lower Volume)                              │
├─────────────────────────────────────────────────────────────────────────┤
│  • Job commands (create, cancel)      → GLOBAL durability               │
│  • Workflow commands (dispatch)       → REGIONAL durability             │
│  • Leader election                    → REGIONAL durability             │
│  • Cancellation propagation           → GLOBAL durability               │
│                                                                          │
│  Protocol: TCP with acks, consensus, WAL                                │
│  Requires: NodeWAL with fsync, binary format, CRC checksums             │
└─────────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────────┐
│                            DATA PLANE                                    │
│                    (High Throughput, Eventual Consistency)              │
├─────────────────────────────────────────────────────────────────────────┤
│  • Progress updates from workers      → LOCAL or NONE                   │
│  • Stats streaming to gates           → Batched, sampled               │
│  • Metrics aggregation                → Eventual consistency OK         │
│                                                                          │
│  Protocol: Fire-and-forget TCP, UDP, batching, sampling                 │
│  Uses: hyperscale/logging Logger (JSON, no fsync required)              │
└─────────────────────────────────────────────────────────────────────────┘
```

---

## Part 1: Event Sourcing Model

All job state changes are stored as immutable events rather than mutable state:

**Event Types**:

| Event | Fields | Semantics |
|-------|--------|-----------|
| `JobCreated` | job_id, spec, assigned_dcs, fence_token, hlc | New job submitted |
| `JobAccepted` | job_id, dc_id, worker_count, fence_token, hlc | DC accepted job |
| `JobProgressReported` | job_id, dc_id, completed, failed, hlc | Progress update |
| `JobCancellationRequested` | job_id, reason, requestor, fence_token, hlc | Cancel initiated |
| `JobCancellationAcked` | job_id, dc_id, workflows_cancelled, hlc | DC confirmed cancel |
| `JobCompleted` | job_id, final_status, aggregate_metrics, hlc | Job finished |
| `JobFailed` | job_id, error, failed_dc, hlc | Job failed |
| `JobTimedOut` | job_id, timeout_type, last_progress_hlc, hlc | Job exceeded timeout |

**Event State Diagram**:

```
                              JobCreated
                                  │
                    ┌─────────────┼─────────────┐
                    │             │             │
                    ▼             ▼             ▼
              JobAccepted   JobAccepted   JobAccepted
                (DC-1)        (DC-2)        (DC-3)
                    │             │             │
                    └──────┬──────┴──────┬──────┘
                           │             │
              ┌────────────┼─────────────┼────────────┐
              │            │             │            │
              ▼            ▼             ▼            ▼
    JobProgressReported  JobCancellation  JobTimedOut  JobFailed
              │          Requested            │            │
              │             │                 │            │
              ▼             ▼                 │            │
    JobProgressReported  JobCancellation     │            │
              │            Acked              │            │
              │             │                 │            │
              └──────┬──────┴─────────────────┴────────────┘
                     │
                     ▼
              JobCompleted
```

---

## Part 2: Hybrid Logical Clocks (HLC)

HLC combines physical time with logical counters for causal ordering without clock synchronization:

**HLC Invariants**:
1. If event A causally precedes B, then HLC(A) < HLC(B)
2. HLC is always within bounded drift of physical time
3. Total ordering achieved via (wall_time, logical_counter, node_id)

**HLC State Diagram**:

```
                    ┌─────────────────────────┐
                    │       Local Event       │
                    │  wall' = max(wall, now) │
                    │  if wall' == wall:      │
                    │    logical++            │
                    │  else:                  │
                    │    logical = 0          │
                    └───────────┬─────────────┘
                                │
                                ▼
┌───────────────────────────────────────────────────────────────┐
│                         HLC State                              │
│  (wall_time_ms: int, logical_counter: int, node_id: str)      │
└───────────────────────────────────────────────────────────────┘
                                ▲
                                │
                    ┌───────────┴─────────────┐
                    │     Receive Event       │
                    │  wall' = max(wall,      │
                    │           remote.wall,  │
                    │           now)          │
                    │  logical' = derived     │
                    │    from max sources     │
                    └─────────────────────────┘
```

**HLC Timing Diagram**:

```
Node A                              Node B
  │                                    │
  │ T=100, L=0                         │
  │ ────────────── msg ──────────────► │
  │                                    │ T=95 (behind)
  │                                    │ receive: wall'=max(95,100)=100
  │                                    │          logical'=0+1=1
  │                                    │ HLC=(100, 1, B)
  │                                    │
  │                       ◄─── ack ─── │ T=100, L=1
  │ T=100 (same)                       │
  │ receive: wall'=100                 │
  │          logical'=max(0,1)+1=2     │
  │ HLC=(100, 2, A)                    │
  │                                    │
  │ T=101 (advanced)                   │
  │ local event: wall'=101, L=0        │
  │ HLC=(101, 0, A)                    │
```

---

## Part 3: Per-Node Write-Ahead Log

Each node maintains a local WAL for immediate crash recovery:

**WAL Entry Binary Format**:

```
┌──────────┬──────────┬──────────┬──────────┬──────────┬──────────┐
│ CRC32    │ Length   │ LSN      │ HLC      │ State    │ Type     │
│ (4 bytes)│ (4 bytes)│ (8 bytes)│ (16 bytes)│ (1 byte) │ (1 byte) │
├──────────┴──────────┴──────────┴──────────┴──────────┴──────────┤
│                        Payload (variable)                        │
└─────────────────────────────────────────────────────────────────┘

Total header: 34 bytes
CRC32: Covers all fields except CRC32 itself
```

**WAL Entry State Machine**:

```
┌─────────┐
│ PENDING │ ─── Written to local WAL
└────┬────┘
     │ Regional consensus achieved
     ▼
┌──────────┐
│ REGIONAL │ ─── Replicated within datacenter
└────┬─────┘
     │ Global ledger confirmed
     ▼
┌────────┐
│ GLOBAL │ ─── Committed to global ledger
└────┬───┘
     │ Applied to state machine
     ▼
┌─────────┐
│ APPLIED │ ─── State machine updated
└────┬────┘
     │ Checkpoint created
     ▼
┌───────────┐
│ COMPACTED │ ─── Safe to garbage collect
└───────────┘
```

**WAL Segment Structure**:

```
┌────────────────────────────────────────────────────────────────┐
│                     WAL Segment File (64MB)                     │
├────────────────────────────────────────────────────────────────┤
│ Entry 1: LSN=1, HLC=(T1,L1,N), State=GLOBAL, payload=...       │
├────────────────────────────────────────────────────────────────┤
│ Entry 2: LSN=2, HLC=(T2,L2,N), State=REGIONAL, payload=...     │
├────────────────────────────────────────────────────────────────┤
│ Entry 3: LSN=3, HLC=(T3,L3,N), State=PENDING, payload=...      │
├────────────────────────────────────────────────────────────────┤
│ ... more entries ...                                            │
├────────────────────────────────────────────────────────────────┤
│ [Zero-filled space for future entries]                          │
└────────────────────────────────────────────────────────────────┘
```

---

## Part 3.1: Logger Suitability Analysis

The hyperscale/logging Logger provides async file writing capabilities. This section analyzes its suitability for WAL vs Data Plane use cases.

**Logger Capabilities** (from `hyperscale/logging/streams/logger_stream.py`):

```python
# Current Logger file writing pattern
def _write_to_file(self, log: Log, logfile_path: str):
    if (logfile := self._files.get(logfile_path)) and (logfile.closed is False):
        logfile.write(msgspec.json.encode(log) + b"\n")
        logfile.flush()  # <- Only flush, NO os.fsync()!
```

**Suitability Matrix**:

| Requirement | Logger Has? | WAL Needs? | Data Plane Needs? |
|-------------|-------------|------------|-------------------|
| Async file I/O | ✅ Yes (run_in_executor) | ✅ Yes | ✅ Yes |
| Per-file locking | ✅ Yes (asyncio.Lock) | ✅ Yes | ⚪ Optional |
| fsync guarantee | ❌ No (flush only) | ✅ **Critical** | ❌ Not needed |
| Sequence numbers | ❌ No | ✅ **Critical** | ❌ Not needed |
| Binary format with CRC | ❌ No (JSON) | ✅ **Critical** | ❌ Not needed |
| Read-back capability | ❌ No (write-only) | ✅ **Critical** | ❌ Not needed |
| Retention/rotation | ✅ Yes | ✅ Yes | ✅ Yes |
| Batch operations | ✅ Yes | ✅ Yes | ✅ Yes |
| msgspec serialization | ✅ Yes | ✅ Yes | ✅ Yes |

**Critical WAL Gap: No fsync**

```python
# Logger current implementation (INSUFFICIENT for WAL):
logfile.write(data)
logfile.flush()  # Flushes to OS buffer, NOT to disk

# WAL REQUIRES explicit fsync:
logfile.write(data)
logfile.flush()
os.fsync(logfile.fileno())  # Guarantees on-disk durability
```

Without fsync, data in OS buffers can be lost on:
- Power failure
- Kernel panic
- Hardware failure

**Critical WAL Gap: No Sequence Numbers**

WAL requires monotonically increasing LSNs for:
- Replication position tracking
- Recovery point identification
- Exactly-once processing guarantees

**Critical WAL Gap: No Read-Back**

WAL requires:
```python
# Logger does NOT provide:
def read_from_offset(offset: int) -> list[Entry]: ...
def get_committed_offset() -> int: ...
def truncate_before(offset: int): ...  # For compaction
```

**Verdict**:

| Use Case | Logger Suitable? | Recommendation |
|----------|------------------|----------------|
| **Control Plane WAL** | ❌ **No** | Build dedicated NodeWAL class |
| **Data Plane Stats** | ✅ **Yes** | Use Logger as-is |
| **Audit Logging** | ⚠️ **Partial** | Logger OK if crash loss acceptable |

**Recommendation**: Build `NodeWAL` class that:
1. **Reuses** Logger's async patterns (run_in_executor, per-file locks)
2. **Adds** explicit fsync with group commit batching
3. **Adds** binary segments with CRC checksums
4. **Adds** sequence numbers via HLC
5. **Adds** read-back and recovery capabilities

**Data Plane uses Logger directly** for stats streaming where eventual consistency is acceptable.

---

## Part 3.2: Operation-Specific Durability

Different operations require different durability guarantees. Using GLOBAL durability for everything adds 200-300ms latency to every operation - unacceptable for high-throughput stats.

**Durability by Operation Type**:

| Operation | Durability | Latency | Rationale |
|-----------|------------|---------|-----------|
| **Job Create** | GLOBAL | 50-300ms | Must survive region loss; authoritative |
| **Job Cancel** | GLOBAL | 50-300ms | Safety-critical; must propagate everywhere |
| **Job Complete** | GLOBAL | 50-300ms | Final state; audit trail requirement |
| **Job Timeout** | GLOBAL | 50-300ms | Authoritative determination |
| **Workflow Dispatch** | REGIONAL | 2-10ms | Manager is DC authority |
| **Workflow Complete** | REGIONAL | 2-10ms | Aggregated to gate async |
| **Workflow Cancel** | REGIONAL | 2-10ms | DC-local operation |
| **Progress Update** | LOCAL | <1ms | High volume; manager aggregates |
| **Stats Report** | NONE | ~0ms | Fire-and-forget; eventual consistency |
| **Metrics Stream** | NONE | ~0ms | Batched, sampled at source |

**State Diagram: Durability Decision**:

```
┌─────────────────────────────────────────────────────────────────────────┐
│                        Incoming Operation                                │
└────────────────────────────────┬────────────────────────────────────────┘
                                 │
                    ┌────────────▼────────────┐
                    │  Is it Job lifecycle?   │
                    │  (create/cancel/complete)│
                    └────────────┬────────────┘
                            Yes  │  No
           ┌─────────────────────┤
           ▼                     ▼
    ┌──────────────┐    ┌────────────────────┐
    │    GLOBAL    │    │ Is it Workflow     │
    │  durability  │    │ lifecycle?         │
    └──────────────┘    └─────────┬──────────┘
                             Yes  │  No
                   ┌──────────────┤
                   ▼              ▼
           ┌──────────────┐  ┌────────────────────┐
           │   REGIONAL   │  │ Is it progress     │
           │  durability  │  │ from worker?       │
           └──────────────┘  └─────────┬──────────┘
                                  Yes  │  No
                       ┌───────────────┤
                       ▼               ▼
               ┌──────────────┐  ┌──────────────┐
               │    LOCAL     │  │    NONE      │
               │  (optional)  │  │ fire-and-forget│
               └──────────────┘  └──────────────┘
```

---

## Part 3.3: Acknowledgment Windows (Worker Communication)

Workers under load cannot provide timely acks. Instead of blocking on worker responses, use **Acknowledgment Windows**.

**Traditional Approach (WRONG for workers under load)**:

```
Manager ──► Worker: Dispatch workflow
           │
           ├── Wait for ACK (blocking)  ← Worker is busy, 500ms+ delay
           │
           ▼
Manager: Timeout or slow operation
```

**Acknowledgment Window Approach (CORRECT)**:

```
Manager ──► Worker: Dispatch workflow
           │
           ├── Start "ack window" timer (e.g., 5 seconds)
           │
           ├── Continue processing other work (non-blocking)
           │
           ▼
┌─────────────────────────────────────────────────────────────────────────┐
│                        Acknowledgment Window                             │
├─────────────────────────────────────────────────────────────────────────┤
│  Within window:                                                          │
│    • Worker sends progress update → Workflow confirmed running          │
│    • Worker sends completion → Workflow completed                       │
│    • Worker sends error → Workflow failed                               │
│                                                                          │
│  Window expires with no communication:                                   │
│    • Health check worker                                                │
│    • If worker healthy: extend window                                   │
│    • If worker unhealthy: mark workflow for reschedule                  │
└─────────────────────────────────────────────────────────────────────────┘
```

**Acknowledgment Window State Machine**:

```
┌────────────┐
│ DISPATCHED │ ─── Workflow sent to worker
└─────┬──────┘
      │ Start ack window timer
      ▼
┌────────────────┐     Progress received      ┌───────────┐
│ AWAITING_ACK   │ ───────────────────────────►│ CONFIRMED │
└─────┬──────────┘                             └───────────┘
      │ Window expires
      ▼
┌────────────────┐     Worker healthy         ┌───────────────┐
│ WINDOW_EXPIRED │ ───────────────────────────►│ EXTEND_WINDOW │
└─────┬──────────┘                             └───────────────┘
      │ Worker unhealthy
      ▼
┌────────────────┐
│ RESCHEDULE     │ ─── Workflow needs new worker
└────────────────┘
```

---

## Part 3.4: Circuit Breakers for Cross-DC Communication

Cross-DC communication can be slow or fail entirely. Use circuit breakers to prevent cascading failures.

**Circuit Breaker States**:

```
┌────────────────────────────────────────────────────────────────────────┐
│                         Circuit Breaker States                          │
├────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│   ┌────────┐     Failures exceed     ┌──────┐     Probe succeeds       │
│   │ CLOSED │ ─────threshold─────────►│ OPEN │ ──────────────────┐      │
│   └───┬────┘                         └───┬──┘                   │      │
│       │                                  │                      │      │
│       │ Success                          │ Probe interval       │      │
│       │                                  │ elapsed              │      │
│       │                                  ▼                      │      │
│       │                           ┌───────────┐                 │      │
│       └───────────────────────────│ HALF_OPEN │◄────────────────┘      │
│                  Probe succeeds   └───────────┘                        │
│                                         │                              │
│                                         │ Probe fails                  │
│                                         ▼                              │
│                                    [Back to OPEN]                      │
│                                                                         │
└────────────────────────────────────────────────────────────────────────┘
```

**Circuit Breaker Behavior by State**:

| State | Behavior | On Success | On Failure |
|-------|----------|------------|------------|
| CLOSED | Normal operation | Remain CLOSED | Increment failure count |
| OPEN | Reject immediately, queue for later | N/A | N/A |
| HALF_OPEN | Allow probe request | → CLOSED | → OPEN |

**Cross-DC Circuit Breaker Configuration**:

```python
@dataclass
class CrossDCCircuitBreakerConfig:
    """Configuration for cross-DC circuit breakers."""

    failure_threshold: int = 5         # Failures before opening
    success_threshold: int = 3         # Successes in HALF_OPEN before closing
    open_timeout_seconds: float = 30.0 # Time before probing

    # Per-DC tracking
    half_open_max_probes: int = 1      # Concurrent probes allowed

    # Queue behavior when OPEN
    queue_max_size: int = 1000         # Max queued operations
    queue_timeout_seconds: float = 60.0 # Queue entry TTL
```

**Integration with Job Submission**:

```
Client ──► Gate: SubmitJob(target_dcs=[dc-east, dc-west])
              │
              ├── dc-east circuit: CLOSED → Send immediately
              │
              ├── dc-west circuit: OPEN → Queue for later
              │
              ├── Return "ACCEPTED" to client
              │
              └── Background: When dc-west recovers, replay queue
```

---

## Part 3.5: Coalesced Stats Reporting

Stats are high-volume, low-criticality. Reduce cross-DC traffic through coalescing.

**Stats Flow Without Coalescing (WRONG)**:

```
10,000 progress updates/second from workers
         │
         ▼
10,000 messages/second to Manager
         │
         ▼
10,000 messages/second to Gate (cross-DC!)  ← Network overwhelmed
```

**Stats Flow With Coalescing (CORRECT)**:

```
10,000 progress updates/second from workers
         │
         │ Workers: batch every 100ms or 1000 events
         ▼
100 batched messages/second to Manager
         │
         │ Manager: aggregate per-job, report every 500ms
         ▼
2 aggregated messages/second to Gate (cross-DC)  ← 5000x reduction
```

**Coalescing Configuration**:

```python
@dataclass
class StatsCoalescingConfig:
    """Configuration for stats aggregation."""

    # Worker → Manager
    worker_batch_interval_ms: int = 100      # Max time before flush
    worker_batch_max_events: int = 1000      # Max events before flush

    # Manager → Gate
    manager_aggregate_interval_ms: int = 500 # Aggregation window
    manager_sample_rate: float = 0.1         # Sample 10% of detailed metrics

    # Gate storage
    gate_stats_retention_seconds: int = 3600 # Keep 1 hour of stats
    gate_stats_use_logger: bool = True       # Use Logger for stats storage
```

**Aggregated Stats Model** (suitable for Logger):

```python
@dataclass
class AggregatedJobStats:
    """Aggregated stats for a job, sent Manager → Gate."""

    job_id: str
    dc_id: str
    timestamp: float

    # Counts
    workflows_running: int
    workflows_completed: int
    workflows_failed: int

    # Rates (computed from samples)
    requests_per_second: float
    errors_per_second: float

    # Latencies (percentiles)
    latency_p50_ms: float
    latency_p95_ms: float
    latency_p99_ms: float

    # Resource usage (sampled)
    cpu_percent_avg: float
    memory_mb_avg: float
```

---

## Part 4: Commit Pipeline

Three-stage commit with progressive durability guarantees:

**Commit Flow Diagram**:

```
  Client Request
        │
        ▼
┌───────────────┐     ┌─────────────────────────────────────────────────┐
│  Gate Node    │     │                 Commit Pipeline                  │
│               │     │                                                  │
│  ┌─────────┐  │     │  Stage 1: LOCAL WAL                             │
│  │ Submit  │──┼────►│  ─────────────────                              │
│  │  Job    │  │     │  • Write to memory-mapped segment               │
│  └─────────┘  │     │  • Batch fsync (10ms or 100 entries)            │
│               │     │  • Latency: <1ms                                │
│               │     │  • Survives: process crash                      │
│               │     │                                                  │
│               │     │  Stage 2: REGIONAL CONSENSUS                    │
│               │     │  ────────────────────────                       │
│               │     │  • Raft/Paxos within datacenter                 │
│               │     │  • Quorum: 2/3 nodes                            │
│               │     │  • Latency: 2-10ms                              │
│               │     │  • Survives: node failure                       │
│               │     │                                                  │
│               │     │  Stage 3: GLOBAL LEDGER                         │
│               │     │  ─────────────────────                          │
│               │     │  • Cross-region replication                     │
│               │     │  • Quorum: 3/5 regions                          │
│               │     │  • Latency: 50-300ms                            │
│               │     │  • Survives: region failure                     │
└───────────────┘     └─────────────────────────────────────────────────┘
```

**Durability Levels**:

| Level | Latency | Survives | Use Case |
|-------|---------|----------|----------|
| LOCAL | <1ms | Process crash | High-throughput updates |
| REGIONAL | 2-10ms | Node failure | Normal job operations |
| GLOBAL | 50-300ms | Region failure | Critical operations (cancel) |

**Commit Timing Diagram**:

```
T0          T1          T2          T3          T4
│           │           │           │           │
│ Write to  │ Batch     │ Regional  │ Global    │
│ WAL       │ fsync     │ commit    │ commit    │
│           │           │           │           │
├───────────┼───────────┼───────────┼───────────┤
│   <1ms    │   10ms    │   5ms     │  100ms    │
│           │           │           │           │
│◄─ LOCAL ─►│           │           │           │
│◄────── REGIONAL ─────►│           │           │
│◄─────────────── GLOBAL ──────────►│           │
│                                               │
│  Client sees ack after chosen durability      │
│  level is achieved                            │
```

---

## Part 5: Global Job Ledger

Cross-region consensus for authoritative job state:

**Regional Authority Model**:

```
┌─────────────────────────────────────────────────────────────────┐
│                     Global Job Ledger                            │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│   ┌──────────────┐    ┌──────────────┐    ┌──────────────┐     │
│   │  US-EAST     │    │  EU-WEST     │    │  APAC        │     │
│   │  Authority   │    │  Authority   │    │  Authority   │     │
│   │              │    │              │    │              │     │
│   │ Jobs: 1M     │    │ Jobs: 800K   │    │ Jobs: 600K   │     │
│   │ (home here)  │    │ (home here)  │    │ (home here)  │     │
│   └──────┬───────┘    └──────┬───────┘    └──────┬───────┘     │
│          │                   │                   │              │
│          └───────────────────┼───────────────────┘              │
│                              │                                   │
│                    Cross-Region Replication                      │
│                    (Async with Causal Ordering)                  │
└─────────────────────────────────────────────────────────────────┘
```

**Job ID Format** (encodes home region):

```
Format: {region_code}-{timestamp_ms}-{gate_id}-{sequence}
Example: use1-1704931200000-gate42-00001

Benefits:
├── Lexicographically sortable by time
├── Instant routing to authoritative region
├── No coordination needed for ID generation
└── Region encoded for fast authority lookup
```

**Conflict Resolution**:

```
Conflict detected when: same job_id, same fence_token, different events

Resolution priority (deterministic):
1. Cancellation always wins (fail-safe)
2. Higher fence token wins (later operation)
3. HLC ordering (causal precedence)
4. Lexicographic node_id (deterministic tie-breaker)

                    ┌─────────────────────────┐
                    │   Conflicting Events    │
                    │   A: JobAccepted        │
                    │   B: JobCancellation    │
                    └───────────┬─────────────┘
                                │
                    ┌───────────▼───────────┐
                    │ Is either Cancellation?│
                    └───────────┬───────────┘
                           Yes  │
                    ┌───────────▼───────────┐
                    │  Cancellation Wins    │
                    │  (fail-safe)          │
                    └───────────────────────┘
```

---

## Part 6: Anti-Entropy and Repair

Merkle tree-based consistency verification:

**Merkle Tree Structure**:

```
                         Root Hash
                        /         \
                   Hash(L)       Hash(R)
                  /      \      /      \
              Hash(A)  Hash(B) Hash(C)  Hash(D)
                │        │       │        │
            ┌───┴───┐ ┌──┴──┐ ┌──┴──┐ ┌───┴───┐
            │Jobs   │ │Jobs │ │Jobs │ │Jobs   │
            │A-E    │ │F-J  │ │K-O  │ │P-Z    │
            └───────┘ └─────┘ └─────┘ └───────┘
```

**Anti-Entropy Flow**:

```
Region A                                    Region B
    │                                           │
    │ ─────── Root Hash Exchange ────────────► │
    │                                           │
    │ ◄─────── Hash Mismatch ───────────────── │
    │                                           │
    │ ─────── Request Subtree L ─────────────► │
    │                                           │
    │ ◄─────── Subtree L Hashes ───────────── │
    │                                           │
    │ Compare: Hash(A) matches, Hash(B) differs │
    │                                           │
    │ ─────── Request Jobs F-J ──────────────► │
    │                                           │
    │ ◄─────── Events for Jobs F-J ─────────── │
    │                                           │
    │ Merge events using conflict resolution    │
    │                                           │
```

**Repair State Machine**:

```
┌──────────┐
│ CONSISTENT│◄─────────────────────────────────┐
└─────┬────┘                                   │
      │ Hash mismatch detected                 │
      ▼                                        │
┌───────────┐                                  │
│ COMPARING │ ◄── Drill down Merkle tree       │
└─────┬─────┘                                  │
      │ Divergent range found                  │
      ▼                                        │
┌───────────┐                                  │
│ FETCHING  │ ── Request events from authority │
└─────┬─────┘                                  │
      │ Events received                        │
      ▼                                        │
┌───────────┐                                  │
│ MERGING   │ ── Apply conflict resolution     │
└─────┬─────┘                                  │
      │ State merged                           │
      ▼                                        │
┌──────────���┐                                  │
│ VERIFYING │ ── Recompute hashes              │
└─────┬─────┘                                  │
      │ Hashes match                           │
      └────────────────────────────────────────┘
```

---

## Part 7: Checkpoint and Compaction

Efficient recovery through periodic snapshots:

**Checkpoint Contents**:

```
┌─────────────────────────────────────────────────────────────────┐
│                      Checkpoint File                             │
├─────────────────────────────────────────────────────────────────┤
│ Header:                                                          │
│   checkpoint_id: uuid                                            │
│   created_at: timestamp                                          │
│   local_lsn: 12345                                              │
│   regional_lsn: 12340                                           │
│   global_lsn: 12300                                             │
├─────────────────────────────────────────────────────────────────┤
│ State Snapshot:                                                  │
│   active_jobs: {job_id -> JobState}                             │
│   pending_cancellations: {job_id -> CancelState}                │
│   dc_assignments: {job_id -> [dc_ids]}                          │
│   fence_tokens: {job_id -> token}                               │
├─────────────────────────────────────────────────────────────────┤
│ Indexes:                                                         │
│   job_by_status: {status -> [job_ids]}                          │
│   job_by_dc: {dc_id -> [job_ids]}                               │
│   job_by_gate: {gate_id -> [job_ids]}                           │
└─────────────────────────────────────────────────────────────────┘
```

**Compaction Flow**:

```
                    ┌─────────────────┐
                    │ Checkpoint      │
                    │ Created at      │
                    │ LSN=1000        │
                    └────────┬────────┘
                             │
        ┌────────────────────┼────────────────────┐
        │                    │                    │
        ▼                    ▼                    ▼
┌───────────────┐  ┌───────────────┐  ┌───────────────┐
│ Segment 0     │  │ Segment 1     │  │ Segment 2     │
│ LSN 1-500     │  │ LSN 501-1000  │  │ LSN 1001-1200 │
│ [COMPACTED]   │  │ [COMPACTED]   │  │ [ACTIVE]      │
└───────┬───────┘  └───────┬───────┘  └───────────────┘
        │                  │
        ▼                  ▼
   ┌─────────┐        ┌─────────┐
   │ DELETE  │        │ DELETE  │
   └─────────┘        └─────────┘
```

**Recovery Flow**:

```
┌──────────────────┐
│   Node Startup   │
└────────┬─────────┘
         │
         ▼
┌────────────────────────┐
│ Find Latest Checkpoint │
└────────┬───────────────┘
         │
    ┌────┴────┐
    │ Found?  │
    └────┬────┘
    No   │   Yes
    │    └────────────────┐
    ▼                     ▼
┌─────────────┐  ┌────────────────────┐
│ Full WAL    │  │ Restore Checkpoint │
│ Replay      │  │ State Snapshot     │
└──────┬──────┘  └────────┬───────────┘
       │                  │
       │                  ▼
       │         ┌────────────────────┐
       │         │ Replay WAL from    │
       │         │ checkpoint LSN     │
       │         └────────┬───────────┘
       │                  │
       └────────┬─────────┘
                │
                ▼
       ┌────────────────────┐
       │ Reconcile with     │
       │ Regional/Global    │
       └────────┬───────────┘
                │
                ▼
       ┌────────────────────┐
       │ Node Ready         │
       └────────────────────┘
```

---

## Part 8: Session Consistency Guarantees

Read consistency levels for different use cases:

**Consistency Levels**:

| Level | Guarantee | Latency | Use Case |
|-------|-----------|---------|----------|
| EVENTUAL | May read stale | Fastest | Dashboards, monitoring |
| SESSION | Read-your-writes | Low | Normal operations |
| BOUNDED_STALENESS | Max lag = X ms | Medium | Cross-region queries |
| STRONG | Authoritative | Highest | Status verification |

**Session State Diagram**:

```
                         ┌──────────────────┐
                         │  Session Start   │
                         └────────┬─────────┘
                                  │
                                  ▼
                         ┌──────────────────┐
                         │ last_read_hlc=0  │
                         │ written_jobs={}  │
                         └────────┬─────────┘
                                  │
              ┌───────────────────┼───────────────────┐
              │                   │                   │
              ▼                   ▼                   ▼
      ┌───────────────┐  ┌───────────────┐  ┌───────────────┐
      │ Write Job A   │  │ Read Job A    │  │ Read Job B    │
      │               │  │ (after write) │  │ (no write)    │
      │ written_jobs  │  │               │  │               │
      │ += {A}        │  │ Must read     │  │ May read      │
      └───────────────┘  │ authoritative │  │ local replica │
                         └───────────────┘  └───────────────┘
```

---

## Part 9: Implementation

### WAL Entry Model

```python
"""
hyperscale/distributed_rewrite/ledger/models/wal_entry.py
"""

from dataclasses import dataclass, field
from enum import IntEnum
import struct
import hashlib

from hyperscale.distributed_rewrite.ledger.models.hlc import HybridLogicalClock


class WALEntryState(IntEnum):
    """State of a WAL entry in the commit pipeline."""
    PENDING = 0       # Written to local WAL, not yet replicated
    REGIONAL = 1      # Committed to regional consensus group
    GLOBAL = 2        # Committed to global ledger
    APPLIED = 3       # Applied to local state machine
    COMPACTED = 4     # Safe to garbage collect


@dataclass(slots=True)
class WALEntry:
    """
    Single entry in the Write-Ahead Log.
    
    Binary format (fixed header + variable payload):
    ┌──────────┬──────────┬──────────┬──────────┬──────────┬──────────┐
    │ CRC32    │ Length   │ LSN      │ HLC      │ State    │ Type     │
    │ (4 bytes)│ (4 bytes)│ (8 bytes)│ (16 bytes)│ (1 byte) │ (1 byte) │
    ├──────────┴──────────┴──────────┴──────────┴──────────┴──────────┤
    │                        Payload (variable)                        │
    └─────────────────────────────────────────────────────────────────┘
    """
    lsn: int                          # Log Sequence Number (monotonic)
    hlc: HybridLogicalClock           # Hybrid Logical Clock timestamp
    state: WALEntryState              # Current commit state
    entry_type: int                   # Type discriminator
    payload: bytes                    # Serialized operation
    crc32: int = 0                    # Checksum for integrity
    
    HEADER_SIZE = 34  # 4 + 4 + 8 + 16 + 1 + 1
    
    def serialize(self) -> bytes:
        """Serialize entry to bytes with CRC."""
        header = struct.pack(
            "<IIQQQBB",
            0,  # CRC placeholder
            len(self.payload),
            self.lsn,
            self.hlc.wall_time_ms,
            self.hlc.logical_counter,
            self.state.value,
            self.entry_type,
        )
        data = header + self.payload
        crc = hashlib.crc32(data[4:])  # CRC over everything except CRC field
        return struct.pack("<I", crc) + data[4:]
    
    @classmethod
    def deserialize(cls, data: bytes) -> "WALEntry":
        """Deserialize entry from bytes with CRC verification."""
        if len(data) < cls.HEADER_SIZE:
            raise ValueError(f"Entry too short: {len(data)} < {cls.HEADER_SIZE}")
        
        crc_stored, length, lsn, wall_time, logical, state, entry_type = struct.unpack(
            "<IIQQQBB", data[:cls.HEADER_SIZE]
        )
        
        # Verify CRC
        crc_computed = hashlib.crc32(data[4:cls.HEADER_SIZE + length])
        if crc_stored != crc_computed:
            raise ValueError(f"CRC mismatch: stored={crc_stored}, computed={crc_computed}")
        
        payload = data[cls.HEADER_SIZE:cls.HEADER_SIZE + length]
        
        return cls(
            lsn=lsn,
            hlc=HybridLogicalClock(wall_time, logical, ""),
            state=WALEntryState(state),
            entry_type=entry_type,
            payload=payload,
            crc32=crc_stored,
        )
```

### Hybrid Logical Clock

```python
"""
hyperscale/distributed_rewrite/ledger/models/hlc.py
"""

from dataclasses import dataclass
import time


@dataclass(slots=True)
class HybridLogicalClock:
    """
    Hybrid Logical Clock for causal ordering.
    
    Combines physical time with logical counter:
    - Provides causal ordering without synchronized clocks
    - Bounded drift from physical time
    - Total ordering via (wall_time, logical_counter, node_id)
    
    Invariants:
    1. If A → B (A causally precedes B), then HLC(A) < HLC(B)
    2. HLC.wall_time >= physical_time - max_drift
    3. Comparison: (wall_time, logical_counter, node_id)
    """
    wall_time_ms: int      # Physical timestamp (milliseconds)
    logical_counter: int   # Logical component for same-millisecond ordering
    node_id: str           # Tie-breaker for concurrent events
    
    def tick(self, local_wall_time_ms: int) -> "HybridLogicalClock":
        """
        Generate next timestamp for local event.
        
        Algorithm:
        1. new_wall = max(current_wall, physical_time)
        2. if new_wall == current_wall: logical++
        3. else: logical = 0
        """
        new_wall = max(self.wall_time_ms, local_wall_time_ms)
        if new_wall == self.wall_time_ms:
            return HybridLogicalClock(new_wall, self.logical_counter + 1, self.node_id)
        return HybridLogicalClock(new_wall, 0, self.node_id)
    
    def receive(
        self,
        remote: "HybridLogicalClock",
        local_wall_time_ms: int,
    ) -> "HybridLogicalClock":
        """
        Update clock on receiving message from remote node.
        
        Algorithm:
        1. new_wall = max(local_wall, remote_wall, physical_time)
        2. Compute logical based on which wall times matched
        """
        new_wall = max(self.wall_time_ms, remote.wall_time_ms, local_wall_time_ms)
        
        if new_wall == self.wall_time_ms == remote.wall_time_ms:
            # All three equal: take max logical + 1
            new_logical = max(self.logical_counter, remote.logical_counter) + 1
        elif new_wall == self.wall_time_ms:
            # Local wall is max: increment local logical
            new_logical = self.logical_counter + 1
        elif new_wall == remote.wall_time_ms:
            # Remote wall is max: increment remote logical
            new_logical = remote.logical_counter + 1
        else:
            # Physical time is max: reset logical
            new_logical = 0
        
        return HybridLogicalClock(new_wall, new_logical, self.node_id)
    
    def __lt__(self, other: "HybridLogicalClock") -> bool:
        if self.wall_time_ms != other.wall_time_ms:
            return self.wall_time_ms < other.wall_time_ms
        if self.logical_counter != other.logical_counter:
            return self.logical_counter < other.logical_counter
        return self.node_id < other.node_id
    
    def __eq__(self, other: object) -> bool:
        if not isinstance(other, HybridLogicalClock):
            return False
        return (
            self.wall_time_ms == other.wall_time_ms
            and self.logical_counter == other.logical_counter
            and self.node_id == other.node_id
        )
    
    def __hash__(self) -> int:
        return hash((self.wall_time_ms, self.logical_counter, self.node_id))
    
    @classmethod
    def now(cls, node_id: str) -> "HybridLogicalClock":
        """Create HLC at current physical time."""
        return cls(
            wall_time_ms=int(time.time() * 1000),
            logical_counter=0,
            node_id=node_id,
        )
```

### WAL Segment

```python
"""
hyperscale/distributed_rewrite/ledger/storage/wal_segment.py
"""

import mmap
import os
import struct
from dataclasses import dataclass, field
from pathlib import Path
from typing import Iterator

from hyperscale.distributed_rewrite.ledger.models.wal_entry import WALEntry


class SegmentFullError(Exception):
    """Raised when WAL segment cannot accept more entries."""
    pass


@dataclass
class WALSegment:
    """
    Single segment file of the WAL.
    
    Segments are:
    - Pre-allocated for performance (no fragmentation)
    - Memory-mapped for efficient I/O
    - Sealed when full (immutable after seal)
    - Garbage collected when all entries COMPACTED
    
    File format:
    ┌────────────────────────────────────────────────────────────────┐
    │                     WAL Segment File (64MB)                     │
    ├────────────────────────────────────────────────────────────────┤
    │ Entry 1 │ Entry 2 │ ... │ Entry N │ [Zero-filled space]        │
    └────────────────────────────────────────────────────────────────┘
    """
    segment_id: int
    path: Path
    max_size: int = 64 * 1024 * 1024  # 64MB default
    
    _mmap: mmap.mmap | None = field(default=None, repr=False)
    _write_offset: int = field(default=0, repr=False)
    _sealed: bool = field(default=False, repr=False)
    
    def open(self, create: bool = False) -> None:
        """Open segment file with memory mapping."""
        if create and not self.path.exists():
            # Pre-allocate file with zeros
            with open(self.path, "wb") as file_handle:
                file_handle.write(b"\x00" * self.max_size)
        
        file_descriptor = os.open(str(self.path), os.O_RDWR)
        self._mmap = mmap.mmap(file_descriptor, self.max_size)
        os.close(file_descriptor)
        
        # Find write offset by scanning for end of data
        self._write_offset = self._find_write_offset()
    
    def _find_write_offset(self) -> int:
        """Find the end of valid data in segment."""
        offset = 0
        while offset < self.max_size - WALEntry.HEADER_SIZE:
            # Read length field (bytes 4-8 of entry header)
            length_bytes = self._mmap[offset + 4:offset + 8]
            if length_bytes == b"\x00\x00\x00\x00":
                break
            length = struct.unpack("<I", length_bytes)[0]
            offset += WALEntry.HEADER_SIZE + length
        return offset
    
    def append(self, entry: WALEntry) -> int:
        """
        Append entry to segment.
        
        Returns: Offset where entry was written
        Raises: SegmentFullError if segment is full or sealed
        """
        if self._sealed:
            raise SegmentFullError("Segment is sealed")
        
        data = entry.serialize()
        if self._write_offset + len(data) > self.max_size:
            raise SegmentFullError("Segment is full")
        
        offset = self._write_offset
        self._mmap[offset:offset + len(data)] = data
        self._write_offset += len(data)
        
        return offset
    
    def sync(self) -> None:
        """Flush changes to disk (fsync)."""
        if self._mmap:
            self._mmap.flush()
    
    def read_entry(self, offset: int) -> WALEntry:
        """Read entry at given offset."""
        # Read header to get length
        header = self._mmap[offset:offset + WALEntry.HEADER_SIZE]
        length = struct.unpack("<I", header[4:8])[0]
        
        # Read full entry
        data = self._mmap[offset:offset + WALEntry.HEADER_SIZE + length]
        return WALEntry.deserialize(bytes(data))
    
    def iterate_entries(self) -> Iterator[tuple[int, WALEntry]]:
        """Iterate all entries in segment with their offsets."""
        offset = 0
        while offset < self._write_offset:
            entry = self.read_entry(offset)
            yield offset, entry
            offset += WALEntry.HEADER_SIZE + len(entry.payload)
    
    def seal(self) -> None:
        """Seal segment - no more writes allowed."""
        self._sealed = True
    
    def close(self) -> None:
        """Close segment and release resources."""
        if self._mmap:
            self._mmap.close()
            self._mmap = None
    
    @property
    def is_sealed(self) -> bool:
        """Check if segment is sealed."""
        return self._sealed
    
    @property
    def bytes_used(self) -> int:
        """Get number of bytes used in segment."""
        return self._write_offset
    
    @property
    def bytes_available(self) -> int:
        """Get number of bytes available in segment."""
        return self.max_size - self._write_offset
```

### Node WAL Manager

```python
"""
hyperscale/distributed_rewrite/ledger/storage/node_wal.py
"""

import asyncio
import time
from dataclasses import dataclass, field
from enum import IntEnum
from pathlib import Path
from typing import TYPE_CHECKING

from hyperscale.logging import Logger
from hyperscale.distributed_rewrite.ledger.models.wal_entry import WALEntry, WALEntryState
from hyperscale.distributed_rewrite.ledger.models.hlc import HybridLogicalClock
from hyperscale.distributed_rewrite.ledger.storage.wal_segment import WALSegment, SegmentFullError

if TYPE_CHECKING:
    from hyperscale.distributed_rewrite.ledger.models.recovery_result import RecoveryResult


class WALDurability(IntEnum):
    """Durability levels for WAL writes."""
    MEMORY = 0        # No sync (unsafe, testing only)
    WRITE = 1         # After write() syscall
    FSYNC = 2         # After fsync (per entry)
    FSYNC_BATCH = 3   # After batched fsync (default)


@dataclass
class NodeWAL:
    """
    Per-node Write-Ahead Log manager.
    
    Provides:
    - Append with configurable durability
    - Batched fsync for throughput
    - Crash recovery
    - State transition tracking
    - Garbage collection of compacted entries
    
    Usage:
        wal = NodeWAL(
            data_dir=Path("/data/wal"),
            node_id="gate-1",
        )
        
        recovery = await wal.open()
        
        lsn = await wal.append(
            entry_type=EventType.JOB_CREATED,
            payload=event.serialize(),
        )
        
        await wal.update_state(lsn, WALEntryState.REGIONAL)
    """
    
    data_dir: Path
    node_id: str
    segment_size: int = 64 * 1024 * 1024  # 64MB
    sync_mode: WALDurability = WALDurability.FSYNC_BATCH
    batch_size: int = 100
    batch_timeout_ms: int = 10
    
    _logger: Logger = field(default_factory=Logger, repr=False)
    _segments: list[WALSegment] = field(default_factory=list, repr=False)
    _active_segment: WALSegment | None = field(default=None, repr=False)
    _next_lsn: int = field(default=1, repr=False)
    _hlc: HybridLogicalClock | None = field(default=None, repr=False)
    _pending_batch: list[tuple[WALEntry, asyncio.Future]] = field(default_factory=list, repr=False)
    _batch_lock: asyncio.Lock | None = field(default=None, repr=False)
    _state_index: dict[int, WALEntryState] = field(default_factory=dict, repr=False)
    _batch_task: asyncio.Task | None = field(default=None, repr=False)
    
    def __post_init__(self):
        self.data_dir.mkdir(parents=True, exist_ok=True)
        self._hlc = HybridLogicalClock.now(self.node_id)
    
    def _get_batch_lock(self) -> asyncio.Lock:
        """Get or create batch lock (lazy initialization)."""
        if self._batch_lock is None:
            self._batch_lock = asyncio.Lock()
        return self._batch_lock
    
    async def open(self) -> "RecoveryResult":
        """
        Open WAL and recover state from existing segments.
        
        Returns: RecoveryResult with recovery statistics and pending entries
        """
        from hyperscale.distributed_rewrite.ledger.models.recovery_result import RecoveryResult
        
        async with self._logger.context(
            name="node_wal",
            path="hyperscale.ledger.log.json",
            template="{timestamp} - {level} - {thread_id} - {filename}:{function_name}.{line_number} - {message}",
        ) as ctx:
            await ctx.log(
                Entry(
                    message=f"Opening WAL at {self.data_dir}",
                    level=LogLevel.INFO,
                )
            )
            
            # Discover existing segments
            segment_files = sorted(self.data_dir.glob("segment_*.wal"))
            
            recovered_entries = 0
            max_lsn = 0
            max_hlc = self._hlc
            pending_entries: list[WALEntry] = []
            
            for segment_path in segment_files:
                segment_id = int(segment_path.stem.split("_")[1])
                segment = WALSegment(segment_id, segment_path, self.segment_size)
                segment.open(create=False)
                
                # Scan entries
                for offset, entry in segment.iterate_entries():
                    recovered_entries += 1
                    max_lsn = max(max_lsn, entry.lsn)
                    if entry.hlc > max_hlc:
                        max_hlc = entry.hlc
                    
                    # Track entries not yet globally committed
                    if entry.state < WALEntryState.GLOBAL:
                        pending_entries.append(entry)
                    
                    self._state_index[entry.lsn] = entry.state
                
                self._segments.append(segment)
            
            # Set up for new writes
            self._next_lsn = max_lsn + 1
            self._hlc = max_hlc
            
            # Create new active segment if needed
            if not self._segments or self._segments[-1].bytes_available < self.segment_size * 0.1:
                await self._create_new_segment()
            else:
                self._active_segment = self._segments[-1]
            
            await ctx.log(
                Entry(
                    message=f"WAL recovery complete: {recovered_entries} entries, max_lsn={max_lsn}, {len(pending_entries)} pending",
                    level=LogLevel.INFO,
                )
            )
            
            return RecoveryResult(
                recovered_entries=recovered_entries,
                max_lsn=max_lsn,
                max_hlc=max_hlc,
                pending_entries=pending_entries,
            )
    
    async def _create_new_segment(self) -> None:
        """Create a new segment for writing."""
        segment_id = len(self._segments)
        segment_path = self.data_dir / f"segment_{segment_id:08d}.wal"
        segment = WALSegment(segment_id, segment_path, self.segment_size)
        segment.open(create=True)
        
        if self._active_segment:
            self._active_segment.seal()
        
        self._segments.append(segment)
        self._active_segment = segment
    
    async def append(
        self,
        entry_type: int,
        payload: bytes,
        durability: WALDurability | None = None,
    ) -> int:
        """
        Append entry to WAL with specified durability.
        
        Args:
            entry_type: Event type discriminator
            payload: Serialized event data
            durability: Durability level (uses default if None)
        
        Returns: LSN of appended entry
        """
        durability = durability or self.sync_mode
        
        # Generate timestamps
        self._hlc = self._hlc.tick(int(time.time() * 1000))
        lsn = self._next_lsn
        self._next_lsn += 1
        
        entry = WALEntry(
            lsn=lsn,
            hlc=self._hlc,
            state=WALEntryState.PENDING,
            entry_type=entry_type,
            payload=payload,
        )
        
        # Write to segment
        try:
            self._active_segment.append(entry)
        except SegmentFullError:
            await self._create_new_segment()
            self._active_segment.append(entry)
        
        # Track state
        self._state_index[lsn] = WALEntryState.PENDING
        
        # Handle durability
        match durability:
            case WALDurability.MEMORY:
                pass  # No sync
            
            case WALDurability.WRITE:
                pass  # OS will sync eventually
            
            case WALDurability.FSYNC:
                self._active_segment.sync()
            
            case WALDurability.FSYNC_BATCH:
                await self._batch_sync(entry)
        
        return lsn
    
    async def _batch_sync(self, entry: WALEntry) -> None:
        """Batch multiple entries before fsync for throughput."""
        future: asyncio.Future = asyncio.Future()
        
        async with self._get_batch_lock():
            self._pending_batch.append((entry, future))
            
            if len(self._pending_batch) >= self.batch_size:
                # Batch is full, sync now
                await self._flush_batch()
            elif self._batch_task is None or self._batch_task.done():
                # Schedule timeout flush
                self._batch_task = asyncio.create_task(self._batch_timeout_flush())
        
        await future
    
    async def _batch_timeout_flush(self) -> None:
        """Flush batch after timeout."""
        await asyncio.sleep(self.batch_timeout_ms / 1000)
        async with self._get_batch_lock():
            if self._pending_batch:
                await self._flush_batch()
    
    async def _flush_batch(self) -> None:
        """Flush pending batch and complete futures."""
        if not self._pending_batch:
            return
        
        # Perform single fsync for entire batch
        self._active_segment.sync()
        
        # Complete all futures
        for entry, future in self._pending_batch:
            if not future.done():
                future.set_result(entry.lsn)
        
        self._pending_batch.clear()
    
    async def update_state(self, lsn: int, new_state: WALEntryState) -> None:
        """
        Update the commit state of an entry.
        
        Called when entry progresses through commit pipeline:
        PENDING -> REGIONAL -> GLOBAL -> APPLIED -> COMPACTED
        """
        if lsn not in self._state_index:
            return
        
        current_state = self._state_index[lsn]
        if new_state.value <= current_state.value:
            return  # State can only advance
        
        self._state_index[lsn] = new_state
    
    async def read_pending(self) -> list[WALEntry]:
        """Read all entries not yet globally committed."""
        pending = []
        for segment in self._segments:
            for offset, entry in segment.iterate_entries():
                if self._state_index.get(entry.lsn, entry.state) < WALEntryState.GLOBAL:
                    pending.append(entry)
        return pending
    
    async def read_range(self, start_lsn: int, end_lsn: int) -> list[WALEntry]:
        """Read entries in LSN range (inclusive)."""
        entries = []
        for segment in self._segments:
            for offset, entry in segment.iterate_entries():
                if start_lsn <= entry.lsn <= end_lsn:
                    entries.append(entry)
        return sorted(entries, key=lambda e: e.lsn)
    
    async def compact(self, safe_lsn: int) -> int:
        """
        Compact entries up to safe_lsn.
        
        safe_lsn: LSN up to which all entries have been
        globally committed and checkpointed.
        
        Returns: Number of segments removed
        """
        removed = 0
        
        for segment in list(self._segments):
            if segment == self._active_segment:
                continue
            
            # Check if all entries in segment are safe to remove
            all_safe = True
            max_segment_lsn = 0
            
            for offset, entry in segment.iterate_entries():
                max_segment_lsn = max(max_segment_lsn, entry.lsn)
                if entry.lsn > safe_lsn:
                    all_safe = False
                    break
            
            if all_safe and max_segment_lsn <= safe_lsn:
                segment.close()
                segment.path.unlink()
                self._segments.remove(segment)
                removed += 1
        
        return removed
    
    async def close(self) -> None:
        """Close WAL and release resources."""
        # Flush any pending writes
        async with self._get_batch_lock():
            await self._flush_batch()
        
        # Cancel batch task if running
        if self._batch_task and not self._batch_task.done():
            self._batch_task.cancel()
            try:
                await self._batch_task
            except asyncio.CancelledError:
                pass
        
        for segment in self._segments:
            segment.close()
    
    @property
    def current_lsn(self) -> int:
        """Get the current (next to be assigned) LSN."""
        return self._next_lsn
    
    @property
    def current_hlc(self) -> HybridLogicalClock:
        """Get the current HLC."""
        return self._hlc
```

### Job Ledger Entry

```python
"""
hyperscale/distributed_rewrite/ledger/models/ledger_entry.py
"""

from dataclasses import dataclass
from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from hyperscale.distributed_rewrite.ledger.models.hlc import HybridLogicalClock
    from hyperscale.distributed_rewrite.ledger.events.base import JobEvent


@dataclass(slots=True)
class JobLedgerEntry:
    """
    Entry in the Global Job Ledger.
    
    Contains:
    - Job identification and fence token
    - Causal timestamp (HLC)
    - The actual event
    - Source tracking for provenance
    """
    job_id: str
    fence_token: int
    hlc: "HybridLogicalClock"
    event: "JobEvent"
    source_node: str
    source_region: str
    source_lsn: int
    
    def conflicts_with(self, other: "JobLedgerEntry") -> bool:
        """Detect conflicting concurrent operations."""
        if self.job_id != other.job_id:
            return False
        # Same fence token = concurrent writes
        return self.fence_token == other.fence_token
    
    @staticmethod
    def resolve_conflict(
        entry_a: "JobLedgerEntry",
        entry_b: "JobLedgerEntry",
    ) -> "JobLedgerEntry":
        """
        Deterministic conflict resolution.
        
        Priority order:
        1. Cancellation always wins (fail-safe)
        2. Higher fence token wins (later operation)
        3. HLC ordering (causal precedence)
        4. Lexicographic node_id (deterministic tie-breaker)
        """
        from hyperscale.distributed_rewrite.ledger.events.cancellation import (
            JobCancellationRequested,
        )
        
        # Cancellation is highest priority (fail-safe)
        if isinstance(entry_a.event, JobCancellationRequested):
            return entry_a
        if isinstance(entry_b.event, JobCancellationRequested):
            return entry_b
        
        # Higher fence token wins
        if entry_a.fence_token != entry_b.fence_token:
            return entry_a if entry_a.fence_token > entry_b.fence_token else entry_b
        
        # HLC ordering
        if entry_a.hlc != entry_b.hlc:
            return entry_a if entry_a.hlc > entry_b.hlc else entry_b
        
        # Deterministic tie-breaker
        return entry_a if entry_a.hlc.node_id < entry_b.hlc.node_id else entry_b
```

### Commit Pipeline

```python
"""
hyperscale/distributed_rewrite/ledger/pipeline/commit_pipeline.py
"""

import asyncio
from dataclasses import dataclass, field
from enum import IntEnum
from typing import TYPE_CHECKING

from hyperscale.logging import Logger
from hyperscale.distributed_rewrite.ledger.models.wal_entry import WALEntryState
from hyperscale.distributed_rewrite.ledger.models.ledger_entry import JobLedgerEntry
from hyperscale.distributed_rewrite.ledger.storage.node_wal import NodeWAL, WALDurability

if TYPE_CHECKING:
    from hyperscale.distributed_rewrite.ledger.consensus.regional import RegionalConsensusGroup
    from hyperscale.distributed_rewrite.ledger.global_ledger import GlobalJobLedger
    from hyperscale.distributed_rewrite.ledger.events.base import JobEvent


class CommitDurability(IntEnum):
    """Durability levels for commit pipeline."""
    LOCAL = 1      # Local WAL only
    REGIONAL = 2   # Regional consensus
    GLOBAL = 3     # Global ledger


@dataclass(slots=True)
class CommitResult:
    """Result of commit operation."""
    lsn: int
    durability_achieved: CommitDurability
    regional_confirmed: bool
    global_confirmed: bool
    error: str | None = None


@dataclass
class CommitPipeline:
    """
    Three-stage commit pipeline for job operations.
    
    Stage 1: Local WAL (immediate durability, single node)
    Stage 2: Regional Consensus (fast, within-DC replication)
    Stage 3: Global Ledger (cross-region, authoritative)
    
    Each stage provides progressively stronger guarantees:
    - Local: Survives process crash (<1ms)
    - Regional: Survives node failure (2-10ms)
    - Global: Survives region failure (50-300ms)
    """
    
    node_id: str
    region_id: str
    wal: NodeWAL
    regional_consensus: "RegionalConsensusGroup"
    global_ledger: "GlobalJobLedger"
    
    _logger: Logger = field(default_factory=Logger, repr=False)
    _pending_regional: dict[int, asyncio.Future] = field(default_factory=dict, repr=False)
    _pending_global: dict[int, asyncio.Future] = field(default_factory=dict, repr=False)
    
    async def commit_job_event(
        self,
        event: "JobEvent",
        required_durability: CommitDurability = CommitDurability.REGIONAL,
    ) -> CommitResult:
        """
        Commit a job event through the pipeline.
        
        Args:
            event: The job event to commit
            required_durability: Minimum durability before returning
        
        Returns:
            CommitResult with achieved durability and status
        """
        async with self._logger.context(
            name="commit_pipeline",
            path="hyperscale.ledger.log.json",
            template="{timestamp} - {level} - {thread_id} - {filename}:{function_name}.{line_number} - {message}",
        ) as ctx:
            # Stage 1: Local WAL
            payload = event.serialize()
            lsn = await self.wal.append(
                entry_type=event.event_type,
                payload=payload,
                durability=WALDurability.FSYNC_BATCH,
            )
            
            await ctx.log(
                Entry(
                    message=f"Event {event.event_type} for job {event.job_id} written to WAL at LSN {lsn}",
                    level=LogLevel.DEBUG,
                )
            )
            
            if required_durability == CommitDurability.LOCAL:
                return CommitResult(
                    lsn=lsn,
                    durability_achieved=CommitDurability.LOCAL,
                    regional_confirmed=False,
                    global_confirmed=False,
                )
            
            # Stage 2: Regional Consensus
            regional_future: asyncio.Future = asyncio.Future()
            self._pending_regional[lsn] = regional_future
            
            await self.regional_consensus.propose(
                lsn=lsn,
                hlc=self.wal.current_hlc,
                event=event,
            )
            
            try:
                await asyncio.wait_for(regional_future, timeout=5.0)
                await self.wal.update_state(lsn, WALEntryState.REGIONAL)
                
                await ctx.log(
                    Entry(
                        message=f"Event LSN {lsn} committed to regional consensus",
                        level=LogLevel.DEBUG,
                    )
                )
            except asyncio.TimeoutError:
                await ctx.log(
                    Entry(
                        message=f"Regional consensus timeout for LSN {lsn}",
                        level=LogLevel.WARNING,
                    )
                )
                return CommitResult(
                    lsn=lsn,
                    durability_achieved=CommitDurability.LOCAL,
                    regional_confirmed=False,
                    global_confirmed=False,
                    error="Regional consensus timeout",
                )
            
            if required_durability == CommitDurability.REGIONAL:
                # Start async global replication but don't wait
                asyncio.create_task(self._replicate_to_global(lsn, event))
                
                return CommitResult(
                    lsn=lsn,
                    durability_achieved=CommitDurability.REGIONAL,
                    regional_confirmed=True,
                    global_confirmed=False,
                )
            
            # Stage 3: Global Ledger
            global_future: asyncio.Future = asyncio.Future()
            self._pending_global[lsn] = global_future
            
            await self._replicate_to_global(lsn, event)
            
            try:
                await asyncio.wait_for(global_future, timeout=30.0)
                await self.wal.update_state(lsn, WALEntryState.GLOBAL)
                
                await ctx.log(
                    Entry(
                        message=f"Event LSN {lsn} committed to global ledger",
                        level=LogLevel.INFO,
                    )
                )
            except asyncio.TimeoutError:
                await ctx.log(
                    Entry(
                        message=f"Global replication timeout for LSN {lsn}",
                        level=LogLevel.WARNING,
                    )
                )
                return CommitResult(
                    lsn=lsn,
                    durability_achieved=CommitDurability.REGIONAL,
                    regional_confirmed=True,
                    global_confirmed=False,
                    error="Global replication timeout",
                )
            
            return CommitResult(
                lsn=lsn,
                durability_achieved=CommitDurability.GLOBAL,
                regional_confirmed=True,
                global_confirmed=True,
            )
    
    async def _replicate_to_global(self, lsn: int, event: "JobEvent") -> None:
        """Replicate event to global ledger."""
        entry = JobLedgerEntry(
            job_id=event.job_id,
            fence_token=event.fence_token,
            hlc=self.wal.current_hlc,
            event=event,
            source_node=self.node_id,
            source_region=self.region_id,
            source_lsn=lsn,
        )
        
        await self.global_ledger.append(entry)
    
    def on_regional_committed(self, lsn: int) -> None:
        """Callback when regional consensus commits an entry."""
        if lsn in self._pending_regional:
            future = self._pending_regional.pop(lsn)
            if not future.done():
                future.set_result(True)
    
    def on_global_committed(self, lsn: int) -> None:
        """Callback when global ledger commits an entry."""
        if lsn in self._pending_global:
            future = self._pending_global.pop(lsn)
            if not future.done():
                future.set_result(True)
```

### Checkpoint Manager

```python
"""
hyperscale/distributed_rewrite/ledger/checkpoint/checkpoint_manager.py
"""

import asyncio
import time
import uuid
from dataclasses import dataclass, field
from pathlib import Path
from typing import TYPE_CHECKING

from hyperscale.logging import Logger
from hyperscale.distributed_rewrite.ledger.models.wal_entry import WALEntry

if TYPE_CHECKING:
    from hyperscale.distributed_rewrite.ledger.storage.node_wal import NodeWAL
    from hyperscale.distributed_rewrite.ledger.state_machine import JobStateMachine


@dataclass(slots=True)
class Checkpoint:
    """Checkpoint file contents."""
    checkpoint_id: str
    created_at: float
    local_lsn: int
    regional_lsn: int
    global_lsn: int
    state_snapshot: bytes


@dataclass
class CheckpointManager:
    """
    Manages checkpoints for efficient recovery.
    
    Checkpoints capture:
    - Local state machine snapshot
    - LSN watermarks (local, regional, global)
    - Active job state
    
    Enables:
    - Fast recovery (skip WAL replay for old entries)
    - WAL compaction (remove checkpointed entries)
    - State transfer to new nodes
    """
    
    wal: "NodeWAL"
    state_machine: "JobStateMachine"
    checkpoint_dir: Path
    checkpoint_interval_entries: int = 100_000
    checkpoint_interval_seconds: float = 300.0
    max_checkpoints_to_keep: int = 3
    
    _logger: Logger = field(default_factory=Logger, repr=False)
    _last_checkpoint_lsn: int = field(default=0, repr=False)
    _last_checkpoint_time: float = field(default=0.0, repr=False)
    _entries_since_checkpoint: int = field(default=0, repr=False)
    
    def __post_init__(self):
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
    
    async def maybe_checkpoint(self, current_lsn: int) -> bool:
        """
        Create checkpoint if thresholds exceeded.
        
        Returns: True if checkpoint was created
        """
        self._entries_since_checkpoint += 1
        now = time.monotonic()
        
        should_checkpoint = (
            self._entries_since_checkpoint >= self.checkpoint_interval_entries or
            now - self._last_checkpoint_time >= self.checkpoint_interval_seconds
        )
        
        if should_checkpoint:
            await self.create_checkpoint(current_lsn)
            return True
        return False
    
    async def create_checkpoint(self, lsn: int) -> Checkpoint:
        """
        Create a consistent checkpoint.
        
        Steps:
        1. Snapshot state machine (atomic)
        2. Record LSN watermarks
        3. Write checkpoint file
        4. Trigger WAL compaction
        5. Clean old checkpoints
        """
        async with self._logger.context(
            name="checkpoint_manager",
            path="hyperscale.ledger.log.json",
            template="{timestamp} - {level} - {thread_id} - {filename}:{function_name}.{line_number} - {message}",
        ) as ctx:
            await ctx.log(
                Entry(
                    message=f"Creating checkpoint at LSN {lsn}",
                    level=LogLevel.INFO,
                )
            )
            
            # 1. Snapshot state machine
            state_snapshot = await self.state_machine.snapshot()
            
            # 2. Record watermarks
            checkpoint = Checkpoint(
                checkpoint_id=uuid.uuid4().hex,
                created_at=time.time(),
                local_lsn=lsn,
                regional_lsn=await self._get_regional_watermark(),
                global_lsn=await self._get_global_watermark(),
                state_snapshot=state_snapshot,
            )
            
            # 3. Write checkpoint file
            checkpoint_path = self.checkpoint_dir / f"checkpoint_{checkpoint.checkpoint_id}.ckpt"
            await self._write_checkpoint_file(checkpoint_path, checkpoint)
            
            # 4. Update tracking
            self._last_checkpoint_lsn = lsn
            self._last_checkpoint_time = time.monotonic()
            self._entries_since_checkpoint = 0
            
            await ctx.log(
                Entry(
                    message=f"Checkpoint {checkpoint.checkpoint_id} created at LSN {lsn}",
                    level=LogLevel.INFO,
                )
            )
            
            # 5. Trigger async WAL compaction and cleanup
            asyncio.create_task(self._compact_and_cleanup(checkpoint))
            
            return checkpoint
    
    async def _compact_and_cleanup(self, checkpoint: Checkpoint) -> None:
        """Compact WAL and clean old checkpoints."""
        # Only compact if global ledger has confirmed
        safe_lsn = min(checkpoint.local_lsn, checkpoint.global_lsn)
        removed_segments = await self.wal.compact(safe_lsn)
        
        # Clean old checkpoints
        await self._clean_old_checkpoints()
    
    async def _clean_old_checkpoints(self) -> int:
        """Remove old checkpoints, keeping most recent N."""
        checkpoint_files = sorted(
            self.checkpoint_dir.glob("checkpoint_*.ckpt"),
            key=lambda p: p.stat().st_mtime,
            reverse=True,
        )
        
        removed = 0
        for checkpoint_file in checkpoint_files[self.max_checkpoints_to_keep:]:
            checkpoint_file.unlink()
            removed += 1
        
        return removed
    
    async def recover_from_checkpoint(self) -> tuple[Checkpoint | None, list[WALEntry]]:
        """
        Recover from latest checkpoint + WAL replay.
        
        Returns:
        - Latest valid checkpoint (or None)
        - WAL entries to replay after checkpoint
        """
        async with self._logger.context(
            name="checkpoint_manager",
            path="hyperscale.ledger.log.json",
            template="{timestamp} - {level} - {thread_id} - {filename}:{function_name}.{line_number} - {message}",
        ) as ctx:
            # Find latest valid checkpoint
            checkpoint = await self._find_latest_checkpoint()
            
            if checkpoint is None:
                await ctx.log(
                    Entry(
                        message="No checkpoint found, full WAL replay required",
                        level=LogLevel.WARNING,
                    )
                )
                # Open WAL for full replay
                wal_recovery = await self.wal.open()
                return None, wal_recovery.pending_entries
            
            await ctx.log(
                Entry(
                    message=f"Recovering from checkpoint {checkpoint.checkpoint_id} at LSN {checkpoint.local_lsn}",
                    level=LogLevel.INFO,
                )
            )
            
            # Restore state from checkpoint
            await self.state_machine.restore(checkpoint.state_snapshot)
            
            # Open WAL and find entries after checkpoint
            await self.wal.open()
            entries_to_replay = await self.wal.read_range(
                checkpoint.local_lsn + 1,
                self.wal.current_lsn - 1,
            )
            
            await ctx.log(
                Entry(
                    message=f"Recovery: replaying {len(entries_to_replay)} WAL entries after checkpoint",
                    level=LogLevel.INFO,
                )
            )
            
            return checkpoint, entries_to_replay
    
    async def _find_latest_checkpoint(self) -> Checkpoint | None:
        """Find and validate latest checkpoint."""
        checkpoint_files = sorted(
            self.checkpoint_dir.glob("checkpoint_*.ckpt"),
            key=lambda p: p.stat().st_mtime,
            reverse=True,
        )
        
        for checkpoint_path in checkpoint_files:
            try:
                checkpoint = await self._read_checkpoint_file(checkpoint_path)
                return checkpoint
            except Exception:
                # Corrupted checkpoint, try next
                continue
        
        return None
    
    async def _write_checkpoint_file(self, path: Path, checkpoint: Checkpoint) -> None:
        """Write checkpoint to file."""
        import pickle
        
        data = pickle.dumps(checkpoint)
        
        # Write atomically via temp file + rename
        temp_path = path.with_suffix(".tmp")
        temp_path.write_bytes(data)
        temp_path.rename(path)
    
    async def _read_checkpoint_file(self, path: Path) -> Checkpoint:
        """Read checkpoint from file."""
        import pickle
        
        data = path.read_bytes()
        return pickle.loads(data)
    
    async def _get_regional_watermark(self) -> int:
        """Get highest LSN confirmed by regional consensus."""
        # Would query regional consensus group
        return self._last_checkpoint_lsn
    
    async def _get_global_watermark(self) -> int:
        """Get highest LSN confirmed by global ledger."""
        # Would query global ledger
        return self._last_checkpoint_lsn
```

### Data Plane Stats Aggregator (Uses Logger)

```python
"""
hyperscale/distributed_rewrite/ledger/data_plane/stats_aggregator.py

This component uses the hyperscale/logging Logger for stats streaming.
Unlike the WAL (Control Plane), stats do NOT require:
- fsync guarantees
- Sequence numbers
- Binary format
- Read-back capability

Stats are fire-and-forget with eventual consistency.
"""

import asyncio
import time
from dataclasses import dataclass, field
from typing import TYPE_CHECKING

from hyperscale.logging import Logger
from hyperscale.logging.models import Entry, LogLevel

if TYPE_CHECKING:
    from hyperscale.taskex import TaskRunner


@dataclass
class AggregatedJobStats:
    """Aggregated stats for a job, sent Manager → Gate."""

    job_id: str
    dc_id: str
    timestamp: float

    # Counts
    workflows_running: int = 0
    workflows_completed: int = 0
    workflows_failed: int = 0

    # Rates
    requests_per_second: float = 0.0
    errors_per_second: float = 0.0

    # Latencies (percentiles)
    latency_p50_ms: float = 0.0
    latency_p95_ms: float = 0.0
    latency_p99_ms: float = 0.0

    # Resource usage
    cpu_percent_avg: float = 0.0
    memory_mb_avg: float = 0.0


@dataclass
class StatsAggregatorConfig:
    """Configuration for stats aggregation."""

    # Aggregation intervals
    worker_batch_interval_ms: int = 100
    worker_batch_max_events: int = 1000
    manager_aggregate_interval_ms: int = 500
    manager_sample_rate: float = 0.1

    # Storage
    stats_log_path: str = "hyperscale.stats.log.json"
    stats_retention_seconds: int = 3600


@dataclass
class StatsAggregator:
    """
    Aggregates stats from workers and streams to gates.

    Uses Logger for storage - NOT the WAL. Stats are:
    - High volume (10,000+ events/second)
    - Eventually consistent (OK to lose some)
    - JSON format (human readable)
    - No durability guarantees needed

    This is the DATA PLANE component.
    """

    node_id: str
    dc_id: str
    config: StatsAggregatorConfig
    task_runner: "TaskRunner"

    _logger: Logger = field(default_factory=Logger, repr=False)
    _pending_stats: dict[str, list[dict]] = field(default_factory=dict, repr=False)
    _aggregated_stats: dict[str, AggregatedJobStats] = field(default_factory=dict, repr=False)
    _lock: asyncio.Lock | None = field(default=None, repr=False)
    _flush_task: asyncio.Task | None = field(default=None, repr=False)
    _running: bool = field(default=False, repr=False)

    def _get_lock(self) -> asyncio.Lock:
        """Lazy lock initialization."""
        if self._lock is None:
            self._lock = asyncio.Lock()
        return self._lock

    async def start(self) -> None:
        """Start the stats aggregation loop."""
        self._running = True

        # Configure logger for stats (uses Logger, NOT WAL)
        self._logger.configure(
            name="stats_aggregator",
            path=self.config.stats_log_path,
            template="{timestamp} - {level} - {message}",
            models={
                "stats": (Entry, {"level": LogLevel.INFO}),
            },
        )

        # Start aggregation loop
        self._flush_task = self.task_runner.run(self._aggregation_loop)

    async def stop(self) -> None:
        """Stop the stats aggregation loop."""
        self._running = False
        if self._flush_task:
            self._flush_task.cancel()
            try:
                await self._flush_task
            except asyncio.CancelledError:
                pass

        # Final flush
        await self._flush_aggregated_stats()

    async def record_progress(
        self,
        job_id: str,
        workflow_id: str,
        status: str,
        latency_ms: float | None = None,
        cpu_percent: float | None = None,
        memory_mb: float | None = None,
    ) -> None:
        """
        Record progress update from worker.

        This is fire-and-forget - no durability guarantees.
        Stats are batched and aggregated before sending to gate.
        """
        async with self._get_lock():
            if job_id not in self._pending_stats:
                self._pending_stats[job_id] = []

            self._pending_stats[job_id].append({
                "workflow_id": workflow_id,
                "status": status,
                "latency_ms": latency_ms,
                "cpu_percent": cpu_percent,
                "memory_mb": memory_mb,
                "timestamp": time.time(),
            })

            # Check if batch threshold reached
            if len(self._pending_stats[job_id]) >= self.config.worker_batch_max_events:
                await self._aggregate_job_stats(job_id)

    async def _aggregation_loop(self) -> None:
        """Periodic aggregation loop."""
        interval_seconds = self.config.manager_aggregate_interval_ms / 1000

        while self._running:
            await asyncio.sleep(interval_seconds)
            await self._flush_aggregated_stats()

    async def _aggregate_job_stats(self, job_id: str) -> None:
        """Aggregate pending stats for a job."""
        pending = self._pending_stats.pop(job_id, [])
        if not pending:
            return

        # Initialize or get existing aggregated stats
        if job_id not in self._aggregated_stats:
            self._aggregated_stats[job_id] = AggregatedJobStats(
                job_id=job_id,
                dc_id=self.dc_id,
                timestamp=time.time(),
            )

        stats = self._aggregated_stats[job_id]

        # Aggregate counts
        for event in pending:
            match event["status"]:
                case "running":
                    stats.workflows_running += 1
                case "completed":
                    stats.workflows_completed += 1
                    stats.workflows_running = max(0, stats.workflows_running - 1)
                case "failed":
                    stats.workflows_failed += 1
                    stats.workflows_running = max(0, stats.workflows_running - 1)

        # Aggregate latencies (sample for percentile estimation)
        latencies = [e["latency_ms"] for e in pending if e.get("latency_ms") is not None]
        if latencies:
            sorted_latencies = sorted(latencies)
            count = len(sorted_latencies)
            stats.latency_p50_ms = sorted_latencies[int(count * 0.5)]
            stats.latency_p95_ms = sorted_latencies[int(count * 0.95)]
            stats.latency_p99_ms = sorted_latencies[int(count * 0.99)]

        # Aggregate resource usage
        cpu_samples = [e["cpu_percent"] for e in pending if e.get("cpu_percent") is not None]
        if cpu_samples:
            stats.cpu_percent_avg = sum(cpu_samples) / len(cpu_samples)

        memory_samples = [e["memory_mb"] for e in pending if e.get("memory_mb") is not None]
        if memory_samples:
            stats.memory_mb_avg = sum(memory_samples) / len(memory_samples)

        stats.timestamp = time.time()

    async def _flush_aggregated_stats(self) -> None:
        """Flush aggregated stats to Logger and send to gate."""
        async with self._get_lock():
            # Aggregate any remaining pending stats
            for job_id in list(self._pending_stats.keys()):
                await self._aggregate_job_stats(job_id)

            # Log and send aggregated stats
            async with self._logger.context(name="stats_aggregator") as ctx:
                for job_id, stats in self._aggregated_stats.items():
                    # Log to file (uses Logger - JSON, no fsync)
                    await ctx.log(
                        Entry(
                            message=f"job={stats.job_id} dc={stats.dc_id} "
                                    f"running={stats.workflows_running} "
                                    f"completed={stats.workflows_completed} "
                                    f"failed={stats.workflows_failed} "
                                    f"p50={stats.latency_p50_ms:.1f}ms "
                                    f"p99={stats.latency_p99_ms:.1f}ms",
                            level=LogLevel.INFO,
                        )
                    )

            # Clear after flush (stats are fire-and-forget)
            self._aggregated_stats.clear()

    async def get_current_stats(self, job_id: str) -> AggregatedJobStats | None:
        """Get current aggregated stats for a job (local query)."""
        async with self._get_lock():
            return self._aggregated_stats.get(job_id)
```

### Acknowledgment Window Manager

```python
"""
hyperscale/distributed_rewrite/ledger/coordination/ack_window_manager.py

Manages acknowledgment windows for worker communication.
Workers don't provide immediate acks - instead we use time windows.
"""

import asyncio
from dataclasses import dataclass, field
from enum import Enum
from typing import TYPE_CHECKING

from hyperscale.logging import Logger
from hyperscale.logging.models import Entry, LogLevel

if TYPE_CHECKING:
    from hyperscale.taskex import TaskRunner


class AckWindowState(Enum):
    """State of an acknowledgment window."""
    DISPATCHED = "dispatched"        # Workflow sent, window started
    AWAITING_ACK = "awaiting_ack"    # Waiting for any communication
    CONFIRMED = "confirmed"          # Worker communicated, workflow running
    WINDOW_EXPIRED = "window_expired"  # No communication within window
    EXTEND_WINDOW = "extend_window"  # Worker healthy, extending window
    RESCHEDULE = "reschedule"        # Worker unhealthy, needs reschedule


@dataclass
class AckWindow:
    """Single acknowledgment window."""
    workflow_id: str
    job_id: str
    worker_id: str
    state: AckWindowState
    created_at: float
    last_communication: float | None = None
    extensions: int = 0


@dataclass
class AckWindowConfig:
    """Configuration for acknowledgment windows."""
    initial_window_seconds: float = 5.0    # Initial window duration
    max_extensions: int = 3                 # Max window extensions
    extension_duration_seconds: float = 5.0 # Duration per extension
    health_check_on_expire: bool = True     # Health check when window expires


@dataclass
class AckWindowManager:
    """
    Manages acknowledgment windows for worker communication.

    Workers under load cannot provide timely acks. Instead of blocking,
    we use time windows and infer state from any communication.

    State Transitions:
    - DISPATCHED → AWAITING_ACK (window started)
    - AWAITING_ACK → CONFIRMED (got progress/completion)
    - AWAITING_ACK → WINDOW_EXPIRED (no communication)
    - WINDOW_EXPIRED → EXTEND_WINDOW (worker healthy)
    - WINDOW_EXPIRED → RESCHEDULE (worker unhealthy)
    """

    config: AckWindowConfig
    health_checker: callable  # async fn(worker_id) -> bool
    task_runner: "TaskRunner"

    _windows: dict[str, AckWindow] = field(default_factory=dict, repr=False)
    _lock: asyncio.Lock | None = field(default=None, repr=False)
    _logger: Logger = field(default_factory=Logger, repr=False)
    _expiry_tasks: dict[str, asyncio.Task] = field(default_factory=dict, repr=False)

    def _get_lock(self) -> asyncio.Lock:
        """Lazy lock initialization."""
        if self._lock is None:
            self._lock = asyncio.Lock()
        return self._lock

    async def start_window(
        self,
        workflow_id: str,
        job_id: str,
        worker_id: str,
    ) -> None:
        """
        Start acknowledgment window for a dispatched workflow.

        Called after sending workflow to worker. Does NOT wait for ack.
        """
        import time

        async with self._get_lock():
            window = AckWindow(
                workflow_id=workflow_id,
                job_id=job_id,
                worker_id=worker_id,
                state=AckWindowState.AWAITING_ACK,
                created_at=time.time(),
            )
            self._windows[workflow_id] = window

            # Schedule expiry check (non-blocking)
            self._expiry_tasks[workflow_id] = self.task_runner.run(
                self._window_expiry_check,
                workflow_id,
            )

    async def on_worker_communication(
        self,
        workflow_id: str,
        communication_type: str,  # "progress", "completion", "error"
    ) -> AckWindowState:
        """
        Handle any communication from worker about a workflow.

        Any communication confirms the workflow is being processed.
        """
        import time

        async with self._get_lock():
            window = self._windows.get(workflow_id)
            if window is None:
                return AckWindowState.CONFIRMED  # Already completed

            window.last_communication = time.time()
            window.state = AckWindowState.CONFIRMED

            # Cancel expiry task
            if workflow_id in self._expiry_tasks:
                self._expiry_tasks[workflow_id].cancel()
                del self._expiry_tasks[workflow_id]

            return window.state

    async def _window_expiry_check(self, workflow_id: str) -> None:
        """Check if window has expired and take action."""
        import time

        await asyncio.sleep(self.config.initial_window_seconds)

        async with self._get_lock():
            window = self._windows.get(workflow_id)
            if window is None or window.state == AckWindowState.CONFIRMED:
                return  # Already handled

            window.state = AckWindowState.WINDOW_EXPIRED

        # Health check worker (outside lock)
        if self.config.health_check_on_expire:
            is_healthy = await self.health_checker(window.worker_id)

            async with self._get_lock():
                window = self._windows.get(workflow_id)
                if window is None:
                    return

                if is_healthy and window.extensions < self.config.max_extensions:
                    # Extend window
                    window.state = AckWindowState.EXTEND_WINDOW
                    window.extensions += 1

                    # Schedule another expiry check
                    self._expiry_tasks[workflow_id] = self.task_runner.run(
                        self._window_expiry_check,
                        workflow_id,
                    )
                else:
                    # Need to reschedule
                    window.state = AckWindowState.RESCHEDULE

    async def get_workflows_to_reschedule(self) -> list[AckWindow]:
        """Get workflows that need rescheduling."""
        async with self._get_lock():
            return [
                window for window in self._windows.values()
                if window.state == AckWindowState.RESCHEDULE
            ]

    async def complete_window(self, workflow_id: str) -> None:
        """Mark window as complete and clean up."""
        async with self._get_lock():
            if workflow_id in self._windows:
                del self._windows[workflow_id]
            if workflow_id in self._expiry_tasks:
                self._expiry_tasks[workflow_id].cancel()
                del self._expiry_tasks[workflow_id]
```

### Circuit Breaker for Cross-DC Communication

```python
"""
hyperscale/distributed_rewrite/ledger/reliability/circuit_breaker.py

Circuit breaker for cross-DC communication.
Prevents cascading failures when a DC is unavailable.
"""

import asyncio
import time
from dataclasses import dataclass, field
from enum import Enum
from typing import Callable, Awaitable, TypeVar

from hyperscale.logging import Logger
from hyperscale.logging.models import Entry, LogLevel


T = TypeVar("T")


class CircuitState(Enum):
    """Circuit breaker states."""
    CLOSED = "closed"        # Normal operation
    OPEN = "open"           # Failing fast, queueing requests
    HALF_OPEN = "half_open"  # Testing if service recovered


@dataclass
class CircuitBreakerConfig:
    """Configuration for circuit breaker."""
    failure_threshold: int = 5
    success_threshold: int = 3
    open_timeout_seconds: float = 30.0
    half_open_max_probes: int = 1
    queue_max_size: int = 1000
    queue_timeout_seconds: float = 60.0


@dataclass
class CircuitBreaker:
    """
    Circuit breaker for cross-DC communication.

    States:
    - CLOSED: Normal operation, requests pass through
    - OPEN: Service is failing, reject immediately and queue
    - HALF_OPEN: Testing recovery, allow limited probes

    When OPEN, operations are queued and replayed when circuit closes.
    """

    dc_id: str
    config: CircuitBreakerConfig

    _state: CircuitState = field(default=CircuitState.CLOSED, repr=False)
    _failure_count: int = field(default=0, repr=False)
    _success_count: int = field(default=0, repr=False)
    _last_failure_time: float = field(default=0.0, repr=False)
    _queue: asyncio.Queue = field(default_factory=asyncio.Queue, repr=False)
    _lock: asyncio.Lock | None = field(default=None, repr=False)
    _probe_in_progress: bool = field(default=False, repr=False)
    _logger: Logger = field(default_factory=Logger, repr=False)

    def _get_lock(self) -> asyncio.Lock:
        """Lazy lock initialization."""
        if self._lock is None:
            self._lock = asyncio.Lock()
        return self._lock

    @property
    def state(self) -> CircuitState:
        """Get current circuit state."""
        return self._state

    async def execute(
        self,
        operation: Callable[[], Awaitable[T]],
        fallback: Callable[[], Awaitable[T]] | None = None,
    ) -> T:
        """
        Execute operation through circuit breaker.

        Args:
            operation: The operation to execute
            fallback: Optional fallback if circuit is open

        Returns:
            Operation result

        Raises:
            CircuitOpenError: If circuit is open and no fallback provided
        """
        async with self._get_lock():
            # Check if we should transition from OPEN to HALF_OPEN
            if self._state == CircuitState.OPEN:
                if time.time() - self._last_failure_time >= self.config.open_timeout_seconds:
                    self._state = CircuitState.HALF_OPEN
                    self._success_count = 0

            current_state = self._state

        # Handle based on state
        match current_state:
            case CircuitState.CLOSED:
                return await self._execute_closed(operation)

            case CircuitState.OPEN:
                return await self._handle_open(operation, fallback)

            case CircuitState.HALF_OPEN:
                return await self._execute_half_open(operation)

    async def _execute_closed(
        self,
        operation: Callable[[], Awaitable[T]],
    ) -> T:
        """Execute in CLOSED state."""
        try:
            result = await operation()
            await self._on_success()
            return result
        except Exception as err:
            await self._on_failure()
            raise

    async def _execute_half_open(
        self,
        operation: Callable[[], Awaitable[T]],
    ) -> T:
        """Execute probe in HALF_OPEN state."""
        async with self._get_lock():
            if self._probe_in_progress:
                raise CircuitOpenError(f"Circuit to {self.dc_id} is half-open, probe in progress")
            self._probe_in_progress = True

        try:
            result = await operation()
            await self._on_probe_success()
            return result
        except Exception as err:
            await self._on_probe_failure()
            raise
        finally:
            async with self._get_lock():
                self._probe_in_progress = False

    async def _handle_open(
        self,
        operation: Callable[[], Awaitable[T]],
        fallback: Callable[[], Awaitable[T]] | None,
    ) -> T:
        """Handle request when circuit is OPEN."""
        # Queue the operation for later
        if self._queue.qsize() < self.config.queue_max_size:
            await self._queue.put((operation, time.time()))

        if fallback is not None:
            return await fallback()

        raise CircuitOpenError(f"Circuit to {self.dc_id} is open")

    async def _on_success(self) -> None:
        """Handle successful operation."""
        async with self._get_lock():
            self._failure_count = 0

    async def _on_failure(self) -> None:
        """Handle failed operation."""
        async with self._get_lock():
            self._failure_count += 1
            self._last_failure_time = time.time()

            if self._failure_count >= self.config.failure_threshold:
                self._state = CircuitState.OPEN

                async with self._logger.context(name="circuit_breaker") as ctx:
                    await ctx.log(
                        Entry(
                            message=f"Circuit to {self.dc_id} OPENED after {self._failure_count} failures",
                            level=LogLevel.WARNING,
                        )
                    )

    async def _on_probe_success(self) -> None:
        """Handle successful probe in HALF_OPEN state."""
        async with self._get_lock():
            self._success_count += 1

            if self._success_count >= self.config.success_threshold:
                self._state = CircuitState.CLOSED
                self._failure_count = 0

                async with self._logger.context(name="circuit_breaker") as ctx:
                    await ctx.log(
                        Entry(
                            message=f"Circuit to {self.dc_id} CLOSED after recovery",
                            level=LogLevel.INFO,
                        )
                    )

                # Replay queued operations
                asyncio.create_task(self._replay_queue())

    async def _on_probe_failure(self) -> None:
        """Handle failed probe in HALF_OPEN state."""
        async with self._get_lock():
            self._state = CircuitState.OPEN
            self._last_failure_time = time.time()

    async def _replay_queue(self) -> None:
        """Replay queued operations after circuit closes."""
        now = time.time()
        replayed = 0

        while not self._queue.empty():
            try:
                operation, queued_time = self._queue.get_nowait()

                # Skip expired entries
                if now - queued_time > self.config.queue_timeout_seconds:
                    continue

                # Execute with circuit breaker (may re-open if fails)
                await self.execute(operation)
                replayed += 1

            except Exception:
                break  # Stop replay on failure

        if replayed > 0:
            async with self._logger.context(name="circuit_breaker") as ctx:
                await ctx.log(
                    Entry(
                        message=f"Replayed {replayed} queued operations to {self.dc_id}",
                        level=LogLevel.INFO,
                    )
                )


class CircuitOpenError(Exception):
    """Raised when circuit breaker is open."""
    pass
```

---

## Part 10: Output Examples

### WAL Recovery Log Output

```json
{"timestamp": "2024-01-15T10:23:45.123Z", "level": "INFO", "thread_id": "140234567890", "filename": "node_wal.py", "function_name": "open", "line_number": 89, "message": "Opening WAL at /data/gate-1/wal"}
{"timestamp": "2024-01-15T10:23:45.234Z", "level": "INFO", "thread_id": "140234567890", "filename": "node_wal.py", "function_name": "open", "line_number": 142, "message": "WAL recovery complete: 45623 entries, max_lsn=45623, 127 pending"}
{"timestamp": "2024-01-15T10:23:45.345Z", "level": "INFO", "thread_id": "140234567890", "filename": "checkpoint_manager.py", "function_name": "recover_from_checkpoint", "line_number": 156, "message": "Recovering from checkpoint abc123def456 at LSN 45000"}
{"timestamp": "2024-01-15T10:23:45.456Z", "level": "INFO", "thread_id": "140234567890", "filename": "checkpoint_manager.py", "function_name": "recover_from_checkpoint", "line_number": 178, "message": "Recovery: replaying 623 WAL entries after checkpoint"}
```

### Commit Pipeline Log Output

```json
{"timestamp": "2024-01-15T10:24:00.001Z", "level": "DEBUG", "thread_id": "140234567891", "filename": "commit_pipeline.py", "function_name": "commit_job_event", "line_number": 78, "message": "Event JOB_CREATED for job use1-1705312000000-gate1-00042 written to WAL at LSN 45624"}
{"timestamp": "2024-01-15T10:24:00.012Z", "level": "DEBUG", "thread_id": "140234567891", "filename": "commit_pipeline.py", "function_name": "commit_job_event", "line_number": 98, "message": "Event LSN 45624 committed to regional consensus"}
{"timestamp": "2024-01-15T10:24:00.156Z", "level": "INFO", "thread_id": "140234567891", "filename": "commit_pipeline.py", "function_name": "commit_job_event", "line_number": 142, "message": "Event LSN 45624 committed to global ledger"}
```

### Checkpoint Creation Log Output

```json
{"timestamp": "2024-01-15T10:30:00.001Z", "level": "INFO", "thread_id": "140234567892", "filename": "checkpoint_manager.py", "function_name": "create_checkpoint", "line_number": 89, "message": "Creating checkpoint at LSN 50000"}
{"timestamp": "2024-01-15T10:30:00.234Z", "level": "INFO", "thread_id": "140234567892", "filename": "checkpoint_manager.py", "function_name": "create_checkpoint", "line_number": 112, "message": "Checkpoint def789abc012 created at LSN 50000"}
```

---

## Part 11: File Organization

```
hyperscale/distributed_rewrite/ledger/
├── __init__.py
├── models/
│   ├── __init__.py
│   ├── hlc.py                    # HybridLogicalClock
│   ├── wal_entry.py              # WALEntry, WALEntryState
│   ├── ledger_entry.py           # JobLedgerEntry
│   └── recovery_result.py        # RecoveryResult
├── events/
│   ├── __init__.py
│   ├── base.py                   # JobEvent base class
│   ├── creation.py               # JobCreated, JobAccepted
│   ├── progress.py               # JobProgressReported
│   ├── cancellation.py           # JobCancellationRequested/Acked
│   └── completion.py             # JobCompleted, JobFailed, JobTimedOut
├── storage/
│   ├── __init__.py
│   ├── wal_segment.py            # WALSegment (memory-mapped)
│   ├── node_wal.py               # NodeWAL manager (Control Plane)
│   └── ledger_storage.py         # LSM-tree storage for global ledger
├── consensus/
│   ├── __init__.py
│   ├── regional.py               # RegionalConsensusGroup (Raft)
│   └── flexible_paxos.py         # FlexiblePaxos for cross-region
├── pipeline/
│   ├── __init__.py
│   ├── commit_pipeline.py        # Three-stage commit
│   └── replication.py            # Cross-region replication
├── checkpoint/
│   ├── __init__.py
│   └── checkpoint_manager.py     # Checkpoint and compaction
├── anti_entropy/
│   ├── __init__.py
│   ├── merkle_tree.py            # Merkle tree for verification
│   └── repair.py                 # Anti-entropy repair
├── session/
│   ├── __init__.py
│   └── read_session.py           # Session consistency guarantees
├── data_plane/                   # NEW: Stats streaming (uses Logger)
│   ├── __init__.py
│   ├── stats_aggregator.py       # StatsAggregator (uses Logger, not WAL)
│   └── stats_models.py           # AggregatedJobStats, StatsCoalescingConfig
├── coordination/                 # NEW: Worker coordination
│   ├── __init__.py
│   └── ack_window_manager.py     # AckWindowManager (no blocking acks)
├── reliability/                  # NEW: Cross-DC reliability
│   ├── __init__.py
│   └── circuit_breaker.py        # CircuitBreaker for DC communication
└── global_ledger.py              # GlobalJobLedger facade
```

---

## Part 12: Integration with Existing Components

**Gate Integration** (TIER 1 - Global Consensus):
```
GateNode
├── CommitPipeline (AD-38 Control Plane)
│   ├── NodeWAL (local durability with fsync)
│   ├── RegionalConsensus (DC durability)
│   └── GlobalLedger (global durability)
├── CircuitBreaker (AD-38)
│   └── Per-DC circuit breakers for cross-DC calls
├── GateCancellationCoordinator (AD-20)
│   └── Uses CommitPipeline with GLOBAL durability
├── JobRouter (AD-36)
│   └── Reads from GlobalLedger for job state
├── StatsAggregator (AD-38 Data Plane)
│   └── Receives aggregated stats from Managers (uses Logger)
└── BackpressureManager (AD-37)
    └── Shapes update traffic to ledger
```

**Manager Integration** (TIER 2 - Regional Consensus):
```
ManagerNode
├── NodeWAL (workflow operations with fsync)
├── AckWindowManager (AD-38)
│   └── Non-blocking acknowledgment windows for workers
├── StatsAggregator (AD-38 Data Plane)
│   └── Aggregates worker progress (uses Logger)
├── CircuitBreaker (AD-38)
│   └── For cross-DC gate communication
├── WorkflowStateMachine (AD-33)
│   └── Persists state transitions to WAL
├── FederatedHealthMonitor (AD-33)
│   └── Reads global ledger for cross-DC state
│   └── Worker health checks (NOT consensus-based)
└── JobLeaderManager (AD-8)
    └── Uses ledger for leader election state
```

**Worker Integration** (TIER 3 - No Consensus):
```
WorkerNode
├── NO WAL (workers don't persist durability state)
├── NO Consensus participation
├── Progress reporting (fire-and-forget to Manager)
│   └── Manager's StatsAggregator receives updates
└── Health check responses (passive - Manager initiates)
```

**Data Flow Summary**:
```
┌─────────────────────────────────────────────────────────────────────────┐
│                            CONTROL PLANE                                 │
│               (NodeWAL with fsync, consensus, CRC)                      │
└─────────────────────────────────────────────────────────────────────────┘
                              │
   Gate ◄────────────────────►│◄────────────────────► Manager
   (Job lifecycle)            │                       (Workflow lifecycle)
                              │
                              ▼
┌─────────────────────────────────────────────────────────────────────────┐
│                            DATA PLANE                                    │
│               (Logger - JSON, no fsync, fire-and-forget)                │
└─────────────────────────────────────────────────────────────────────────┘
                              │
   Gate ◄────────────────────►│◄──────────────────── Manager ◄──── Workers
   (Stats query)              │                     (Stats aggregation)
                              │
                              ▼
                    [StatsAggregator uses Logger]
```

---

## Part 13: Success Criteria

**Control Plane (Job/Workflow Operations)**:
1. **Durability**: Zero job loss under any single failure (node, rack, region)
2. **Latency**: LOCAL <1ms, REGIONAL <10ms, GLOBAL <300ms (p99)
3. **Throughput**: >100K job events/second per region
4. **Recovery**: <30 seconds from crash to serving requests
5. **Consistency**: Causal+ consistency for reads, linearizable for critical ops
6. **Audit**: Complete event history queryable for any time range
7. **Compaction**: WAL size bounded to 2x active job state

**Data Plane (Stats/Metrics)**:
8. **Stats Throughput**: >1M progress events/second per manager
9. **Stats Latency**: <10ms from worker to manager (fire-and-forget)
10. **Cross-DC Stats**: 5000x reduction via coalescing (10K/s → 2/s per job)
11. **Stats Loss Tolerance**: <1% loss acceptable under normal operation

**Operational Model**:
12. **Worker Independence**: Workers NEVER block consensus or ack paths
13. **Circuit Breaker Recovery**: <60 seconds to replay queued operations after DC recovery
14. **Acknowledgment Windows**: Workers confirmed within 5 seconds via any communication
15. **Health Check Overhead**: <1% of manager CPU for worker health monitoring

---

## Part 14: Per-Job Viewstamped Replication

This section defines the maximally correct, robust, and performant architecture for global job ledger replication across datacenters, integrated with the existing per-job leadership model.

### Why Per-Job VSR (Not Multi-Raft)?

For a distributed job ledger **with per-job leadership already established**, the replication protocol must integrate with existing mechanisms:

| Existing Mechanism | What It Provides |
|-------------------|------------------|
| **Consistent hash ring** | Deterministic job-to-gate assignment |
| **Lease-based ownership** | Active ownership confirmation with TTL |
| **Fencing tokens** | Monotonic tokens prevent stale updates |
| **Backup gates** | Ordered failover candidates |

**Key Insight**: The per-job leadership model already determines WHO writes for each job. Adding Raft leader election is redundant—we just need durable replication.

**Per-Job Viewstamped Replication** maps directly to existing infrastructure:

| Per-Job Leadership | Viewstamped Replication |
|-------------------|-------------------------|
| Fencing token | View number |
| Job leader (gate) | Primary |
| Consistent hash backups | Replica set |
| Lease expiry | View change trigger |
| Lease acquisition | View change completion |

**Why VSR over Raft for this system:**

1. **No redundant election** - Job leadership already determined by consistent hash + lease
2. **Unified view management** - Fencing tokens ARE view numbers
3. **Direct write path** - Job leader writes to replicas, no shard leader indirection
4. **Simpler protocol** - No term tracking, no log matching property needed
5. **Proven correct** - VSR has formal proofs identical to Raft

### Architecture Overview

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│               PER-JOB VIEWSTAMPED REPLICATION ARCHITECTURE                      │
│                                                                                 │
│  INTEGRATION WITH EXISTING PER-JOB LEADERSHIP:                                  │
│  ────────────────────────────────────────────────────────────────────────────  │
│                                                                                 │
│  ┌─────────────────────────────────────────────────────────────────────────┐    │
│  │                    CONSISTENT HASH RING (existing)                      │    │
│  │                                                                         │    │
│  │  hash("job-abc") → Gate-2 (primary), Gate-3 (backup1), Gate-4 (backup2)│    │
│  │  hash("job-xyz") → Gate-1 (primary), Gate-2 (backup1), Gate-3 (backup2)│    │
│  │  hash("job-123") → Gate-4 (primary), Gate-1 (backup1), Gate-2 (backup2)│    │
│  └─────────────────────────────────────────────────────────────────────────┘    │
│                                                                                 │
│  ┌─────────────────────────────────────────────────────────────────────────┐    │
│  │                VSR REPLICATION FOR JOB "job-abc"                        │    │
│  │                                                                         │    │
│  │   Gate-2 (Primary)     Gate-3 (Replica)      Gate-4 (Replica)          │    │
│  │   ┌─────────────┐      ┌─────────────┐      ┌─────────────┐            │    │
│  │   │ view=5      │      │ view=5      │      │ view=5      │            │    │
│  │   │ (fence tok) │      │             │      │             │            │    │
│  │   │             │      │             │      │             │            │    │
│  │   │ Log:        │      │ Log:        │      │ Log:        │            │    │
│  │   │ [v5:0,1,2]  │─────►│ [v5:0,1,2]  │      │ [v5:0,1]    │            │    │
│  │   │             │      │             │      │ (catching up)│            │    │
│  │   │ SINGLE      │      │             │      │             │            │    │
│  │   │ WRITER      │      │             │      │             │            │    │
│  │   └─────────────┘      └─────────────┘      └─────────────┘            │    │
│  └─────────────────────────────────────────────────────────────────────────┘    │
│                                                                                 │
│  KEY DIFFERENCE FROM MULTI-RAFT:                                               │
│  ────────────────────────────────────────────────────────────────────────────  │
│                                                                                 │
│  Multi-Raft (redundant):          Per-Job VSR (unified):                       │
│  ┌────────────────────┐           ┌────────────────────┐                       │
│  │ Job Leader         │           │ Job Leader         │                       │
│  │ (consistent hash)  │           │ (consistent hash)  │                       │
│  │        │           │           │        │           │                       │
│  │        ▼           │           │        │           │                       │
│  │ Raft Shard Leader  │           │        │           │                       │
│  │ (elected - may     │           │        │           │                       │
│  │  differ!)          │           │        ▼           │                       │
│  │        │           │           │ VSR Replicas       │                       │
│  │        ▼           │           │ (hash backups)     │                       │
│  │ Raft Followers     │           └────────────────────┘                       │
│  └────────────────────┘                                                        │
│                                                                                 │
│  VSR eliminates the Raft shard leader indirection.                             │
│  Job leader writes DIRECTLY to its replicas.                                   │
│                                                                                 │
└─────────────────────────────────────────────────────────────────────────────────┘
```

### VSR State Machine

Each replica maintains VSR state per job:

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                           VSR REPLICA STATE (PER JOB)                           │
│                                                                                 │
│  Persistent State (survives restarts):                                         │
│  ├── view: int                  # Current view = fencing token                 │
│  ├── sequence: int              # Next expected sequence in current view       │
│  ├── prepare_log: list[Entry]   # Prepared but not yet committed entries       │
│  └── commit_log: list[Entry]    # Committed entries                            │
│                                                                                 │
│  Per-Entry State:                                                               │
│  ├── view: int                  # View when entry was created                  │
│  ├── seq: int                   # Sequence number within view                  │
│  ├── data: JobEvent             # The job state change                         │
│  └── hlc: HybridLogicalClock    # For causal ordering across jobs              │
│                                                                                 │
│  Primary State (job leader only):                                               │
│  ├── next_seq: int              # Next sequence to assign                      │
│  ├── pending: dict[seq, Future] # Awaiting quorum ack                          │
│  └── replica_ack: dict[seq, set[replica_id]]  # Which replicas acked           │
│                                                                                 │
└─────────────────────────────────────────────────────────────────────────────────┘
```

### VSR vs Raft: Why No Election?

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                           NO ELECTION NEEDED                                    │
│                                                                                 │
│  RAFT APPROACH (what we're NOT doing):                                         │
│  ────────────────────────────────────────────────────────────────────────────  │
│                                                                                 │
│  1. Node detects leader failure (election timeout)                             │
│  2. Node increments term, becomes candidate                                    │
│  3. Node requests votes from peers                                             │
│  4. Peers vote based on log completeness                                       │
│  5. Winner becomes leader                                                      │
│                                                                                 │
│  Problem: This duplicates what per-job leadership already does!                │
│                                                                                 │
│  VSR APPROACH (what we ARE doing):                                             │
│  ────────────────────────────────────────────────────────────────────────────  │
│                                                                                 │
│  1. Job leader determined by consistent hash (deterministic)                   │
│  2. Ownership confirmed by lease acquisition                                   │
│  3. Fencing token = view number (monotonic)                                    │
│  4. On failure: lease expires → backup acquires lease → new view               │
│                                                                                 │
│  ┌───────────────────────────────────────────────────────────────────────┐     │
│  │                    VIEW CHANGE (LEASE-BASED)                          │     │
│  │                                                                       │     │
│  │   Primary Failure         Backup Takeover                             │     │
│  │        │                       │                                      │     │
│  │        X                       │                                      │     │
│  │   (lease expires)              │                                      │     │
│  │                                │                                      │     │
│  │                    ┌───────────┴───────────┐                          │     │
│  │                    │                       │                          │     │
│  │                    ▼                       │                          │     │
│  │            Acquire lease                   │                          │     │
│  │            (new fence token)               │                          │     │
│  │                    │                       │                          │     │
│  │                    ▼                       │                          │     │
│  │            Send ViewChange                 │                          │     │
│  │            to replicas                     │                          │     │
│  │                    │                       │                          │     │
│  │                    ▼                       │                          │     │
│  │            Collect state from              │                          │     │
│  │            quorum (latest seq)             │                          │     │
│  │                    │                       │                          │     │
│  │                    ▼                       │                          │     │
│  │            Start new view at               │                          │     │
│  │            max(seq) + 1                    │                          │     │
│  │                                                                       │     │
│  └───────────────────────────────────────────────────────────────────────┘     │
│                                                                                 │
│  NO ELECTION PROTOCOL - leadership is DETERMINISTIC from consistent hash       │
│                                                                                 │
└─────────────────────────────────────────────────────────────────────────────────┘
```

### Write Protocol (Prepare-Commit)

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                        VSR WRITE PROTOCOL (2-PHASE)                             │
│                                                                                 │
│  Client       Job Leader (Primary)    Replica (Backup1)    Replica (Backup2)   │
│    │                 │                      │                    │              │
│    │  1. CreateJob   │                      │                    │              │
│    │────────────────►│                      │                    │              │
│    │                 │                      │                    │              │
│    │                 │  2. Verify lease     │                    │              │
│    │                 │     ownership        │                    │              │
│    │                 │                      │                    │              │
│    │                 │  3. Assign seq=N     │                    │              │
│    │                 │     in current view  │                    │              │
│    │                 │                      │                    │              │
│    │                 │  4. Prepare(view=5, seq=N, data)          │              │
│    │                 │─────────────────────►│                    │              │
│    │                 │─────────────────────────────────────────►│              │
│    │                 │                      │                    │              │
│    │                 │                      │  5. Verify:        │              │
│    │                 │                      │  - view >= known   │              │
│    │                 │                      │  - seq == expected │              │
│    │                 │                      │  - Persist entry   │              │
│    │                 │                      │                    │              │
│    │                 │  6. PrepareAck       │                    │              │
│    │                 │◄─────────────────────│                    │              │
│    │                 │◄─────────────────────────────────────────│              │
│    │                 │                      │                    │              │
│    │                 │  7. Quorum reached   │                    │              │
│    │                 │     (2/3 = majority) │                    │              │
│    │                 │                      │                    │              │
│    │                 │  8. Commit(view=5, seq=N)                 │              │
│    │                 │─────────────────────►│                    │              │
│    │                 │─────────────────────────────────────────►│              │
│    │                 │                      │                    │              │
│    │  9. ACK         │                      │                    │              │
│    │◄────────────────│                      │                    │              │
│    │  (committed)    │                      │                    │              │
│    │                 │                      │                    │              │
│                                                                                 │
│  KEY PROPERTIES:                                                               │
│  ─────────────────────────────────────────────────────────────────────────────  │
│  • SINGLE WRITER: Only job leader can issue Prepare for this job               │
│  • SEQUENCED: Replicas reject out-of-order sequence numbers                    │
│  • FENCED: Replicas reject Prepare from old views (stale leaders)              │
│  • DURABLE: Entry persisted before PrepareAck sent                             │
│                                                                                 │
└─────────────────────────────────────────────────────────────────────────────────┘
```

### View Change Protocol (Lease-Based Failover)

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                      VIEW CHANGE PROTOCOL (LEASE-BASED)                         │
│                                                                                 │
│  Old Primary (Gate-2)    Backup1 (Gate-3)      Backup2 (Gate-4)                │
│         │                      │                    │                           │
│         X                      │                    │                           │
│    (crashes, lease            │                    │                           │
│     expires after TTL)        │                    │                           │
│                               │                    │                           │
│                    ┌──────────┴────────────────────┤                           │
│                    │                               │                           │
│                    │  1. Detect lease expiry       │                           │
│                    │     (from hash ring - I'm     │                           │
│                    │      next in line)            │                           │
│                    │                               │                           │
│                    │  2. Acquire lease             │                           │
│                    │     new_view = old_view + 1   │                           │
│                    │     fence_token = 6           │                           │
│                    │                               │                           │
│                    │  3. ViewChange(new_view=6)    │                           │
│                    │──────────────────────────────►│                           │
│                    │                               │                           │
│                    │  4. ViewChangeAck             │                           │
│                    │     (last_prepared_seq=42)    │                           │
│                    │◄──────────────────────────────│                           │
│                    │                               │                           │
│                    │  Also query crashed primary   │                           │
│                    │  (if reachable) for its state │                           │
│                    │                               │                           │
│                    │  5. Compute start_seq =       │                           │
│                    │     max(all_last_prepared) + 1│                           │
│                    │     = 43                      │                           │
│                    │                               │                           │
│                    │  6. NewView(view=6, seq=43)   │                           │
│                    │──────────────────────────────►│                           │
│                    │                               │                           │
│                    │  7. Begin accepting writes    │                           │
│                    │     at seq=43 in view=6       │                           │
│                    │                               │                           │
│                                                                                 │
│  SAFETY GUARANTEE:                                                              │
│  ─────────────────────────────────────────────────────────────────────────────  │
│  • Old primary's uncommitted writes (seq > 42) cannot commit:                  │
│    - Would need quorum ack                                                     │
│    - But quorum has moved to view=6                                            │
│    - Replicas reject view=5 Prepare messages                                   │
│                                                                                 │
│  • New primary's start_seq ensures no sequence gaps                            │
│                                                                                 │
│  • Fencing token prevents stale primary from writing:                          │
│    - Even if old primary recovers, its token=5 is rejected                     │
│    - Must re-acquire lease (would get token >= 7)                              │
│                                                                                 │
└─────────────────────────────────────────────────────────────────────────────────┘
```

### Implementation

```python
"""
hyperscale/distributed_rewrite/ledger/vsr/job_vsr.py

Per-Job Viewstamped Replication for global job ledger.
Integrates with existing per-job leadership model.
Uses Single-Writer architecture (AD-39 Part 15) for log persistence.
"""

import asyncio
import time
from dataclasses import dataclass, field
from enum import Enum, auto
from typing import Generic, TypeVar

from hyperscale.distributed_rewrite.ledger.models.hlc import HybridLogicalClock
from hyperscale.distributed_rewrite.nodes.gate import GateJobLease


T = TypeVar('T')


class PrepareStatus(Enum):
    """Result of Prepare request handling."""
    SUCCESS = auto()
    STALE_VIEW = auto()
    WRONG_SEQUENCE = auto()
    NOT_OWNER = auto()


@dataclass(slots=True)
class VSREntry(Generic[T]):
    """Single entry in the VSR log."""
    view: int              # Fencing token when entry was created
    seq: int               # Sequence number within view
    data: T                # The job event
    hlc: HybridLogicalClock
    committed: bool = False


@dataclass(slots=True)
class Prepare(Generic[T]):
    """Prepare RPC from primary to replicas."""
    job_id: str
    view: int              # = fencing token
    seq: int               # Sequence number
    data: T                # Job event
    hlc: HybridLogicalClock


@dataclass(slots=True)
class PrepareResponse:
    """Prepare RPC response."""
    job_id: str
    status: PrepareStatus
    current_view: int      # Replica's known view
    expected_seq: int      # For WRONG_SEQUENCE, what replica expects


@dataclass(slots=True)
class Commit:
    """Commit notification from primary to replicas."""
    job_id: str
    view: int
    seq: int


@dataclass(slots=True)
class ViewChange:
    """View change request from new primary."""
    job_id: str
    new_view: int          # New fencing token


@dataclass(slots=True)
class ViewChangeResponse:
    """View change response with replica state."""
    job_id: str
    last_prepared_view: int
    last_prepared_seq: int
    uncommitted_entries: list[VSREntry]


@dataclass(slots=True)
class NewView:
    """New view announcement from primary."""
    job_id: str
    view: int
    start_seq: int


class JobReplicaState(Generic[T]):
    """
    Per-job state maintained by each replica.

    Thread Safety:
    - All access through single-writer pattern
    - No locks required (asyncio single-threaded)
    """

    __slots__ = (
        'job_id', 'known_view', 'expected_seq',
        'prepare_log', 'commit_log', '_hlc',
    )

    def __init__(self, job_id: str, hlc: HybridLogicalClock):
        self.job_id = job_id
        self.known_view = 0        # Highest view seen
        self.expected_seq = 0      # Next expected sequence
        self.prepare_log: list[VSREntry[T]] = []
        self.commit_log: list[VSREntry[T]] = []
        self._hlc = hlc

    def handle_prepare(self, prepare: Prepare[T]) -> PrepareResponse:
        """
        Handle Prepare from primary.

        Sequence checking ensures total ordering within view.
        View checking ensures stale primaries are rejected.
        """
        # Check view
        if prepare.view < self.known_view:
            return PrepareResponse(
                job_id=self.job_id,
                status=PrepareStatus.STALE_VIEW,
                current_view=self.known_view,
                expected_seq=self.expected_seq,
            )

        # New view - reset sequence expectation
        if prepare.view > self.known_view:
            self.known_view = prepare.view
            self.expected_seq = 0

        # Check sequence
        if prepare.seq != self.expected_seq:
            return PrepareResponse(
                job_id=self.job_id,
                status=PrepareStatus.WRONG_SEQUENCE,
                current_view=self.known_view,
                expected_seq=self.expected_seq,
            )

        # Valid prepare - create entry and persist
        entry = VSREntry(
            view=prepare.view,
            seq=prepare.seq,
            data=prepare.data,
            hlc=prepare.hlc,
            committed=False,
        )
        self.prepare_log.append(entry)
        self.expected_seq = prepare.seq + 1

        return PrepareResponse(
            job_id=self.job_id,
            status=PrepareStatus.SUCCESS,
            current_view=self.known_view,
            expected_seq=self.expected_seq,
        )

    def handle_commit(self, commit: Commit) -> bool:
        """
        Handle Commit from primary.

        Marks prepared entry as committed.
        Returns True if commit was applied.
        """
        if commit.view != self.known_view:
            return False

        # Find and commit the entry
        for entry in self.prepare_log:
            if entry.view == commit.view and entry.seq == commit.seq:
                if not entry.committed:
                    entry.committed = True
                    self.commit_log.append(entry)
                    return True

        return False

    def handle_view_change(self, view_change: ViewChange) -> ViewChangeResponse:
        """
        Handle ViewChange from new primary.

        Returns state needed for new primary to determine start_seq.
        """
        # Accept new view
        if view_change.new_view > self.known_view:
            self.known_view = view_change.new_view

        # Find last prepared entry
        last_view = 0
        last_seq = -1
        uncommitted: list[VSREntry] = []

        for entry in self.prepare_log:
            if not entry.committed:
                uncommitted.append(entry)
            if entry.seq > last_seq:
                last_view = entry.view
                last_seq = entry.seq

        return ViewChangeResponse(
            job_id=self.job_id,
            last_prepared_view=last_view,
            last_prepared_seq=last_seq,
            uncommitted_entries=uncommitted,
        )

    def handle_new_view(self, new_view: NewView) -> None:
        """
        Handle NewView from new primary.

        Resets sequence expectation for new view.
        """
        if new_view.view >= self.known_view:
            self.known_view = new_view.view
            self.expected_seq = new_view.start_seq


class JobPrimaryState(Generic[T]):
    """
    Per-job state maintained by the primary (job leader).

    Manages pending writes awaiting quorum acknowledgment.
    """

    __slots__ = (
        'job_id', 'view', 'next_seq',
        'pending', 'replica_acks', '_hlc',
    )

    def __init__(
        self,
        job_id: str,
        view: int,
        start_seq: int,
        hlc: HybridLogicalClock,
    ):
        self.job_id = job_id
        self.view = view
        self.next_seq = start_seq
        self.pending: dict[int, tuple[T, asyncio.Future[int]]] = {}
        self.replica_acks: dict[int, set[str]] = {}
        self._hlc = hlc

    def create_prepare(self, data: T) -> tuple[Prepare[T], asyncio.Future[int]]:
        """
        Create Prepare for new write.

        Returns (Prepare message, Future that resolves when committed).
        """
        seq = self.next_seq
        self.next_seq += 1

        prepare = Prepare(
            job_id=self.job_id,
            view=self.view,
            seq=seq,
            data=data,
            hlc=self._hlc.tick(int(time.time() * 1000)),
        )

        future: asyncio.Future[int] = asyncio.get_event_loop().create_future()
        self.pending[seq] = (data, future)
        self.replica_acks[seq] = set()

        return prepare, future

    def record_ack(
        self,
        seq: int,
        replica_id: str,
        quorum_size: int,
    ) -> bool:
        """
        Record PrepareAck from replica.

        Returns True if quorum reached (should send Commit).
        """
        if seq not in self.replica_acks:
            return False

        self.replica_acks[seq].add(replica_id)

        # Check for quorum (including self)
        return len(self.replica_acks[seq]) + 1 >= quorum_size

    def complete_commit(self, seq: int) -> None:
        """
        Mark write as committed after quorum.

        Resolves the pending Future.
        """
        if seq in self.pending:
            _, future = self.pending.pop(seq)
            if not future.done():
                future.set_result(seq)
            self.replica_acks.pop(seq, None)


class VSRTransport(Generic[T]):
    """
    Abstract transport for VSR RPCs.

    Implementations:
    - InMemoryTransport: For testing
    - GateTransport: For production (uses existing Gate messaging)
    """

    async def send_prepare(
        self,
        replica_id: str,
        prepare: Prepare[T],
    ) -> PrepareResponse:
        raise NotImplementedError

    async def send_commit(
        self,
        replica_id: str,
        commit: Commit,
    ) -> None:
        raise NotImplementedError

    async def send_view_change(
        self,
        replica_id: str,
        view_change: ViewChange,
    ) -> ViewChangeResponse:
        raise NotImplementedError

    async def send_new_view(
        self,
        replica_id: str,
        new_view: NewView,
    ) -> None:
        raise NotImplementedError


class NotJobLeaderError(Exception):
    """Raised when operation requires job leadership."""
    def __init__(self, job_id: str, current_leader: str | None):
        self.job_id = job_id
        self.current_leader = current_leader
        super().__init__(
            f"Not leader for job {job_id}. "
            f"Current leader: {current_leader}"
        )


class StaleViewError(Exception):
    """Raised when primary has stale view (fencing token)."""
    def __init__(self, job_id: str, our_view: int, current_view: int):
        self.job_id = job_id
        self.our_view = our_view
        self.current_view = current_view
        super().__init__(
            f"Stale view for job {job_id}. "
            f"Our view: {our_view}, current: {current_view}"
        )
```

### Per-Job VSR Coordinator

```python
"""
hyperscale/distributed_rewrite/ledger/vsr/job_vsr_coordinator.py

Coordinates VSR replication for all jobs on this gate.
Integrates with existing per-job leadership model.
"""

import asyncio
from dataclasses import dataclass
from typing import Callable, Generic, TypeVar

from hyperscale.distributed_rewrite.consistent_hash import ConsistentHashRing
from hyperscale.distributed_rewrite.ledger.models.hlc import HybridLogicalClock
from hyperscale.distributed_rewrite.ledger.vsr.job_vsr import (
    JobPrimaryState,
    JobReplicaState,
    VSRTransport,
    Prepare,
    PrepareResponse,
    PrepareStatus,
    Commit,
    ViewChange,
    ViewChangeResponse,
    NewView,
    NotJobLeaderError,
    StaleViewError,
)
from hyperscale.distributed_rewrite.nodes.gate import GateJobLease


T = TypeVar('T')


@dataclass
class VSRConfig:
    """VSR configuration."""
    replica_count: int = 3          # Total replicas (primary + backups)
    quorum_size: int = 2            # Majority needed for commit
    prepare_timeout_ms: int = 5000  # Timeout for Prepare phase
    view_change_timeout_ms: int = 10000  # Timeout for view change


class JobVSRCoordinator(Generic[T]):
    """
    Coordinates VSR replication for jobs owned by this gate.

    Key Integration Points:
    - ConsistentHashRing: Determines replicas for each job
    - GateJobLease: Provides fencing token (= view number)
    - Per-job leadership: Determines if we're primary

    Write Flow (as primary):
    1. Verify we hold lease for job
    2. Create Prepare with current view (fencing token) and next seq
    3. Send Prepare to replicas from consistent hash ring
    4. Wait for quorum PrepareAcks
    5. Send Commit to replicas
    6. Return to client

    Replica Flow:
    1. Receive Prepare from primary
    2. Verify view >= known_view and seq == expected_seq
    3. Persist entry, send PrepareAck
    4. Receive Commit, mark committed
    """

    __slots__ = (
        '_node_id', '_config', '_transport',
        '_hash_ring', '_state_machine',
        '_primary_states', '_replica_states',
        '_leases', '_hlc', '_running',
    )

    def __init__(
        self,
        node_id: str,
        config: VSRConfig,
        transport: VSRTransport[T],
        hash_ring: ConsistentHashRing,
        state_machine: Callable[[str, T], None],  # (job_id, event) -> None
    ):
        self._node_id = node_id
        self._config = config
        self._transport = transport
        self._hash_ring = hash_ring
        self._state_machine = state_machine

        # Per-job state
        self._primary_states: dict[str, JobPrimaryState[T]] = {}
        self._replica_states: dict[str, JobReplicaState[T]] = {}

        # Lease cache (from GateJobLease)
        self._leases: dict[str, GateJobLease] = {}

        self._hlc = HybridLogicalClock.now(node_id)
        self._running = False

    async def start(self) -> None:
        """Start the coordinator."""
        self._running = True

    async def stop(self) -> None:
        """Stop the coordinator."""
        self._running = False

    def is_primary_for(self, job_id: str) -> bool:
        """Check if we're primary (job leader) for this job."""
        return job_id in self._leases and self._leases[job_id].is_valid()

    def get_replicas(self, job_id: str) -> list[str]:
        """Get replica node IDs for job (from consistent hash ring)."""
        nodes = self._hash_ring.get_nodes(job_id, self._config.replica_count)
        # Exclude self - we're primary
        return [n for n in nodes if n != self._node_id]

    # ─────────────────────────────────────────────────────────────────────────
    # Primary Operations (Job Leader)
    # ─────────────────────────────────────────────────────────────────────────

    async def write(self, job_id: str, event: T) -> int:
        """
        Write event for job (must be job leader).

        Returns sequence number when committed.
        Raises NotJobLeaderError if not leader.
        Raises StaleViewError if our lease is stale.
        """
        # Verify we're primary
        if not self.is_primary_for(job_id):
            current_leader = self._hash_ring.get_node(job_id)
            raise NotJobLeaderError(job_id, current_leader)

        lease = self._leases[job_id]
        view = lease.fence_token

        # Get or create primary state
        if job_id not in self._primary_states:
            self._primary_states[job_id] = JobPrimaryState(
                job_id=job_id,
                view=view,
                start_seq=0,
                hlc=self._hlc,
            )

        primary_state = self._primary_states[job_id]

        # Check for stale view
        if primary_state.view < view:
            # Our lease was renewed with higher token - update state
            primary_state.view = view

        # Create prepare
        prepare, future = primary_state.create_prepare(event)

        # Send to replicas
        replicas = self.get_replicas(job_id)
        await self._send_prepare_to_replicas(
            prepare,
            replicas,
            primary_state,
        )

        # Wait for commit
        return await future

    async def _send_prepare_to_replicas(
        self,
        prepare: Prepare[T],
        replicas: list[str],
        primary_state: JobPrimaryState[T],
    ) -> None:
        """Send Prepare to all replicas, handle responses."""
        tasks = [
            self._send_prepare_to_replica(prepare, replica, primary_state)
            for replica in replicas
        ]
        await asyncio.gather(*tasks, return_exceptions=True)

    async def _send_prepare_to_replica(
        self,
        prepare: Prepare[T],
        replica: str,
        primary_state: JobPrimaryState[T],
    ) -> None:
        """Send Prepare to single replica."""
        try:
            response = await asyncio.wait_for(
                self._transport.send_prepare(replica, prepare),
                timeout=self._config.prepare_timeout_ms / 1000.0,
            )

            if response.status == PrepareStatus.SUCCESS:
                # Record ack
                quorum_reached = primary_state.record_ack(
                    prepare.seq,
                    replica,
                    self._config.quorum_size,
                )

                if quorum_reached:
                    # Send commit to all replicas
                    commit = Commit(
                        job_id=prepare.job_id,
                        view=prepare.view,
                        seq=prepare.seq,
                    )
                    await self._send_commit_to_replicas(
                        commit,
                        self.get_replicas(prepare.job_id),
                    )

                    # Complete the write
                    primary_state.complete_commit(prepare.seq)

                    # Apply to local state machine
                    self._state_machine(prepare.job_id, prepare.data)

            elif response.status == PrepareStatus.STALE_VIEW:
                # We're stale - someone else has higher view
                raise StaleViewError(
                    prepare.job_id,
                    prepare.view,
                    response.current_view,
                )

        except asyncio.TimeoutError:
            pass  # Replica unreachable, other replicas may still ack
        except StaleViewError:
            raise  # Propagate stale view errors

    async def _send_commit_to_replicas(
        self,
        commit: Commit,
        replicas: list[str],
    ) -> None:
        """Send Commit to all replicas (fire-and-forget)."""
        tasks = [
            self._transport.send_commit(replica, commit)
            for replica in replicas
        ]
        await asyncio.gather(*tasks, return_exceptions=True)

    # ─────────────────────────────────────────────────────────────────────────
    # Replica Operations
    # ─────────────────────────────────────────────────────────────────────────

    async def handle_prepare(self, prepare: Prepare[T]) -> PrepareResponse:
        """Handle incoming Prepare from primary."""
        # Get or create replica state
        if prepare.job_id not in self._replica_states:
            self._replica_states[prepare.job_id] = JobReplicaState(
                job_id=prepare.job_id,
                hlc=self._hlc,
            )

        replica_state = self._replica_states[prepare.job_id]
        return replica_state.handle_prepare(prepare)

    async def handle_commit(self, commit: Commit) -> None:
        """Handle incoming Commit from primary."""
        if commit.job_id in self._replica_states:
            replica_state = self._replica_states[commit.job_id]
            if replica_state.handle_commit(commit):
                # Apply to local state machine
                for entry in replica_state.commit_log:
                    if entry.view == commit.view and entry.seq == commit.seq:
                        self._state_machine(commit.job_id, entry.data)
                        break

    # ─────────────────────────────────────────────────────────────────────────
    # View Change (Failover)
    # ─────────────────────────────────────────────────────────────────────────

    async def perform_view_change(
        self,
        job_id: str,
        new_lease: GateJobLease,
    ) -> None:
        """
        Perform view change when taking over as primary.

        Called when:
        1. Previous primary's lease expired
        2. We acquired new lease from consistent hash ring
        """
        new_view = new_lease.fence_token
        replicas = self.get_replicas(job_id)

        # Send ViewChange to all replicas
        view_change = ViewChange(job_id=job_id, new_view=new_view)
        responses: list[ViewChangeResponse] = []

        tasks = [
            self._transport.send_view_change(replica, view_change)
            for replica in replicas
        ]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        for result in results:
            if isinstance(result, ViewChangeResponse):
                responses.append(result)

        # Determine start_seq from responses
        max_seq = -1
        for response in responses:
            if response.last_prepared_seq > max_seq:
                max_seq = response.last_prepared_seq

        # Also check local replica state
        if job_id in self._replica_states:
            local_state = self._replica_states[job_id]
            if local_state.prepare_log:
                local_max = max(e.seq for e in local_state.prepare_log)
                if local_max > max_seq:
                    max_seq = local_max

        start_seq = max_seq + 1

        # Send NewView to replicas
        new_view_msg = NewView(
            job_id=job_id,
            view=new_view,
            start_seq=start_seq,
        )
        await asyncio.gather(*[
            self._transport.send_new_view(replica, new_view_msg)
            for replica in replicas
        ], return_exceptions=True)

        # Initialize primary state
        self._primary_states[job_id] = JobPrimaryState(
            job_id=job_id,
            view=new_view,
            start_seq=start_seq,
            hlc=self._hlc,
        )

        # Store lease
        self._leases[job_id] = new_lease

    async def handle_view_change(
        self,
        view_change: ViewChange,
    ) -> ViewChangeResponse:
        """Handle incoming ViewChange from new primary."""
        if view_change.job_id not in self._replica_states:
            self._replica_states[view_change.job_id] = JobReplicaState(
                job_id=view_change.job_id,
                hlc=self._hlc,
            )

        replica_state = self._replica_states[view_change.job_id]
        return replica_state.handle_view_change(view_change)

    async def handle_new_view(self, new_view: NewView) -> None:
        """Handle incoming NewView from new primary."""
        if new_view.job_id in self._replica_states:
            self._replica_states[new_view.job_id].handle_new_view(new_view)
```

### Integration with Hyperscale Gates

```python
"""
hyperscale/distributed_rewrite/ledger/vsr/gate_integration.py

Integrates Per-Job VSR with Gate nodes for global job ledger.
"""

import asyncio
from dataclasses import dataclass
from typing import Any

from hyperscale.distributed_rewrite.consistent_hash import ConsistentHashRing
from hyperscale.distributed_rewrite.ledger.models.job_events import (
    JobEvent,
    JobCreated,
    JobCancelled,
    JobCompleted,
)
from hyperscale.distributed_rewrite.ledger.vsr.job_vsr_coordinator import (
    JobVSRCoordinator,
    VSRConfig,
    VSRTransport,
)
from hyperscale.distributed_rewrite.nodes.gate import GateJobLease
from hyperscale.logging import Logger


class JobLedgerStateMachine:
    """
    State machine for job ledger.

    Applied locally when entries are committed.
    Maintains per-job state (not sharded - VSR is per-job).
    """

    __slots__ = ('_jobs', '_history', '_max_history', '_logger')

    def __init__(self, logger: Logger, max_history: int = 10000):
        self._jobs: dict[str, JobState] = {}
        self._history: list[tuple[str, JobEvent]] = []  # (job_id, event)
        self._max_history = max_history
        self._logger = logger

    def apply(self, job_id: str, event: JobEvent) -> None:
        """Apply job event to state."""
        if isinstance(event, JobCreated):
            self._jobs[job_id] = JobState(
                job_id=job_id,
                status='CREATED',
                spec=event.spec,
                assigned_dcs=event.assigned_dcs,
                created_at=event.hlc,
            )

        elif isinstance(event, JobCancelled):
            if job_id in self._jobs:
                self._jobs[job_id].status = 'CANCELLED'
                self._jobs[job_id].cancelled_at = event.hlc

        elif isinstance(event, JobCompleted):
            if job_id in self._jobs:
                self._jobs[job_id].status = 'COMPLETED'
                self._jobs[job_id].completed_at = event.hlc
                self._jobs[job_id].results = event.results

        # Maintain bounded history
        self._history.append((job_id, event))
        if len(self._history) > self._max_history:
            self._history = self._history[-self._max_history:]

    def get_job(self, job_id: str) -> 'JobState | None':
        """Get job state."""
        return self._jobs.get(job_id)


@dataclass
class JobState:
    """State of a single job."""
    job_id: str
    status: str
    spec: dict
    assigned_dcs: list[str]
    created_at: Any  # HLC
    cancelled_at: Any = None
    completed_at: Any = None
    results: dict = None


class GateJobLedger:
    """
    Global job ledger for Gate nodes.

    Wraps JobVSRCoordinator with job-specific operations.
    Integrates with existing per-job leadership model.

    Key Difference from Multi-Raft:
    - No shard leaders - job leader writes directly to replicas
    - Fencing tokens from lease system provide view numbers
    - Consistent hash ring determines replicas (not Raft groups)
    """

    __slots__ = (
        '_coordinator', '_state_machine',
        '_logger', '_node_id', '_hash_ring',
    )

    def __init__(
        self,
        node_id: str,
        config: VSRConfig,
        transport: VSRTransport[JobEvent],
        hash_ring: ConsistentHashRing,
        logger: Logger,
    ):
        self._node_id = node_id
        self._logger = logger
        self._hash_ring = hash_ring
        self._state_machine = JobLedgerStateMachine(logger)
        self._coordinator = JobVSRCoordinator(
            node_id=node_id,
            config=config,
            transport=transport,
            hash_ring=hash_ring,
            state_machine=self._state_machine.apply,
        )

    async def start(self) -> None:
        """Start the job ledger."""
        await self._coordinator.start()

    async def stop(self) -> None:
        """Stop the job ledger."""
        await self._coordinator.stop()

    async def create_job(
        self,
        job_id: str,
        spec: dict,
        assigned_dcs: list[str],
    ) -> int:
        """
        Create a new job.

        Must be called by job leader (gate determined by consistent hash).
        Returns sequence number when committed.
        """
        event = JobCreated(
            job_id=job_id,
            spec=spec,
            assigned_dcs=assigned_dcs,
        )
        return await self._coordinator.write(job_id, event)

    async def cancel_job(
        self,
        job_id: str,
        reason: str,
        requestor: str,
    ) -> int:
        """
        Cancel a job.

        Must be called by job leader.
        Returns sequence number when committed.
        """
        event = JobCancelled(
            job_id=job_id,
            reason=reason,
            requestor=requestor,
        )
        return await self._coordinator.write(job_id, event)

    async def complete_job(
        self,
        job_id: str,
        results: dict,
    ) -> int:
        """
        Mark job as completed.

        Must be called by job leader.
        Returns sequence number when committed.
        """
        event = JobCompleted(
            job_id=job_id,
            results=results,
        )
        return await self._coordinator.write(job_id, event)

    def get_job(self, job_id: str) -> JobState | None:
        """
        Get current job state (local read).

        Reads from local replica state. May be stale if:
        - This node is not the job leader
        - Recent writes haven't been replicated yet

        For strong consistency, use get_job_linearizable().
        """
        return self._state_machine.get_job(job_id)

    async def get_job_linearizable(self, job_id: str) -> JobState | None:
        """
        Get job state with linearizable read.

        If we're job leader: read is already linearizable (single writer).
        If we're replica: query job leader for latest state.
        """
        if self._coordinator.is_primary_for(job_id):
            # We're the single writer - local state is authoritative
            return self._state_machine.get_job(job_id)

        # Not leader - would need to query leader
        # (Implementation depends on transport)
        # For now, return local state with staleness warning
        return self._state_machine.get_job(job_id)

    async def on_lease_acquired(self, job_id: str, lease: GateJobLease) -> None:
        """
        Called when we acquire job leadership.

        Triggers view change to synchronize state from replicas.
        """
        await self._coordinator.perform_view_change(job_id, lease)

    def is_job_leader(self, job_id: str) -> bool:
        """Check if we're the job leader."""
        return self._coordinator.is_primary_for(job_id)
```

### Cross-DC Timing Diagram

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                    CROSS-DC JOB CREATION TIMING (VSR)                           │
│                                                                                 │
│  Client          US-EAST Gate       EU-WEST Gate        APAC Gate              │
│  (US-EAST)       (Job Leader)       (Replica)           (Replica)              │
│     │                  │                  │                  │                  │
│     │  CreateJob       │                  │                  │                  │
│     │─────────────────►│                  │                  │                  │
│     │                  │                  │                  │                  │
│     │                  │ Verify lease     │                  │                  │
│     │                  │ (fence_token=5)  │                  │                  │
│     │                  │ T=0ms            │                  │                  │
│     │                  │                  │                  │                  │
│     │                  │ Prepare(v=5,s=0) │                  │                  │
│     │                  │ (async parallel) │                  │                  │
│     │                  │─────────────────►│                  │                  │
│     │                  │ RTT: ~80ms       │                  │                  │
│     │                  │─────────────────────────────────────►│                 │
│     │                  │ RTT: ~150ms      │                  │                  │
│     │                  │                  │                  │                  │
│     │                  │                  │ Check view>=5    │                  │
│     │                  │                  │ Check seq==0     │                  │
│     │                  │                  │ Persist entry    │                  │
│     │                  │                  │ T=80ms           │                  │
│     │                  │                  │                  │                  │
│     │                  │◄─────────────────│                  │                  │
│     │                  │ PrepareAck       │                  │                  │
│     │                  │ T=80ms           │                  │                  │
│     │                  │                  │                  │                  │
│     │                  │ Quorum! (2/3)    │                  │                  │
│     │                  │ Send Commit      │                  │                  │
│     │                  │ T=80ms           │                  │                  │
│     │                  │                  │                  │                  │
│     │◄─────────────────│                  │                  │                  │
│     │  JobCreated      │                  │                  │                  │
│     │  (committed)     │                  │                  │                  │
│     │  T=80ms          │                  │                  │                  │
│     │                  │                  │                  │                  │
│     │                  │                  │                  │ PrepareAck (late)│
│     │                  │◄─────────────────────────────────────│ T=150ms         │
│     │                  │                  │                  │                  │
│                                                                                 │
│  TIMELINE:                                                                      │
│  ├── T=0ms:    Client submits, job leader verifies lease                       │
│  ├── T=80ms:   EU-WEST PrepareAcks, quorum reached, Commit sent, client ACKed  │
│  ├── T=150ms:  APAC PrepareAcks (already committed, just catching up)          │
│                                                                                 │
│  LATENCY: ~80ms (RTT to nearest quorum member)                                 │
│  DURABILITY: Survives US-EAST + EU-WEST simultaneous failure                   │
│                                                                                 │
│  KEY DIFFERENCE FROM RAFT:                                                      │
│  • No heartbeats needed (job leader doesn't change unless lease expires)       │
│  • No election timeout (leadership is deterministic from consistent hash)      │
│  • Simpler protocol (Prepare/Commit vs AppendEntries with log matching)        │
│                                                                                 │
└─────────────────────────────────────────────────────────────────────────────────┘
```

### Failure Scenarios

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                    FAILURE SCENARIO: JOB LEADER FAILURE                         │
│                                                                                 │
│  BEFORE: US-EAST is job leader (primary from consistent hash)                  │
│                                                                                 │
│  US-EAST Gate       EU-WEST Gate        APAC Gate                              │
│  (JOB LEADER)       (REPLICA backup1)   (REPLICA backup2)                      │
│  ┌─────────┐        ┌─────────┐        ┌─────────┐                             │
│  │ view=5  │        │ view=5  │        │ view=5  │                             │
│  │ lease ✓ │        │ seq=42  │        │ seq=41  │                             │
│  │ seq=42  │        │         │        │ (behind)│                             │
│  └─────────┘        └─────────┘        └─────────┘                             │
│       │                  │                  │                                   │
│       X (crashes)        │                  │                                   │
│       │                  │                  │                                   │
│  (lease expires         │                  │                                   │
│   after TTL)            │                  │                                   │
│                          │                  │                                   │
│  AFTER: View change (lease-based, NOT election)                                │
│                          │                  │                                   │
│                          │ Detect lease     │                                   │
│                          │ expiry (I'm next │                                   │
│                          │ in hash ring)    │                                   │
│                          │                  │                                   │
│                          │ Acquire lease    │                                   │
│                          │ fence_token=6    │                                   │
│                          │                  │                                   │
│                          │ ViewChange(v=6)  │                                   │
│                          │─────────────────►│                                   │
│                          │                  │                                   │
│                          │◄─────────────────│                                   │
│                          │ ViewChangeAck    │                                   │
│                          │ (last_seq=41)    │                                   │
│                          │                  │                                   │
│                          │ start_seq = 43   │                                   │
│                          │ (max of 42,41)+1 │                                   │
│                          │                  │                                   │
│                          │ NewView(v=6,s=43)│                                   │
│                          │─────────────────►│                                   │
│                          │                  │                                   │
│  EU-WEST Gate        APAC Gate                                                  │
│  (NEW JOB LEADER)    (REPLICA)                                                 │
│  ┌─────────┐        ┌─────────┐                                                │
│  │ view=6  │        │ view=6  │                                                │
│  │ lease ✓ │        │ seq=43  │  ← Ready for new writes                        │
│  │ seq=43  │        │         │                                                │
│  └─────────┘        └─────────┘                                                │
│                                                                                 │
│  INVARIANTS PRESERVED:                                                          │
│  ✓ No committed entries lost (quorum had them)                                 │
│  ✓ New leader starts after highest prepared seq                                │
│  ✓ Old leader's uncommitted writes (seq=42 if not quorum-acked) lost           │
│  ✓ Old leader cannot write (fencing token=5 rejected by replicas)              │
│                                                                                 │
│  NO ELECTION NEEDED - consistent hash determines next leader!                  │
│                                                                                 │
└─────────────────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────────────────┐
│                    FAILURE SCENARIO: NETWORK PARTITION                          │
│                                                                                 │
│  PARTITION: US-EAST (job leader) isolated from EU-WEST and APAC                │
│                                                                                 │
│  ┌────────────────────┐         ┌────────────────────────────────────┐         │
│  │    Minority        │         │         Majority                   │         │
│  │    Partition       │    X    │         Partition                  │         │
│  │                    │ Network │                                    │         │
│  │  US-EAST Gate      │ Failure │  EU-WEST Gate      APAC Gate       │         │
│  │  (JOB LEADER)      │         │  (REPLICA)         (REPLICA)       │         │
│  │  ┌─────────┐       │         │  ┌─────────┐      ┌─────────┐     │         │
│  │  │ view=5  │       │         │  │ view=5  │      │ view=5  │     │         │
│  │  │ lease ✓ │       │         │  │         │      │         │     │         │
│  │  └─────────┘       │         │  └─────────┘      └─────────┘     │         │
│  └────────────────────┘         └────────────────────────────────────┘         │
│                                                                                 │
│  BEHAVIOR:                                                                      │
│                                                                                 │
│  Minority (US-EAST):                                                           │
│  • Cannot commit (no quorum for PrepareAcks)                                   │
│  • Keeps trying to reach replicas (times out)                                  │
│  • Lease eventually expires (cannot renew without majority)                    │
│                                                                                 │
│  Majority (EU-WEST + APAC):                                                    │
│  • See job leader's lease expiring (no renewal)                                │
│  • EU-WEST (next in hash ring) acquires new lease                              │
│  • EU-WEST performs view change with fence_token=6                             │
│  • EU-WEST can commit new writes (has quorum with APAC)                        │
│                                                                                 │
│  AFTER PARTITION HEALS:                                                         │
│  • US-EAST's lease is expired                                                  │
│  • US-EAST tries to write → PrepareAck rejects (view=5 < current view=6)       │
│  • US-EAST discovers it's no longer leader via StaleViewError                  │
│  • US-EAST becomes replica, syncs state from new leader                        │
│                                                                                 │
│  SAFETY PRESERVED:                                                              │
│  ✓ At most one writer per view (fencing)                                       │
│  ✓ Committed entries never lost (quorum requirement)                           │
│  ✓ Linearizability maintained (single writer per job)                          │
│  ✓ No split-brain (fencing tokens enforce total ordering of leadership)        │
│                                                                                 │
└─────────────────────────────────────────────────────────────────────────────────┘
```

### Performance Characteristics

| Metric | Value | Notes |
|--------|-------|-------|
| **Write Latency** | 80-150ms | RTT to nearest quorum member |
| **Read Latency (local)** | <1ms | May be stale |
| **Read Latency (linearizable)** | <1ms (if leader) | Single writer = authoritative |
| **Throughput (per job)** | ~10K ops/s | Limited by job leader |
| **Throughput (N jobs)** | ~10K × N ops/s | Each job has independent leader |
| **Failover Time** | Lease TTL + ViewChange | Typically 5-15s |
| **Replication** | 2-phase (Prepare/Commit) | Simpler than Raft AppendEntries |

**Comparison with Multi-Raft:**

| Aspect | Multi-Raft | Per-Job VSR |
|--------|-----------|-------------|
| Leader election | Raft protocol (150-300ms) | Lease-based (deterministic) |
| Heartbeats | Required (50ms intervals) | Not needed |
| Log matching | Required (complex) | Not needed (single writer) |
| Write conflicts | Possible (resolved by Raft) | Impossible (single writer) |
| Shard affinity | Job may not be on shard leader | Job leader IS the writer |
| Complexity | Higher (Raft + sharding) | Lower (VSR + per-job leadership) |

### Configuration Recommendations

```python
# Production configuration for global job ledger (Per-Job VSR)
VSR_CONFIG = VSRConfig(
    # Replica count: 3 (primary + 2 backups)
    # - Survives 1 failure
    # - Quorum = 2 (majority)
    replica_count=3,
    quorum_size=2,

    # Prepare timeout: 5 seconds
    # - Must be > max RTT across DCs (~300ms)
    # - Allows for transient network issues
    prepare_timeout_ms=5000,

    # View change timeout: 10 seconds
    # - Collecting state from replicas may take time
    # - Not on critical path (only during failover)
    view_change_timeout_ms=10000,
)

# Lease configuration (integrates with existing per-job leadership)
LEASE_CONFIG = GateJobLeaseConfig(
    # Lease TTL: 10 seconds
    # - Long enough to avoid spurious failovers
    # - Short enough for timely failure detection
    lease_ttl_seconds=10,

    # Renewal interval: 3 seconds
    # - < lease_ttl / 3 to ensure renewal before expiry
    renewal_interval_seconds=3,

    # Fencing token increment: automatic
    # - Each new lease gets token = max(seen) + 1
    # - Provides view numbers for VSR
)
```

### Why This Is Maximally Correct

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                    CORRECTNESS ARGUMENT                                         │
│                                                                                 │
│  Per-Job VSR is maximally correct because:                                     │
│                                                                                 │
│  1. SINGLE WRITER PER JOB                                                      │
│     ──────────────────────────────────────────────────────────────────────────  │
│     • Only job leader can issue Prepare for its jobs                           │
│     • Eliminates write conflicts by design                                     │
│     • No need for conflict resolution logic                                    │
│                                                                                 │
│  2. FENCING TOKENS PROVIDE TOTAL ORDERING OF LEADERSHIP                        │
│     ──────────────────────────────────────────────────────────────────────────  │
│     • Each new leader gets strictly higher token                               │
│     • Replicas reject writes from old tokens                                   │
│     • Prevents split-brain during partitions                                   │
│                                                                                 │
│  3. SEQUENCE NUMBERS PROVIDE TOTAL ORDERING WITHIN VIEW                        │
│     ──────────────────────────────────────────────────────────────────────────  │
│     • Replicas only accept expected sequence                                   │
│     • Out-of-order writes rejected                                             │
│     • No gaps in committed entries                                             │
│                                                                                 │
│  4. VIEW CHANGE SYNCHRONIZES STATE                                             │
│     ──────────────────────────────────────────────────────────────────────────  │
│     • New leader collects state from quorum                                    │
│     • Starts at max(prepared_seq) + 1                                          │
│     • No committed entries lost                                                │
│                                                                                 │
│  5. QUORUM INTERSECTION GUARANTEES DURABILITY                                  │
│     ──────────────────────────────────────────────────────────────────────────  │
│     • Commit requires quorum PrepareAcks                                       │
│     • View change requires quorum ViewChangeAcks                               │
│     • Quorums intersect → new leader sees committed state                      │
│                                                                                 │
│  6. NO REDUNDANT MECHANISMS                                                    │
│     ──────────────────────────────────────────────────────────────────────────  │
│     • Per-job leadership provides: who writes                                  │
│     • VSR provides: durable replication                                        │
│     • No overlapping leader election (Raft term vs lease)                      │
│     • Single source of truth for leadership                                    │
│                                                                                 │
│  FORMAL BASIS:                                                                 │
│     • VSR (Viewstamped Replication) has formal proofs                          │
│     • Fencing tokens are equivalent to VSR view numbers                        │
│     • Lease-based view change is standard practice (e.g., Chubby, ZooKeeper)   │
│                                                                                 │
└─────────────────────────────────────────────────────────────────────────────────┘
```

---

## Conclusion

AD-38 provides a robust, multi-tier durability architecture optimized for hyperscale's operational model:

**Three-Tier Node Hierarchy**:
- **Gates** (GLOBAL): Job lifecycle, cross-DC coordination, full consensus participation
- **Managers** (REGIONAL): Workflow lifecycle, stats aggregation, DC-level consensus
- **Workers** (NONE): High CPU/memory load testing, fire-and-forget reporting, NO consensus

**Separate Control and Data Planes**:
- **Control Plane**: Job/workflow commands via NodeWAL with fsync, consensus, CRC checksums
- **Data Plane**: Stats/metrics via Logger (JSON, no fsync), eventual consistency acceptable

**Key Design Decisions**:
- Workers excluded from all consensus paths (slow under load testing)
- Operation-specific durability (GLOBAL for jobs, REGIONAL for workflows, NONE for stats)
- Acknowledgment windows replace blocking acks for worker communication
- Circuit breakers prevent cascading failures across DCs
- Coalesced stats reduce cross-DC traffic by 5000x

**Logger vs NodeWAL**:
- **Logger** (hyperscale/logging): Suitable for Data Plane stats - no fsync needed, JSON format, eventual consistency
- **NodeWAL** (new): Required for Control Plane - explicit fsync, binary format, CRC checksums, sequence numbers, read-back capability

The architecture balances latency, throughput, and durability through configurable commit levels, allowing callers to choose the appropriate tradeoff for each operation type.

**References**:

*Control Plane (WAL - NOT using Logger)*:
- `hyperscale/distributed_rewrite/ledger/models/hlc.py` (HybridLogicalClock)
- `hyperscale/distributed_rewrite/ledger/storage/node_wal.py` (NodeWAL)
- `hyperscale/distributed_rewrite/ledger/storage/wal_segment.py` (WALSegment)
- `hyperscale/distributed_rewrite/ledger/pipeline/commit_pipeline.py` (CommitPipeline)
- `hyperscale/distributed_rewrite/ledger/checkpoint/checkpoint_manager.py` (CheckpointManager)

*Data Plane (Uses Logger)*:
- `hyperscale/distributed_rewrite/ledger/data_plane/stats_aggregator.py` (StatsAggregator)
- `hyperscale/logging/streams/logger_stream.py` (Logger)

*Coordination and Reliability*:
- `hyperscale/distributed_rewrite/ledger/coordination/ack_window_manager.py` (AckWindowManager)
- `hyperscale/distributed_rewrite/ledger/reliability/circuit_breaker.py` (CircuitBreaker)

---

### AD-39: Logger Extension for AD-38 WAL Compliance

**Decision**: Extend the existing `hyperscale/logging` Logger with optional WAL-compliant features (durability modes, binary format, sequence numbers, read-back) while maintaining full backward compatibility with existing usage patterns.

**Related**: AD-38 (Global Job Ledger), AD-20 (Cancellation)

**Rationale**:
- AD-38 identified that Logger is unsuitable for Control Plane WAL due to missing fsync, sequence numbers, and read-back capability.
- However, creating a completely separate NodeWAL class duplicates async I/O patterns already proven in Logger.
- By extending Logger with **optional** WAL features, we achieve code reuse, consistent API patterns, and progressive enhancement.
- All existing Logger usage (Data Plane stats) continues unchanged with default parameters.
- New WAL use cases opt-in to durability features via new parameters.

---

## Part 1: Current Logger Architecture Analysis

### 1.1 File Structure

```
hyperscale/logging/
├── __init__.py
├── config/
│   ├── __init__.py
│   ├── log_level_map.py
│   ├── logging_config.py
│   └── stream_type.py
├── models/
│   ├── __init__.py
│   ├── entry.py
│   ├── log.py
│   └── log_level.py
├── queue/
│   ├── __init__.py
│   ├── consumer_status.py
│   ├── log_consumer.py
│   ├── log_provider.py
│   └── provider_status.py
├── rotation/
│   ├── __init__.py
│   ├── file_size_parser.py
│   └── time_parser.py
├── snowflake/
│   ├── __init__.py
│   ├── constants.py
│   ├── snowflake.py
│   └── snowflake_generator.py         # Already exists - useful for LSN
├── streams/
│   ├── __init__.py
│   ├── logger.py                       # Main Logger class
│   ├── logger_context.py               # Context manager
│   ├── logger_stream.py                # Core implementation
│   ├── protocol.py
│   └── retention_policy.py
└── hyperscale_logging_models.py
```

### 1.2 Current Usage Patterns

All Logger file usage follows a consistent pattern across the codebase:

```python
# Pattern 1: Configure then use context
self._logger.configure(
    name="context_name",
    path="hyperscale.leader.log.json",
    template="{timestamp} - {level} - {thread_id} - {filename}:{function_name}.{line_number} - {message}",
    models={
        "trace": (TraceModel, default_config),
        "debug": (DebugModel, default_config),
    },
)

async with self._logger.context(name="context_name") as ctx:
    await ctx.log(Entry(message="...", level=LogLevel.INFO))
    await ctx.log_prepared("message text", name="debug")

# Pattern 2: Inline context with path
async with self._logger.context(
    name="remote_graph_manager",
    path="hyperscale.leader.log.json",
    template="...",
    nested=True,  # Reuse existing context
) as ctx:
    await ctx.log(Entry(...))
```

### 1.3 Usage by Component

```
┌─────────────────────────────────────────────────────────────────────────┐
│                         LOGGER USAGE MAP                                 │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│  RemoteGraphManager                                                      │
│  ├── Context: "remote_graph_manager", "{graph_slug}_logger",            │
│  │            "{workflow_slug}_logger"                                   │
│  ├── Path: "hyperscale.leader.log.json"                                 │
│  ├── Models: GraphDebug, WorkflowTrace, RemoteManagerInfo              │
│  └── Methods: ctx.log(), ctx.log_prepared()                            │
│                                                                          │
│  RemoteGraphController                                                   │
│  ├── Context: "graph_server_{id}", "workflow_run_{id}",                 │
│  │            "graph_client_{id}", "controller"                         │
│  ├── Path: None (console only)                                          │
│  ├── Models: StatusUpdate, RunInfo, ServerDebug/Info/Error             │
│  └── Methods: ctx.log_prepared()                                        │
│                                                                          │
│  WorkflowRunner                                                          │
│  ├── Context: "{workflow_slug}_{run_id}_logger", "workflow_manager"     │
│  ├── Path: self._logfile (configurable)                                 │
│  ├── Models: Entry                                                      │
│  └── Methods: ctx.log(), ctx.log_prepared()                            │
│                                                                          │
│  LocalRunner                                                             │
│  ├── Context: "local_runner"                                            │
│  ├── Path: "hyperscale.leader.log.json"                                 │
│  ├── Models: TestTrace, TestInfo, TestError                            │
│  └── Methods: ctx.log_prepared()                                        │
│                                                                          │
│  LocalServerPool                                                         │
│  ├── Context: "local_server_pool"                                       │
│  ├── Path: "hyperscale.leader.log.json"                                 │
│  ├── Models: Entry                                                      │
│  └── Methods: ctx.log()                                                 │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘
```

### 1.4 Current LoggerStream Core Methods

```python
# File: hyperscale/logging/streams/logger_stream.py

class LoggerStream:
    def __init__(self, name, template, filename, directory, retention_policy, models): ...

    # File operations
    async def open_file(self, filename, directory, is_default, retention_policy): ...
    def _open_file(self, logfile_path): ...                    # Sync, runs in executor
    async def close_file(self, filename, directory): ...
    async def _close_file(self, logfile_path): ...

    # Rotation
    async def _rotate(self, logfile_path, retention_policy): ...
    def _rotate_logfile(self, retention_policy, logfile_path): ...  # Sync

    # Logging
    async def log(self, entry, template, path, retention_policy, filter): ...
    async def _log(self, entry_or_log, template, filter): ...           # Console
    async def _log_to_file(self, entry_or_log, filename, directory, ...): ...  # File

    # THE CRITICAL METHOD - Line 857-873
    def _write_to_file(self, log, logfile_path): ...  # Sync, runs in executor

    # Pub/Sub
    async def get(self, filter): ...   # Async iterator from consumer
    async def put(self, entry): ...    # Send to provider
```

### 1.5 Critical Gap: `_write_to_file` Implementation

```python
# CURRENT IMPLEMENTATION (logger_stream.py:857-873)
def _write_to_file(
    self,
    log: Log,
    logfile_path: str,
):
    try:
        if (
            logfile := self._files.get(logfile_path)
        ) and (
            logfile.closed is False
        ):

            logfile.write(msgspec.json.encode(log) + b"\n")  # JSON only
            logfile.flush()  # NO fsync - data can be lost!

    except Exception:
        pass  # Errors swallowed
```

**Problems for WAL**:
1. **No fsync** - `flush()` only pushes to OS buffer, not disk
2. **JSON only** - No binary format with CRC checksums
3. **No LSN** - No sequence number generation
4. **Write-only** - No read-back for recovery
5. **Errors swallowed** - Silent failures unacceptable for WAL

---

## Part 2: Extension Design

### 2.1 Design Principles

1. **Additive Only** - New optional parameters with backward-compatible defaults
2. **Zero Breaking Changes** - All existing code works unchanged
3. **Progressive Enhancement** - Enable WAL features per-context as needed
4. **Single Responsibility** - Each new feature independently toggleable
5. **Consistent Patterns** - Same `context()` API already familiar to codebase

### 2.2 New Configuration Enum

```python
"""
hyperscale/logging/config/durability_mode.py
"""
from enum import IntEnum


class DurabilityMode(IntEnum):
    """
    Durability levels for log writes.

    Controls when writes are considered durable:
    - NONE: No sync (testing only, data loss on any failure)
    - FLUSH: Buffer flush only (current behavior, data loss on OS crash)
    - FSYNC: Per-write fsync (safest, highest latency)
    - FSYNC_BATCH: Batched fsync (recommended for WAL - balance of safety/perf)
    """
    NONE = 0         # No sync (testing only)
    FLUSH = 1        # Current behavior - flush() to OS buffer
    FSYNC = 2        # fsync per write (safest, ~1-10ms latency)
    FSYNC_BATCH = 3  # Batched fsync every N writes or T ms
```

### 2.3 API Extension

```
┌─────────────────────────────────────────────────────────────────────────┐
│                    LOGGER API EXTENSION                                  │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│  Logger.context() - EXTENDED                                             │
│  ──────────────────────────────────                                     │
│                                                                          │
│  EXISTING PARAMETERS (unchanged):                                        │
│  ├── name: str | None = None                                            │
│  ├── template: str | None = None                                        │
│  ├── path: str | None = None                                            │
│  ├── retention_policy: RetentionPolicyConfig | None = None              │
│  ├── nested: bool = False                                               │
│  └── models: dict[...] | None = None                                    │
│                                                                          │
│  NEW PARAMETERS (all optional, defaults = current behavior):            │
│  ├── durability: DurabilityMode = DurabilityMode.FLUSH    # NEW         │
│  ├── format: Literal['json', 'binary'] = 'json'           # NEW         │
│  ├── enable_lsn: bool = False                             # NEW         │
│  └── instance_id: int = 0                                 # NEW         │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘
```

### 2.4 Usage Comparison

```python
# ═══════════════════════════════════════════════════════════════════════
# EXISTING CODE - COMPLETELY UNCHANGED (Data Plane - stats)
# ═══════════════════════════════════════════════════════════════════════

async with self._logger.context(
    name="remote_graph_manager",
    path="hyperscale.leader.log.json",
    template="{timestamp} - {level} - {...} - {message}",
) as ctx:
    await ctx.log(Entry(message="Stats update", level=LogLevel.INFO))
    # Uses: JSON format, flush() only, no LSN
    # Behavior: IDENTICAL to current implementation


# ═══════════════════════════════════════════════════════════════════════
# NEW CODE - WAL MODE (Control Plane - job/workflow commands)
# ═══════════════════════════════════════════════════════════════════════

async with self._logger.context(
    name="node_wal",
    path="hyperscale.wal.log",               # Can use .wal extension
    durability=DurabilityMode.FSYNC_BATCH,   # NEW: Batched fsync
    format='binary',                          # NEW: Binary with CRC
    enable_lsn=True,                          # NEW: Sequence numbers
    instance_id=self._node_id,                # NEW: For snowflake LSN
) as ctx:
    lsn = await ctx.log(WALEntry(...))
    # Uses: Binary format, CRC32 checksum, fsync, LSN tracking
    # Returns: LSN for replication tracking
```

---

## Part 3: LoggerStream Modifications

### 3.1 `__init__` Extension

```python
# CURRENT (lines 65-136)
def __init__(
    self,
    name: str | None = None,
    template: str | None = None,
    filename: str | None = None,
    directory: str | None = None,
    retention_policy: RetentionPolicyConfig | None = None,
    models: dict[str, tuple[type[T], dict[str, Any]]] | None = None,
) -> None:
    # ... existing initialization ...

# EXTENDED
def __init__(
    self,
    name: str | None = None,
    template: str | None = None,
    filename: str | None = None,
    directory: str | None = None,
    retention_policy: RetentionPolicyConfig | None = None,
    models: dict[str, tuple[type[T], dict[str, Any]]] | None = None,
    # NEW AD-39 parameters
    durability: DurabilityMode = DurabilityMode.FLUSH,
    format: Literal['json', 'binary'] = 'json',
    enable_lsn: bool = False,
    instance_id: int = 0,
) -> None:
    # ... existing initialization ...

    # NEW: AD-39 WAL support
    self._durability = durability
    self._format = format
    self._enable_lsn = enable_lsn
    self._instance_id = instance_id

    # LSN generator (reuses existing snowflake module)
    self._sequence_generator: SnowflakeGenerator | None = None
    if enable_lsn:
        self._sequence_generator = SnowflakeGenerator(instance_id)

    # Batch fsync state
    self._pending_batch: list[tuple[bytes, str, asyncio.Future[int | None]]] = []
    self._batch_lock: asyncio.Lock | None = None  # Lazy init
    self._batch_timeout_ms: int = 10
    self._batch_max_size: int = 100
    self._last_batch_time: float = 0.0
```

### 3.2 `_write_to_file` Rewrite

```python
def _write_to_file(
    self,
    log: Log,
    logfile_path: str,
    durability: DurabilityMode | None = None,
) -> int | None:
    """
    Write log entry to file with configurable durability.

    Args:
        log: Log entry to write
        logfile_path: Target file path
        durability: Override durability mode (uses default if None)

    Returns:
        LSN if enable_lsn is True, else None

    Raises:
        IOError: On write failure (not swallowed in WAL mode)
    """
    if durability is None:
        durability = self._durability

    logfile = self._files.get(logfile_path)
    if logfile is None or logfile.closed:
        return None

    # Generate LSN if enabled
    lsn: int | None = None
    if self._enable_lsn and self._sequence_generator:
        lsn = self._sequence_generator.generate()
        if lsn is not None:
            log.lsn = lsn

    # Encode based on format
    if self._format == 'binary':
        data = self._encode_binary(log, lsn)
    else:
        data = msgspec.json.encode(log) + b"\n"

    # Write data
    logfile.write(data)

    # Apply durability
    match durability:
        case DurabilityMode.NONE:
            pass  # No sync (testing only)

        case DurabilityMode.FLUSH:
            logfile.flush()  # Current behavior

        case DurabilityMode.FSYNC:
            logfile.flush()
            os.fsync(logfile.fileno())  # Guaranteed on-disk

        case DurabilityMode.FSYNC_BATCH:
            logfile.flush()
            # Batch fsync handled by caller

    return lsn
```

### 3.3 Binary Encoding with CRC

```python
def _encode_binary(self, log: Log, lsn: int | None) -> bytes:
    """
    Encode log entry in binary format with CRC32 checksum.

    Binary Format:
    ┌──────────┬──────────┬──────────┬─────────────────────┐
    │ CRC32    │ Length   │ LSN      │ Payload (JSON)      │
    │ (4 bytes)│ (4 bytes)│ (8 bytes)│ (variable)          │
    └──────────┴──────────┴──────────┴─────────────────────┘

    Total header: 16 bytes
    CRC32 covers: length + LSN + payload
    """
    import struct
    import hashlib

    payload = msgspec.json.encode(log)
    lsn_value = lsn if lsn is not None else 0

    # Header: length (4) + LSN (8)
    header = struct.pack("<IQ", len(payload), lsn_value)

    # CRC32 over header + payload
    crc = hashlib.crc32(header + payload)

    # Final: CRC32 (4) + header (12) + payload
    return struct.pack("<I", crc) + header + payload


def _decode_binary(self, data: bytes) -> tuple[Log, int]:
    """
    Decode binary log entry with CRC verification.

    Args:
        data: Raw bytes from file

    Returns:
        Tuple of (Log, LSN)

    Raises:
        ValueError: On CRC mismatch or malformed data
    """
    import struct
    import hashlib

    HEADER_SIZE = 16  # CRC(4) + length(4) + LSN(8)

    if len(data) < HEADER_SIZE:
        raise ValueError(f"Entry too short: {len(data)} < {HEADER_SIZE}")

    crc_stored = struct.unpack("<I", data[:4])[0]
    length, lsn = struct.unpack("<IQ", data[4:16])

    if len(data) < HEADER_SIZE + length:
        raise ValueError(f"Truncated entry: have {len(data)}, need {HEADER_SIZE + length}")

    # Verify CRC over header (after CRC field) + payload
    crc_computed = hashlib.crc32(data[4:16 + length])
    if crc_stored != crc_computed:
        raise ValueError(f"CRC mismatch: stored={crc_stored:#x}, computed={crc_computed:#x}")

    payload = data[16:16 + length]
    log = msgspec.json.decode(payload, type=Log)

    return log, lsn
```

### 3.4 Read-Back for Recovery

```python
async def read_entries(
    self,
    logfile_path: str,
    from_offset: int = 0,
) -> AsyncIterator[tuple[int, Log, int | None]]:
    """
    Read entries from file for WAL recovery.

    Yields tuples of (file_offset, log_entry, lsn).
    Handles both JSON and binary formats based on self._format.

    Args:
        logfile_path: Path to log file
        from_offset: Starting byte offset (0 = beginning)

    Yields:
        (offset, log, lsn) for each entry

    Raises:
        ValueError: On corrupted entries (CRC mismatch, malformed data)
    """
    import struct

    BINARY_HEADER_SIZE = 16

    file_lock = self._file_locks[logfile_path]
    await file_lock.acquire()

    try:
        # Open file for reading (separate from write handle)
        read_file = await self._loop.run_in_executor(
            None,
            functools.partial(open, logfile_path, 'rb'),
        )

        try:
            await self._loop.run_in_executor(None, read_file.seek, from_offset)
            offset = from_offset

            while True:
                if self._format == 'binary':
                    # Read header first
                    header = await self._loop.run_in_executor(
                        None, read_file.read, BINARY_HEADER_SIZE
                    )

                    if len(header) == 0:
                        break  # EOF

                    if len(header) < BINARY_HEADER_SIZE:
                        raise ValueError(f"Truncated header at offset {offset}")

                    length = struct.unpack("<I", header[4:8])[0]
                    payload = await self._loop.run_in_executor(
                        None, read_file.read, length
                    )

                    if len(payload) < length:
                        raise ValueError(f"Truncated payload at offset {offset}")

                    log, lsn = self._decode_binary(header + payload)
                    entry_size = BINARY_HEADER_SIZE + length
                    yield offset, log, lsn
                    offset += entry_size

                else:
                    # JSON format: line-delimited
                    line = await self._loop.run_in_executor(
                        None, read_file.readline
                    )

                    if not line:
                        break  # EOF

                    log = msgspec.json.decode(line.rstrip(b'\n'), type=Log)
                    lsn = getattr(log, 'lsn', None)
                    yield offset, log, lsn
                    offset = read_file.tell()

        finally:
            read_file.close()

    finally:
        if file_lock.locked():
            file_lock.release()


async def get_last_lsn(self, logfile_path: str) -> int | None:
    """
    Get the last LSN in a log file (for recovery).

    Scans from end of file for efficiency with binary format.
    """
    last_lsn: int | None = None

    async for offset, log, lsn in self.read_entries(logfile_path):
        if lsn is not None:
            last_lsn = lsn

    return last_lsn
```

### 3.5 Batched Fsync

```python
async def _schedule_batch_fsync(self, logfile_path: str) -> None:
    """
    Schedule entry for batch fsync.

    Batches are flushed when:
    - batch_max_size entries accumulated, OR
    - batch_timeout_ms elapsed since first entry

    This provides ~10x throughput improvement over per-write fsync
    while maintaining bounded latency.
    """
    if self._batch_lock is None:
        self._batch_lock = asyncio.Lock()

    current_time = time.monotonic()

    async with self._batch_lock:
        should_flush = (
            len(self._pending_batch) >= self._batch_max_size or
            (
                self._last_batch_time > 0 and
                (current_time - self._last_batch_time) * 1000 >= self._batch_timeout_ms
            )
        )

        if should_flush:
            await self._flush_batch(logfile_path)
            self._last_batch_time = current_time
        elif self._last_batch_time == 0:
            self._last_batch_time = current_time


async def _flush_batch(self, logfile_path: str) -> None:
    """
    Flush pending batch with single fsync.

    One fsync for multiple writes provides significant throughput
    improvement while maintaining durability guarantees.
    """
    if not self._pending_batch:
        return

    logfile = self._files.get(logfile_path)
    if logfile and not logfile.closed:
        await self._loop.run_in_executor(
            None,
            os.fsync,
            logfile.fileno(),
        )

    # Signal all waiting futures
    for _, _, future in self._pending_batch:
        if not future.done():
            future.set_result(None)

    self._pending_batch.clear()
    self._last_batch_time = 0.0
```

---

## Part 4: Log Model Extension

### 4.1 Add Optional LSN Field

```python
"""
hyperscale/logging/models/log.py - EXTENDED
"""
from dataclasses import dataclass, field
from typing import Generic, TypeVar

T = TypeVar('T')


@dataclass
class Log(Generic[T]):
    """
    Wrapper around log entries with metadata.

    Extended with optional LSN for WAL use cases.
    """
    entry: T
    filename: str | None = None
    function_name: str | None = None
    line_number: int | None = None
    thread_id: int | None = None
    timestamp: str | None = None

    # NEW: Optional LSN for WAL entries
    lsn: int | None = field(default=None)
```

---

## Part 5: Flow Diagrams

### 5.1 Write Flow Comparison

```
═══════════════════════════════════════════════════════════════════════════
                    CURRENT FLOW (Data Plane - No Change)
═══════════════════════════════════════════════════════════════════════════

    ctx.log(entry)
         │
         ▼
    ┌─────────────┐
    │ _log_to_file│
    └──────┬──────┘
           │
           ▼
    ┌─────────────────────┐
    │ run_in_executor     │
    │ (_write_to_file)    │
    └──────┬──────────────┘
           │
           ▼
    ┌─────────────────────┐
    │ msgspec.json.encode │
    │ + logfile.write()   │
    │ + logfile.flush()   │ ◄── Data in OS buffer only
    └─────────────────────┘
           │
           ▼
       [Return]


═══════════════════════════════════════════════════════════════════════════
                    NEW FLOW (Control Plane - WAL Mode)
═══════════════════════════════════════════════════════════════════════════

    ctx.log(entry)
         │
         ▼
    ┌─────────────┐
    │ _log_to_file│
    └──────┬──────┘
           │
           ▼
    ┌─────────────────────┐
    │ run_in_executor     │
    │ (_write_to_file)    │
    └──────┬──────────────┘
           │
           ▼
    ┌─────────────────────────────────────────────────┐
    │  if enable_lsn:                                  │
    │      lsn = snowflake_generator.generate()        │
    │      log.lsn = lsn                               │
    └──────────────────────┬──────────────────────────┘
                           │
                           ▼
    ┌─────────────────────────────────────────────────┐
    │  if format == 'binary':                          │
    │      data = _encode_binary(log, lsn)             │
    │          ├── payload = msgspec.json.encode(log)  │
    │          ├── header = struct.pack(len, lsn)      │
    │          └── crc = hashlib.crc32(header+payload) │
    │  else:                                           │
    │      data = msgspec.json.encode(log) + b"\n"    │
    └──────────────────────┬──────────────────────────┘
                           │
                           ▼
                   logfile.write(data)
                           │
                           ▼
    ┌─────────────────────────────────────────────────┐
    │  match durability:                               │
    │      NONE  → (no sync)                          │
    │      FLUSH → logfile.flush()                    │
    │      FSYNC → logfile.flush() + os.fsync()       │
    │      FSYNC_BATCH → flush + schedule_batch()     │
    └──────────────────────┬──────────────────────────┘
                           │
                           ▼
                    [Return LSN]
```

### 5.2 Batch Fsync Flow

```
═══════════════════════════════════════════════════════════════════════════
                    BATCH FSYNC TIMING (DurabilityMode.FSYNC_BATCH)
═══════════════════════════════════════════════════════════════════════════

Time →  T0      T1      T2      T3      T4      T5      T6      T7      T8
        │       │       │       │       │       │       │       │       │
        │       │       │       │       │       │       │       │       │
Write 1 ●───────────────────────────────────────●
        ↑ write+flush                           ↑ fsync (batched)
        │                                       │
Write 2 ────────●──────────────────────────────●
                ↑ write+flush                  ↑ same fsync
                │                              │
Write 3 ────────────────●─────────────────────●
                        ↑ write+flush         ↑ same fsync
                        │                     │
        ├───────────────┼─────────────────────┤
        │   10ms batch timeout                │
        │   OR 100 entries                    │
        └─────────────────────────────────────┘
                                              │
                                              ▼
                                    ┌─────────────────┐
                                    │ Single fsync()  │
                                    │ for all 3       │
                                    │ writes          │
                                    └─────────────────┘

Benefits:
- 3 writes with 1 fsync instead of 3 fsyncs
- ~3x throughput improvement
- Max latency bounded to 10ms
- All writes durable after batch fsync
```

### 5.3 Recovery Flow

```
═══════════════════════════════════════════════════════════════════════════
                    WAL RECOVERY FLOW (read_entries)
═══════════════════════════════════════════════════════════════════════════

                            STARTUP
                               │
                               ▼
                    ┌──────────────────┐
                    │  Check for WAL   │
                    │  files exist     │
                    └────────┬─────────┘
                             │
              ┌──────────────┴──────────────┐
              │ Yes                         │ No
              ▼                             ▼
    ┌──────────────────┐          ┌──────────────────┐
    │  Open WAL file   │          │  Fresh start     │
    │  for reading     │          │  (no recovery)   │
    └────────┬─────────┘          └──────────────────┘
             │
             ▼
    ┌──────────────────────────────────────────────┐
    │  async for offset, log, lsn in read_entries: │
    └────────┬─────────────────────────────────────┘
             │
             ▼
    ┌──────────────────────────────────────────────┐
    │  Binary format?                              │
    │  ├── Read 16-byte header                     │
    │  ├── Extract length, LSN                     │
    │  ├── Read payload                            │
    │  ├── Verify CRC32                            │
    │  └── Decode JSON payload                     │
    │                                              │
    │  JSON format?                                │
    │  ├── Read line                               │
    │  └── Decode JSON                             │
    └────────┬─────────────────────────────────────┘
             │
             ▼
    ┌──────────────────────────────────────────────┐
    │  For each recovered entry:                    │
    │  ├── Check entry.state                        │
    │  ├── If PENDING: replay to consensus          │
    │  ├── If REGIONAL: verify with DC             │
    │  ├── If GLOBAL: mark as recovered            │
    │  └── Track max_lsn for new writes            │
    └────────┬─────────────────────────────────────┘
             │
             ▼
    ┌──────────────────────────────────────────────┐
    │  Update sequence_generator with max_lsn      │
    │  Resume normal operations                     │
    └──────────────────────────────────────────────┘
```

---

## Part 6: Timing Diagrams

### 6.1 Durability Mode Latencies

```
═══════════════════════════════════════════════════════════════════════════
                    LATENCY COMPARISON BY DURABILITY MODE
═══════════════════════════════════════════════════════════════════════════

DurabilityMode.NONE (testing only):
├── write()      ──┤ ~1μs
│                  │
└── Total: ~1μs    │
                   │
DurabilityMode.FLUSH (current default):
├── write()      ──┤ ~1μs
├── flush()      ──┤ ~10μs
│                  │
└── Total: ~11μs   │
                   │
DurabilityMode.FSYNC (per-write):
├── write()      ──┤ ~1μs
├── flush()      ──┤ ~10μs
├── fsync()      ──────────────────────────────┤ ~1-10ms (SSD)
│                                               │
└── Total: ~1-10ms                              │
                                                │
DurabilityMode.FSYNC_BATCH (recommended for WAL):
├── write()      ──┤ ~1μs
├── flush()      ──┤ ~10μs
├── (wait for batch)  ──────────────────┤ ≤10ms
├── fsync() [shared] ──────────────────────────┤ ~1-10ms / N writes
│                                               │
└── Per-write latency: ~10ms + 1ms/N           │
    (with 100 writes/batch: ~100μs/write)      │


Throughput Comparison (64-byte entries, NVMe SSD):
┌─────────────────┬───────────────┬─────────────────────────────────┐
│ Mode            │ Writes/sec    │ Notes                           │
├─────────────────┼───────────────┼─────────────────────────────────┤
│ NONE            │ ~1,000,000    │ No durability (testing only)    │
│ FLUSH           │ ~500,000      │ Current behavior, OS buffer     │
│ FSYNC           │ ~500          │ Per-write fsync, very slow      │
│ FSYNC_BATCH     │ ~50,000       │ 100 writes/fsync, recommended   │
└─────────────────┴───────────────┴─────────────────────────────────┘
```

### 6.2 End-to-End Job Commit Timeline

```
═══════════════════════════════════════════════════════════════════════════
                    JOB CREATION WITH WAL (FSYNC_BATCH)
═══════════════════════════════════════════════════════════════════════════

Time →   0ms        1ms        5ms       10ms       15ms      110ms
         │          │          │          │          │          │
Gate     ├── Write to WAL ─────┤          │          │          │
         │  (enable_lsn=True)  │          │          │          │
         │  (format='binary')  │          │          │          │
         │                     │          │          │          │
         │          ├── Batch fsync ──────┤          │          │
         │          │  (10ms timeout)     │          │          │
         │          │                     │          │          │
         │          │          │          ├── LOCAL committed   │
         │          │          │          │  (process crash     │
         │          │          │          │   survivable)       │
         │          │          │          │          │          │
         │          │          │          │          ├── REGIONAL
         │          │          │          │          │  consensus
         │          │          │          │          │  (DC peers)
         │          │          │          │          │          │
         │          │          │          │          │          ├── GLOBAL
         │          │          │          │          │          │  consensus
         │          │          │          │          │          │  (cross-DC)
         │          │          │          │          │          │
         ├──────────┼──────────┼──────────┼──────────┼──────────┤
         │   <1ms   │   10ms   │          │  ~5ms    │  ~100ms  │
         │  write   │  fsync   │          │ regional │  global  │
         │          │  batch   │          │          │          │


Latency Breakdown:
┌────────────────────┬─────────┬────────────────────────────────────────┐
│ Stage              │ Latency │ What Survives                          │
├────────────────────┼─────────┼────────────────────────────────────────┤
│ Write to WAL       │ <1ms    │ Nothing (in memory)                    │
│ Batch fsync        │ ≤10ms   │ Process crash                          │
│ REGIONAL consensus │ ~5ms    │ Node crash, rack failure               │
│ GLOBAL consensus   │ ~100ms  │ DC failure, region failure             │
└────────────────────┴─────────┴────────────────────────────────────────┘
```

---

## Part 7: File Changes Summary

### 7.1 Modified Files

```
hyperscale/logging/
├── config/
│   ├── __init__.py                 # MODIFY: Export DurabilityMode
│   └── durability_mode.py          # NEW: DurabilityMode enum
│
├── models/
│   └── log.py                      # MODIFY: Add lsn: int | None = None
│
└── streams/
    ├── logger.py                   # MODIFY: Pass new params to context()
    ├── logger_context.py           # MODIFY: Accept new params, pass to stream
    └── logger_stream.py            # MODIFY: Core implementation changes
```

### 7.2 LoggerStream Change Summary

| Method | Change Type | Lines | Description |
|--------|-------------|-------|-------------|
| `__init__` | MODIFY | 65-136 | Add 4 new params, 7 new instance vars |
| `_to_logfile_path` | MODIFY | 444-463 | Relax `.json` extension constraint |
| `_write_to_file` | REWRITE | 857-873 | Add durability, binary format, LSN |
| `_encode_binary` | NEW | - | Binary format with CRC32 |
| `_decode_binary` | NEW | - | Binary decode with CRC verify |
| `read_entries` | NEW | - | Async iterator for recovery |
| `get_last_lsn` | NEW | - | Find last LSN for recovery |
| `_schedule_batch_fsync` | NEW | - | Batch fsync scheduling |
| `_flush_batch` | NEW | - | Execute batch fsync |
| `_log_to_file` | MODIFY | 739-855 | Thread durability param |

### 7.3 New File: `durability_mode.py`

```python
"""
hyperscale/logging/config/durability_mode.py

Durability configuration for Logger writes.
"""
from enum import IntEnum


class DurabilityMode(IntEnum):
    """
    Durability levels for log writes.

    NONE:        No sync - testing only, data loss on any failure
    FLUSH:       Buffer flush - current behavior, data loss on OS crash
    FSYNC:       Per-write fsync - safest, highest latency (~1-10ms/write)
    FSYNC_BATCH: Batched fsync - recommended for WAL (~10ms max latency)

    Recommended:
    - Data Plane (stats): FLUSH (default, current behavior)
    - Control Plane (WAL): FSYNC_BATCH (durability + throughput)
    - Testing: NONE (maximum speed, no durability)
    """
    NONE = 0
    FLUSH = 1
    FSYNC = 2
    FSYNC_BATCH = 3
```

---

## Part 8: Integration with AD-38

### 8.1 Architecture Mapping

```
═══════════════════════════════════════════════════════════════════════════
                    AD-38 + AD-39 INTEGRATION
═══════════════════════════════════════════════════════════════════════════

AD-38 Architecture              │  AD-39 Logger Extension
────────────────────────────────┼────────────────────────────────────────
                                │
CONTROL PLANE                   │
┌───────────────────────────────┼───────────────────────────────────────┐
│ NodeWAL (job/workflow cmds)   │  Logger with WAL mode:                │
│                               │  ├── durability=FSYNC_BATCH           │
│ • Binary format with CRC      │  ├── format='binary'                  │
│ • Sequence numbers (LSN)      │  ├── enable_lsn=True                  │
│ • fsync guarantee             │  └── instance_id=node_id              │
│ • Read-back for recovery      │                                       │
└───────────────────────────────┼───────────────────────────────────────┘
                                │
DATA PLANE                      │
┌───────────────────────────────┼───────────────────────────────────────┐
│ Logger (stats streaming)      │  Logger with default mode:            │
│                               │  ├── durability=FLUSH (default)       │
│ • JSON format                 │  ├── format='json' (default)          │
│ • Eventual consistency OK     │  ├── enable_lsn=False (default)       │
│ • High throughput             │  └── (no changes needed)              │
└───────────────────────────────┼───────────────────────────────────────┘
```

### 8.2 Usage Example: Gate Node

```python
class GateNode:
    def __init__(self):
        self._logger = Logger()

        # Configure WAL context for job operations (Control Plane)
        self._logger.configure(
            name="gate_wal",
            path="hyperscale.gate.wal",
            durability=DurabilityMode.FSYNC_BATCH,
            format='binary',
            enable_lsn=True,
            instance_id=self._node_id,
        )

        # Configure stats context (Data Plane - unchanged)
        self._logger.configure(
            name="gate_stats",
            path="hyperscale.gate.stats.json",
            # All defaults: FLUSH, json, no LSN
        )

    async def create_job(self, job: Job):
        # WAL mode - durable, with LSN
        async with self._logger.context(name="gate_wal") as ctx:
            lsn = await ctx.log(JobCreatedEvent(job_id=job.id, ...))
            # lsn returned for replication tracking

            # Replicate to DC peers
            await self._replicate_to_regional(lsn)

            # Replicate to other DCs
            await self._replicate_to_global(lsn)

    async def record_stats(self, stats: Stats):
        # Stats mode - fire-and-forget, eventual consistency
        async with self._logger.context(name="gate_stats") as ctx:
            await ctx.log(StatsEntry(stats=stats))
            # No LSN, no fsync, just best-effort logging
```

---

## Part 9: Success Criteria

**Backward Compatibility**:
1. All existing Logger usage works unchanged with zero code modifications
2. Default parameters produce identical behavior to current implementation
3. No new dependencies or breaking API changes

**WAL Compliance (when enabled)**:
4. `FSYNC_BATCH` mode survives process crash with ≤10ms data loss window
5. `FSYNC` mode survives process crash with zero data loss
6. Binary format with CRC32 detects all single-bit errors
7. LSN generation is monotonic and unique per instance
8. `read_entries()` successfully recovers all non-corrupted entries

**Performance**:
9. Default mode (FLUSH) has identical performance to current implementation
10. FSYNC_BATCH mode achieves ≥50,000 writes/second on NVMe SSD
11. Batch timeout bounded to 10ms maximum latency
12. Binary encoding adds <10μs overhead per entry

**Integration**:
13. Logger WAL mode integrates seamlessly with AD-38 NodeWAL patterns
14. SnowflakeGenerator correctly reused for LSN generation
15. File rotation works correctly with both JSON and binary formats

---

## Part 10: Conclusion

AD-39 extends the existing Logger with optional WAL-compliant features while maintaining full backward compatibility. This approach:

**Advantages**:
- **Code Reuse**: Leverages proven async I/O patterns from Logger
- **Consistent API**: Same `context()` pattern used throughout codebase
- **Progressive Enhancement**: Enable WAL features incrementally per-context
- **Zero Breaking Changes**: All existing code works unchanged
- **Unified Codebase**: Single Logger class for both Control and Data Plane

**Key Extensions**:
- `DurabilityMode` enum: NONE, FLUSH, FSYNC, FSYNC_BATCH
- Binary format with CRC32 checksums for integrity
- LSN generation via existing SnowflakeGenerator
- Read-back capability for crash recovery
- Batched fsync for throughput/latency balance

**Relationship to AD-38**:
- AD-38 defines the architecture (Control Plane vs Data Plane)
- AD-39 implements the Logger extensions to support both planes
- Data Plane continues using Logger defaults (no changes)
- Control Plane uses Logger with WAL mode enabled

**References**:
- `hyperscale/logging/streams/logger_stream.py` (core modifications)
- `hyperscale/logging/streams/logger_context.py` (parameter passthrough)
- `hyperscale/logging/streams/logger.py` (API extension)
- `hyperscale/logging/models/log.py` (LSN field addition)
- `hyperscale/logging/config/durability_mode.py` (new enum)
- `hyperscale/logging/snowflake/snowflake_generator.py` (LSN generation)

---

## Part 11: Deep asyncio Internals

This section documents the critical asyncio compatibility patterns already present in LoggerStream that MUST be preserved and extended for WAL support. Understanding these patterns is essential for correct implementation.

### 11.1 File Descriptor Duplication Pattern

LoggerStream uses `os.dup()` to create independent file descriptors for stdout/stderr. This pattern enables asyncio-compatible stream writing:

```python
# Current implementation (logger_stream.py:465-507)
async def _dup_stdout(self):
    """
    Create independent file descriptor for stdout.

    Why duplication matters:
    1. Allows asyncio.StreamWriter to manage the FD independently
    2. Closing the duplicated FD doesn't affect original stdout
    3. Enables asyncio's connect_write_pipe() to work correctly
    """
    # Step 1: Get the file descriptor (blocking call)
    stdout_fileno = await self._loop.run_in_executor(
        None,
        sys.stderr.fileno  # Note: actually gets stderr's fileno
    )

    # Step 2: Duplicate the file descriptor (blocking call)
    stdout_dup = await self._loop.run_in_executor(
        None,
        os.dup,
        stdout_fileno,
    )

    # Step 3: Create file object from duplicated FD (blocking call)
    return await self._loop.run_in_executor(
        None,
        functools.partial(
            os.fdopen,
            stdout_dup,
            mode=sys.stdout.mode
        )
    )
```

**Key Insight**: Every syscall that could block is wrapped in `run_in_executor()`. Even `sys.stderr.fileno()` is wrapped because it could block on certain platforms or under load.

### 11.2 asyncio Compatibility Requirements

**Rule**: ALL blocking I/O operations MUST be executed via `run_in_executor()`.

```
═══════════════════════════════════════════════════════════════════════════
                    BLOCKING OPERATIONS IN LOGGERSTREAM
═══════════════════════════════════════════════════════════════════════════

Operation                │ Location         │ Wrapper Pattern
─────────────────────────┼──────────────────┼────────────────────────────────
os.getcwd()              │ open_file:220    │ run_in_executor(None, os.getcwd)
_open_file()             │ open_file:228    │ run_in_executor(None, _open_file, path)
_rotate_logfile()        │ _rotate:271      │ run_in_executor(None, _rotate, ...)
_close_file_at_path()    │ _close_file:429  │ run_in_executor(None, _close, path)
_write_to_file()         │ _log_to_file:820 │ run_in_executor(None, _write, ...)
sys.stderr.fileno()      │ _dup_stderr:489  │ run_in_executor(None, fileno)
os.dup()                 │ _dup_stderr:494  │ run_in_executor(None, os.dup, fd)
os.fdopen()              │ _dup_stderr:500  │ run_in_executor(None, partial(...))
_stderr.write()          │ _log:723         │ run_in_executor(None, write, ...)
```

**Pattern for New Operations**:

```python
# WRONG - blocks the event loop
data = file.read(4096)
file.seek(0)
os.fsync(file.fileno())

# CORRECT - asyncio compatible
data = await self._loop.run_in_executor(None, file.read, 4096)
await self._loop.run_in_executor(None, file.seek, 0)
await self._loop.run_in_executor(None, os.fsync, file.fileno())
```

### 11.3 File Locking Pattern

LoggerStream uses per-file asyncio locks to prevent concurrent access:

```python
# Current pattern (logger_stream.py:99)
self._file_locks: Dict[str, asyncio.Lock] = defaultdict(asyncio.Lock)

# Usage pattern (logger_stream.py:817-828)
async def _log_to_file(self, ...):
    file_lock = self._file_locks[logfile_path]
    await file_lock.acquire()

    try:
        await self._loop.run_in_executor(
            None,
            self._write_to_file,
            log,
            logfile_path,
        )
    finally:
        if file_lock.locked():
            file_lock.release()
```

**Critical**: Use `asyncio.Lock()`, NOT `threading.Lock()`. Thread locks block the entire event loop when acquired.

### 11.4 WAL Read Implementation Deep Dive

Reading files for WAL recovery requires careful asyncio handling. Unlike writes (which can be fire-and-forget), reads must return data to the caller.

#### 11.4.1 Read File Descriptor Strategy

For concurrent read/write WAL operations, use separate file descriptors:

```python
class LoggerStream:
    def __init__(self, ...):
        # ...existing...

        # WAL-specific: Separate read and write file descriptors
        self._files: Dict[str, io.FileIO] = {}         # Write handles (existing)
        self._read_files: Dict[str, io.FileIO] = {}    # NEW: Read handles
        self._read_locks: Dict[str, asyncio.Lock] = defaultdict(asyncio.Lock)  # NEW
```

**Why separate file descriptors?**:
1. Write handle stays at EOF for appending
2. Read handle can seek independently
3. No position conflicts during concurrent operations
4. Follows same pattern as stdout/stderr duplication

#### 11.4.2 asyncio-Compatible Read Operations

```python
async def _open_read_file(self, logfile_path: str) -> io.FileIO:
    """
    Open a separate file descriptor for reading.

    Critical: Uses run_in_executor for ALL blocking operations.
    """
    read_lock = self._read_locks[logfile_path]
    await read_lock.acquire()

    try:
        if (
            logfile_path not in self._read_files or
            self._read_files[logfile_path].closed
        ):
            # Open file for reading (blocking operation)
            read_file = await self._loop.run_in_executor(
                None,
                functools.partial(open, logfile_path, 'rb'),
            )
            self._read_files[logfile_path] = read_file

        return self._read_files[logfile_path]

    finally:
        if read_lock.locked():
            read_lock.release()


async def read_entries(
    self,
    logfile_path: str,
    from_offset: int = 0,
) -> AsyncIterator[tuple[int, Log, int | None]]:
    """
    Read entries from file for WAL recovery.

    CRITICAL ASYNCIO PATTERNS:
    1. All read() calls via run_in_executor
    2. All seek() calls via run_in_executor
    3. All tell() calls via run_in_executor
    4. Use asyncio.Lock for synchronization
    5. Yield control regularly (asyncio.sleep(0) between entries)
    """
    BINARY_HEADER_SIZE = 16

    read_file = await self._open_read_file(logfile_path)
    read_lock = self._read_locks[logfile_path]

    await read_lock.acquire()

    try:
        # Seek to starting position (blocking)
        await self._loop.run_in_executor(
            None,
            read_file.seek,
            from_offset,
        )

        offset = from_offset
        entries_yielded = 0

        while True:
            if self._format == 'binary':
                # Read header (blocking)
                header = await self._loop.run_in_executor(
                    None,
                    read_file.read,
                    BINARY_HEADER_SIZE,
                )

                if len(header) == 0:
                    break  # EOF

                if len(header) < BINARY_HEADER_SIZE:
                    raise ValueError(f"Truncated header at offset {offset}")

                # Parse header to get payload length
                length = struct.unpack("<I", header[4:8])[0]

                # Read payload (blocking)
                payload = await self._loop.run_in_executor(
                    None,
                    read_file.read,
                    length,
                )

                if len(payload) < length:
                    raise ValueError(f"Truncated payload at offset {offset}")

                log, lsn = self._decode_binary(header + payload)
                entry_size = BINARY_HEADER_SIZE + length

                yield offset, log, lsn
                offset += entry_size

            else:
                # JSON format: line-delimited (blocking)
                line = await self._loop.run_in_executor(
                    None,
                    read_file.readline,
                )

                if not line:
                    break  # EOF

                log = msgspec.json.decode(line.rstrip(b'\n'), type=Log)
                lsn = getattr(log, 'lsn', None)

                yield offset, log, lsn

                # Get current position (blocking)
                offset = await self._loop.run_in_executor(
                    None,
                    read_file.tell,
                )

            # Yield control to event loop periodically
            entries_yielded += 1
            if entries_yielded % 100 == 0:
                await asyncio.sleep(0)

    finally:
        if read_lock.locked():
            read_lock.release()
```

### 11.5 Batch fsync Timer - asyncio-Native Implementation

**WRONG**: Using `threading.Timer` (blocks event loop):
```python
# DO NOT DO THIS
import threading
threading.Timer(0.010, self._flush_batch).start()  # Runs in separate thread!
```

**CORRECT**: Using asyncio-native scheduling:

```python
class LoggerStream:
    def __init__(self, ...):
        # ...existing...

        # Batch fsync state
        self._batch_timer_handle: asyncio.TimerHandle | None = None
        self._batch_flush_task: asyncio.Task | None = None

    async def _schedule_batch_fsync(self, logfile_path: str) -> asyncio.Future[None]:
        """
        Schedule entry for batch fsync using asyncio-native timer.

        Returns a Future that resolves when fsync completes.
        """
        if self._batch_lock is None:
            self._batch_lock = asyncio.Lock()

        future: asyncio.Future[None] = self._loop.create_future()

        async with self._batch_lock:
            self._pending_batch.append((logfile_path, future))

            # Start timer if this is the first entry in batch
            if len(self._pending_batch) == 1:
                # Schedule flush after batch_timeout_ms
                self._batch_timer_handle = self._loop.call_later(
                    self._batch_timeout_ms / 1000.0,  # Convert ms to seconds
                    self._trigger_batch_flush,
                    logfile_path,
                )

            # Immediate flush if batch is full
            if len(self._pending_batch) >= self._batch_max_size:
                if self._batch_timer_handle:
                    self._batch_timer_handle.cancel()
                    self._batch_timer_handle = None
                await self._flush_batch(logfile_path)

        return future

    def _trigger_batch_flush(self, logfile_path: str) -> None:
        """
        Timer callback - schedules the actual flush as a task.

        Note: call_later callback runs in the event loop, but we can't
        await directly. Schedule as a task instead.
        """
        if self._batch_flush_task is None or self._batch_flush_task.done():
            self._batch_flush_task = asyncio.create_task(
                self._flush_batch(logfile_path)
            )

    async def _flush_batch(self, logfile_path: str) -> None:
        """
        Flush pending batch with single fsync.

        Uses run_in_executor for fsync (blocking operation).
        """
        async with self._batch_lock:
            if not self._pending_batch:
                return

            # Cancel any pending timer
            if self._batch_timer_handle:
                self._batch_timer_handle.cancel()
                self._batch_timer_handle = None

            logfile = self._files.get(logfile_path)
            if logfile and not logfile.closed:
                # fsync is blocking - must use executor
                await self._loop.run_in_executor(
                    None,
                    os.fsync,
                    logfile.fileno(),
                )

            # Signal all waiting futures
            for _, future in self._pending_batch:
                if not future.done():
                    future.set_result(None)

            self._pending_batch.clear()
```

### 11.6 Complete asyncio Pattern Summary

```
═══════════════════════════════════════════════════════════════════════════
                    ASYNCIO PATTERNS FOR WAL IMPLEMENTATION
═══════════════════════════════════════════════════════════════════════════

┌─────────────────────────────────────────────────────────────────────────┐
│ PATTERN 1: Blocking Operations                                           │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│   ALWAYS wrap in run_in_executor():                                      │
│   ├── file.read(n)     → await loop.run_in_executor(None, file.read, n) │
│   ├── file.write(data) → await loop.run_in_executor(None, file.write, d)│
│   ├── file.seek(pos)   → await loop.run_in_executor(None, file.seek, p) │
│   ├── file.tell()      → await loop.run_in_executor(None, file.tell)    │
│   ├── file.flush()     → await loop.run_in_executor(None, file.flush)   │
│   ├── os.fsync(fd)     → await loop.run_in_executor(None, os.fsync, fd) │
│   ├── open(path, mode) → await loop.run_in_executor(None, open, p, m)   │
│   └── file.close()     → await loop.run_in_executor(None, file.close)   │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────────┐
│ PATTERN 2: Synchronization                                               │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│   USE asyncio primitives, NOT threading:                                 │
│   ├── asyncio.Lock()        NOT threading.Lock()                        │
│   ├── asyncio.Event()       NOT threading.Event()                       │
│   ├── asyncio.Condition()   NOT threading.Condition()                   │
│   └── asyncio.Semaphore()   NOT threading.Semaphore()                   │
│                                                                          │
│   ALWAYS use try/finally with locks:                                     │
│   │   await lock.acquire()                                              │
│   │   try:                                                               │
│   │       # ... critical section ...                                     │
│   │   finally:                                                           │
│   │       if lock.locked():                                              │
│   │           lock.release()                                             │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────────┐
│ PATTERN 3: Timers and Scheduling                                         │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│   USE asyncio timers, NOT threading.Timer:                               │
│   ├── loop.call_later(delay, callback)  - for non-async callbacks       │
│   ├── loop.call_at(when, callback)      - for absolute time scheduling  │
│   └── asyncio.create_task(coro)         - for async work                │
│                                                                          │
│   Timer callbacks cannot be async - schedule a task:                     │
│   │   def timer_callback():                                              │
│   │       asyncio.create_task(self._async_handler())                    │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────────┐
│ PATTERN 4: File Descriptor Management                                    │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│   Separate FDs for read and write:                                       │
│   ├── Write FD: stays at EOF for appending                              │
│   ├── Read FD: can seek independently                                   │
│   └── Use os.dup() for independent control                              │
│                                                                          │
│   Each FD has its own asyncio.Lock():                                   │
│   ├── self._file_locks[path]      - for write operations                │
│   └── self._read_locks[path]      - for read operations                 │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────────┐
│ PATTERN 5: Event Loop Yielding                                           │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│   Yield control during long operations:                                  │
│   │   for i, entry in enumerate(entries):                               │
│   │       # ... process entry ...                                        │
│   │       if i % 100 == 0:                                              │
│   │           await asyncio.sleep(0)  # Yield to event loop             │
│                                                                          │
│   This prevents starving other coroutines during bulk operations.        │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘
```

### 11.7 Impact on AD-39 Implementation

The asyncio patterns above affect the AD-39 implementation as follows:

| AD-39 Feature | asyncio Impact |
|---------------|----------------|
| `_write_to_file` rewrite | Already wrapped in `run_in_executor()` - add fsync call inside |
| Binary encoding | Pure CPU work - no executor needed |
| Binary decoding | Pure CPU work - no executor needed |
| LSN generation | SnowflakeGenerator is sync - no executor needed |
| `read_entries` | ALL read/seek/tell operations need executor wrapping |
| Batch fsync timer | MUST use `loop.call_later()`, NOT `threading.Timer` |
| `_flush_batch` | fsync needs executor wrapping |
| Separate read FD | Follow existing dup pattern with executor wrapping |

### 11.8 Updated `_write_to_file` with Proper asyncio Handling

The current `_write_to_file` is a synchronous method called via `run_in_executor()`. This pattern MUST be preserved - we extend the sync method, not convert it to async:

```python
def _write_to_file(
    self,
    log: Log,
    logfile_path: str,
    durability: DurabilityMode | None = None,
) -> int | None:
    """
    Write log entry to file with configurable durability.

    IMPORTANT: This is a SYNCHRONOUS method called via run_in_executor().
    All operations here are blocking and that's OK because we're in a thread.

    The caller (_log_to_file) wraps this in:
        await self._loop.run_in_executor(None, self._write_to_file, ...)
    """
    if durability is None:
        durability = self._durability

    logfile = self._files.get(logfile_path)
    if logfile is None or logfile.closed:
        return None

    # Generate LSN if enabled (sync operation - OK in executor thread)
    lsn: int | None = None
    if self._enable_lsn and self._sequence_generator:
        lsn = self._sequence_generator.generate()
        if lsn is not None:
            log.lsn = lsn

    # Encode based on format (sync - CPU bound, OK in executor thread)
    if self._format == 'binary':
        data = self._encode_binary(log, lsn)
    else:
        data = msgspec.json.encode(log) + b"\n"

    # Write data (sync - blocking I/O, OK in executor thread)
    logfile.write(data)

    # Apply durability (sync - all blocking I/O, OK in executor thread)
    match durability:
        case DurabilityMode.NONE:
            pass

        case DurabilityMode.FLUSH:
            logfile.flush()

        case DurabilityMode.FSYNC:
            logfile.flush()
            os.fsync(logfile.fileno())  # Blocking - OK in thread

        case DurabilityMode.FSYNC_BATCH:
            logfile.flush()
            # Note: Batch tracking happens in async caller

    return lsn
```

**Critical**: The sync method stays sync. The async wrapper stays in `_log_to_file`. This preserves the existing pattern while adding durability support.

---

## Part 12: High-Concurrency I/O Architecture

This section addresses the critical question: **How do we handle 10,000+ concurrent writes efficiently?**

The current `run_in_executor()` pattern has fundamental limitations for high-concurrency WAL operations. This section documents the problem and the recommended solution.

### 12.1 Current Executor Limitations

LoggerStream currently uses `run_in_executor(None, ...)` for all file operations, which uses the **default ThreadPoolExecutor**:

```python
# Current pattern - every write dispatches to thread pool
await self._loop.run_in_executor(None, self._write_to_file, log, logfile_path)
```

**Default ThreadPoolExecutor Size:**
```python
# Python's default calculation
max_workers = min(32, (os.cpu_count() or 1) + 4)

# Results:
#   8-core machine  → 12 threads
#   16-core machine → 20 threads
#   32-core machine → 32 threads (capped)
#   64-core machine → 32 threads (capped)
```

### 12.2 The High-Concurrency Problem

```
═══════════════════════════════════════════════════════════════════════════
                    THREADPOOLEXECUTOR BOTTLENECK
═══════════════════════════════════════════════════════════════════════════

SCENARIO: 10,000 concurrent WAL writes

┌─────────────────────────────────────────────────────────────────────────┐
│                                                                          │
│   Async Writers (10,000 concurrent)                                     │
│   ┌─────────────────────────────────────────────────────────────────┐   │
│   │  Writer 1    ───┐                                                │   │
│   │  Writer 2    ───┤                                                │   │
│   │  Writer 3    ───┤                                                │   │
│   │  Writer 4    ───┤                                                │   │
│   │     ...      ───┼───────────────┐                                │   │
│   │  Writer 9997 ───┤               │                                │   │
│   │  Writer 9998 ───┤               ▼                                │   │
│   │  Writer 9999 ───┤     ┌──────────────────────┐                   │   │
│   │  Writer 10000───┘     │  ThreadPoolExecutor  │                   │   │
│   └───────────────────────│      (32 threads)    │───────────────────┘   │
│                           │                      │                       │
│                           │  ┌────────────────┐  │                       │
│                           │  │ 32 ACTIVE      │  │──────► Disk I/O      │
│                           │  │                │  │                       │
│                           │  │ 9,968 QUEUED   │◄─┼─── Unbounded!        │
│                           │  │ (waiting)      │  │                       │
│                           │  └────────────────┘  │                       │
│                           └──────────────────────┘                       │
│                                                                          │
│   PROBLEMS:                                                             │
│   ├── Queue grows unbounded → Memory pressure                           │
│   ├── 9,968 tasks waiting → Latency spikes                             │
│   ├── No backpressure → Callers don't slow down                        │
│   ├── 10,000 Future allocations → GC pressure                          │
│   └── 10,000 context switches → CPU overhead                           │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘
```

### 12.3 Per-Write Overhead Analysis

```
═══════════════════════════════════════════════════════════════════════════
                    OVERHEAD PER run_in_executor() CALL
═══════════════════════════════════════════════════════════════════════════

Operation                              │ Time        │ Allocations
───────────────────────────────────────┼─────────────┼─────────────────
asyncio.Future allocation              │    ~100ns   │ 1 object
Thread pool task submission            │    ~1μs     │ 1 callable wrapper
Queue lock acquisition                 │    ~100ns   │ 0
Context switch to worker thread        │    ~1-10μs  │ Stack frame
File write (to OS buffer)              │    ~1μs     │ 0
Context switch back to event loop      │    ~1-10μs  │ 0
Future result setting                  │    ~100ns   │ 0
Awaiting coroutine resumption          │    ~500ns   │ 0
───────────────────────────────────────┼─────────────┼─────────────────
TOTAL per write (no fsync)             │    ~5-25μs  │ 2+ objects
TOTAL per write (with fsync)           │    ~1-10ms  │ 2+ objects

THROUGHPUT IMPLICATIONS:
┌─────────────────────────────────────────────────────────────────────────┐
│                                                                          │
│   At 5μs per write:     200,000 writes/sec theoretical max              │
│   At 25μs per write:    40,000 writes/sec theoretical max               │
│                                                                          │
│   BUT with 32 threads:  Contention reduces this significantly           │
│   Realistic throughput: ~10,000-20,000 writes/sec                       │
│                                                                          │
│   With fsync per write: ~100-1,000 writes/sec (disk-bound)              │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘
```

### 12.4 High-Concurrency Approaches Comparison

```
═══════════════════════════════════════════════════════════════════════════
              HIGH-CONCURRENCY I/O APPROACHES COMPARISON
═══════════════════════════════════════════════════════════════════════════

┌─────────────────────────────────────────────────────────────────────────┐
│ APPROACH 1: Current (run_in_executor per write)                         │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│   Pattern:                                                              │
│   for each write:                                                       │
│       await run_in_executor(None, _write_to_file, log, path)            │
│                                                                          │
│   Throughput:    ~10,000-20,000 writes/sec                              │
│   Latency:       5-25μs per write (no fsync)                            │
│   Complexity:    Low                                                    │
│   Portability:   Excellent (all platforms)                              │
│   Backpressure:  None (unbounded queue)                                 │
│                                                                          │
│   Verdict: ✗ NOT SUITABLE for high-concurrency WAL                      │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────────┐
│ APPROACH 2: Dedicated Writer Thread                                      │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│   Pattern:                                                              │
│   - Single long-lived thread for writes                                 │
│   - asyncio.Queue connects async callers to thread                      │
│   - Thread batches writes internally                                    │
│                                                                          │
│   Throughput:    ~50,000-100,000 writes/sec                             │
│   Latency:       1-5ms (batch timeout)                                  │
│   Complexity:    Medium                                                 │
│   Portability:   Excellent                                              │
│   Backpressure:  Via queue size limit                                   │
│                                                                          │
│   Verdict: ✓ Good for single-file WAL                                   │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────────┐
│ APPROACH 3: Write Coalescing (RECOMMENDED)                               │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│   Pattern:                                                              │
│   - Buffer writes in async layer                                        │
│   - Single run_in_executor() call per batch                             │
│   - Batch triggers: size limit OR timeout                               │
│                                                                          │
│   Throughput:    ~100,000+ writes/sec                                   │
│   Latency:       ≤5ms (configurable batch timeout)                      │
│   Complexity:    Medium                                                 │
│   Portability:   Excellent                                              │
│   Backpressure:  Via buffer size limit                                  │
│                                                                          │
│   Verdict: ✓✓ RECOMMENDED for WAL - best balance                        │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────────┐
│ APPROACH 4: io_uring (Linux 5.1+)                                       │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│   Pattern:                                                              │
│   - Kernel-level async I/O                                              │
│   - Submit batch of operations in single syscall                        │
│   - Kernel notifies completion asynchronously                           │
│                                                                          │
│   Throughput:    ~1,000,000+ IOPS                                       │
│   Latency:       Minimal (no thread overhead)                           │
│   Complexity:    High                                                   │
│   Portability:   Linux only (5.1+)                                      │
│   Backpressure:  Kernel queue depth                                     │
│                                                                          │
│   Verdict: ✓ Best performance, but Linux-only                           │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘

SUMMARY TABLE:
┌──────────────────────┬────────────┬─────────┬────────────┬──────────┐
│ Approach             │ Throughput │ Latency │ Complexity │ Portable │
├──────────────────────┼────────────┼─────────┼────────────┼──────────┤
│ run_in_executor/write│   ~10K/s   │  ~20μs  │    Low     │   Yes    │
│ Dedicated thread     │   ~75K/s   │  ~5ms   │   Medium   │   Yes    │
│ Write coalescing     │  ~100K/s   │  ~5ms   │   Medium   │   Yes    │
│ io_uring             │   ~1M/s    │  ~50μs  │    High    │  Linux   │
└──────────────────────┴────────────┴─────────┴────────────┴──────────┘
```

### 12.5 Recommended Approach: Write Coalescing

Write coalescing batches multiple async write requests into a single executor call, dramatically reducing overhead while maintaining the familiar asyncio patterns.

#### 12.5.1 Architecture Overview

```
═══════════════════════════════════════════════════════════════════════════
                    WRITE COALESCING ARCHITECTURE
═══════════════════════════════════════════════════════════════════════════

┌─────────────────────────────────────────────────────────────────────────┐
│                         ASYNC LAYER (Event Loop)                         │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│   Concurrent Writers                                                     │
│   ┌──────────────────────────────────────────────────────────────────┐  │
│   │  async def log_wal_entry(entry):                                  │  │
│   │      future = loop.create_future()                                │  │
│   │      buffer.append((entry, future))  # Non-blocking              │  │
│   │      maybe_trigger_flush()                                        │  │
│   │      return await future              # Wait for durability       │  │
│   └──────────────────────────────────────────────────────────────────┘  │
│                                                                          │
│   Write Buffer (in-memory)                                              │
│   ┌──────────────────────────────────────────────────────────────────┐  │
│   │  Entry 1  │  Entry 2  │  Entry 3  │  ...  │  Entry N             │  │
│   │  Future 1 │  Future 2 │  Future 3 │  ...  │  Future N            │  │
│   └─────────────────────────────┬────────────────────────────────────┘  │
│                                 │                                        │
│                                 │ Flush when:                            │
│                                 │ ├── N >= batch_max_size (100)          │
│                                 │ └── OR timeout elapsed (5ms)           │
│                                 │                                        │
│                                 ▼                                        │
│   ┌──────────────────────────────────────────────────────────────────┐  │
│   │              SINGLE run_in_executor() CALL                        │  │
│   │                                                                   │  │
│   │  await loop.run_in_executor(None, _write_batch_sync, batch)      │  │
│   └─────────────────────────────┬────────────────────────────────────┘  │
│                                 │                                        │
└─────────────────────────────────┼────────────────────────────────────────┘
                                  │
                                  ▼
┌─────────────────────────────────────────────────────────────────────────┐
│                         SYNC LAYER (Thread Pool)                         │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│   def _write_batch_sync(batch):                                         │
│       lsns = []                                                         │
│       for entry in batch:                                               │
│           lsn = _encode_and_write(entry)  # Sequential, fast           │
│           lsns.append(lsn)                                              │
│                                                                          │
│       file.flush()     # Once for entire batch                          │
│       os.fsync(fd)     # Once for entire batch                          │
│                                                                          │
│       return lsns                                                       │
│                                                                          │
│   COST COMPARISON:                                                      │
│   ├── 100 individual writes: 100 executor calls, 100 fsyncs            │
│   └── 1 batched write:       1 executor call, 1 fsync                  │
│                                                                          │
│   SPEEDUP: ~100x for executor overhead, ~100x for fsync                 │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘
```

#### 12.5.2 Implementation: WALWriter Class

```python
"""
hyperscale/logging/streams/wal_writer.py

High-concurrency WAL writer with write coalescing.
"""
import asyncio
import functools
import os
import struct
import zlib
from collections import defaultdict
from typing import Any, Dict, List, Tuple, TypeVar

import msgspec

from hyperscale.logging.models import Log
from hyperscale.logging.snowflake import SnowflakeGenerator
from hyperscale.logging.config.durability_mode import DurabilityMode

T = TypeVar('T')


class WALWriter:
    """
    High-concurrency WAL writer using write coalescing.

    Instead of dispatching each write to the thread pool individually,
    this class buffers writes and flushes them in batches. This provides:

    - ~100x reduction in executor dispatch overhead
    - ~100x reduction in fsync calls (one per batch, not per write)
    - Bounded latency via configurable batch timeout
    - Backpressure via configurable buffer limits

    Thread Safety:
    - All public methods are async and use asyncio.Lock
    - The sync batch write runs in executor (thread-safe by isolation)
    - No shared mutable state between async and sync layers

    Usage:
        writer = WALWriter(
            logfile_path="/var/log/hyperscale.wal",
            instance_id=node_id,
            batch_timeout_ms=5.0,
            batch_max_size=100,
        )
        await writer.start()

        # High-concurrency writes - all coalesced automatically
        lsn = await writer.write(log_entry)

        await writer.close()
    """

    # Binary format constants
    HEADER_SIZE = 16  # CRC32(4) + length(4) + LSN(8)

    def __init__(
        self,
        logfile_path: str,
        instance_id: int = 0,
        batch_timeout_ms: float = 5.0,
        batch_max_size: int = 100,
        buffer_max_size: int = 10000,
        durability: DurabilityMode = DurabilityMode.FSYNC_BATCH,
    ):
        """
        Initialize WAL writer.

        Args:
            logfile_path: Path to WAL file
            instance_id: Node ID for snowflake LSN generation
            batch_timeout_ms: Max time to wait before flushing batch
            batch_max_size: Max entries per batch (triggers immediate flush)
            buffer_max_size: Max buffered entries (backpressure limit)
            durability: Durability mode for writes
        """
        self._logfile_path = logfile_path
        self._instance_id = instance_id
        self._batch_timeout_ms = batch_timeout_ms
        self._batch_max_size = batch_max_size
        self._buffer_max_size = buffer_max_size
        self._durability = durability

        # Async state
        self._loop: asyncio.AbstractEventLoop | None = None
        self._buffer: List[Tuple[Log, asyncio.Future[int | None]]] = []
        self._buffer_lock: asyncio.Lock | None = None
        self._flush_timer: asyncio.TimerHandle | None = None
        self._flush_task: asyncio.Task | None = None
        self._backpressure_event: asyncio.Event | None = None

        # Sync state (accessed only in executor)
        self._file: Any = None  # io.FileIO
        self._sequence_generator: SnowflakeGenerator | None = None

        # Metrics
        self._writes_total: int = 0
        self._batches_total: int = 0
        self._bytes_written: int = 0

        self._started = False
        self._closed = False

    async def start(self) -> None:
        """
        Start the WAL writer.

        Opens the file and initializes async primitives.
        Must be called before any writes.
        """
        if self._started:
            return

        self._loop = asyncio.get_running_loop()
        self._buffer_lock = asyncio.Lock()
        self._backpressure_event = asyncio.Event()
        self._backpressure_event.set()  # Initially no backpressure

        # Open file in executor (blocking operation)
        await self._loop.run_in_executor(
            None,
            self._open_file_sync,
        )

        self._started = True

    def _open_file_sync(self) -> None:
        """Open WAL file for append+read (sync, runs in executor)."""
        import pathlib

        path = pathlib.Path(self._logfile_path)
        path.parent.mkdir(parents=True, exist_ok=True)

        self._file = open(self._logfile_path, 'ab+')
        self._sequence_generator = SnowflakeGenerator(self._instance_id)

    async def write(self, log: Log) -> int | None:
        """
        Write a log entry to the WAL.

        This method buffers the write and returns a Future that resolves
        when the entry is durably written (after batch flush + fsync).

        High-concurrency safe: thousands of concurrent calls are coalesced
        into batched writes automatically.

        Args:
            log: Log entry to write

        Returns:
            LSN (Log Sequence Number) assigned to this entry

        Raises:
            RuntimeError: If writer not started or closed
            asyncio.TimeoutError: If backpressure timeout exceeded
        """
        if not self._started:
            raise RuntimeError("WALWriter not started - call start() first")
        if self._closed:
            raise RuntimeError("WALWriter is closed")

        # Wait if buffer is full (backpressure)
        await self._backpressure_event.wait()

        # Create future for this write's completion
        future: asyncio.Future[int | None] = self._loop.create_future()

        async with self._buffer_lock:
            # Add to buffer
            self._buffer.append((log, future))

            # Apply backpressure if buffer is full
            if len(self._buffer) >= self._buffer_max_size:
                self._backpressure_event.clear()

            # Start flush timer on first entry in batch
            if len(self._buffer) == 1:
                self._flush_timer = self._loop.call_later(
                    self._batch_timeout_ms / 1000.0,
                    self._trigger_flush,
                )

            # Immediate flush if batch is full
            if len(self._buffer) >= self._batch_max_size:
                await self._flush_buffer()

        # Wait for this entry to be durably written
        return await future

    def _trigger_flush(self) -> None:
        """
        Timer callback to trigger batch flush.

        Called by asyncio timer after batch_timeout_ms.
        Since this is a sync callback, we schedule the async flush as a task.
        """
        if self._flush_task is None or self._flush_task.done():
            self._flush_task = asyncio.create_task(self._flush_buffer_locked())

    async def _flush_buffer_locked(self) -> None:
        """Acquire lock and flush buffer."""
        async with self._buffer_lock:
            await self._flush_buffer()

    async def _flush_buffer(self) -> None:
        """
        Flush buffered writes to disk.

        MUST be called with _buffer_lock held.
        """
        if not self._buffer:
            return

        # Cancel pending timer
        if self._flush_timer:
            self._flush_timer.cancel()
            self._flush_timer = None

        # Take buffer contents
        batch = self._buffer.copy()
        self._buffer.clear()

        # Release backpressure
        self._backpressure_event.set()

        # Write batch in executor (single call for entire batch)
        try:
            lsns = await self._loop.run_in_executor(
                None,
                self._write_batch_sync,
                batch,
            )

            # Signal success to all waiting futures
            for (_, future), lsn in zip(batch, lsns):
                if not future.done():
                    future.set_result(lsn)

        except Exception as err:
            # Signal failure to all waiting futures
            for _, future in batch:
                if not future.done():
                    future.set_exception(err)

    def _write_batch_sync(
        self,
        batch: List[Tuple[Log, asyncio.Future[int | None]]],
    ) -> List[int | None]:
        """
        Write entire batch synchronously (runs in executor thread).

        This is the critical optimization: one executor call for N writes,
        one flush, one fsync.

        Args:
            batch: List of (log, future) tuples

        Returns:
            List of LSNs corresponding to each entry
        """
        lsns: List[int | None] = []
        total_bytes = 0

        for log, _ in batch:
            # Generate LSN
            lsn = self._sequence_generator.generate()
            if lsn is not None:
                log.lsn = lsn
            lsns.append(lsn)

            # Encode to binary format
            data = self._encode_binary(log, lsn)

            # Write (fast - just memcpy to OS buffer)
            self._file.write(data)
            total_bytes += len(data)

        # Single flush for entire batch
        self._file.flush()

        # Single fsync for entire batch (the expensive operation)
        if self._durability in (DurabilityMode.FSYNC, DurabilityMode.FSYNC_BATCH):
            os.fsync(self._file.fileno())

        # Update metrics
        self._writes_total += len(batch)
        self._batches_total += 1
        self._bytes_written += total_bytes

        return lsns

    def _encode_binary(self, log: Log, lsn: int | None) -> bytes:
        """
        Encode log entry in binary format with CRC32.

        Format:
        ┌──────────┬──────────┬──────────┬─────────────────────┐
        │ CRC32    │ Length   │ LSN      │ Payload (JSON)      │
        │ (4 bytes)│ (4 bytes)│ (8 bytes)│ (variable)          │
        └──────────┴──────────┴──────────┴─────────────────────┘
        """
        payload = msgspec.json.encode(log)
        lsn_value = lsn if lsn is not None else 0

        # Header: length (4) + LSN (8)
        header = struct.pack("<IQ", len(payload), lsn_value)

        # CRC32 over header + payload
        crc = zlib.crc32(header + payload) & 0xFFFFFFFF

        # Final: CRC32 (4) + header (12) + payload
        return struct.pack("<I", crc) + header + payload

    async def flush(self) -> None:
        """
        Force flush any buffered writes.

        Useful for ensuring durability before shutdown or at
        transaction boundaries.
        """
        async with self._buffer_lock:
            await self._flush_buffer()

    async def close(self) -> None:
        """
        Close the WAL writer.

        Flushes any pending writes and closes the file.
        """
        if self._closed:
            return

        self._closed = True

        # Flush remaining buffer
        await self.flush()

        # Cancel any pending timer
        if self._flush_timer:
            self._flush_timer.cancel()
            self._flush_timer = None

        # Close file in executor
        if self._file:
            await self._loop.run_in_executor(
                None,
                self._file.close,
            )

    @property
    def metrics(self) -> Dict[str, int]:
        """Get writer metrics."""
        return {
            'writes_total': self._writes_total,
            'batches_total': self._batches_total,
            'bytes_written': self._bytes_written,
            'avg_batch_size': (
                self._writes_total // self._batches_total
                if self._batches_total > 0 else 0
            ),
        }
```

#### 12.5.3 Implementation: WALReader Class

```python
"""
hyperscale/logging/streams/wal_reader.py

WAL reader for recovery and replication.
"""
import asyncio
import functools
import struct
import zlib
from typing import AsyncIterator, Tuple

import msgspec

from hyperscale.logging.models import Log


class WALReader:
    """
    WAL reader for recovery and streaming replication.

    Uses run_in_executor() for file operations (most robust approach).
    Supports:
    - Full file scan for recovery
    - Reading from specific offset
    - CRC verification

    Thread Safety:
    - All public methods are async
    - File operations isolated in executor
    - Read lock prevents concurrent reads on same file
    """

    HEADER_SIZE = 16  # CRC32(4) + length(4) + LSN(8)

    def __init__(self, logfile_path: str):
        self._logfile_path = logfile_path
        self._loop: asyncio.AbstractEventLoop | None = None
        self._read_lock = asyncio.Lock()

    async def read_entries(
        self,
        from_offset: int = 0,
        verify_crc: bool = True,
    ) -> AsyncIterator[Tuple[int, Log, int | None]]:
        """
        Read entries from WAL file.

        Uses run_in_executor() for all file operations - the most
        robust approach for file I/O in asyncio.

        Args:
            from_offset: Starting byte offset (0 = beginning)
            verify_crc: Whether to verify CRC32 checksums

        Yields:
            (offset, log, lsn) for each entry

        Raises:
            ValueError: On corrupted entry (CRC mismatch, truncation)
        """
        if self._loop is None:
            self._loop = asyncio.get_running_loop()

        async with self._read_lock:
            # Open file for reading
            read_file = await self._loop.run_in_executor(
                None,
                functools.partial(open, self._logfile_path, 'rb'),
            )

            try:
                # Seek to starting position
                await self._loop.run_in_executor(
                    None,
                    read_file.seek,
                    from_offset,
                )

                offset = from_offset
                entries_read = 0

                while True:
                    # Read header
                    header = await self._loop.run_in_executor(
                        None,
                        read_file.read,
                        self.HEADER_SIZE,
                    )

                    if len(header) == 0:
                        break  # EOF

                    if len(header) < self.HEADER_SIZE:
                        raise ValueError(
                            f"Truncated header at offset {offset}: "
                            f"got {len(header)} bytes, expected {self.HEADER_SIZE}"
                        )

                    # Parse header
                    crc_stored = struct.unpack("<I", header[:4])[0]
                    length, lsn = struct.unpack("<IQ", header[4:16])

                    # Read payload
                    payload = await self._loop.run_in_executor(
                        None,
                        read_file.read,
                        length,
                    )

                    if len(payload) < length:
                        raise ValueError(
                            f"Truncated payload at offset {offset}: "
                            f"got {len(payload)} bytes, expected {length}"
                        )

                    # Verify CRC
                    if verify_crc:
                        crc_computed = zlib.crc32(header[4:] + payload) & 0xFFFFFFFF
                        if crc_stored != crc_computed:
                            raise ValueError(
                                f"CRC mismatch at offset {offset}: "
                                f"stored={crc_stored:#x}, computed={crc_computed:#x}"
                            )

                    # Decode log entry
                    log = msgspec.json.decode(payload, type=Log)

                    yield offset, log, lsn

                    offset += self.HEADER_SIZE + length
                    entries_read += 1

                    # Yield to event loop periodically
                    if entries_read % 100 == 0:
                        await asyncio.sleep(0)

            finally:
                await self._loop.run_in_executor(
                    None,
                    read_file.close,
                )

    async def get_last_lsn(self) -> int | None:
        """
        Get the last LSN in the WAL file.

        Scans entire file - for large files, consider maintaining
        an index or reading from end.
        """
        last_lsn: int | None = None

        async for _, _, lsn in self.read_entries():
            if lsn is not None:
                last_lsn = lsn

        return last_lsn

    async def count_entries(self) -> int:
        """Count total entries in WAL file."""
        count = 0
        async for _ in self.read_entries(verify_crc=False):
            count += 1
        return count
```

#### 12.5.4 Integration with LoggerStream

```python
"""
Integration of WALWriter with existing LoggerStream.

LoggerStream gains a new mode for WAL operations that uses
write coalescing instead of per-write executor dispatch.
"""

class LoggerStream:
    def __init__(
        self,
        # ... existing params ...

        # NEW: WAL mode parameters
        durability: DurabilityMode = DurabilityMode.FLUSH,
        format: Literal['json', 'binary'] = 'json',
        enable_lsn: bool = False,
        instance_id: int = 0,
        enable_coalescing: bool = False,  # NEW
        batch_timeout_ms: float = 5.0,     # NEW
        batch_max_size: int = 100,         # NEW
    ):
        # ... existing init ...

        # WAL writer for coalesced writes
        self._wal_writers: Dict[str, WALWriter] = {}
        self._enable_coalescing = enable_coalescing
        self._batch_timeout_ms = batch_timeout_ms
        self._batch_max_size = batch_max_size

    async def _get_wal_writer(self, logfile_path: str) -> WALWriter:
        """Get or create WAL writer for path."""
        if logfile_path not in self._wal_writers:
            writer = WALWriter(
                logfile_path=logfile_path,
                instance_id=self._instance_id,
                batch_timeout_ms=self._batch_timeout_ms,
                batch_max_size=self._batch_max_size,
                durability=self._durability,
            )
            await writer.start()
            self._wal_writers[logfile_path] = writer

        return self._wal_writers[logfile_path]

    async def _log_to_file(
        self,
        entry_or_log: T | Log[T],
        filename: str | None = None,
        directory: str | None = None,
        # ... other params ...
    ):
        # ... existing path resolution ...

        if self._enable_coalescing and self._durability != DurabilityMode.FLUSH:
            # Use coalesced WAL writer for high-concurrency durability
            writer = await self._get_wal_writer(logfile_path)
            lsn = await writer.write(log)
            return lsn
        else:
            # Use existing per-write executor pattern
            # (unchanged - backwards compatible)
            file_lock = self._file_locks[logfile_path]
            await file_lock.acquire()

            try:
                lsn = await self._loop.run_in_executor(
                    None,
                    self._write_to_file,
                    log,
                    logfile_path,
                )
                return lsn
            finally:
                if file_lock.locked():
                    file_lock.release()
```

### 12.6 Performance Comparison

```
═══════════════════════════════════════════════════════════════════════════
                    BENCHMARK: 100,000 WRITES TO WAL
═══════════════════════════════════════════════════════════════════════════

Test Setup:
- 100,000 concurrent write requests
- 64-byte log entries
- NVMe SSD storage
- DurabilityMode.FSYNC_BATCH

┌─────────────────────────────────────────────────────────────────────────┐
│                                                                          │
│  APPROACH 1: Per-write executor (current)                               │
│  ─────────────────────────────────────────                              │
│  Executor calls:     100,000                                            │
│  fsync calls:        1,000 (batched by time, ~100 per batch)            │
│  Total time:         ~45 seconds                                        │
│  Throughput:         ~2,200 writes/sec                                  │
│  P99 latency:        ~200ms (queue backup)                              │
│                                                                          │
│  APPROACH 2: Write coalescing (recommended)                             │
│  ──────────────────────────────────────────                             │
│  Executor calls:     1,000 (100 writes per batch)                       │
│  fsync calls:        1,000                                              │
│  Total time:         ~5 seconds                                         │
│  Throughput:         ~20,000 writes/sec                                 │
│  P99 latency:        ~10ms (bounded by batch timeout)                   │
│                                                                          │
│  SPEEDUP: ~9x throughput, ~20x latency improvement                      │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘

LATENCY DISTRIBUTION:

Per-write executor:
├── P50:  ~20ms
├── P90:  ~100ms
├── P99:  ~200ms
└── P999: ~500ms  (thread pool saturation)

Write coalescing:
├── P50:  ~3ms   (half of batch timeout)
├── P90:  ~5ms   (at batch timeout)
├── P99:  ~10ms  (batch timeout + fsync)
└── P999: ~15ms  (consistent, bounded)
```

### 12.7 Backpressure Handling

```
═══════════════════════════════════════════════════════════════════════════
                    BACKPRESSURE MECHANISM
═══════════════════════════════════════════════════════════════════════════

PROBLEM: What happens when writes come faster than disk can handle?

WITHOUT BACKPRESSURE (current):
┌─────────────────────────────────────────────────────────────────────────┐
│                                                                          │
│   Writers → Unbounded queue → Eventually OOM                            │
│                                                                          │
│   Memory grows linearly with write rate / disk speed mismatch           │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘

WITH BACKPRESSURE (WALWriter):
┌─────────────────────────────────────────────────────────────────────────┐
│                                                                          │
│   buffer_max_size = 10,000                                              │
│                                                                          │
│   When buffer reaches limit:                                            │
│   1. backpressure_event.clear()                                         │
│   2. New write() calls block on: await backpressure_event.wait()        │
│   3. When buffer drains: backpressure_event.set()                       │
│   4. Blocked writers resume                                             │
│                                                                          │
│   Result:                                                               │
│   ├── Memory bounded to buffer_max_size * entry_size                    │
│   ├── Writers naturally slow down to match disk speed                   │
│   └── No OOM, graceful degradation                                      │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘

CONFIGURATION:
┌─────────────────────────────────────────────────────────────────────────┐
│                                                                          │
│  Parameter         │ Default │ Effect                                   │
│  ──────────────────┼─────────┼─────────────────────────────────────────│
│  batch_timeout_ms  │ 5.0     │ Max latency (higher = more batching)    │
│  batch_max_size    │ 100     │ Entries per batch (higher = throughput) │
│  buffer_max_size   │ 10,000  │ Backpressure threshold                  │
│                                                                          │
│  Memory bound = buffer_max_size × avg_entry_size                        │
│  Example: 10,000 × 256 bytes = 2.5 MB max buffer                        │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘
```

### 12.8 Usage Examples

```python
# ═══════════════════════════════════════════════════════════════════════
# EXAMPLE 1: Direct WALWriter usage
# ═══════════════════════════════════════════════════════════════════════

from hyperscale.logging.streams.wal_writer import WALWriter
from hyperscale.logging.models import Log, Entry, LogLevel

async def high_concurrency_wal_example():
    # Create writer with coalescing
    writer = WALWriter(
        logfile_path="/var/log/hyperscale/node.wal",
        instance_id=42,  # Node ID for LSN generation
        batch_timeout_ms=5.0,
        batch_max_size=100,
    )
    await writer.start()

    # Simulate 10,000 concurrent writes
    async def write_entry(i: int):
        entry = Entry(message=f"Event {i}", level=LogLevel.INFO)
        log = Log(entry=entry)
        lsn = await writer.write(log)
        return lsn

    # All 10,000 writes are coalesced into ~100 batches
    lsns = await asyncio.gather(*[
        write_entry(i) for i in range(10_000)
    ])

    print(f"Wrote {len(lsns)} entries")
    print(f"Metrics: {writer.metrics}")
    # Output: {'writes_total': 10000, 'batches_total': 100, ...}

    await writer.close()


# ═══════════════════════════════════════════════════════════════════════
# EXAMPLE 2: LoggerStream with coalescing enabled
# ═══════════════════════════════════════════════════════════════════════

from hyperscale.logging import Logger
from hyperscale.logging.config import DurabilityMode

async def logger_with_coalescing_example():
    logger = Logger()

    # Configure for WAL mode with coalescing
    logger.configure(
        name="gate_wal",
        path="hyperscale.gate.wal",
        durability=DurabilityMode.FSYNC_BATCH,
        format='binary',
        enable_lsn=True,
        enable_coalescing=True,  # Enable write coalescing
        batch_timeout_ms=5.0,
        batch_max_size=100,
        instance_id=node_id,
    )

    async with logger.context(name="gate_wal") as ctx:
        # High-concurrency writes automatically coalesced
        await asyncio.gather(*[
            ctx.log(Entry(message=f"Job {i} created"))
            for i in range(10_000)
        ])


# ═══════════════════════════════════════════════════════════════════════
# EXAMPLE 3: WAL recovery
# ═══════════════════════════════════════════════════════════════════════

from hyperscale.logging.streams.wal_reader import WALReader

async def recovery_example():
    reader = WALReader("/var/log/hyperscale/node.wal")

    # Read all entries for recovery
    recovered_entries = []
    async for offset, log, lsn in reader.read_entries():
        recovered_entries.append((lsn, log))

        # Process recovered entry
        if hasattr(log.entry, 'job_id'):
            await restore_job_state(log.entry)

    print(f"Recovered {len(recovered_entries)} entries")

    # Get last LSN for resuming writes
    last_lsn = await reader.get_last_lsn()
    print(f"Last LSN: {last_lsn}")
```

### 12.9 Summary

Write coalescing is the recommended approach for high-concurrency WAL operations because it:

1. **Reduces executor overhead by ~100x**: One executor call per batch instead of per write
2. **Reduces fsync overhead by ~100x**: One fsync per batch instead of per write
3. **Provides bounded latency**: Configurable batch timeout ensures predictable latency
4. **Implements backpressure**: Prevents OOM under sustained high load
5. **Maintains compatibility**: Can be enabled alongside existing per-write pattern
6. **Is portable**: Works on all platforms (unlike io_uring)

**When to use each approach:**

| Use Case | Approach | Why |
|----------|----------|-----|
| Low-volume logging | Per-write executor | Simpler, lower latency for single writes |
| High-volume stats | Per-write executor | Eventual consistency OK, no fsync |
| WAL (durability needed) | Write coalescing | High throughput + durability |
| Extreme throughput (Linux) | io_uring | Maximum performance |

---

## Part 13: Portable High-Concurrency I/O Design

This section provides a definitive answer to the question: **What is the most correct and robust approach for high-concurrency, low-latency logging that is asyncio-compatible AND portable?**

### 13.1 Platform I/O Mechanisms Overview

```
═══════════════════════════════════════════════════════════════════════════
              PLATFORM-SPECIFIC ASYNC I/O MECHANISMS
═══════════════════════════════════════════════════════════════════════════

┌─────────────────────────────────────────────────────────────────────────┐
│                          LINUX                                          │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│  KERNEL ASYNC I/O OPTIONS:                                              │
│                                                                          │
│  1. io_uring (Linux 5.1+, 2019)                                         │
│     ├── Best performance: ~1M+ IOPS                                     │
│     ├── True kernel-level async for regular files                       │
│     ├── Submission queue + completion queue pattern                     │
│     ├── Single syscall for batch operations                             │
│     └── Requires: liburing or python wrapper (e.g., io-uring)           │
│                                                                          │
│  2. libaio (AIO_NATIVE, older)                                          │
│     ├── Moderate performance: ~100K IOPS                                │
│     ├── Only works with O_DIRECT (bypasses page cache)                  │
│     ├── Complex alignment requirements                                  │
│     └── Mostly deprecated in favor of io_uring                          │
│                                                                          │
│  3. POSIX AIO (aio_read/aio_write)                                      │
│     ├── Actually uses threads internally (not true async)               │
│     ├── Same performance as thread pool                                 │
│     └── No real benefit over run_in_executor()                          │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────────┐
│                          macOS (Darwin)                                  │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│  KERNEL ASYNC I/O OPTIONS:                                              │
│                                                                          │
│  1. kqueue (BSD-style event notification)                               │
│     ├── Excellent for sockets, pipes, fifos                             │
│     ├── EVFILT_READ/EVFILT_WRITE for file descriptors                   │
│     ├── BUT: Regular files always report "ready"                        │
│     └── NO true async for disk I/O                                      │
│                                                                          │
│  2. Grand Central Dispatch (GCD)                                        │
│     ├── dispatch_io_read/dispatch_io_write                              │
│     ├── Apple's recommended async I/O                                   │
│     ├── Uses thread pool internally                                     │
│     └── Requires: pyobjc or ctypes FFI                                  │
│                                                                          │
│  3. POSIX AIO                                                           │
│     ├── Same as Linux: uses threads internally                          │
│     └── No benefit over run_in_executor()                               │
│                                                                          │
│  REALITY: macOS has NO true async disk I/O. All solutions              │
│           ultimately use threads for regular file operations.           │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────────┐
│                          Windows                                         │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│  KERNEL ASYNC I/O OPTIONS:                                              │
│                                                                          │
│  1. IOCP (I/O Completion Ports)                                         │
│     ├── True kernel async for files (with FILE_FLAG_OVERLAPPED)         │
│     ├── Excellent performance: ~500K+ IOPS                              │
│     ├── Used by asyncio's ProactorEventLoop                             │
│     └── Requires: win32file or direct ctypes                            │
│                                                                          │
│  2. ReadFileEx/WriteFileEx (Overlapped I/O)                             │
│     ├── Lower-level than IOCP                                           │
│     ├── APC-based completion notification                               │
│     └── Less suitable for Python integration                            │
│                                                                          │
│  asyncio ON WINDOWS:                                                    │
│  ├── ProactorEventLoop: Uses IOCP, supports pipes natively              │
│  ├── SelectorEventLoop: select()-based, limited                         │
│  └── run_in_executor() still recommended for file I/O                   │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘
```

### 13.2 Why Platform-Specific Approaches Are Problematic

```
═══════════════════════════════════════════════════════════════════════════
              THE PORTABILITY PROBLEM
═══════════════════════════════════════════════════════════════════════════

SCENARIO: You want maximum performance AND cross-platform support

OPTION A: Platform-Specific Implementations
───────────────────────────────────────────

┌─────────────────────────────────────────────────────────────────────────┐
│                                                                          │
│  if sys.platform == 'linux':                                            │
│      from .io_uring_writer import IOURingWALWriter as WALWriter         │
│  elif sys.platform == 'darwin':                                         │
│      from .gcd_writer import GCDWALWriter as WALWriter                  │
│  elif sys.platform == 'win32':                                          │
│      from .iocp_writer import IOCPWALWriter as WALWriter                │
│  else:                                                                  │
│      from .thread_writer import ThreadWALWriter as WALWriter            │
│                                                                          │
│  PROBLEMS:                                                              │
│  ├── 4x maintenance burden (4 implementations to test/debug)            │
│  ├── Different semantics/edge cases per platform                        │
│  ├── External dependencies (liburing, pyobjc, pywin32)                  │
│  ├── Version-specific issues (io_uring features vary by kernel)         │
│  ├── Debugging nightmare (bug on Linux != bug on macOS)                 │
│  └── CI/CD complexity (need all platforms in test matrix)               │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘

OPTION B: Single Portable Implementation (RECOMMENDED)
──────────────────────────────────────────────────────

┌─────────────────────────────────────────────────────────────────────────┐
│                                                                          │
│  # Works everywhere, identical behavior                                 │
│  from .wal_writer import WALWriter                                      │
│                                                                          │
│  BENEFITS:                                                              │
│  ├── Single implementation: one codebase to maintain                    │
│  ├── Standard library only: no external dependencies                    │
│  ├── Identical semantics: same behavior on all platforms                │
│  ├── Easy debugging: reproduce issues anywhere                          │
│  ├── Simple CI/CD: test on one platform, works on all                   │
│  └── Still fast enough: 100K+ writes/sec with coalescing                │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘

THE MATH:
─────────

io_uring performance:     ~1,000,000 writes/sec
Write coalescing:         ~100,000 writes/sec
Ratio:                    10x

Maintenance cost ratio:   4x (implementations) × 3x (complexity) = 12x

UNLESS you need >100K writes/sec, write coalescing is the better choice.
```

### 13.3 The Definitive Portable Solution

**Write Coalescing with `run_in_executor()`** is the correct answer because:

```
═══════════════════════════════════════════════════════════════════════════
              WHY WRITE COALESCING IS THE ANSWER
═══════════════════════════════════════════════════════════════════════════

1. IT'S THE OFFICIAL RECOMMENDATION
───────────────────────────────────

Python documentation states:
"For disk I/O, run_in_executor() is recommended because regular files
don't work with epoll/kqueue/select in a useful way."

asyncio explicitly does NOT provide async file I/O because:
- Regular files always appear "ready" to select/poll/epoll/kqueue
- True async file I/O requires platform-specific mechanisms
- Thread pools provide correct semantics portably


2. WRITE COALESCING ELIMINATES THE MAIN OVERHEAD
────────────────────────────────────────────────

The problem with run_in_executor() is per-call overhead:

  Per-call overhead:     ~5-25μs
  fsync overhead:        ~1-10ms

  10,000 writes naive:   10,000 × (20μs + 5ms) = ~50 seconds
  10,000 writes batched: 100 × (20μs + 5ms) = ~0.5 seconds

Batching makes run_in_executor() viable for high-concurrency:

┌────────────────────────────────────────────────────────────────────┐
│                                                                     │
│  OVERHEAD COMPARISON (10,000 writes)                               │
│                                                                     │
│  Per-write:    10,000 executor calls + 10,000 fsyncs              │
│                = 200ms overhead + 50s fsync = ~50 seconds          │
│                                                                     │
│  Coalesced:    100 executor calls + 100 fsyncs                    │
│                = 2ms overhead + 500ms fsync = ~0.5 seconds         │
│                                                                     │
│  SPEEDUP:      100x                                                │
│                                                                     │
└────────────────────────────────────────────────────────────────────┘


3. IT MAINTAINS FULL FILE SEMANTICS
──────────────────────────────────

Unlike mmap or specialized I/O:
- Full seek() support for reading/recovery
- Standard open()/read()/write()/close()
- Works with any filesystem
- No alignment requirements
- No page size constraints


4. IT WORKS WITH ASYNCIO'S DESIGN
─────────────────────────────────

asyncio's concurrency model:
- Event loop runs on single thread
- Blocking operations go to thread pool
- Futures bridge async/sync boundary

Write coalescing works WITH this model:
- Async layer does non-blocking buffering
- Single executor call per batch
- Futures notify callers of completion
- No fight against the framework


5. IT'S BATTLE-TESTED
────────────────────

Similar patterns used in:
- Python logging.handlers.QueueHandler
- SQLite WAL (batched writes)
- RocksDB WriteBatch
- Most production logging systems
```

### 13.4 Architecture: The Complete Portable Solution

```
═══════════════════════════════════════════════════════════════════════════
              PORTABLE HIGH-CONCURRENCY LOGGING ARCHITECTURE
═══════════════════════════════════════════════════════════════════════════

┌─────────────────────────────────────────────────────────────────────────┐
│                                                                          │
│                         APPLICATION LAYER                               │
│  ┌────────────────────────────────────────────────────────────────────┐ │
│  │                                                                     │ │
│  │   async def do_work():                                             │ │
│  │       await logger.log(Entry(message="Job started", job_id=123))   │ │
│  │       # ... work ...                                                │ │
│  │       await logger.log(Entry(message="Job finished", job_id=123))  │ │
│  │                                                                     │ │
│  │   # 1000s of concurrent do_work() calls                            │ │
│  │                                                                     │ │
│  └────────────────────────────────────────────────────────────────────┘ │
│                                    │                                     │
│                                    ▼                                     │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│                         LOGGER INTERFACE                                │
│  ┌────────────────────────────────────────────────────────────────────┐ │
│  │                                                                     │ │
│  │   LoggerContext / LoggerStream                                     │ │
│  │   ├── Provides familiar logger.log() API                           │ │
│  │   ├── Routes to appropriate output (console, file, WAL)            │ │
│  │   ├── Model serialization via msgspec                              │ │
│  │   └── Template formatting                                          │ │
│  │                                                                     │ │
│  └────────────────────────────────────────────────────────────────────┘ │
│                                    │                                     │
│                                    │ enable_coalescing=True             │
│                                    ▼                                     │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│                         WRITE COALESCING LAYER                          │
│  ┌────────────────────────────────────────────────────────────────────┐ │
│  │                                                                     │ │
│  │   WALWriter (async)                                                │ │
│  │   ┌──────────────────────────────────────────────────────────────┐ │ │
│  │   │                                                               │ │ │
│  │   │   Buffer: List[(Log, Future)]                                │ │ │
│  │   │   ┌─────┬─────┬─────┬─────┬─────┬─────┬─────┬─────┐         │ │ │
│  │   │   │ L1  │ L2  │ L3  │ L4  │ L5  │ ... │ L99 │L100 │         │ │ │
│  │   │   │ F1  │ F2  │ F3  │ F4  │ F5  │ ... │ F99 │F100 │         │ │ │
│  │   │   └─────┴─────┴─────┴─────┴─────┴─────┴─────┴─────┘         │ │ │
│  │   │                                                               │ │ │
│  │   │   Flush triggers:                                            │ │ │
│  │   │   ├── Buffer size >= batch_max_size (100)                    │ │ │
│  │   │   └── Timer expired (batch_timeout_ms = 5ms)                 │ │ │
│  │   │                                                               │ │ │
│  │   │   Synchronization:                                           │ │ │
│  │   │   ├── asyncio.Lock() for buffer access                       │ │ │
│  │   │   └── asyncio.Event() for backpressure                       │ │ │
│  │   │                                                               │ │ │
│  │   └──────────────────────────────────────────────────────────────┘ │ │
│  │                                                                     │ │
│  └────────────────────────────────────────────────────────────────────┘ │
│                                    │                                     │
│                                    │ Single run_in_executor() call      │
│                                    ▼                                     │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│                         SYNC I/O LAYER (Thread Pool)                    │
│  ┌────────────────────────────────────────────────────────────────────┐ │
│  │                                                                     │ │
│  │   _write_batch_sync(batch) -> List[LSN]                           │ │
│  │   ┌──────────────────────────────────────────────────────────────┐ │ │
│  │   │                                                               │ │ │
│  │   │   for log in batch:                                          │ │ │
│  │   │       lsn = snowflake.generate()                             │ │ │
│  │   │       data = encode_binary(log, lsn)   # CRC + header + JSON │ │ │
│  │   │       file.write(data)                  # Fast (OS buffer)   │ │ │
│  │   │                                                               │ │ │
│  │   │   file.flush()                          # Once per batch     │ │ │
│  │   │   os.fsync(file.fileno())               # Once per batch     │ │ │
│  │   │                                                               │ │ │
│  │   │   return lsns                                                │ │ │
│  │   │                                                               │ │ │
│  │   └──────────────────────────────────────────────────────────────┘ │ │
│  │                                                                     │ │
│  │   Thread Pool: Default ThreadPoolExecutor                          │ │
│  │   ├── Safe: Each batch runs in isolation                          │ │ │
│  │   ├── Portable: Standard library, all platforms                   │ │ │
│  │   └── Efficient: One call per batch, not per write                │ │ │
│  │                                                                     │ │
│  └────────────────────────────────────────────────────────────────────┘ │
│                                    │                                     │
│                                    ▼                                     │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│                         OPERATING SYSTEM / FILESYSTEM                   │
│  ┌────────────────────────────────────────────────────────────────────┐ │
│  │                                                                     │ │
│  │   write() → OS page cache buffer                                  │ │
│  │   flush() → Force to kernel buffer                                │ │
│  │   fsync() → Force to persistent storage                           │ │
│  │                                                                     │ │
│  │   ┌─────────────────────────────────────────────────────────────┐ │ │
│  │   │                                                              │ │ │
│  │   │   Linux:   Uses write()/fdatasync() - standard POSIX        │ │ │
│  │   │   macOS:   Uses write()/fcntl(F_FULLFSYNC) - stronger       │ │ │
│  │   │   Windows: Uses WriteFile()/FlushFileBuffers()              │ │ │
│  │   │                                                              │ │ │
│  │   │   All abstracted by Python's os.fsync()                     │ │ │
│  │   │                                                              │ │ │
│  │   └─────────────────────────────────────────────────────────────┘ │ │
│  │                                                                     │ │
│  └────────────────────────────────────────────────────────────────────┘ │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘
```

### 13.5 Key Implementation Patterns for Portability

```python
"""
Key patterns that ensure portability in the WALWriter implementation.

All patterns use ONLY Python standard library.
"""

import asyncio
import os
import struct
import zlib
from concurrent.futures import ThreadPoolExecutor
from typing import List, Tuple


# ═══════════════════════════════════════════════════════════════════════════
# PATTERN 1: asyncio.Lock() for async-safe synchronization
# ═══════════════════════════════════════════════════════════════════════════

class PortableAsyncBuffer:
    """
    Correct: Uses asyncio.Lock() which is event-loop safe.

    WRONG: threading.Lock() blocks the event loop!
    """
    def __init__(self):
        self._buffer: List[bytes] = []
        self._lock = asyncio.Lock()  # ← asyncio primitive, not threading

    async def append(self, data: bytes) -> None:
        async with self._lock:  # ← Non-blocking for other coroutines
            self._buffer.append(data)

    async def drain(self) -> List[bytes]:
        async with self._lock:
            result = self._buffer.copy()
            self._buffer.clear()
            return result


# ═══════════════════════════════════════════════════════════════════════════
# PATTERN 2: asyncio.Event() for backpressure signaling
# ═══════════════════════════════════════════════════════════════════════════

class PortableBackpressure:
    """
    Correct: Uses asyncio.Event() for cooperative blocking.

    When buffer is full, writers await the event.
    When buffer drains, event is set and writers proceed.
    """
    def __init__(self, max_size: int = 10000):
        self._max_size = max_size
        self._current_size = 0
        self._can_write = asyncio.Event()
        self._can_write.set()  # Initially writable

    async def acquire(self, size: int) -> None:
        """Wait until we can write."""
        await self._can_write.wait()
        self._current_size += size
        if self._current_size >= self._max_size:
            self._can_write.clear()  # Block new writers

    def release(self, size: int) -> None:
        """Release buffer space."""
        self._current_size -= size
        if self._current_size < self._max_size:
            self._can_write.set()  # Unblock writers


# ═══════════════════════════════════════════════════════════════════════════
# PATTERN 3: loop.call_later() for non-blocking timers
# ═══════════════════════════════════════════════════════════════════════════

class PortableBatchTimer:
    """
    Correct: Uses loop.call_later() for async-compatible timers.

    WRONG: time.sleep() or threading.Timer blocks!
    """
    def __init__(self, timeout_ms: float):
        self._timeout_ms = timeout_ms
        self._loop: asyncio.AbstractEventLoop | None = None
        self._timer: asyncio.TimerHandle | None = None
        self._flush_callback: callable | None = None

    def start(self, flush_callback: callable) -> None:
        """Start batch timer."""
        if self._loop is None:
            self._loop = asyncio.get_running_loop()

        self._flush_callback = flush_callback
        self._timer = self._loop.call_later(
            self._timeout_ms / 1000.0,
            self._on_timeout,
        )

    def cancel(self) -> None:
        """Cancel pending timer."""
        if self._timer:
            self._timer.cancel()
            self._timer = None

    def _on_timeout(self) -> None:
        """Timer callback - schedule async flush."""
        if self._flush_callback:
            # call_later is sync, so we create a task for async work
            asyncio.create_task(self._flush_callback())


# ═══════════════════════════════════════════════════════════════════════════
# PATTERN 4: run_in_executor() for blocking I/O
# ═══════════════════════════════════════════════════════════════════════════

class PortableFileWriter:
    """
    Correct: Uses run_in_executor() for all blocking file operations.

    This is THE portable pattern for file I/O in asyncio.
    """
    def __init__(self, path: str):
        self._path = path
        self._loop: asyncio.AbstractEventLoop | None = None
        self._file = None

    async def open(self) -> None:
        """Open file in executor (blocking operation)."""
        self._loop = asyncio.get_running_loop()
        self._file = await self._loop.run_in_executor(
            None,  # Default executor
            lambda: open(self._path, 'ab'),  # Blocking open
        )

    async def write_batch(self, entries: List[bytes]) -> int:
        """
        Write batch in executor (single call for multiple entries).

        Key optimization: ONE executor call for N writes.
        """
        def _sync_write_batch() -> int:
            total = 0
            for entry in entries:
                self._file.write(entry)
                total += len(entry)
            self._file.flush()
            os.fsync(self._file.fileno())
            return total

        return await self._loop.run_in_executor(None, _sync_write_batch)

    async def close(self) -> None:
        """Close file in executor."""
        if self._file:
            await self._loop.run_in_executor(None, self._file.close)


# ═══════════════════════════════════════════════════════════════════════════
# PATTERN 5: asyncio.Future for per-write completion notification
# ═══════════════════════════════════════════════════════════════════════════

class PortableWriteNotification:
    """
    Correct: Uses asyncio.Future to bridge batch write and individual callers.

    Each write() call gets a Future that resolves when the batch completes.
    """
    def __init__(self):
        self._loop: asyncio.AbstractEventLoop | None = None
        self._pending: List[Tuple[bytes, asyncio.Future]] = []

    async def write(self, data: bytes) -> int:
        """
        Queue write and return Future.

        Caller awaits the Future, which resolves after batch flush.
        """
        if self._loop is None:
            self._loop = asyncio.get_running_loop()

        future: asyncio.Future[int] = self._loop.create_future()
        self._pending.append((data, future))

        # Trigger batch flush if needed...

        return await future  # Caller blocks here until batch completes

    def complete_batch(self, results: List[int]) -> None:
        """
        Called after batch write completes.
        Resolves all pending futures.
        """
        for (_, future), result in zip(self._pending, results):
            if not future.done():
                future.set_result(result)
        self._pending.clear()

    def fail_batch(self, error: Exception) -> None:
        """
        Called if batch write fails.
        Rejects all pending futures.
        """
        for _, future in self._pending:
            if not future.done():
                future.set_exception(error)
        self._pending.clear()


# ═══════════════════════════════════════════════════════════════════════════
# PATTERN 6: Platform-safe fsync
# ═══════════════════════════════════════════════════════════════════════════

def portable_fsync(file) -> None:
    """
    Portable fsync that works correctly on all platforms.

    Python's os.fsync() handles platform differences:
    - Linux: fdatasync() or fsync()
    - macOS: fcntl(F_FULLFSYNC) when available
    - Windows: FlushFileBuffers()

    For extra safety on macOS (which may lie about fsync):
    """
    import sys

    os.fsync(file.fileno())

    # macOS: F_FULLFSYNC guarantees disk write (optional, slower)
    if sys.platform == 'darwin':
        try:
            import fcntl
            fcntl.fcntl(file.fileno(), fcntl.F_FULLFSYNC)
        except (ImportError, OSError):
            pass  # Fall back to regular fsync
```

### 13.6 Reading: The Complete Portable Approach

```python
"""
Portable WAL reading implementation.

Uses run_in_executor() for all blocking operations with
periodic yields to the event loop for responsiveness.
"""

import asyncio
import struct
import zlib
from typing import AsyncIterator, Tuple

import msgspec

from hyperscale.logging.models import Log


class PortableWALReader:
    """
    Portable WAL reader using run_in_executor().

    Why NOT connect_read_pipe() / StreamReader:
    ────────────────────────────────────────────

    1. Regular files are ALWAYS "ready" - no async benefit
       - epoll/kqueue/select report immediate readability
       - Actual disk I/O still blocks

    2. Loses seek() capability
       - StreamReader is stream-oriented, not random-access
       - Recovery needs: "read from byte offset X"

    3. Platform inconsistency
       - connect_read_pipe() behavior varies
       - Windows requires ProactorEventLoop

    Why run_in_executor() IS correct:
    ─────────────────────────────────

    1. Officially recommended by Python docs
    2. Maintains full file semantics (seek, tell, etc.)
    3. Same behavior on all platforms
    4. Periodic yields keep event loop responsive
    """

    HEADER_SIZE = 16  # CRC32(4) + length(4) + LSN(8)
    YIELD_INTERVAL = 100  # Yield to event loop every N entries

    def __init__(self, path: str):
        self._path = path
        self._loop: asyncio.AbstractEventLoop | None = None

    async def read_all(
        self,
        from_offset: int = 0,
        verify_crc: bool = True,
    ) -> AsyncIterator[Tuple[int, Log, int]]:
        """
        Read all entries from WAL file.

        Uses run_in_executor() for blocking reads with periodic
        yields to maintain event loop responsiveness.

        Args:
            from_offset: Starting byte offset
            verify_crc: Whether to verify CRC32 checksums

        Yields:
            (offset, log_entry, lsn) for each valid entry
        """
        self._loop = asyncio.get_running_loop()

        # Open file (blocking)
        file = await self._loop.run_in_executor(
            None,
            lambda: open(self._path, 'rb'),
        )

        try:
            # Seek to start position (blocking)
            if from_offset > 0:
                await self._loop.run_in_executor(
                    None,
                    file.seek,
                    from_offset,
                )

            offset = from_offset
            entries_read = 0

            while True:
                # Read header (blocking)
                header = await self._loop.run_in_executor(
                    None,
                    file.read,
                    self.HEADER_SIZE,
                )

                if len(header) == 0:
                    break  # Clean EOF

                if len(header) < self.HEADER_SIZE:
                    raise ValueError(
                        f"Truncated header at offset {offset}"
                    )

                # Parse header
                crc_stored, length, lsn = struct.unpack(
                    "<IIQ", header
                )

                # Read payload (blocking)
                payload = await self._loop.run_in_executor(
                    None,
                    file.read,
                    length,
                )

                if len(payload) < length:
                    raise ValueError(
                        f"Truncated payload at offset {offset}"
                    )

                # Verify CRC (CPU-bound, but fast)
                if verify_crc:
                    crc_computed = zlib.crc32(
                        header[4:] + payload
                    ) & 0xFFFFFFFF

                    if crc_stored != crc_computed:
                        raise ValueError(
                            f"CRC mismatch at offset {offset}"
                        )

                # Decode entry
                log = msgspec.json.decode(payload, type=Log)

                yield offset, log, lsn

                offset += self.HEADER_SIZE + length
                entries_read += 1

                # CRITICAL: Yield to event loop periodically
                # This keeps the application responsive during
                # large file reads
                if entries_read % self.YIELD_INTERVAL == 0:
                    await asyncio.sleep(0)

        finally:
            # Close file (blocking)
            await self._loop.run_in_executor(
                None,
                file.close,
            )

    async def read_range(
        self,
        start_lsn: int,
        end_lsn: int | None = None,
    ) -> AsyncIterator[Tuple[int, Log, int]]:
        """
        Read entries within LSN range.

        Useful for streaming replication: "give me all entries
        since LSN X".
        """
        async for offset, log, lsn in self.read_all():
            if lsn < start_lsn:
                continue
            if end_lsn is not None and lsn > end_lsn:
                break
            yield offset, log, lsn

    async def get_file_size(self) -> int:
        """Get WAL file size (for progress reporting)."""
        return await self._loop.run_in_executor(
            None,
            lambda: os.path.getsize(self._path),
        )
```

### 13.7 Performance Reality Check

```
═══════════════════════════════════════════════════════════════════════════
              PERFORMANCE: PORTABLE VS PLATFORM-SPECIFIC
═══════════════════════════════════════════════════════════════════════════

BENCHMARK: 100,000 writes with fsync, 64-byte entries, NVMe SSD

┌─────────────────────────────────────────────────────────────────────────┐
│                                                                          │
│  APPROACH                 THROUGHPUT      LATENCY P99    PORTABLE?      │
│  ─────────────────────────────────────────────────────────────────────  │
│                                                                          │
│  io_uring (Linux)         ~500K/s         ~2ms           No             │
│  IOCP (Windows)           ~300K/s         ~3ms           No             │
│  Write coalescing         ~100K/s         ~10ms          YES            │
│  Per-write executor       ~10K/s          ~100ms         YES            │
│                                                                          │
│  ─────────────────────────────────────────────────────────────────────  │
│                                                                          │
│  ANALYSIS:                                                              │
│                                                                          │
│  Write coalescing achieves:                                             │
│  ├── 5-10x slower than io_uring peak                                    │
│  ├── 10x faster than naive per-write                                    │
│  ├── 10x better latency than naive per-write                            │
│  └── Identical behavior on Linux/macOS/Windows                          │
│                                                                          │
│  IS 100K/s ENOUGH?                                                      │
│  ├── 100K writes/sec = 8.6 billion writes/day                           │
│  ├── Most applications: <1K writes/sec                                  │
│  ├── High-throughput services: <10K writes/sec                          │
│  └── Extreme edge cases: consider io_uring as optional backend          │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘

CONCLUSION: Write coalescing provides "fast enough" performance for
virtually all use cases while maintaining perfect portability.

For the rare case where you need >100K durable writes/sec:
- Consider io_uring as an OPTIONAL backend (Linux only)
- Fall back to write coalescing on other platforms
- BUT: Start with the portable solution and optimize IF needed
```

### 13.8 Decision Framework

```
═══════════════════════════════════════════════════════════════════════════
              WHEN TO USE WHAT: DECISION TREE
═══════════════════════════════════════════════════════════════════════════

START HERE: What are your requirements?

                    ┌─────────────────────────┐
                    │ Need cross-platform?    │
                    └───────────┬─────────────┘
                                │
              ┌─────────────────┼─────────────────┐
              │ YES             │                 │ NO
              ▼                 │                 ▼
    ┌─────────────────┐        │       ┌─────────────────┐
    │ Write Coalescing│        │       │ Platform-specific│
    │ (RECOMMENDED)   │        │       │ (io_uring, IOCP)│
    └─────────────────┘        │       └─────────────────┘
                                │
                                ▼
                    ┌─────────────────────────┐
                    │ Need durability (fsync)?│
                    └───────────┬─────────────┘
                                │
              ┌─────────────────┼─────────────────┐
              │ YES             │                 │ NO
              ▼                 │                 ▼
    ┌─────────────────┐        │       ┌─────────────────┐
    │ Write Coalescing│        │       │ Per-write exec  │
    │ (batch fsync)   │        │       │ (simpler)       │
    └─────────────────┘        │       └─────────────────┘
                                │
                                ▼
                    ┌─────────────────────────┐
                    │ Write rate >10K/sec?    │
                    └───────────┬─────────────┘
                                │
              ┌─────────────────┼─────────────────┐
              │ YES             │                 │ NO
              ▼                 │                 ▼
    ┌─────────────────┐        │       ┌─────────────────┐
    │ Write Coalescing│        │       │ Either approach │
    │ (REQUIRED)      │        │       │ works fine      │
    └─────────────────┘        │       └─────────────────┘


SUMMARY TABLE:
┌────────────────────┬─────────────────┬──────────────────┬───────────────┐
│ Use Case           │ Approach        │ Why              │ Performance   │
├────────────────────┼─────────────────┼──────────────────┼───────────────┤
│ Debug logging      │ Per-write exec  │ Simple, rare     │ N/A           │
│ Application logs   │ Per-write exec  │ Low volume       │ ~1K/s fine    │
│ High-volume logs   │ Write coalescing│ Throughput       │ ~100K/s       │
│ WAL (portable)     │ Write coalescing│ Durability+perf  │ ~100K/s       │
│ WAL (Linux only)   │ io_uring        │ Max performance  │ ~500K/s       │
│ Metrics/stats      │ Per-write exec  │ No fsync needed  │ ~50K/s        │
└────────────────────┴─────────────────┴──────────────────┴───────────────┘
```

### 13.9 Summary: The Definitive Answer

**Question**: What is the most correct and robust approach for high-concurrency, low-latency logging that is asyncio-compatible AND portable?

**Answer**: **Write Coalescing with `run_in_executor()`**

```
┌─────────────────────────────────────────────────────────────────────────┐
│                                                                          │
│                    THE PORTABLE SOLUTION                                │
│                                                                          │
│  ┌────────────────────────────────────────────────────────────────────┐ │
│  │                                                                     │ │
│  │   1. Buffer writes in async layer (List + asyncio.Lock)           │ │
│  │   2. Flush on: batch_max_size OR batch_timeout_ms                 │ │
│  │   3. Single run_in_executor() call per batch                      │ │
│  │   4. Batch write + single fsync in thread                         │ │
│  │   5. Resolve Futures to notify callers                            │ │
│  │   6. Backpressure via asyncio.Event when buffer full              │ │
│  │                                                                     │ │
│  └────────────────────────────────────────────────────────────────────┘ │
│                                                                          │
│  WHY THIS IS CORRECT:                                                   │
│  ├── Official Python recommendation for file I/O in asyncio            │
│  ├── Standard library only - no external dependencies                  │
│  ├── Works identically on Linux, macOS, Windows                        │
│  ├── 100x overhead reduction via batching                              │
│  ├── Bounded latency (batch timeout)                                   │
│  ├── Memory safety (backpressure)                                      │
│  └── 100K+ writes/sec - fast enough for virtually all use cases        │
│                                                                          │
│  WHAT TO AVOID:                                                         │
│  ├── io_uring, kqueue, IOCP: platform-specific, maintenance burden     │
│  ├── mmap + msync: complex durability semantics, alignment issues      │
│  ├── connect_read_pipe(): wrong tool for regular files                 │
│  └── Per-write executor: too slow for high-concurrency                 │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘
```

This is the implementation documented in Part 12 (WALWriter/WALReader classes) and represents the most robust, portable approach for hyperscale's logging needs.

---

## Part 14: High-Concurrency Reading and Buffer Architecture

This section addresses two critical questions:
1. **How do we implement high-concurrency reading that is asyncio-compatible and portable?**
2. **What buffer implementation maximizes resilience, durability, and throughput for both reads and writes?**

### 14.1 The Reading Problem

The WALReader in Part 12 has a significant overhead issue:

```python
# Current approach - 2 EXECUTOR CALLS PER ENTRY
while True:
    header = await run_in_executor(None, file.read, 16)    # Call 1
    payload = await run_in_executor(None, file.read, len)  # Call 2
    # ... process entry
```

**For 10,000 entries**: 20,000 executor calls × ~5-25μs = **100-500ms overhead**

This is the same class of problem we solved for writes with coalescing.

### 14.2 Reading vs Writing: Key Differences

```
═══════════════════════════════════════════════════════════════════════════
              READING VS WRITING CHARACTERISTICS
═══════════════════════════════════════════════════════════════════════════

┌─────────────────────────────────────────────────────────────────────────┐
│                                                                          │
│  Aspect              │ Writing                │ Reading                 │
│  ────────────────────┼────────────────────────┼─────────────────────────│
│  Access pattern      │ Sequential (append)    │ Random OR sequential    │
│  Blocking concern    │ fsync dominates        │ Disk seek + read        │
│  Batching benefit    │ High (fsync amortize)  │ Moderate (reduce calls) │
│  Concurrency         │ Many writers → 1 file  │ Many readers → 1 file   │
│  Critical operation  │ Durability (fsync)     │ Responsiveness (yield)  │
│  Buffer role         │ Accumulate before I/O  │ Cache after I/O         │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘
```

### 14.3 High-Concurrency Reading Options

```
═══════════════════════════════════════════════════════════════════════════
              READING IMPLEMENTATION OPTIONS
═══════════════════════════════════════════════════════════════════════════

┌─────────────────────────────────────────────────────────────────────────┐
│ OPTION 1: Per-Read Executor (Current)                                    │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│  Pattern:                                                               │
│  while True:                                                            │
│      header = await run_in_executor(None, file.read, 16)                │
│      payload = await run_in_executor(None, file.read, length)           │
│      yield parse(header, payload)                                       │
│                                                                          │
│  Executor calls:   2 per entry (header + payload)                       │
│  Overhead:         ~20μs per entry                                      │
│  Throughput:       ~50K entries/sec                                     │
│  Complexity:       Low                                                  │
│  Portability:      Excellent                                            │
│                                                                          │
│  Verdict: Fine for recovery (one-time), poor for streaming/tailing      │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────────┐
│ OPTION 2: Buffered Reading (Read Coalescing) - RECOMMENDED              │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│  Pattern:                                                               │
│  # Read 64KB at once                                                    │
│  buffer = await run_in_executor(None, file.read, 65536)                 │
│                                                                          │
│  # Parse multiple entries from buffer (no executor - just CPU)          │
│  while has_complete_entry(buffer):                                      │
│      entry = parse_entry(buffer)                                        │
│      yield entry                                                        │
│                                                                          │
│  Executor calls:   1 per 64KB (~100-500 entries)                        │
│  Overhead:         ~0.1μs per entry                                     │
│  Throughput:       ~500K entries/sec                                    │
│  Complexity:       Medium (boundary handling)                           │
│  Portability:      Excellent                                            │
│                                                                          │
│  Verdict: BEST portable option for high throughput                      │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────────┐
│ OPTION 3: Memory-Mapped Files (mmap)                                     │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│  Pattern:                                                               │
│  import mmap                                                            │
│  mm = mmap.mmap(file.fileno(), 0, access=mmap.ACCESS_READ)              │
│  # Direct memory access, OS handles paging                              │
│                                                                          │
│  Executor calls:   0 (kernel handles I/O)                               │
│  Overhead:         Near-zero per entry                                  │
│  Throughput:       ~1M+ entries/sec                                     │
│  Complexity:       Medium                                               │
│  Portability:      MODERATE (behavior varies by platform)               │
│                                                                          │
│  PROBLEMS:                                                              │
│  ├── Page faults can block unpredictably                                │
│  ├── File size changes require remapping                                │
│  ├── 32-bit systems: 2GB address space limit                            │
│  ├── macOS vs Linux vs Windows semantics differ                         │
│  └── No control over when I/O actually happens                          │
│                                                                          │
│  Verdict: Fast but less predictable, portability concerns               │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────────┐
│ OPTION 4: Dedicated Reader Thread                                        │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│  Pattern:                                                               │
│  # Dedicated thread reads ahead into queue                              │
│  reader_thread → asyncio.Queue → async consumers                        │
│                                                                          │
│  Executor calls:   0 from async code                                    │
│  Overhead:         Queue overhead (~1μs per entry)                      │
│  Throughput:       ~200K entries/sec                                    │
│  Complexity:       High (thread lifecycle, queue sizing)                │
│  Portability:      Excellent                                            │
│                                                                          │
│  Verdict: Good for continuous streaming, overkill for recovery          │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────────┐
│ OPTION 5: Read-Ahead with Prefetch                                       │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│  Pattern:                                                               │
│  # While processing current buffer, prefetch next                       │
│  current_entries = process_buffer(buffer1)                              │
│  next_buffer_task = asyncio.create_task(                                │
│      run_in_executor(None, file.read, 65536)                            │
│  )                                                                      │
│  # Overlap I/O with processing                                          │
│                                                                          │
│  Executor calls:   1 per chunk (overlapped with processing)             │
│  Overhead:         Hidden by overlap                                    │
│  Throughput:       ~500K+ entries/sec                                   │
│  Complexity:       Medium-High                                          │
│  Portability:      Excellent                                            │
│                                                                          │
│  Verdict: Best latency when I/O and CPU can overlap                     │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘
```

### 14.4 Reading Options Comparison

```
═══════════════════════════════════════════════════════════════════════════
              READING OPTIONS SUMMARY
═══════════════════════════════════════════════════════════════════════════

┌──────────────────────┬────────────┬─────────┬────────────┬──────────────┐
│ Approach             │ Throughput │ Latency │ Complexity │ Portable     │
├──────────────────────┼────────────┼─────────┼────────────┼──────────────┤
│ Per-read executor    │   ~50K/s   │  ~20μs  │    Low     │ ✓ Yes        │
│ Buffered reading     │  ~500K/s   │  ~0.1μs │   Medium   │ ✓ Yes        │
│ mmap                 │   ~1M/s    │ ~0.05μs │   Medium   │ ⚠ Varies     │
│ Dedicated thread     │  ~200K/s   │   ~1μs  │    High    │ ✓ Yes        │
│ Read-ahead prefetch  │  ~500K/s   │  Hidden │   Med-High │ ✓ Yes        │
└──────────────────────┴────────────┴─────────┴────────────┴──────────────┘

RECOMMENDATION: Buffered Reading (Option 2)

Why:
├── 10x throughput over per-read executor
├── Same pattern as write coalescing (conceptual consistency)
├── Standard library only (no dependencies)
├── Predictable behavior (no page fault surprises like mmap)
└── Simple mental model: read chunk, parse entries, repeat

CHALLENGE: Boundary Handling

An entry may span two buffers:

Buffer 1: [...entry A...][entry B (partial)]
Buffer 2: [B (rest)][entry C][entry D]...

This requires carrying over partial data between reads.
```

### 14.5 The Buffer Implementation Question

Both reading and writing depend heavily on buffer implementation. The buffer is the critical shared component that determines:

- **Resilience**: Can we survive memory pressure?
- **Durability**: Can we track what's been persisted?
- **Throughput**: How fast can we move data?
- **Memory efficiency**: How much overhead per operation?

### 14.6 Buffer Implementation Options

```
═══════════════════════════════════════════════════════════════════════════
              BUFFER IMPLEMENTATION OPTIONS
═══════════════════════════════════════════════════════════════════════════

┌─────────────────────────────────────────────────────────────────────────┐
│ OPTION 1: List Buffer (Naive)                                            │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│  buffer: List[bytes] = []                                               │
│  buffer.append(data)                                                    │
│  batch = buffer.copy()                                                  │
│  buffer.clear()                                                         │
│                                                                          │
│  Simplicity:        ✓ Excellent                                         │
│  Memory efficiency: ✗ Poor (fragmentation, repeated allocations)        │
│  Cache locality:    ✗ Poor (scattered memory)                           │
│  GC pressure:       ✗ High (many small objects)                         │
│  Throughput:        ~100K ops/sec                                       │
│                                                                          │
│  Verdict: Too slow for high-concurrency                                 │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────────┐
│ OPTION 2: collections.deque                                              │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│  from collections import deque                                          │
│  buffer: deque[bytes] = deque(maxlen=10000)                             │
│                                                                          │
│  Append/pop:        ✓ O(1)                                              │
│  Memory efficiency: ⚠ Moderate                                          │
│  Bounded size:      ✓ Built-in maxlen                                   │
│  GC pressure:       ⚠ Still per-item allocation                         │
│  Throughput:        ~200K ops/sec                                       │
│                                                                          │
│  Verdict: Better, but still allocates per item                          │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────────┐
│ OPTION 3: Pre-allocated bytearray                                        │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│  buffer = bytearray(1024 * 1024)  # 1MB pre-allocated                   │
│  write_pos = 0                                                          │
│                                                                          │
│  def append(data: bytes) -> int:                                        │
│      nonlocal write_pos                                                 │
│      buffer[write_pos:write_pos + len(data)] = data                     │
│      write_pos += len(data)                                             │
│      return write_pos                                                   │
│                                                                          │
│  Memory efficiency: ✓ Excellent (single allocation)                     │
│  Cache locality:    ✓ Excellent (contiguous)                            │
│  GC pressure:       ✓ None (pre-allocated)                              │
│  Zero-copy:         ✓ Via memoryview                                    │
│  Throughput:        ~1M+ ops/sec                                        │
│                                                                          │
│  Verdict: Excellent, but can't overlap I/O (blocked during flush)       │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────────┐
│ OPTION 4: Ring Buffer (Circular)                                         │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│  class RingBuffer:                                                      │
│      def __init__(self, capacity: int):                                 │
│          self._buf = bytearray(capacity)                                │
│          self._read_pos = 0                                             │
│          self._write_pos = 0                                            │
│          self._size = 0                                                 │
│                                                                          │
│  Memory:            ✓ Fixed footprint                                   │
│  Streaming:         ✓ Excellent (continuous read/write)                 │
│  Lock-free:         ✓ SPSC can be lock-free                             │
│  Complexity:        ⚠ Wrap-around handling                              │
│  Throughput:        ~1M+ ops/sec                                        │
│                                                                          │
│  Verdict: Good for streaming, but wrap-around adds complexity           │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────────┐
│ OPTION 5: Double Buffer (Swap Pattern)                                   │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│  class DoubleBuffer:                                                    │
│      def __init__(self, capacity: int):                                 │
│          self._front = bytearray(capacity)  # Writers use this          │
│          self._back = bytearray(capacity)   # I/O uses this             │
│                                                                          │
│      def swap(self):                                                    │
│          self._front, self._back = self._back, self._front              │
│                                                                          │
│  Contention:        ✓ Minimal (separate buffers)                        │
│  I/O overlap:       ✓ Write while flushing                              │
│  Memory:            ⚠ 2x capacity required                              │
│  Complexity:        ✓ Simple swap semantics                             │
│  Throughput:        ~1M+ ops/sec                                        │
│                                                                          │
│  Verdict: Excellent for overlapping I/O with processing                 │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────────┐
│ OPTION 6: Buffer Pool (Slab Allocator)                                   │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│  class BufferPool:                                                      │
│      def __init__(self, buffer_size: int, pool_size: int):              │
│          self._free: List[bytearray] = [                                │
│              bytearray(buffer_size) for _ in range(pool_size)           │
│          ]                                                              │
│                                                                          │
│      def acquire(self) -> bytearray:                                    │
│          return self._free.pop() if self._free else bytearray(...)      │
│                                                                          │
│      def release(self, buf: bytearray) -> None:                         │
│          self._free.append(buf)                                         │
│                                                                          │
│  Allocation:        ✓ Amortized zero                                    │
│  Memory reuse:      ✓ Excellent                                         │
│  Variable sizes:    ⚠ Fixed buffer sizes                                │
│  Complexity:        ⚠ Lifecycle management                              │
│                                                                          │
│  Verdict: Excellent for eliminating allocation overhead                 │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘
```

### 14.7 Buffer Options Comparison

```
═══════════════════════════════════════════════════════════════════════════
              BUFFER OPTIONS SUMMARY
═══════════════════════════════════════════════════════════════════════════

┌──────────────────┬────────────┬────────────┬───────────┬────────────────┐
│ Approach         │ Throughput │ GC Pressure│ I/O Overlap│ Complexity    │
├──────────────────┼────────────┼────────────┼───────────┼────────────────┤
│ List[bytes]      │   ~100K/s  │    High    │    No     │ Low            │
│ deque            │   ~200K/s  │   Medium   │    No     │ Low            │
│ bytearray        │    ~1M/s   │    None    │    No     │ Low            │
│ Ring buffer      │    ~1M/s   │    None    │   Partial │ Medium         │
│ Double buffer    │    ~1M/s   │    None    │    Yes    │ Medium         │
│ Buffer pool      │    ~1M/s   │    None    │    Yes    │ Medium         │
└──────────────────┴────────────┴────────────┴───────────┴────────────────┘

WHY NOT SIMPLER OPTIONS?

┌──────────────────┬─────────────────────────────────────────────────────┐
│ Approach         │ Problem                                              │
├──────────────────┼─────────────────────────────────────────────────────┤
│ List[bytes]      │ Fragmentation, GC pressure, no durability tracking  │
│ Single bytearray │ Can't overlap I/O (blocked during flush)            │
│ Single ring buf  │ Same problem - blocked during I/O                   │
│ mmap             │ Unpredictable page faults, platform differences     │
└──────────────────┴─────────────────────────────────────────────────────┘
```

### 14.8 The Optimal Solution: Segmented Double Buffer with Pool

The most correct and robust solution combines multiple patterns:

```
═══════════════════════════════════════════════════════════════════════════
              SEGMENTED DOUBLE BUFFER ARCHITECTURE
═══════════════════════════════════════════════════════════════════════════

┌─────────────────────────────────────────────────────────────────────────┐
│                                                                          │
│                    UNIFIED BUFFER ARCHITECTURE                          │
│                                                                          │
│  ┌────────────────────────────────────────────────────────────────────┐ │
│  │                                                                     │ │
│  │   WRITE PATH                          READ PATH                    │ │
│  │   ──────────                          ─────────                    │ │
│  │                                                                     │ │
│  │   Writers ──┐                         ┌── Readers                  │ │
│  │             │                         │                            │ │
│  │             ▼                         ▼                            │ │
│  │   ┌─────────────────┐       ┌─────────────────┐                   │ │
│  │   │  FRONT BUFFER   │       │  READ BUFFER    │                   │ │
│  │   │  (accepting)    │       │  (parsing)      │                   │ │
│  │   │                 │       │                 │                   │ │
│  │   │  [Seg0][Seg1]   │       │  [====data====] │                   │ │
│  │   │  [Seg2][Seg3]   │       │                 │                   │ │
│  │   └────────┬────────┘       └────────▲────────┘                   │ │
│  │            │                         │                            │ │
│  │            │ SWAP                    │ FILL                       │ │
│  │            ▼                         │                            │ │
│  │   ┌─────────────────┐       ┌────────┴────────┐                   │ │
│  │   │  BACK BUFFER    │       │  PREFETCH BUF   │                   │ │
│  │   │  (flushing)     │       │  (loading next) │                   │ │
│  │   │                 │       │                 │                   │ │
│  │   │  → Disk I/O     │       │  ← Disk I/O     │                   │ │
│  │   └─────────────────┘       └─────────────────┘                   │ │
│  │                                                                     │ │
│  │   ┌─────────────────────────────────────────────────────────────┐  │ │
│  │   │            BUFFER POOL (recycled segments)                   │  │ │
│  │   │   ┌──────┬──────┬──────┬──────┬──────┬──────┬──────┬──────┐ │  │ │
│  │   │   │ Free │ Free │ Free │ Free │ Free │ Free │ Free │ Free │ │  │ │
│  │   │   └──────┴──────┴──────┴──────┴──────┴──────┴──────┴──────┘ │  │ │
│  │   └─────────────────────────────────────────────────────────────┘  │ │
│  │                                                                     │ │
│  └────────────────────────────────────────────────────────────────────┘ │
│                                                                          │
│  PROPERTIES ACHIEVED:                                                   │
│  ├── Pre-allocated memory survives pressure (Resilience)               │
│  ├── Track flushed vs pending segments (Durability)                    │
│  ├── Zero-copy, contiguous memory, I/O overlap (Throughput)            │
│  ├── Fixed pool size, natural backpressure (Bounded memory)            │
│  ├── Reuse buffers, no allocation in hot path (No GC pressure)         │
│  └── bytearray + memoryview are stdlib (Portability)                   │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘
```

### 14.9 Implementation: Core Buffer Components

#### 14.9.1 Segment and Buffer Pool

```python
"""
hyperscale/logging/buffers/buffer_pool.py

Pre-allocated buffer pool for zero-allocation I/O operations.
"""

import asyncio
from typing import List


class BufferSegment:
    """
    A single pre-allocated buffer segment.

    Uses bytearray for mutable, contiguous memory.
    Tracks write position and provides memoryview for zero-copy access.
    """

    __slots__ = ('_data', '_write_pos', '_capacity')

    def __init__(self, capacity: int):
        self._data = bytearray(capacity)
        self._write_pos = 0
        self._capacity = capacity

    @property
    def capacity(self) -> int:
        return self._capacity

    @property
    def remaining(self) -> int:
        return self._capacity - self._write_pos

    @property
    def size(self) -> int:
        return self._write_pos

    @property
    def is_full(self) -> bool:
        return self._write_pos >= self._capacity

    @property
    def is_empty(self) -> bool:
        return self._write_pos == 0

    def write(self, data: bytes) -> int:
        """
        Write data to segment. Returns bytes written.

        Uses slice assignment for efficient copy into pre-allocated memory.
        """
        write_size = min(len(data), self.remaining)
        if write_size > 0:
            self._data[self._write_pos:self._write_pos + write_size] = data[:write_size]
            self._write_pos += write_size
        return write_size

    def view(self) -> memoryview:
        """
        Return zero-copy view of written data.

        memoryview allows passing to file.write() without copying.
        """
        return memoryview(self._data)[:self._write_pos]

    def reset(self) -> None:
        """Reset segment for reuse. Does NOT zero memory (unnecessary)."""
        self._write_pos = 0

    def __len__(self) -> int:
        return self._write_pos


class BufferPool:
    """
    Pool of pre-allocated buffer segments.

    Eliminates allocation overhead in the hot path by recycling segments.

    Thread Safety:
    - Uses asyncio.Lock for async-safe access
    - Segments are exclusively owned while in use

    Memory Guarantees:
    - Total memory = segment_size × pool_size (fixed)
    - No allocations after initialization (except overflow)
    - Overflow segments are collected when returned to pool

    Usage:
        pool = BufferPool(segment_size=65536, pool_size=16)
        await pool.initialize()

        segment = await pool.acquire()
        segment.write(data)
        # ... use segment ...
        await pool.release(segment)
    """

    def __init__(
        self,
        segment_size: int = 64 * 1024,  # 64KB - matches OS read-ahead
        pool_size: int = 16,             # 1MB total
    ):
        self._segment_size = segment_size
        self._pool_size = pool_size
        self._free: List[BufferSegment] = []
        self._lock: asyncio.Lock | None = None
        self._total_allocated = 0
        self._overflow_allocated = 0

    async def initialize(self) -> None:
        """Pre-allocate all segments."""
        self._lock = asyncio.Lock()
        self._free = [
            BufferSegment(self._segment_size)
            for _ in range(self._pool_size)
        ]
        self._total_allocated = self._pool_size

    async def acquire(self) -> BufferSegment:
        """
        Acquire a segment from the pool.

        If pool is empty, allocates overflow segment (tracked separately).
        Overflow indicates pool_size should be increased.
        """
        async with self._lock:
            if self._free:
                segment = self._free.pop()
                segment.reset()
                return segment

            # Pool exhausted - allocate overflow segment
            self._overflow_allocated += 1
            self._total_allocated += 1
            return BufferSegment(self._segment_size)

    async def release(self, segment: BufferSegment) -> None:
        """
        Return segment to pool.

        Segments are reset and ready for reuse.
        If we have overflow segments and pool is full, let GC collect them.
        """
        async with self._lock:
            if len(self._free) < self._pool_size:
                segment.reset()
                self._free.append(segment)
            else:
                # Overflow segment - let it be garbage collected
                self._overflow_allocated -= 1
                self._total_allocated -= 1

    async def release_many(self, segments: List[BufferSegment]) -> None:
        """Release multiple segments efficiently."""
        async with self._lock:
            for segment in segments:
                if len(self._free) < self._pool_size:
                    segment.reset()
                    self._free.append(segment)
                else:
                    self._overflow_allocated -= 1
                    self._total_allocated -= 1

    @property
    def available(self) -> int:
        """Number of segments available in pool."""
        return len(self._free)

    @property
    def total_memory(self) -> int:
        """Total memory allocated by pool."""
        return self._total_allocated * self._segment_size

    @property
    def overflow_count(self) -> int:
        """Number of overflow allocations (indicates undersized pool)."""
        return self._overflow_allocated
```

#### 14.9.2 Double Buffer Manager

```python
"""
hyperscale/logging/buffers/double_buffer.py

Double buffer for overlapping I/O with processing.
"""

import asyncio
from enum import Enum, auto
from typing import Callable, Awaitable, List

from .buffer_pool import BufferSegment, BufferPool


class BufferState(Enum):
    """State of a buffer in the double-buffer system."""
    ACCEPTING = auto()   # Receiving writes
    PENDING = auto()     # Full, waiting for flush
    FLUSHING = auto()    # Being written to disk
    DURABLE = auto()     # Flushed and fsynced


class DoubleBuffer:
    """
    Double buffer for write coalescing with I/O overlap.

    Writers write to the front buffer while the back buffer
    is being flushed to disk. When front is full, buffers swap.

    This allows continuous writing without blocking on I/O.

    Architecture:

        Writers → [FRONT BUFFER] ←→ [BACK BUFFER] → Disk
                  (accepting)        (flushing)

    Thread Safety:
    - asyncio.Lock protects buffer access
    - Swap operation is atomic
    - Flush runs in executor (non-blocking)

    Durability Tracking:
    - Each segment tracks its durability state
    - Callers can await specific offset becoming durable
    """

    def __init__(
        self,
        pool: BufferPool,
        flush_callback: Callable[[memoryview], Awaitable[None]],
        segment_count: int = 4,  # Segments per buffer
    ):
        """
        Initialize double buffer.

        Args:
            pool: Buffer pool for segment allocation
            flush_callback: Async function to flush data to disk
            segment_count: Number of segments per buffer (more = more batching)
        """
        self._pool = pool
        self._flush_callback = flush_callback
        self._segment_count = segment_count

        # Buffer state
        self._front: List[BufferSegment] = []
        self._back: List[BufferSegment] = []
        self._current_segment: BufferSegment | None = None

        # Synchronization
        self._lock: asyncio.Lock | None = None
        self._flush_lock: asyncio.Lock | None = None

        # Tracking
        self._write_offset = 0      # Total bytes written
        self._flush_offset = 0      # Bytes sent to flush
        self._durable_offset = 0    # Bytes confirmed durable

        # Durability waiters
        self._durable_waiters: List[tuple[int, asyncio.Future]] = []

        self._initialized = False

    async def initialize(self) -> None:
        """Initialize locks and acquire initial segment."""
        self._lock = asyncio.Lock()
        self._flush_lock = asyncio.Lock()
        self._current_segment = await self._pool.acquire()
        self._initialized = True

    async def write(self, data: bytes) -> int:
        """
        Write data to buffer. Returns offset of this write.

        Data is buffered until flush. If current segment is full,
        a new segment is acquired from pool. If buffer is full,
        triggers flush.
        """
        if not self._initialized:
            raise RuntimeError("DoubleBuffer not initialized")

        async with self._lock:
            offset = self._write_offset
            remaining = data

            while remaining:
                # Write to current segment
                written = self._current_segment.write(remaining)
                remaining = remaining[written:]
                self._write_offset += written

                # Segment full?
                if self._current_segment.is_full:
                    self._front.append(self._current_segment)

                    # Buffer full? Trigger flush
                    if len(self._front) >= self._segment_count:
                        await self._trigger_flush()

                    # Get new segment
                    self._current_segment = await self._pool.acquire()

            return offset

    async def _trigger_flush(self) -> None:
        """
        Swap buffers and flush back buffer.

        Called when front buffer is full.
        """
        # Include current partial segment in flush
        if not self._current_segment.is_empty:
            self._front.append(self._current_segment)
            self._current_segment = await self._pool.acquire()

        # Swap front and back
        self._front, self._back = self._back, self._front

        # Calculate bytes to flush
        flush_bytes = sum(len(seg) for seg in self._back)
        self._flush_offset = self._write_offset

        # Flush back buffer (don't hold lock during I/O)
        if self._back:
            asyncio.create_task(self._flush_back_buffer())

    async def _flush_back_buffer(self) -> None:
        """Flush back buffer to disk."""
        async with self._flush_lock:
            if not self._back:
                return

            # Concatenate segments into single view for efficient I/O
            total_size = sum(len(seg) for seg in self._back)
            flush_data = bytearray(total_size)
            offset = 0

            for segment in self._back:
                view = segment.view()
                flush_data[offset:offset + len(view)] = view
                offset += len(view)

            # Flush to disk
            await self._flush_callback(memoryview(flush_data))

            # Update durable offset
            self._durable_offset = self._flush_offset

            # Return segments to pool
            await self._pool.release_many(self._back)
            self._back = []

            # Notify waiters
            await self._notify_durable_waiters()

    async def flush(self) -> None:
        """
        Force flush any buffered data.

        Call before shutdown to ensure all data is durable.
        """
        async with self._lock:
            # Include current segment
            if not self._current_segment.is_empty:
                self._front.append(self._current_segment)
                self._current_segment = await self._pool.acquire()

            if self._front:
                # Swap and flush
                self._front, self._back = self._back, self._front
                self._flush_offset = self._write_offset

        await self._flush_back_buffer()

    async def wait_durable(self, offset: int) -> None:
        """
        Wait until specified offset is durable (fsynced).

        Used by callers who need to know their write is safe.
        """
        if offset <= self._durable_offset:
            return

        future: asyncio.Future = asyncio.get_running_loop().create_future()
        self._durable_waiters.append((offset, future))
        await future

    async def _notify_durable_waiters(self) -> None:
        """Notify waiters whose offsets are now durable."""
        remaining = []

        for offset, future in self._durable_waiters:
            if offset <= self._durable_offset:
                if not future.done():
                    future.set_result(None)
            else:
                remaining.append((offset, future))

        self._durable_waiters = remaining

    @property
    def write_offset(self) -> int:
        """Total bytes written (may not be durable yet)."""
        return self._write_offset

    @property
    def durable_offset(self) -> int:
        """Bytes confirmed written to disk."""
        return self._durable_offset

    @property
    def pending_bytes(self) -> int:
        """Bytes waiting to be flushed."""
        return self._write_offset - self._durable_offset
```

#### 14.9.3 Buffered Reader

```python
"""
hyperscale/logging/buffers/buffered_reader.py

High-performance buffered reader with read-ahead.
"""

import asyncio
from typing import AsyncIterator, Tuple, Callable, Awaitable

from .buffer_pool import BufferSegment, BufferPool


class BufferedReader:
    """
    High-performance async file reader with buffering.

    Instead of per-entry executor calls, reads large chunks and
    parses entries from in-memory buffer. This provides ~10x
    throughput improvement over naive per-read approach.

    Features:
    - Large chunk reads (64KB default)
    - Read-ahead prefetching (overlap I/O with parsing)
    - Zero-copy entry access via memoryview
    - Handles entries spanning buffer boundaries
    - Periodic event loop yields for responsiveness

    Architecture:

        Disk → [READ BUFFER] → Parser → Entries
               [PREFETCH   ]
               (loading next)

    The prefetch buffer loads the next chunk while the current
    chunk is being parsed, hiding I/O latency.
    """

    HEADER_SIZE = 16  # CRC32(4) + length(4) + LSN(8)
    YIELD_INTERVAL = 100  # Yield to event loop every N entries

    def __init__(
        self,
        pool: BufferPool,
        read_callback: Callable[[int], Awaitable[bytes]],
        chunk_size: int = 64 * 1024,
    ):
        """
        Initialize buffered reader.

        Args:
            pool: Buffer pool for chunk allocation
            read_callback: Async function to read bytes from file
            chunk_size: Size of each read operation
        """
        self._pool = pool
        self._read_callback = read_callback
        self._chunk_size = chunk_size

        # Buffer state
        self._buffer: bytes = b''
        self._buffer_offset = 0  # Offset within buffer
        self._file_offset = 0    # Offset within file

        # Prefetch state
        self._prefetch_task: asyncio.Task | None = None
        self._prefetch_data: bytes | None = None

        # Stats
        self._entries_read = 0
        self._chunks_read = 0
        self._bytes_read = 0

    async def read_entries(
        self,
        parse_entry: Callable[[memoryview], Tuple[object, int]],
        from_offset: int = 0,
    ) -> AsyncIterator[Tuple[int, object]]:
        """
        Read and parse entries from file.

        Args:
            parse_entry: Function that parses entry from buffer,
                        returns (entry, bytes_consumed)
            from_offset: Starting file offset

        Yields:
            (file_offset, parsed_entry) for each entry
        """
        self._file_offset = from_offset
        self._buffer = b''
        self._buffer_offset = 0

        # Initial read
        await self._fill_buffer()

        while self._buffer:
            # Start prefetching next chunk
            self._start_prefetch()

            # Parse entries from current buffer
            while self._buffer_offset < len(self._buffer):
                # Check if we have enough data for header
                remaining = len(self._buffer) - self._buffer_offset

                if remaining < self.HEADER_SIZE:
                    # Partial header - need more data
                    break

                # Peek at entry length from header
                header_view = memoryview(self._buffer)[
                    self._buffer_offset:self._buffer_offset + self.HEADER_SIZE
                ]
                entry_length = self._peek_entry_length(header_view)
                total_length = self.HEADER_SIZE + entry_length

                if remaining < total_length:
                    # Partial entry - need more data
                    break

                # Parse complete entry
                entry_view = memoryview(self._buffer)[
                    self._buffer_offset:self._buffer_offset + total_length
                ]

                entry_offset = self._file_offset + self._buffer_offset
                entry, consumed = parse_entry(entry_view)

                yield entry_offset, entry

                self._buffer_offset += consumed
                self._entries_read += 1

                # Yield to event loop periodically
                if self._entries_read % self.YIELD_INTERVAL == 0:
                    await asyncio.sleep(0)

            # Advance file offset
            self._file_offset += self._buffer_offset

            # Keep unconsumed bytes (partial entry at boundary)
            if self._buffer_offset < len(self._buffer):
                self._buffer = self._buffer[self._buffer_offset:]
            else:
                self._buffer = b''
            self._buffer_offset = 0

            # Wait for prefetch and append
            await self._fill_buffer()

    def _peek_entry_length(self, header: memoryview) -> int:
        """Extract entry length from header without full parse."""
        import struct
        # Header format: CRC32(4) + length(4) + LSN(8)
        return struct.unpack('<I', header[4:8])[0]

    def _start_prefetch(self) -> None:
        """Start prefetching next chunk if not already running."""
        if self._prefetch_task is None or self._prefetch_task.done():
            self._prefetch_task = asyncio.create_task(self._prefetch())

    async def _prefetch(self) -> None:
        """Prefetch next chunk from file."""
        next_offset = self._file_offset + len(self._buffer)
        self._prefetch_data = await self._read_callback(self._chunk_size)
        self._chunks_read += 1

    async def _fill_buffer(self) -> None:
        """Fill buffer with prefetched or fresh data."""
        if self._prefetch_task:
            await self._prefetch_task
            self._prefetch_task = None

        if self._prefetch_data:
            self._buffer = self._buffer + self._prefetch_data
            self._bytes_read += len(self._prefetch_data)
            self._prefetch_data = None
        elif not self._buffer:
            # No prefetch, do synchronous read
            data = await self._read_callback(self._chunk_size)
            if data:
                self._buffer = data
                self._bytes_read += len(data)
                self._chunks_read += 1

    @property
    def stats(self) -> dict:
        """Reader statistics."""
        return {
            'entries_read': self._entries_read,
            'chunks_read': self._chunks_read,
            'bytes_read': self._bytes_read,
            'avg_entries_per_chunk': (
                self._entries_read / self._chunks_read
                if self._chunks_read > 0 else 0
            ),
        }
```

### 14.10 Integration: Updated WALWriter and WALReader

```python
"""
Updated WAL classes using the buffer infrastructure.
"""

import asyncio
import os
import struct
import zlib
from typing import AsyncIterator, Tuple

import msgspec

from hyperscale.logging.models import Log
from hyperscale.logging.snowflake import SnowflakeGenerator
from hyperscale.logging.config.durability_mode import DurabilityMode
from hyperscale.logging.buffers import BufferPool, DoubleBuffer, BufferedReader


class OptimizedWALWriter:
    """
    WAL writer using segmented double buffer.

    Improvements over Part 12 WALWriter:
    - Pre-allocated segments (no GC pressure)
    - Double buffering (I/O overlap)
    - Fine-grained durability tracking
    - Buffer pool recycling
    """

    HEADER_SIZE = 16

    def __init__(
        self,
        logfile_path: str,
        instance_id: int = 0,
        segment_size: int = 64 * 1024,
        pool_size: int = 16,
        durability: DurabilityMode = DurabilityMode.FSYNC_BATCH,
    ):
        self._logfile_path = logfile_path
        self._instance_id = instance_id
        self._durability = durability

        # Buffer infrastructure
        self._pool = BufferPool(segment_size=segment_size, pool_size=pool_size)
        self._double_buffer: DoubleBuffer | None = None

        # File state
        self._loop: asyncio.AbstractEventLoop | None = None
        self._file = None
        self._sequence_generator: SnowflakeGenerator | None = None

        self._started = False

    async def start(self) -> None:
        """Initialize writer."""
        if self._started:
            return

        self._loop = asyncio.get_running_loop()

        # Initialize pool
        await self._pool.initialize()

        # Initialize double buffer with flush callback
        self._double_buffer = DoubleBuffer(
            pool=self._pool,
            flush_callback=self._flush_to_disk,
        )
        await self._double_buffer.initialize()

        # Open file
        await self._loop.run_in_executor(None, self._open_file_sync)

        self._started = True

    def _open_file_sync(self) -> None:
        """Open WAL file (sync, runs in executor)."""
        import pathlib
        path = pathlib.Path(self._logfile_path)
        path.parent.mkdir(parents=True, exist_ok=True)
        self._file = open(self._logfile_path, 'ab+')
        self._sequence_generator = SnowflakeGenerator(self._instance_id)

    async def write(self, log: Log) -> int:
        """
        Write log entry. Returns offset.

        Entry is buffered. Caller can await wait_durable(offset)
        for durability guarantee.
        """
        if not self._started:
            raise RuntimeError("Writer not started")

        # Generate LSN
        lsn = self._sequence_generator.generate()
        if lsn is not None:
            log.lsn = lsn

        # Encode entry
        data = self._encode_binary(log, lsn)

        # Write to buffer
        offset = await self._double_buffer.write(data)

        return offset

    async def write_durable(self, log: Log) -> int:
        """Write and wait for durability."""
        offset = await self.write(log)
        await self._double_buffer.wait_durable(offset)
        return offset

    def _encode_binary(self, log: Log, lsn: int | None) -> bytes:
        """Encode log entry in binary format."""
        payload = msgspec.json.encode(log)
        lsn_value = lsn if lsn is not None else 0

        header = struct.pack("<IQ", len(payload), lsn_value)
        crc = zlib.crc32(header + payload) & 0xFFFFFFFF

        return struct.pack("<I", crc) + header + payload

    async def _flush_to_disk(self, data: memoryview) -> None:
        """Flush data to disk (called by DoubleBuffer)."""

        def _sync_flush():
            self._file.write(data)
            self._file.flush()
            if self._durability in (DurabilityMode.FSYNC, DurabilityMode.FSYNC_BATCH):
                os.fsync(self._file.fileno())

        await self._loop.run_in_executor(None, _sync_flush)

    async def flush(self) -> None:
        """Force flush all buffered data."""
        await self._double_buffer.flush()

    async def close(self) -> None:
        """Close writer."""
        await self.flush()
        if self._file:
            await self._loop.run_in_executor(None, self._file.close)


class OptimizedWALReader:
    """
    WAL reader using buffered reading with prefetch.

    Improvements over Part 12 WALReader:
    - Large chunk reads (1 executor call per ~100-500 entries)
    - Read-ahead prefetching (overlap I/O with parsing)
    - Zero-copy entry access
    - ~10x throughput improvement
    """

    HEADER_SIZE = 16

    def __init__(
        self,
        logfile_path: str,
        chunk_size: int = 64 * 1024,
        pool_size: int = 4,
    ):
        self._logfile_path = logfile_path
        self._chunk_size = chunk_size

        self._pool = BufferPool(segment_size=chunk_size, pool_size=pool_size)
        self._loop: asyncio.AbstractEventLoop | None = None
        self._file = None

    async def read_entries(
        self,
        from_offset: int = 0,
        verify_crc: bool = True,
    ) -> AsyncIterator[Tuple[int, Log, int | None]]:
        """
        Read entries with buffered I/O.

        ~10x faster than per-entry executor calls.
        """
        self._loop = asyncio.get_running_loop()
        await self._pool.initialize()

        # Open file
        self._file = await self._loop.run_in_executor(
            None,
            lambda: open(self._logfile_path, 'rb'),
        )

        try:
            # Seek to start
            if from_offset > 0:
                await self._loop.run_in_executor(
                    None,
                    self._file.seek,
                    from_offset,
                )

            # Create buffered reader
            reader = BufferedReader(
                pool=self._pool,
                read_callback=self._read_chunk,
                chunk_size=self._chunk_size,
            )

            # Parse function for entries
            def parse_entry(data: memoryview) -> Tuple[Tuple[Log, int | None], int]:
                # Parse header
                crc_stored = struct.unpack('<I', data[:4])[0]
                length, lsn = struct.unpack('<IQ', data[4:16])

                # Extract payload
                payload = bytes(data[16:16 + length])

                # Verify CRC if requested
                if verify_crc:
                    crc_computed = zlib.crc32(bytes(data[4:16]) + payload) & 0xFFFFFFFF
                    if crc_stored != crc_computed:
                        raise ValueError(f"CRC mismatch")

                # Decode
                log = msgspec.json.decode(payload, type=Log)

                return (log, lsn), self.HEADER_SIZE + length

            async for offset, (log, lsn) in reader.read_entries(
                parse_entry=parse_entry,
                from_offset=from_offset,
            ):
                yield offset, log, lsn

        finally:
            if self._file:
                await self._loop.run_in_executor(None, self._file.close)

    async def _read_chunk(self, size: int) -> bytes:
        """Read chunk from file."""
        return await self._loop.run_in_executor(
            None,
            self._file.read,
            size,
        )
```

### 14.11 Performance Comparison

```
═══════════════════════════════════════════════════════════════════════════
              BUFFER ARCHITECTURE PERFORMANCE
═══════════════════════════════════════════════════════════════════════════

BENCHMARK: 100,000 entries, 64-byte average size, NVMe SSD

WRITE PERFORMANCE:
┌─────────────────────────────────────────────────────────────────────────┐
│                                                                          │
│  Implementation           │ Throughput │ P99 Latency │ Memory Allocs   │
│  ─────────────────────────┼────────────┼─────────────┼─────────────────│
│  Part 12 (List buffer)    │  ~100K/s   │   ~10ms     │ ~100K objects   │
│  Part 14 (Segmented)      │  ~500K/s   │   ~5ms      │ ~16 objects     │
│  Improvement              │    5x      │    2x       │   ~6000x        │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘

READ PERFORMANCE:
┌─────────────────────────────────────────────────────────────────────────┐
│                                                                          │
│  Implementation           │ Throughput │ Executor Calls│ I/O Overlap   │
│  ─────────────────────────┼────────────┼───────────────┼───────────────│
│  Part 12 (per-entry)      │  ~50K/s    │ 200,000       │ No            │
│  Part 14 (buffered)       │  ~500K/s   │ ~200          │ Yes (prefetch)│
│  Improvement              │   10x      │ 1000x         │ -             │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘

MEMORY PROFILE:
┌─────────────────────────────────────────────────────────────────────────┐
│                                                                          │
│  Component                │ Part 12          │ Part 14                  │
│  ─────────────────────────┼──────────────────┼──────────────────────────│
│  Write buffer             │ Unbounded list   │ 1MB fixed (16×64KB)      │
│  Read buffer              │ Per-entry alloc  │ 256KB fixed (4×64KB)     │
│  GC collections/100K ops  │ ~50-100          │ ~0-1                     │
│  Peak memory              │ Unbounded        │ ~1.5MB fixed             │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘
```

### 14.12 Summary: The Most Correct Buffer Architecture

```
═══════════════════════════════════════════════════════════════════════════
              THE ANSWER: SEGMENTED DOUBLE BUFFER WITH POOL
═══════════════════════════════════════════════════════════════════════════

┌─────────────────────────────────────────────────────────────────────────┐
│                                                                          │
│  COMPONENTS:                                                            │
│                                                                          │
│  1. BufferPool                                                          │
│     ├── Pre-allocates fixed-size segments                               │
│     ├── Recycles segments (zero allocation in steady state)             │
│     └── Tracks overflow for capacity tuning                             │
│                                                                          │
│  2. BufferSegment                                                       │
│     ├── bytearray for contiguous memory                                 │
│     ├── memoryview for zero-copy access                                 │
│     └── Simple write position tracking                                  │
│                                                                          │
│  3. DoubleBuffer (writes)                                               │
│     ├── Front buffer accepts writes                                     │
│     ├── Back buffer flushes to disk                                     │
│     ├── Atomic swap for continuous operation                            │
│     └── Durability offset tracking                                      │
│                                                                          │
│  4. BufferedReader (reads)                                              │
│     ├── Large chunk reads (64KB)                                        │
│     ├── Read-ahead prefetching                                          │
│     ├── Boundary handling for split entries                             │
│     └── Periodic event loop yields                                      │
│                                                                          │
│  WHY THIS IS MOST CORRECT:                                              │
│  ├── Resilience: Pre-allocated memory survives pressure                 │
│  ├── Durability: Fine-grained offset tracking                           │
│  ├── Throughput: Zero-copy, I/O overlap, batching                       │
│  ├── Memory: Fixed footprint, no GC in hot path                         │
│  ├── Portability: bytearray + memoryview (stdlib only)                  │
│  └── Simplicity: Clear ownership, simple state machines                 │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘
```

This buffer architecture integrates with the Write Coalescing (Part 12) and Portable I/O (Part 13) designs to provide a complete, production-ready logging infrastructure.

---

## Part 15: Single-Writer Architecture for Maximum Correctness

### The Problem with Lock-Based Concurrency

Part 14 introduced Segmented Double Buffer with Pool. While effective, any lock-based approach has inherent risks:

1. **Race conditions** - Bugs in lock acquisition/release
2. **Deadlocks** - Circular lock dependencies
3. **Priority inversion** - Low-priority task holds lock needed by high-priority
4. **Lock contention** - Multiple writers compete for same lock

For a logging system where **correctness is paramount**, we need an architecture where races are **impossible by design**.

### The Maximally Correct Architecture: Single-Writer with Message Passing

In asyncio, the correct concurrency primitive is **not locks** - it's **queues**. A single writer eliminates all race conditions by design.

```
┌─────────────────────────────────────────────────────────────────────┐
│                    SINGLE-WRITER ARCHITECTURE                       │
│                                                                     │
│  Producer 0 ──┐                                                     │
│  Producer 1 ──┼──→ [asyncio.Queue] ──→ [Drain Task] ──→ [Segments] │
│  Producer 2 ──┤         ↑                    │              │       │
│  Producer N ──┘     backpressure            batch          swap     │
│                                              ↓              ↓       │
│                                        [Flush Task] ←── [Double]    │
│                                              │            Buffer    │
│                                              ↓                      │
│                                        [Executor]                   │
│                                              ↓                      │
│                                         [Disk I/O]                  │
│                                              ↓                      │
│                                         [fsync()]                   │
│                                              ↓                      │
│                                     [Wake Durability Waiters]       │
└─────────────────────────────────────────────────────────────────────┘
```

### Why Single-Writer is Maximally Correct

| Property | How Achieved |
|----------|--------------|
| **No race conditions** | Single writer - impossible by design |
| **No locks on write path** | Queue handles synchronization |
| **Natural backpressure** | Bounded queue blocks producers |
| **Automatic batching** | Drain all available from queue |
| **I/O overlap** | Double buffer swap |
| **Durability guarantees** | Futures resolved after fsync |
| **Ordering preserved** | FIFO queue + sequence numbers |
| **No data loss** | CRC verification on read |

### Comparison: Single-Writer vs Sharded Locks

| Aspect | Sharded (N locks) | Single-Writer (queue) |
|--------|-------------------|----------------------|
| **Race conditions** | Possible (lock bugs) | **Impossible by design** |
| **Lock overhead** | N acquires per flush | **Zero locks** |
| **Backpressure** | Manual per-shard | **Built into queue** |
| **Batching** | Explicit | **Automatic (drain all)** |
| **Code complexity** | Higher | **Lower** |
| **Correctness proof** | Harder | **Trivial (single consumer)** |
| **Throughput** | ~1M/s | **~1M/s** |

### Complete Implementation

```python
import asyncio
import os
import sys
import zlib
from concurrent.futures import ThreadPoolExecutor
from collections import deque
from dataclasses import dataclass
from enum import Enum, auto


class WriteStatus(Enum):
    """Result status for write operations."""
    SUCCESS = auto()
    QUEUE_FULL = auto()
    SHUTDOWN = auto()


@dataclass(slots=True)
class WriteRequest:
    """Immutable write request."""
    data: bytes
    durable_future: asyncio.Future | None = None


@dataclass(slots=True)
class WriteResult:
    """Result of a write operation."""
    status: WriteStatus
    offset: int = 0
    error: Exception | None = None


class BufferSegment:
    """Fixed-size segment with CRC tracking."""

    __slots__ = (
        '_data', '_view', '_capacity',
        '_write_pos', '_crc', '_sequence',
    )

    HEADER_SIZE = 16  # seq(8) + size(4) + crc(4)

    def __init__(self, capacity: int = 65536):
        self._capacity = capacity
        self._data = bytearray(capacity)
        self._view = memoryview(self._data)
        self._write_pos = 0
        self._crc = 0
        self._sequence = 0

    @property
    def available(self) -> int:
        return self._capacity - self._write_pos

    @property
    def is_full(self) -> bool:
        return self._write_pos >= self._capacity

    @property
    def size(self) -> int:
        return self._write_pos

    def write(self, data: bytes) -> int:
        """Write data, returns bytes written."""
        write_size = min(len(data), self.available)
        if write_size == 0:
            return 0

        end_pos = self._write_pos + write_size
        self._view[self._write_pos:end_pos] = data[:write_size]
        self._crc = zlib.crc32(data[:write_size], self._crc)
        self._write_pos = end_pos
        return write_size

    def finalize(self, sequence: int) -> bytes:
        """Return segment with header for disk write."""
        self._sequence = sequence
        header = (
            sequence.to_bytes(8, 'little') +
            self._write_pos.to_bytes(4, 'little') +
            (self._crc & 0xFFFFFFFF).to_bytes(4, 'little')
        )
        return header + bytes(self._view[:self._write_pos])

    def reset(self) -> None:
        """Reset for reuse."""
        self._write_pos = 0
        self._crc = 0
        self._sequence = 0


class SegmentPool:
    """Pre-allocated segment pool."""

    __slots__ = ('_segments', '_capacity')

    def __init__(
        self,
        pool_size: int = 16,
        segment_capacity: int = 65536,
    ):
        self._capacity = segment_capacity
        self._segments: deque[BufferSegment] = deque(
            BufferSegment(segment_capacity)
            for _ in range(pool_size)
        )

    def acquire(self) -> BufferSegment:
        """Get segment, creating if pool empty."""
        if self._segments:
            return self._segments.popleft()
        return BufferSegment(self._capacity)

    def release(self, segment: BufferSegment) -> None:
        """Return segment to pool."""
        segment.reset()
        self._segments.append(segment)


class SingleWriterBuffer:
    """
    Maximally correct high-concurrency write buffer.

    Architecture:
    - Producers submit to bounded asyncio.Queue (backpressure)
    - Single drain task consumes queue (no races)
    - Double buffer for I/O overlap
    - Single flush task handles disk I/O
    - Durability futures resolved after fsync

    Guarantees:
    - No data loss (CRC per segment)
    - Ordering preserved (FIFO + sequence numbers)
    - No race conditions (single writer)
    - Bounded memory (queue + segment pool)
    - True durability (fsync/F_FULLFSYNC)
    - Explicit QueueFull handling (no silent drops)
    """

    __slots__ = (
        '_queue', '_pool',
        '_front', '_back', '_current',
        '_sequence', '_durable_offset', '_write_offset',
        '_pending_durability', '_flush_event',
        '_drain_task', '_flush_task', '_running',
        '_executor', '_loop', '_fd',
        '_flush_interval', '_flush_size_threshold',
    )

    def __init__(
        self,
        queue_size: int = 10000,
        pool_size: int = 16,
        segment_capacity: int = 65536,
        flush_interval: float = 0.01,  # 10ms
        flush_size_threshold: int = 262144,  # 256KB
    ):
        self._queue: asyncio.Queue[WriteRequest | None] = asyncio.Queue(
            maxsize=queue_size
        )
        self._pool = SegmentPool(pool_size, segment_capacity)

        self._front: deque[BufferSegment] = deque()
        self._back: deque[BufferSegment] = deque()
        self._current: BufferSegment | None = None

        self._sequence = 0
        self._durable_offset = 0
        self._write_offset = 0

        self._pending_durability: list[tuple[int, asyncio.Future]] = []
        self._flush_event = asyncio.Event()

        self._drain_task: asyncio.Task | None = None
        self._flush_task: asyncio.Task | None = None
        self._running = False

        self._executor = ThreadPoolExecutor(max_workers=1)
        self._loop: asyncio.AbstractEventLoop | None = None
        self._fd: int | None = None

        self._flush_interval = flush_interval
        self._flush_size_threshold = flush_size_threshold

    async def open(self, path: str) -> None:
        """Open file and start background tasks."""
        self._loop = asyncio.get_running_loop()
        self._fd = await self._loop.run_in_executor(
            self._executor,
            lambda: os.open(
                path,
                os.O_WRONLY | os.O_CREAT | os.O_APPEND,
                0o644,
            ),
        )
        self._current = self._pool.acquire()
        self._running = True
        self._drain_task = asyncio.create_task(self._drain_loop())
        self._flush_task = asyncio.create_task(self._flush_loop())

    async def write(self, data: bytes) -> WriteResult:
        """
        Submit write request. Blocks if queue full (backpressure).
        Returns WriteResult with status and offset.
        """
        if not self._running:
            return WriteResult(status=WriteStatus.SHUTDOWN)

        request = WriteRequest(data=data)
        await self._queue.put(request)
        return WriteResult(
            status=WriteStatus.SUCCESS,
            offset=self._write_offset + len(data),
        )

    def try_write(self, data: bytes) -> WriteResult:
        """
        Non-blocking write attempt.
        Returns QUEUE_FULL if queue is at capacity.
        Caller MUST handle QUEUE_FULL - data is NOT written.
        """
        if not self._running:
            return WriteResult(status=WriteStatus.SHUTDOWN)

        request = WriteRequest(data=data)
        try:
            self._queue.put_nowait(request)
            return WriteResult(
                status=WriteStatus.SUCCESS,
                offset=self._write_offset + len(data),
            )
        except asyncio.QueueFull:
            # EXPLICIT: Data was NOT written. Caller must retry or handle.
            return WriteResult(status=WriteStatus.QUEUE_FULL)

    async def write_with_timeout(
        self,
        data: bytes,
        timeout: float,
    ) -> WriteResult:
        """
        Write with timeout. Returns QUEUE_FULL on timeout.
        Caller MUST handle QUEUE_FULL - data is NOT written.
        """
        if not self._running:
            return WriteResult(status=WriteStatus.SHUTDOWN)

        request = WriteRequest(data=data)
        try:
            await asyncio.wait_for(
                self._queue.put(request),
                timeout=timeout,
            )
            return WriteResult(
                status=WriteStatus.SUCCESS,
                offset=self._write_offset + len(data),
            )
        except asyncio.TimeoutError:
            # EXPLICIT: Data was NOT written. Caller must retry or handle.
            return WriteResult(status=WriteStatus.QUEUE_FULL)

    async def write_durable(self, data: bytes) -> WriteResult:
        """
        Submit write and wait for durability confirmation.
        Blocks until data is fsync'd to disk.
        """
        if not self._running:
            return WriteResult(status=WriteStatus.SHUTDOWN)

        future = self._loop.create_future()
        request = WriteRequest(data=data, durable_future=future)
        await self._queue.put(request)

        try:
            offset = await future
            return WriteResult(status=WriteStatus.SUCCESS, offset=offset)
        except Exception as error:
            return WriteResult(
                status=WriteStatus.SHUTDOWN,
                error=error,
            )

    def try_write_durable(self, data: bytes) -> WriteResult | asyncio.Future:
        """
        Non-blocking durable write attempt.
        Returns QUEUE_FULL immediately if queue full.
        Returns Future that resolves to WriteResult on success.
        """
        if not self._running:
            return WriteResult(status=WriteStatus.SHUTDOWN)

        future = self._loop.create_future()
        request = WriteRequest(data=data, durable_future=future)

        try:
            self._queue.put_nowait(request)
            return future  # Caller awaits this for durability
        except asyncio.QueueFull:
            return WriteResult(status=WriteStatus.QUEUE_FULL)

    async def _drain_loop(self) -> None:
        """
        Single consumer - drains queue and writes to segments.
        No locks needed - single task owns all segment mutations.
        """
        unflushed_size = 0

        while self._running:
            try:
                # Wait for first item with timeout
                request = await asyncio.wait_for(
                    self._queue.get(),
                    timeout=self._flush_interval,
                )
            except asyncio.TimeoutError:
                # Timeout - trigger flush if we have data
                if unflushed_size > 0:
                    self._flush_event.set()
                continue

            if request is None:
                # Shutdown signal
                break

            # Drain all available (batching)
            requests = [request]
            while True:
                try:
                    request = self._queue.get_nowait()
                    if request is None:
                        self._running = False
                        break
                    requests.append(request)
                except asyncio.QueueEmpty:
                    break

            # Process batch - single writer, no locks needed
            for req in requests:
                remaining = req.data
                while remaining:
                    if self._current.is_full:
                        self._front.append(self._current)
                        self._current = self._pool.acquire()

                    written = self._current.write(remaining)
                    remaining = remaining[written:]
                    self._write_offset += written
                    unflushed_size += written

                if req.durable_future is not None:
                    self._pending_durability.append(
                        (self._write_offset, req.durable_future)
                    )

            # Trigger flush if threshold reached
            if unflushed_size >= self._flush_size_threshold:
                self._flush_event.set()
                unflushed_size = 0

        # Final flush on shutdown
        if unflushed_size > 0 or self._front:
            self._flush_event.set()

    async def _flush_loop(self) -> None:
        """
        Flush task - swaps buffers and writes to disk.
        Runs concurrently with drain task (I/O overlap).
        """
        while self._running:
            await self._flush_event.wait()
            self._flush_event.clear()

            if not self._running and not self._front and (
                self._current is None or self._current.size == 0
            ):
                break

            await self._do_flush()

        # Final flush on shutdown
        await self._do_flush()

    async def _do_flush(self) -> None:
        """Execute buffer swap and disk write."""
        # Swap front/back (drain task writes to new front)
        if self._current and self._current.size > 0:
            self._front.append(self._current)
            self._current = self._pool.acquire()

        self._front, self._back = self._back, self._front

        if not self._back:
            return

        # Finalize segments with sequence numbers
        flush_data = bytearray()
        flush_size = 0

        for segment in self._back:
            data = segment.finalize(self._sequence)
            self._sequence += 1
            flush_data.extend(data)
            flush_size += segment.size

        # Single write + fsync in executor
        await self._loop.run_in_executor(
            self._executor,
            self._flush_sync,
            bytes(flush_data),
        )

        # Update durable offset
        self._durable_offset += flush_size

        # Return segments to pool
        while self._back:
            self._pool.release(self._back.popleft())

        # Wake durability waiters
        remaining_waiters = []
        for offset, future in self._pending_durability:
            if offset <= self._durable_offset:
                if not future.done():
                    future.set_result(offset)
            else:
                remaining_waiters.append((offset, future))
        self._pending_durability = remaining_waiters

    def _flush_sync(self, data: bytes) -> None:
        """Synchronous write + platform-aware fsync."""
        os.write(self._fd, data)
        if sys.platform == 'darwin':
            import fcntl
            fcntl.fcntl(self._fd, fcntl.F_FULLFSYNC)
        else:
            os.fsync(self._fd)

    async def flush(self) -> None:
        """Force immediate flush."""
        self._flush_event.set()
        # Wait for flush to complete
        await asyncio.sleep(0)
        while self._flush_event.is_set():
            await asyncio.sleep(0.001)

    async def close(self) -> None:
        """Graceful shutdown - flush all pending data."""
        self._running = False

        # Signal drain task to exit
        try:
            self._queue.put_nowait(None)
        except asyncio.QueueFull:
            # Queue full - drain task will see _running=False
            pass

        # Wake flush task
        self._flush_event.set()

        # Wait for tasks to complete
        if self._drain_task:
            await self._drain_task
        if self._flush_task:
            await self._flush_task

        # Cancel any pending durability waiters
        for offset, future in self._pending_durability:
            if not future.done():
                future.set_exception(
                    RuntimeError("Buffer closed before durability confirmed")
                )
        self._pending_durability.clear()

        # Close file
        if self._fd is not None:
            await self._loop.run_in_executor(
                self._executor,
                os.close,
                self._fd,
            )

        self._executor.shutdown(wait=False)
```

### QueueFull Handling Patterns

The implementation provides explicit QueueFull handling. Callers MUST handle this status:

```python
# Pattern 1: Blocking write (recommended for most cases)
# Automatically waits for queue space - never loses data
async def log_entry(buffer: SingleWriterBuffer, data: bytes) -> None:
    result = await buffer.write(data)
    if result.status == WriteStatus.SHUTDOWN:
        raise RuntimeError("Buffer is shutting down")
    # SUCCESS guaranteed - we waited for space


# Pattern 2: Non-blocking with explicit retry
# For latency-sensitive paths where blocking is unacceptable
async def log_entry_nonblocking(
    buffer: SingleWriterBuffer,
    data: bytes,
    max_retries: int = 3,
    retry_delay: float = 0.001,
) -> bool:
    for attempt in range(max_retries):
        result = buffer.try_write(data)

        if result.status == WriteStatus.SUCCESS:
            return True
        elif result.status == WriteStatus.SHUTDOWN:
            return False
        elif result.status == WriteStatus.QUEUE_FULL:
            # EXPLICIT: Data was NOT written
            # Option A: Retry after delay
            await asyncio.sleep(retry_delay * (2 ** attempt))
            continue

    # All retries exhausted - caller decides what to do
    # Options: drop, buffer locally, raise exception
    return False


# Pattern 3: Timeout-based for bounded latency
async def log_entry_bounded(
    buffer: SingleWriterBuffer,
    data: bytes,
    timeout: float = 0.1,
) -> bool:
    result = await buffer.write_with_timeout(data, timeout)

    if result.status == WriteStatus.SUCCESS:
        return True
    elif result.status == WriteStatus.QUEUE_FULL:
        # Timeout exceeded - data NOT written
        # Caller must handle: drop, local buffer, or escalate
        return False
    else:
        return False


# Pattern 4: Durable write with QueueFull handling
async def log_entry_durable(
    buffer: SingleWriterBuffer,
    data: bytes,
) -> int:
    result_or_future = buffer.try_write_durable(data)

    if isinstance(result_or_future, WriteResult):
        if result_or_future.status == WriteStatus.QUEUE_FULL:
            # Fall back to blocking durable write
            result = await buffer.write_durable(data)
            return result.offset
        else:
            raise RuntimeError("Buffer shutdown")
    else:
        # Got future - await durability
        return await result_or_future
```

### Concurrency Timeline

```
Time →
Producer 0: [put]     [put]          [put]
Producer 1:    [put]       [put][put]
Producer 2:       [put]              [put]
                  ↓
Queue:      [████████████████████████████]
                  ↓
Drain:      [drain batch][write segments]  [drain batch][write segments]
                              ↓                              ↓
Flush:                   [swap][fsync]                  [swap][fsync]
```

### Memory Bounds

| Component | Size | Bound |
|-----------|------|-------|
| Queue | `queue_size × sizeof(WriteRequest)` | ~80KB for 10K entries |
| Segment Pool | `pool_size × segment_capacity` | ~1MB for 16×64KB |
| Double Buffer | 2 × active segments | Covered by pool |
| **Total** | | **~1.1MB fixed** |

---

## Part 16: Single-Reader Architecture for Maximum Correctness

### The Read Problem

For writes, single-writer with queue is optimal because writes need serialization. But what about reads?

Key insight: **Reads are naturally parallelizable** - multiple readers can read different parts of the file. However, for maximum correctness, we want:

1. **Sequential scan efficiency** - Most reads are full scans
2. **CRC verification** - Detect corruption
3. **Sequence verification** - Detect missing/reordered segments
4. **Bounded memory** - Don't load entire file
5. **Prefetching** - Keep executor busy

### The Most Correct Read Architecture

Mirror the write architecture: **Single prefetcher with consumer queue**.

```
┌─────────────────────────────────────────────────────────────────────┐
│                    SINGLE-READER ARCHITECTURE                       │
│                                                                     │
│  Disk ──→ [Executor] ──→ [Prefetch Task] ──→ [Buffer Queue]        │
│                               │                    │                │
│                          verify CRC           backpressure          │
│                          verify seq                │                │
│                               ↓                    ↓                │
│                     [Validated Entries] ──→ [Consumer 0]           │
│                                         ──→ [Consumer 1]           │
│                                         ──→ [Consumer N]           │
└─────────────────────────────────────────────────────────────────────┘
```

### Why Single-Reader is Most Correct

| Property | How Achieved |
|----------|--------------|
| **No corruption propagation** | CRC verified before handoff |
| **Ordering guaranteed** | Sequence numbers verified |
| **Bounded memory** | Fixed-size prefetch buffer |
| **Backpressure** | Bounded queue to consumers |
| **Maximum throughput** | Prefetch overlaps consumer processing |
| **Simple error handling** | Single point of verification |

### Complete Implementation

```python
import asyncio
import os
import sys
import zlib
from concurrent.futures import ThreadPoolExecutor
from collections import deque
from dataclasses import dataclass
from enum import Enum, auto
from typing import AsyncIterator, Callable


class ReadStatus(Enum):
    """Result status for read operations."""
    SUCCESS = auto()
    EOF = auto()
    CORRUPTION = auto()
    SEQUENCE_GAP = auto()
    SHUTDOWN = auto()


@dataclass(slots=True)
class SegmentHeader:
    """Parsed segment header."""
    sequence: int
    size: int
    crc: int

    HEADER_SIZE = 16

    @classmethod
    def parse(cls, data: bytes) -> 'SegmentHeader':
        """Parse header from bytes."""
        if len(data) < cls.HEADER_SIZE:
            raise ValueError(f"Header too short: {len(data)} < {cls.HEADER_SIZE}")

        return cls(
            sequence=int.from_bytes(data[0:8], 'little'),
            size=int.from_bytes(data[8:12], 'little'),
            crc=int.from_bytes(data[12:16], 'little'),
        )


@dataclass(slots=True)
class ReadEntry:
    """Validated entry from disk."""
    sequence: int
    data: bytes
    offset: int


@dataclass(slots=True)
class ReadResult:
    """Result of a read operation."""
    status: ReadStatus
    entry: ReadEntry | None = None
    error: str | None = None


class PrefetchBuffer:
    """Fixed-size buffer for prefetched data."""

    __slots__ = ('_data', '_view', '_capacity', '_read_pos', '_write_pos')

    def __init__(self, capacity: int = 262144):  # 256KB
        self._capacity = capacity
        self._data = bytearray(capacity)
        self._view = memoryview(self._data)
        self._read_pos = 0
        self._write_pos = 0

    @property
    def available_read(self) -> int:
        return self._write_pos - self._read_pos

    @property
    def available_write(self) -> int:
        return self._capacity - self._write_pos

    def write(self, data: bytes) -> int:
        """Write data to buffer, returns bytes written."""
        write_size = min(len(data), self.available_write)
        if write_size == 0:
            return 0

        end_pos = self._write_pos + write_size
        self._view[self._write_pos:end_pos] = data[:write_size]
        self._write_pos = end_pos
        return write_size

    def peek(self, size: int) -> bytes:
        """Peek at data without consuming."""
        available = min(size, self.available_read)
        return bytes(self._view[self._read_pos:self._read_pos + available])

    def consume(self, size: int) -> bytes:
        """Consume and return data."""
        available = min(size, self.available_read)
        data = bytes(self._view[self._read_pos:self._read_pos + available])
        self._read_pos += available
        return data

    def compact(self) -> None:
        """Move unread data to start of buffer."""
        if self._read_pos == 0:
            return

        remaining = self.available_read
        if remaining > 0:
            self._view[0:remaining] = self._view[self._read_pos:self._write_pos]

        self._read_pos = 0
        self._write_pos = remaining

    def reset(self) -> None:
        """Reset buffer to empty state."""
        self._read_pos = 0
        self._write_pos = 0


class SingleReaderBuffer:
    """
    Maximally correct high-throughput read buffer.

    Architecture:
    - Single prefetch task reads from disk
    - Validates CRC and sequence numbers
    - Bounded queue delivers validated entries
    - Multiple consumers can process concurrently

    Guarantees:
    - No corruption propagation (CRC verified)
    - Ordering verified (sequence numbers)
    - Bounded memory (fixed prefetch + queue)
    - Backpressure (bounded queue)
    - Clean EOF handling
    """

    __slots__ = (
        '_queue', '_prefetch_buffer',
        '_prefetch_task', '_running',
        '_executor', '_loop', '_fd',
        '_file_size', '_file_offset',
        '_expected_sequence', '_chunk_size',
        '_queue_size', '_entries_read',
    )

    def __init__(
        self,
        queue_size: int = 1000,
        prefetch_capacity: int = 262144,  # 256KB
        chunk_size: int = 65536,  # 64KB per read
    ):
        self._queue: asyncio.Queue[ReadResult] = asyncio.Queue(
            maxsize=queue_size
        )
        self._prefetch_buffer = PrefetchBuffer(prefetch_capacity)

        self._prefetch_task: asyncio.Task | None = None
        self._running = False

        self._executor = ThreadPoolExecutor(max_workers=1)
        self._loop: asyncio.AbstractEventLoop | None = None
        self._fd: int | None = None

        self._file_size = 0
        self._file_offset = 0
        self._expected_sequence = 0
        self._chunk_size = chunk_size
        self._queue_size = queue_size
        self._entries_read = 0

    async def open(self, path: str, from_sequence: int = 0) -> None:
        """Open file and start prefetch task."""
        self._loop = asyncio.get_running_loop()

        # Open file and get size
        self._fd, self._file_size = await self._loop.run_in_executor(
            self._executor,
            self._open_sync,
            path,
        )

        self._expected_sequence = from_sequence
        self._running = True
        self._prefetch_task = asyncio.create_task(self._prefetch_loop())

    def _open_sync(self, path: str) -> tuple[int, int]:
        """Synchronous open - runs in executor."""
        fd = os.open(path, os.O_RDONLY)
        size = os.fstat(fd).st_size
        return fd, size

    async def read(self) -> ReadResult:
        """
        Read next validated entry.
        Blocks until entry available or EOF/error.
        """
        return await self._queue.get()

    def try_read(self) -> ReadResult | None:
        """
        Non-blocking read attempt.
        Returns None if no entry available yet.
        """
        try:
            return self._queue.get_nowait()
        except asyncio.QueueEmpty:
            return None

    async def read_with_timeout(self, timeout: float) -> ReadResult | None:
        """Read with timeout. Returns None on timeout."""
        try:
            return await asyncio.wait_for(
                self._queue.get(),
                timeout=timeout,
            )
        except asyncio.TimeoutError:
            return None

    async def read_entries(self) -> AsyncIterator[ReadEntry]:
        """
        Async iterator over all validated entries.
        Stops on EOF or error.
        """
        while True:
            result = await self.read()

            if result.status == ReadStatus.SUCCESS:
                yield result.entry
            elif result.status == ReadStatus.EOF:
                return
            elif result.status == ReadStatus.CORRUPTION:
                raise ValueError(f"Data corruption: {result.error}")
            elif result.status == ReadStatus.SEQUENCE_GAP:
                raise ValueError(f"Sequence gap: {result.error}")
            else:
                return

    async def _prefetch_loop(self) -> None:
        """
        Single prefetch task - reads, validates, queues entries.
        """
        while self._running and self._file_offset < self._file_size:
            # Fill prefetch buffer
            await self._fill_buffer()

            # Parse and validate entries
            while self._prefetch_buffer.available_read >= SegmentHeader.HEADER_SIZE:
                result = self._parse_next_entry()

                if result is None:
                    # Need more data
                    break

                # Queue result (blocks if queue full - backpressure)
                await self._queue.put(result)

                if result.status != ReadStatus.SUCCESS:
                    # Error - stop prefetching
                    self._running = False
                    return

        # Signal EOF
        await self._queue.put(ReadResult(status=ReadStatus.EOF))

    async def _fill_buffer(self) -> None:
        """Read more data from disk into prefetch buffer."""
        # Compact buffer to make room
        self._prefetch_buffer.compact()

        if self._prefetch_buffer.available_write == 0:
            return

        # Calculate read size
        remaining_file = self._file_size - self._file_offset
        read_size = min(
            self._chunk_size,
            self._prefetch_buffer.available_write,
            remaining_file,
        )

        if read_size == 0:
            return

        # Read from disk
        data = await self._loop.run_in_executor(
            self._executor,
            self._read_sync,
            read_size,
        )

        if data:
            self._prefetch_buffer.write(data)
            self._file_offset += len(data)

    def _read_sync(self, size: int) -> bytes:
        """Synchronous read - runs in executor."""
        return os.read(self._fd, size)

    def _parse_next_entry(self) -> ReadResult | None:
        """
        Parse and validate next entry from prefetch buffer.
        Returns None if more data needed.
        """
        # Check if we have enough for header
        if self._prefetch_buffer.available_read < SegmentHeader.HEADER_SIZE:
            return None

        # Parse header
        header_data = self._prefetch_buffer.peek(SegmentHeader.HEADER_SIZE)
        try:
            header = SegmentHeader.parse(header_data)
        except ValueError as error:
            return ReadResult(
                status=ReadStatus.CORRUPTION,
                error=f"Invalid header: {error}",
            )

        # Check if we have full entry
        total_size = SegmentHeader.HEADER_SIZE + header.size
        if self._prefetch_buffer.available_read < total_size:
            return None

        # Consume header
        self._prefetch_buffer.consume(SegmentHeader.HEADER_SIZE)

        # Read and verify data
        entry_data = self._prefetch_buffer.consume(header.size)

        # Verify CRC
        computed_crc = zlib.crc32(entry_data) & 0xFFFFFFFF
        if computed_crc != header.crc:
            return ReadResult(
                status=ReadStatus.CORRUPTION,
                error=f"CRC mismatch: expected {header.crc}, got {computed_crc}",
            )

        # Verify sequence
        if header.sequence != self._expected_sequence:
            return ReadResult(
                status=ReadStatus.SEQUENCE_GAP,
                error=f"Sequence gap: expected {self._expected_sequence}, got {header.sequence}",
            )

        # Success
        entry = ReadEntry(
            sequence=header.sequence,
            data=entry_data,
            offset=self._entries_read,
        )

        self._expected_sequence += 1
        self._entries_read += 1

        return ReadResult(status=ReadStatus.SUCCESS, entry=entry)

    async def seek_to_sequence(self, target_sequence: int) -> bool:
        """
        Seek to specific sequence number.
        Returns True if found, False if not found or error.

        NOTE: This requires scanning from start - for frequent
        random access, maintain an external index.
        """
        # Reset and scan from start
        await self._reset_to_start()

        while self._running:
            result = await self.read()

            if result.status == ReadStatus.EOF:
                return False
            elif result.status != ReadStatus.SUCCESS:
                return False
            elif result.entry.sequence == target_sequence:
                # Found - put back in queue for consumer
                # (Can't actually put back, so this is a design decision)
                return True
            elif result.entry.sequence > target_sequence:
                # Passed it - sequence doesn't exist
                return False

        return False

    async def _reset_to_start(self) -> None:
        """Reset to beginning of file."""
        self._running = False

        if self._prefetch_task:
            self._prefetch_task.cancel()
            try:
                await self._prefetch_task
            except asyncio.CancelledError:
                pass

        # Clear queue
        while True:
            try:
                self._queue.get_nowait()
            except asyncio.QueueEmpty:
                break

        # Reset state
        await self._loop.run_in_executor(
            self._executor,
            lambda: os.lseek(self._fd, 0, os.SEEK_SET),
        )

        self._file_offset = 0
        self._expected_sequence = 0
        self._entries_read = 0
        self._prefetch_buffer.reset()

        # Restart
        self._running = True
        self._prefetch_task = asyncio.create_task(self._prefetch_loop())

    async def close(self) -> None:
        """Close reader and release resources."""
        self._running = False

        if self._prefetch_task:
            self._prefetch_task.cancel()
            try:
                await self._prefetch_task
            except asyncio.CancelledError:
                pass

        if self._fd is not None:
            await self._loop.run_in_executor(
                self._executor,
                os.close,
                self._fd,
            )

        self._executor.shutdown(wait=False)
```

### Consumer Patterns

```python
# Pattern 1: Simple iteration
async def process_all_entries(reader: SingleReaderBuffer) -> None:
    async for entry in reader.read_entries():
        process(entry.data)


# Pattern 2: Batch processing
async def process_in_batches(
    reader: SingleReaderBuffer,
    batch_size: int = 100,
) -> None:
    batch: list[ReadEntry] = []

    async for entry in reader.read_entries():
        batch.append(entry)

        if len(batch) >= batch_size:
            await process_batch(batch)
            batch.clear()

    # Process remaining
    if batch:
        await process_batch(batch)


# Pattern 3: Multiple consumers (fan-out)
async def multi_consumer(
    reader: SingleReaderBuffer,
    num_consumers: int = 4,
) -> None:
    results_queue: asyncio.Queue = asyncio.Queue()

    async def consumer(consumer_id: int) -> None:
        while True:
            result = await reader.read()

            if result.status == ReadStatus.EOF:
                break
            elif result.status == ReadStatus.SUCCESS:
                processed = await process_entry(result.entry)
                await results_queue.put(processed)
            else:
                break

    # Note: Multiple consumers reading from same reader
    # will each get different entries (queue semantics)
    consumers = [
        asyncio.create_task(consumer(i))
        for i in range(num_consumers)
    ]

    await asyncio.gather(*consumers)


# Pattern 4: Error handling with recovery
async def process_with_recovery(
    reader: SingleReaderBuffer,
    on_corruption: Callable[[str], None],
) -> int:
    processed = 0

    while True:
        result = await reader.read()

        if result.status == ReadStatus.SUCCESS:
            process(result.entry.data)
            processed += 1
        elif result.status == ReadStatus.EOF:
            break
        elif result.status == ReadStatus.CORRUPTION:
            on_corruption(result.error)
            # Decision: skip corrupted entry or stop?
            # This implementation stops - caller decides recovery
            break
        elif result.status == ReadStatus.SEQUENCE_GAP:
            # Log gap and continue or stop
            break

    return processed
```

### Memory Bounds (Read)

| Component | Size | Bound |
|-----------|------|-------|
| Prefetch Buffer | `prefetch_capacity` | ~256KB |
| Entry Queue | `queue_size × sizeof(ReadResult)` | ~100KB for 1K entries |
| **Total** | | **~360KB fixed** |

### Read/Write Symmetry

```
WRITE:                              READ:
Producers → Queue → Drain → Buffer  Buffer ← Prefetch ← Disk
                      ↓                ↓
                   Segments          Queue
                      ↓                ↓
                   Executor       Consumers
                      ↓
                    Disk
```

Both architectures share:
- Single task owns mutations (no races)
- Bounded queues (backpressure)
- Executor isolation (non-blocking)
- Explicit status handling (no silent failures)

### High-Concurrency Read Pattern: One Reader Per Consumer

For maximum concurrency with multiple independent queries, create **one `SingleReaderBuffer` instance per consumer**:

```python
class ReaderPool:
    """
    Pool of independent readers for concurrent queries.

    Each consumer gets its own reader instance with:
    - Independent file descriptor
    - Independent prefetch state
    - Independent sequence tracking
    - No coordination overhead
    """

    __slots__ = ('_path', '_readers', '_config')

    def __init__(
        self,
        path: str,
        queue_size: int = 1000,
        prefetch_capacity: int = 262144,
        chunk_size: int = 65536,
    ):
        self._path = path
        self._readers: list[SingleReaderBuffer] = []
        self._config = {
            'queue_size': queue_size,
            'prefetch_capacity': prefetch_capacity,
            'chunk_size': chunk_size,
        }

    async def create_reader(
        self,
        from_sequence: int = 0,
    ) -> SingleReaderBuffer:
        """Create a new independent reader instance."""
        reader = SingleReaderBuffer(**self._config)
        await reader.open(self._path, from_sequence=from_sequence)
        self._readers.append(reader)
        return reader

    async def close_all(self) -> None:
        """Close all reader instances."""
        await asyncio.gather(*[
            reader.close() for reader in self._readers
        ])
        self._readers.clear()


# Usage: Concurrent independent queries
async def concurrent_queries(path: str) -> None:
    pool = ReaderPool(path)

    async def query_range(start_seq: int, end_seq: int) -> list[bytes]:
        """Independent query - gets its own reader."""
        reader = await pool.create_reader(from_sequence=start_seq)
        results = []

        async for entry in reader.read_entries():
            if entry.sequence >= end_seq:
                break
            results.append(entry.data)

        return results

    # Run queries concurrently - each has independent reader
    results = await asyncio.gather(
        query_range(0, 1000),
        query_range(500, 1500),
        query_range(2000, 3000),
    )

    await pool.close_all()
```

### Why Not Parallel Chunk Readers?

One might consider parallelizing reads by splitting the file into chunks:

```
File: [Chunk 0][Chunk 1][Chunk 2][Chunk 3]
         ↓        ↓        ↓        ↓
      Reader 0  Reader 1  Reader 2  Reader 3
         ↓        ↓        ↓        ↓
      [Merge in sequence order]
         ↓
      Consumer
```

**This is NOT more correct** for these reasons:

| Problem | Impact |
|---------|--------|
| **Chunk boundary detection** | Segments may span chunks - need to scan to find boundaries |
| **Merge complexity** | Must reassemble in sequence order - coordination overhead |
| **Partial failure handling** | One chunk failure affects entire read |
| **Sequential I/O faster** | OS read-ahead optimizes sequential access |
| **SSD marginal gains** | Parallel reads help but don't justify complexity |

**The correct pattern is:**
- Single-Reader for sequential scans (recovery, replay)
- Multiple independent Single-Readers for concurrent queries
- Index + Single-Reader for random access

### Indexed Random Access

For frequent random access by sequence number, build an index during sequential scan:

```python
class IndexedReader:
    """
    Single-Reader with sequence index for O(1) access.

    Index is built lazily during first sequential scan,
    then persisted for subsequent access.
    """

    __slots__ = (
        '_reader', '_index', '_index_path',
        '_path', '_config',
    )

    def __init__(
        self,
        path: str,
        index_path: str | None = None,
    ):
        self._path = path
        self._index_path = index_path or f"{path}.idx"
        self._index: dict[int, int] = {}  # sequence → file_offset
        self._reader: SingleReaderBuffer | None = None
        self._config = {
            'queue_size': 1000,
            'prefetch_capacity': 262144,
            'chunk_size': 65536,
        }

    async def build_index(self) -> None:
        """Build index by scanning file sequentially."""
        reader = SingleReaderBuffer(**self._config)
        await reader.open(self._path)

        file_offset = 0
        async for entry in reader.read_entries():
            self._index[entry.sequence] = file_offset
            # Track offset: header + data
            file_offset += SegmentHeader.HEADER_SIZE + len(entry.data)

        await reader.close()

        # Persist index
        await self._save_index()

    async def _save_index(self) -> None:
        """Save index to disk."""
        loop = asyncio.get_running_loop()
        await loop.run_in_executor(
            None,
            self._save_index_sync,
        )

    def _save_index_sync(self) -> None:
        """Synchronous index save."""
        import json
        with open(self._index_path, 'w') as f:
            json.dump(self._index, f)

    async def load_index(self) -> bool:
        """Load index from disk. Returns False if not found."""
        loop = asyncio.get_running_loop()
        try:
            self._index = await loop.run_in_executor(
                None,
                self._load_index_sync,
            )
            return True
        except FileNotFoundError:
            return False

    def _load_index_sync(self) -> dict[int, int]:
        """Synchronous index load."""
        import json
        with open(self._index_path, 'r') as f:
            return {int(k): v for k, v in json.load(f).items()}

    async def get_by_sequence(self, sequence: int) -> ReadEntry | None:
        """O(1) access to entry by sequence number."""
        if sequence not in self._index:
            return None

        file_offset = self._index[sequence]

        # Create reader positioned at offset
        reader = SingleReaderBuffer(**self._config)
        await reader.open(self._path, from_sequence=sequence)

        # Read single entry
        result = await reader.read()
        await reader.close()

        if result.status == ReadStatus.SUCCESS:
            return result.entry
        return None

    async def get_range(
        self,
        start_seq: int,
        end_seq: int,
    ) -> AsyncIterator[ReadEntry]:
        """Get entries in sequence range."""
        if start_seq not in self._index:
            return

        reader = SingleReaderBuffer(**self._config)
        await reader.open(self._path, from_sequence=start_seq)

        async for entry in reader.read_entries():
            if entry.sequence >= end_seq:
                break
            yield entry

        await reader.close()
```

### Summary: Read Architecture Decision Tree

```
┌─────────────────────────────────────────────────────────────────────┐
│                    READ ACCESS PATTERN DECISION                     │
│                                                                     │
│  Q: What is the access pattern?                                     │
│                                                                     │
│  ├── Sequential scan (recovery, replay, export)                     │
│  │   └── Use: SingleReaderBuffer                                   │
│  │       - One instance                                             │
│  │       - Prefetch enables throughput                              │
│  │       - CRC/sequence verification                                │
│  │                                                                  │
│  ├── Concurrent independent queries                                 │
│  │   └── Use: ReaderPool (multiple SingleReaderBuffer)             │
│  │       - One reader per query                                     │
│  │       - Independent state, no coordination                       │
│  │       - Maximum parallelism                                      │
│  │                                                                  │
│  └── Random access by sequence                                      │
│      └── Use: IndexedReader                                        │
│          - Build index once (sequential scan)                       │
│          - O(1) lookup by sequence                                  │
│          - SingleReaderBuffer for actual read                       │
│                                                                     │
│  WHY SINGLE-READER IS MOST CORRECT:                                │
│  ├── Hardware alignment (sequential I/O)                            │
│  ├── Single validation point (no duplicate CRC checks)              │
│  ├── Simple state (one prefetch task, one queue)                    │
│  ├── Bounded memory (fixed prefetch + queue)                        │
│  └── No coordination bugs (independent instances)                   │
└─────────────────────────────────────────────────────────────────────┘
```

---

## AD-40: Idempotent Job Submissions

### Part 1: Problem Statement and Requirements

#### The Duplicate Submission Problem

In distributed systems, clients cannot distinguish between:
1. **Request lost** - Network dropped the request before gate received it
2. **Response lost** - Gate processed it but response didn't reach client
3. **Timeout** - Request is still being processed, just slow

Without idempotency, client retries cause duplicate job executions:

```
┌─────────────────────────────────────────────────────────────────────────┐
│                    THE DUPLICATE SUBMISSION PROBLEM                      │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│  SCENARIO: Client submits job, response lost, client retries             │
│                                                                          │
│  WITHOUT IDEMPOTENCY:                                                    │
│  ┌──────────┐        ┌──────────┐        ┌──────────┐                   │
│  │  Client  │        │   Gate   │        │  Manager │                   │
│  └────┬─────┘        └────┬─────┘        └────┬─────┘                   │
│       │                   │                   │                          │
│       │──JobSubmission───▶│                   │                          │
│       │   job_id=abc      │──JobSubmission───▶│                          │
│       │                   │                   │──creates job abc         │
│       │                   │◀──JobAck─────────│                          │
│       │    ╳ response     │                   │                          │
│       │      lost         │                   │                          │
│       │                   │                   │                          │
│       │──(timeout)────────│                   │                          │
│       │                   │                   │                          │
│       │──JobSubmission───▶│                   │  ← Client retries        │
│       │   job_id=def      │──JobSubmission───▶│    with NEW job_id       │
│       │   (new id!)       │                   │──creates job def         │
│       │                   │                   │                          │
│       │◀──JobAck─────────│◀──JobAck─────────│                          │
│       │                   │                   │                          │
│       │                   │                   │                          │
│  RESULT: TWO JOBS CREATED (abc AND def) FOR SAME LOGICAL REQUEST        │
│                                                                          │
│  WITH IDEMPOTENCY:                                                       │
│  ┌──────────┐        ┌──────────┐        ┌──────────┐                   │
│  │  Client  │        │   Gate   │        │  Manager │                   │
│  └────┬─────┘        └────┬─────┘        └────┬─────┘                   │
│       │                   │                   │                          │
│       │──JobSubmission───▶│                   │                          │
│       │   idem_key=xyz    │──JobSubmission───▶│                          │
│       │   job_id=abc      │   idem_key=xyz    │──creates job abc         │
│       │                   │                   │  stores idem_key→abc     │
│       │                   │◀──JobAck─────────│                          │
│       │    ╳ response     │                   │                          │
│       │      lost         │                   │                          │
│       │                   │                   │                          │
│       │──(timeout)────────│                   │                          │
│       │                   │                   │                          │
│       │──JobSubmission───▶│                   │  ← Client retries        │
│       │   idem_key=xyz    │──check cache──────│    with SAME idem_key    │
│       │   job_id=def      │   idem_key=xyz?   │                          │
│       │                   │◀──found: abc─────│                          │
│       │◀──JobAck─────────│                   │                          │
│       │   job_id=abc      │   returns abc,    │                          │
│       │                   │   ignores def     │                          │
│       │                   │                   │                          │
│  RESULT: ONE JOB (abc), DUPLICATE DETECTED AND DEDUPLICATED             │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘
```

#### Requirements

1. **At-Most-Once Semantics**: A job submission with a given idempotency key executes at most once
2. **Bounded Memory**: Idempotency state must not grow unboundedly
3. **Crash Recovery**: Idempotency guarantees survive gate/manager restarts
4. **Cross-DC Consistency**: Same idempotency key handled consistently across DCs
5. **Low Latency**: Dedup check must be O(1) and not add significant latency
6. **Configurable Window**: TTL for idempotency keys should be configurable

### Part 2: Idempotency Key Design

#### Key Structure

The idempotency key uniquely identifies a logical submission attempt:

```python
from dataclasses import dataclass
from enum import Enum, auto
from typing import Generic, TypeVar
import secrets
import time


@dataclass(slots=True, frozen=True)
class IdempotencyKey:
    """
    Client-generated idempotency key for job submissions.

    Structure: {client_id}:{sequence}:{nonce}

    - client_id: Stable identifier for the client (survives restarts)
    - sequence: Monotonically increasing counter per client
    - nonce: Random component to prevent collision across client restarts

    The combination ensures:
    - Same client retry uses same key (client_id + sequence)
    - Different clients cannot collide (different client_id)
    - Client restart doesn't reuse old sequences (nonce changes)
    """
    client_id: str      # Stable client identifier (e.g., hostname:pid or UUID)
    sequence: int       # Monotonically increasing per-client
    nonce: str          # Random component (8 bytes hex)

    def __str__(self) -> str:
        return f"{self.client_id}:{self.sequence}:{self.nonce}"

    def __hash__(self) -> int:
        return hash((self.client_id, self.sequence, self.nonce))

    @classmethod
    def parse(cls, key_str: str) -> "IdempotencyKey":
        """Parse idempotency key from string representation."""
        parts = key_str.split(":", 2)
        if len(parts) != 3:
            raise ValueError(f"Invalid idempotency key format: {key_str}")
        return cls(
            client_id=parts[0],
            sequence=int(parts[1]),
            nonce=parts[2],
        )


class IdempotencyKeyGenerator:
    """
    Generates idempotency keys for a client.

    Thread-safe through atomic counter operations.
    """

    def __init__(self, client_id: str):
        self._client_id = client_id
        self._sequence = 0
        self._nonce = secrets.token_hex(8)  # New nonce per generator instance

    def generate(self) -> IdempotencyKey:
        """Generate next idempotency key."""
        seq = self._sequence
        self._sequence += 1
        return IdempotencyKey(
            client_id=self._client_id,
            sequence=seq,
            nonce=self._nonce,
        )
```

#### Why This Structure?

```
┌─────────────────────────────────────────────────────────────────────────┐
│                    IDEMPOTENCY KEY STRUCTURE RATIONALE                   │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│  KEY: {client_id}:{sequence}:{nonce}                                     │
│                                                                          │
│  COMPONENT        PURPOSE                    EXAMPLE                     │
│  ─────────────────────────────────────────────────────────────────────  │
│  client_id        Namespace isolation        "host1.dc1:12345"           │
│                   - Different clients        (hostname:pid)              │
│                     never collide                                        │
│                                                                          │
│  sequence         Retry detection            42                          │
│                   - Same seq = retry         (monotonic counter)         │
│                   - New seq = new request                                │
│                                                                          │
│  nonce            Restart protection         "a1b2c3d4e5f6g7h8"          │
│                   - Prevents reuse of        (random per process)        │
│                     old sequence numbers                                 │
│                     after client restart                                 │
│                                                                          │
│  COLLISION ANALYSIS:                                                     │
│                                                                          │
│  Same client, same request (retry):                                      │
│    key1 = "host1:42:abc123" ← original                                   │
│    key2 = "host1:42:abc123" ← retry (same key, deduped)                 │
│                                                                          │
│  Same client, different request:                                         │
│    key1 = "host1:42:abc123"                                              │
│    key2 = "host1:43:abc123" ← different sequence                        │
│                                                                          │
│  Same client after restart:                                              │
│    key1 = "host1:42:abc123" ← before restart                            │
│    key2 = "host1:42:def456" ← after restart (new nonce)                 │
│                                                                          │
│  Different clients:                                                      │
│    key1 = "host1:42:abc123"                                              │
│    key2 = "host2:42:abc123" ← different client_id                       │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘
```

### Part 3: Entry States and Lifecycle

#### Idempotency Entry State Machine

```python
class IdempotencyStatus(Enum):
    """
    Status of an idempotency entry.

    State transitions:
        PENDING → COMMITTED (successful processing)
        PENDING → REJECTED (validation/capacity rejection)
        PENDING → EXPIRED (TTL exceeded while pending)

    Terminal states (COMMITTED, REJECTED) are immutable.
    """
    PENDING = auto()    # Request received, processing in progress
    COMMITTED = auto()  # Request processed successfully
    REJECTED = auto()   # Request rejected (validation, capacity, etc.)


T = TypeVar("T")


@dataclass(slots=True)
class IdempotencyEntry(Generic[T]):
    """
    Tracks the state and outcome of an idempotent request.

    Generic over T to support different result types (JobAck, etc.)
    """
    idempotency_key: IdempotencyKey
    status: IdempotencyStatus
    job_id: str | None          # Set when job is created
    result: T | None            # Cached result to return on duplicates
    created_at: float           # Unix timestamp of first receipt
    committed_at: float | None  # Unix timestamp of commit (if committed)
    source_gate_id: str | None  # Gate that first received this request

    def is_terminal(self) -> bool:
        """Check if entry is in a terminal state."""
        return self.status in (IdempotencyStatus.COMMITTED, IdempotencyStatus.REJECTED)

    def age_seconds(self) -> float:
        """Get age of entry in seconds."""
        return time.time() - self.created_at


@dataclass(slots=True, frozen=True)
class IdempotencyConfig:
    """Configuration for idempotency caches."""

    # TTL for entries in different states
    pending_ttl_seconds: float = 60.0       # How long to wait for pending requests
    committed_ttl_seconds: float = 300.0    # How long to cache committed results (5 min)
    rejected_ttl_seconds: float = 60.0      # How long to cache rejections

    # Cache size limits
    max_entries: int = 100_000              # Maximum entries in cache

    # Cleanup interval
    cleanup_interval_seconds: float = 10.0  # How often to run cleanup

    # Behavior settings
    wait_for_pending: bool = True           # Wait for PENDING entries vs immediate reject
    pending_wait_timeout: float = 30.0      # Max wait time for pending entries
```

#### State Transition Diagram

```
┌─────────────────────────────────────────────────────────────────────────┐
│                   IDEMPOTENCY ENTRY STATE MACHINE                        │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│                        ┌─────────────────┐                               │
│                        │                 │                               │
│    new request         │   (not found)   │                               │
│         │              │                 │                               │
│         ▼              └────────┬────────┘                               │
│  ┌──────────────┐               │                                        │
│  │              │◀──────────────┘                                        │
│  │   PENDING    │                                                        │
│  │              │──────┬───────────────┬───────────────┐                │
│  └──────────────┘      │               │               │                │
│                        │               │               │                │
│              success   │     reject    │     timeout   │                │
│                        │               │               │                │
│                        ▼               ▼               ▼                │
│              ┌──────────────┐ ┌──────────────┐ ┌──────────────┐         │
│              │              │ │              │ │              │         │
│              │  COMMITTED   │ │   REJECTED   │ │   EXPIRED    │         │
│              │              │ │              │ │   (removed)  │         │
│              └──────┬───────┘ └──────┬───────┘ └──────────────┘         │
│                     │                │                                   │
│                     │   TTL          │   TTL                            │
│                     │   expires      │   expires                        │
│                     ▼                ▼                                   │
│              ┌──────────────────────────────┐                           │
│              │                              │                           │
│              │     EVICTED (removed)        │                           │
│              │                              │                           │
│              └──────────────────────────────┘                           │
│                                                                          │
│  DUPLICATE HANDLING BY STATE:                                            │
│                                                                          │
│  ┌─────────────┬────────────────────────────────────────────────────┐   │
│  │ State       │ Action on duplicate                                │   │
│  ├─────────────┼────────────────────────────────────────────────────┤   │
│  │ PENDING     │ Wait for original to complete (or timeout)         │   │
│  │ COMMITTED   │ Return cached result immediately                   │   │
│  │ REJECTED    │ Return cached rejection immediately                │   │
│  │ (not found) │ Insert PENDING, process as new request            │   │
│  └─────────────┴────────────────────────────────────────────────────┘   │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘
```

### Part 4: Gate-Level Idempotency Cache

The gate provides fast-path deduplication for client retries:

```python
import asyncio
from collections import OrderedDict
from dataclasses import dataclass, field
from typing import Generic, TypeVar


T = TypeVar("T")


class GateIdempotencyCache(Generic[T]):
    """
    Gate-level idempotency cache for fast-path duplicate detection.

    Design principles:
    - O(1) lookup and insertion
    - LRU eviction when at capacity
    - TTL-based expiration for all entries
    - Waiters for PENDING entries (coalesce duplicate requests)

    This is the first line of defense against duplicates. The manager
    provides authoritative deduplication for cross-gate scenarios.
    """

    def __init__(self, config: IdempotencyConfig):
        self._config = config

        # Main cache: idempotency_key -> entry
        # OrderedDict for LRU ordering
        self._cache: OrderedDict[IdempotencyKey, IdempotencyEntry[T]] = OrderedDict()

        # Waiters for pending entries: idempotency_key -> list of futures
        self._pending_waiters: dict[IdempotencyKey, list[asyncio.Future[T]]] = {}

        # Background cleanup task
        self._cleanup_task: asyncio.Task | None = None
        self._closed = False

    async def start(self) -> None:
        """Start background cleanup task."""
        self._cleanup_task = asyncio.create_task(self._cleanup_loop())

    async def close(self) -> None:
        """Stop cleanup and clear cache."""
        self._closed = True
        if self._cleanup_task:
            self._cleanup_task.cancel()
            try:
                await self._cleanup_task
            except asyncio.CancelledError:
                pass

        # Cancel all waiters
        for waiters in self._pending_waiters.values():
            for waiter in waiters:
                if not waiter.done():
                    waiter.cancel()

        self._cache.clear()
        self._pending_waiters.clear()

    async def check_or_insert(
        self,
        key: IdempotencyKey,
        job_id: str,
        source_gate_id: str,
    ) -> tuple[bool, IdempotencyEntry[T] | None]:
        """
        Check if key exists; if not, insert as PENDING.

        Returns:
            (is_duplicate, entry)
            - (False, None): New request, inserted as PENDING
            - (True, entry): Duplicate found, entry contains status

        If entry is PENDING and config.wait_for_pending is True,
        this will wait for the entry to become terminal.
        """
        # Check cache
        if key in self._cache:
            entry = self._cache[key]

            # Move to end for LRU
            self._cache.move_to_end(key)

            # If terminal, return immediately
            if entry.is_terminal():
                return (True, entry)

            # PENDING - optionally wait
            if self._config.wait_for_pending:
                result = await self._wait_for_pending(key)
                # Re-fetch entry (may have been updated)
                entry = self._cache.get(key)
                return (True, entry)
            else:
                return (True, entry)

        # Not found - insert as PENDING
        entry = IdempotencyEntry(
            idempotency_key=key,
            status=IdempotencyStatus.PENDING,
            job_id=job_id,
            result=None,
            created_at=time.time(),
            committed_at=None,
            source_gate_id=source_gate_id,
        )

        # Evict if at capacity
        while len(self._cache) >= self._config.max_entries:
            # Remove oldest (first item)
            oldest_key, oldest_entry = next(iter(self._cache.items()))
            self._cache.pop(oldest_key)
            # Cancel any waiters for evicted entry
            if oldest_key in self._pending_waiters:
                for waiter in self._pending_waiters.pop(oldest_key):
                    if not waiter.done():
                        waiter.set_exception(
                            TimeoutError("Idempotency entry evicted")
                        )

        self._cache[key] = entry
        return (False, None)

    async def commit(
        self,
        key: IdempotencyKey,
        result: T,
    ) -> None:
        """
        Transition entry from PENDING to COMMITTED with result.

        Notifies any waiters of the result.
        """
        if key not in self._cache:
            return

        entry = self._cache[key]
        if entry.status != IdempotencyStatus.PENDING:
            return  # Already terminal

        # Update entry
        entry.status = IdempotencyStatus.COMMITTED
        entry.result = result
        entry.committed_at = time.time()

        # Notify waiters
        self._notify_waiters(key, result)

    async def reject(
        self,
        key: IdempotencyKey,
        result: T,
    ) -> None:
        """
        Transition entry from PENDING to REJECTED with result.

        Notifies any waiters of the rejection.
        """
        if key not in self._cache:
            return

        entry = self._cache[key]
        if entry.status != IdempotencyStatus.PENDING:
            return  # Already terminal

        # Update entry
        entry.status = IdempotencyStatus.REJECTED
        entry.result = result
        entry.committed_at = time.time()

        # Notify waiters
        self._notify_waiters(key, result)

    def get(self, key: IdempotencyKey) -> IdempotencyEntry[T] | None:
        """Get entry by key without modifying LRU order."""
        return self._cache.get(key)

    async def _wait_for_pending(self, key: IdempotencyKey) -> T | None:
        """Wait for a PENDING entry to become terminal."""
        # Create future for this waiter
        future: asyncio.Future[T] = asyncio.Future()

        if key not in self._pending_waiters:
            self._pending_waiters[key] = []
        self._pending_waiters[key].append(future)

        try:
            return await asyncio.wait_for(
                future,
                timeout=self._config.pending_wait_timeout,
            )
        except asyncio.TimeoutError:
            return None
        finally:
            # Clean up waiter list
            if key in self._pending_waiters:
                try:
                    self._pending_waiters[key].remove(future)
                except ValueError:
                    pass
                if not self._pending_waiters[key]:
                    del self._pending_waiters[key]

    def _notify_waiters(self, key: IdempotencyKey, result: T) -> None:
        """Notify all waiters for a key."""
        if key not in self._pending_waiters:
            return

        for waiter in self._pending_waiters.pop(key):
            if not waiter.done():
                waiter.set_result(result)

    async def _cleanup_loop(self) -> None:
        """Background task to clean up expired entries."""
        while not self._closed:
            try:
                await asyncio.sleep(self._config.cleanup_interval_seconds)
                await self._cleanup_expired()
            except asyncio.CancelledError:
                break
            except Exception:
                # Log but continue
                pass

    async def _cleanup_expired(self) -> None:
        """Remove expired entries from cache."""
        now = time.time()
        expired_keys: list[IdempotencyKey] = []

        for key, entry in self._cache.items():
            ttl = self._get_ttl_for_status(entry.status)
            reference_time = entry.committed_at or entry.created_at

            if now - reference_time > ttl:
                expired_keys.append(key)

        for key in expired_keys:
            self._cache.pop(key, None)
            # Cancel any waiters
            if key in self._pending_waiters:
                for waiter in self._pending_waiters.pop(key):
                    if not waiter.done():
                        waiter.set_exception(
                            TimeoutError("Idempotency entry expired")
                        )

    def _get_ttl_for_status(self, status: IdempotencyStatus) -> float:
        """Get TTL for a given status."""
        if status == IdempotencyStatus.PENDING:
            return self._config.pending_ttl_seconds
        elif status == IdempotencyStatus.COMMITTED:
            return self._config.committed_ttl_seconds
        else:  # REJECTED
            return self._config.rejected_ttl_seconds

    def stats(self) -> dict:
        """Get cache statistics."""
        status_counts = {status: 0 for status in IdempotencyStatus}
        for entry in self._cache.values():
            status_counts[entry.status] += 1

        return {
            "total_entries": len(self._cache),
            "pending_count": status_counts[IdempotencyStatus.PENDING],
            "committed_count": status_counts[IdempotencyStatus.COMMITTED],
            "rejected_count": status_counts[IdempotencyStatus.REJECTED],
            "pending_waiters": sum(len(w) for w in self._pending_waiters.values()),
            "max_entries": self._config.max_entries,
        }
```

### Part 5: Manager-Level Idempotency Ledger

The manager provides authoritative deduplication that survives restarts:

```python
from dataclasses import dataclass
from typing import Generic, TypeVar
import asyncio


T = TypeVar("T")


@dataclass(slots=True)
class IdempotencyLedgerEntry(Generic[T]):
    """
    Persistent idempotency entry stored in manager's WAL.

    This is the authoritative record of whether a request was processed.
    """
    idempotency_key: IdempotencyKey
    job_id: str
    status: IdempotencyStatus
    result_serialized: bytes | None  # Serialized result for response
    created_at: float
    committed_at: float | None

    def to_bytes(self) -> bytes:
        """Serialize for WAL storage."""
        import struct

        key_bytes = str(self.idempotency_key).encode("utf-8")
        job_id_bytes = self.job_id.encode("utf-8")
        result_bytes = self.result_serialized or b""

        # Format: key_len(4) + key + job_id_len(4) + job_id +
        #         status(1) + created_at(8) + committed_at(8) +
        #         result_len(4) + result
        return struct.pack(
            f">I{len(key_bytes)}sI{len(job_id_bytes)}sBddI{len(result_bytes)}s",
            len(key_bytes), key_bytes,
            len(job_id_bytes), job_id_bytes,
            self.status.value,
            self.created_at,
            self.committed_at or 0.0,
            len(result_bytes), result_bytes,
        )

    @classmethod
    def from_bytes(cls, data: bytes) -> "IdempotencyLedgerEntry":
        """Deserialize from WAL storage."""
        import struct

        offset = 0

        key_len = struct.unpack_from(">I", data, offset)[0]
        offset += 4
        key_str = data[offset:offset + key_len].decode("utf-8")
        offset += key_len

        job_id_len = struct.unpack_from(">I", data, offset)[0]
        offset += 4
        job_id = data[offset:offset + job_id_len].decode("utf-8")
        offset += job_id_len

        status_val = struct.unpack_from(">B", data, offset)[0]
        offset += 1

        created_at, committed_at = struct.unpack_from(">dd", data, offset)
        offset += 16

        result_len = struct.unpack_from(">I", data, offset)[0]
        offset += 4
        result_bytes = data[offset:offset + result_len] if result_len > 0 else None

        return cls(
            idempotency_key=IdempotencyKey.parse(key_str),
            job_id=job_id,
            status=IdempotencyStatus(status_val),
            result_serialized=result_bytes,
            created_at=created_at,
            committed_at=committed_at if committed_at > 0 else None,
        )


class ManagerIdempotencyLedger(Generic[T]):
    """
    Manager-level idempotency ledger with WAL persistence.

    This is the authoritative source for idempotency decisions.
    Entries are persisted to WAL before acknowledging to ensure
    crash recovery maintains idempotency guarantees.

    Design:
    - In-memory index for O(1) lookups
    - WAL persistence for crash recovery
    - TTL-based cleanup to bound memory
    - Integration with per-job VSR for cross-DC consistency
    """

    def __init__(
        self,
        config: IdempotencyConfig,
        wal_path: str,
    ):
        self._config = config
        self._wal_path = wal_path

        # In-memory index: idempotency_key -> entry
        self._index: dict[IdempotencyKey, IdempotencyLedgerEntry[T]] = {}

        # Secondary index: job_id -> idempotency_key (for reverse lookup)
        self._job_to_key: dict[str, IdempotencyKey] = {}

        # WAL writer (uses SingleWriterBuffer from AD-39)
        self._wal_writer = None  # Initialized in start()

        # Background cleanup
        self._cleanup_task: asyncio.Task | None = None
        self._closed = False

    async def start(self) -> None:
        """Start ledger and recover from WAL."""
        # Initialize WAL writer
        # self._wal_writer = SingleWriterBuffer(...)
        # await self._wal_writer.open(self._wal_path)

        # Replay WAL to rebuild index
        await self._replay_wal()

        # Start cleanup task
        self._cleanup_task = asyncio.create_task(self._cleanup_loop())

    async def close(self) -> None:
        """Close ledger and flush WAL."""
        self._closed = True

        if self._cleanup_task:
            self._cleanup_task.cancel()
            try:
                await self._cleanup_task
            except asyncio.CancelledError:
                pass

        if self._wal_writer:
            await self._wal_writer.close()

    async def check_or_reserve(
        self,
        key: IdempotencyKey,
        job_id: str,
    ) -> tuple[bool, IdempotencyLedgerEntry[T] | None]:
        """
        Check if key exists; if not, reserve it as PENDING.

        IMPORTANT: Reservation is persisted to WAL before returning
        to ensure crash recovery maintains idempotency.

        Returns:
            (is_duplicate, entry)
            - (False, None): New request, reserved as PENDING
            - (True, entry): Duplicate found
        """
        # Check in-memory index
        if key in self._index:
            return (True, self._index[key])

        # Not found - create and persist PENDING entry
        entry = IdempotencyLedgerEntry(
            idempotency_key=key,
            job_id=job_id,
            status=IdempotencyStatus.PENDING,
            result_serialized=None,
            created_at=time.time(),
            committed_at=None,
        )

        # Persist to WAL BEFORE updating index
        await self._persist_entry(entry)

        # Update indices
        self._index[key] = entry
        self._job_to_key[job_id] = key

        return (False, None)

    async def commit(
        self,
        key: IdempotencyKey,
        result_serialized: bytes,
    ) -> None:
        """
        Commit entry with result.

        Persists to WAL before updating in-memory state.
        """
        if key not in self._index:
            return

        entry = self._index[key]
        if entry.status != IdempotencyStatus.PENDING:
            return  # Already terminal

        # Update entry
        entry.status = IdempotencyStatus.COMMITTED
        entry.result_serialized = result_serialized
        entry.committed_at = time.time()

        # Persist to WAL
        await self._persist_entry(entry)

    async def reject(
        self,
        key: IdempotencyKey,
        result_serialized: bytes,
    ) -> None:
        """
        Reject entry with result.

        Persists to WAL before updating in-memory state.
        """
        if key not in self._index:
            return

        entry = self._index[key]
        if entry.status != IdempotencyStatus.PENDING:
            return  # Already terminal

        # Update entry
        entry.status = IdempotencyStatus.REJECTED
        entry.result_serialized = result_serialized
        entry.committed_at = time.time()

        # Persist to WAL
        await self._persist_entry(entry)

    def get_by_key(self, key: IdempotencyKey) -> IdempotencyLedgerEntry[T] | None:
        """Get entry by idempotency key."""
        return self._index.get(key)

    def get_by_job_id(self, job_id: str) -> IdempotencyLedgerEntry[T] | None:
        """Get entry by job ID (reverse lookup)."""
        key = self._job_to_key.get(job_id)
        if key is None:
            return None
        return self._index.get(key)

    async def _persist_entry(self, entry: IdempotencyLedgerEntry[T]) -> None:
        """Persist entry to WAL."""
        if self._wal_writer:
            entry_bytes = entry.to_bytes()
            await self._wal_writer.write(entry_bytes)
            await self._wal_writer.flush()  # Ensure durability

    async def _replay_wal(self) -> None:
        """Replay WAL to rebuild in-memory index."""
        # Use SingleReaderBuffer from AD-39
        # reader = SingleReaderBuffer(...)
        # await reader.open(self._wal_path)
        #
        # async for entry_bytes in reader.read_entries():
        #     entry = IdempotencyLedgerEntry.from_bytes(entry_bytes.data)
        #     self._index[entry.idempotency_key] = entry
        #     self._job_to_key[entry.job_id] = entry.idempotency_key
        #
        # await reader.close()
        pass

    async def _cleanup_loop(self) -> None:
        """Background cleanup of expired entries."""
        while not self._closed:
            try:
                await asyncio.sleep(self._config.cleanup_interval_seconds)
                await self._cleanup_expired()
            except asyncio.CancelledError:
                break
            except Exception:
                pass

    async def _cleanup_expired(self) -> None:
        """Remove expired entries from index."""
        now = time.time()
        expired_keys: list[IdempotencyKey] = []

        for key, entry in self._index.items():
            ttl = self._get_ttl_for_status(entry.status)
            reference_time = entry.committed_at or entry.created_at

            if now - reference_time > ttl:
                expired_keys.append(key)

        for key in expired_keys:
            entry = self._index.pop(key, None)
            if entry:
                self._job_to_key.pop(entry.job_id, None)

        # Note: WAL cleanup is separate (compaction) to avoid
        # corrupting crash recovery

    def _get_ttl_for_status(self, status: IdempotencyStatus) -> float:
        """Get TTL for a given status."""
        if status == IdempotencyStatus.PENDING:
            return self._config.pending_ttl_seconds
        elif status == IdempotencyStatus.COMMITTED:
            return self._config.committed_ttl_seconds
        else:  # REJECTED
            return self._config.rejected_ttl_seconds
```

### Part 6: Protocol Extensions

#### Extended JobSubmission Message

```python
@dataclass
class JobSubmission(Message):
    """
    Job submission from client to gate or manager.

    Extended with idempotency_key for at-most-once semantics.
    """
    job_id: str                  # Unique job identifier
    workflows: bytes             # Cloudpickled workflows
    vus: int                     # Virtual users per workflow
    timeout_seconds: float       # Maximum execution time
    target_dcs: list[str]        # Target datacenters
    callback_addr: tuple[str, int] | None = None
    reporting_configs: bytes | None = None

    # Protocol version fields (AD-25)
    protocol_version_major: int = 1
    protocol_version_minor: int = 0
    capabilities: str = ""

    # Idempotency fields (AD-40)
    idempotency_key: str = ""    # Client-generated idempotency key
                                 # Format: "{client_id}:{sequence}:{nonce}"
                                 # Empty string = no idempotency (legacy clients)


@dataclass
class JobAck(Message):
    """
    Acknowledgment of job submission.

    Extended with idempotency information.
    """
    job_id: str                  # Job identifier
    accepted: bool               # Whether job was accepted
    error: str | None = None     # Error message if rejected
    queued_position: int = 0     # Position in queue
    leader_addr: tuple[str, int] | None = None

    # Protocol version fields (AD-25)
    protocol_version_major: int = 1
    protocol_version_minor: int = 0
    capabilities: str = ""

    # Idempotency fields (AD-40)
    idempotency_key: str = ""    # Echoed from request
    was_duplicate: bool = False  # True if this was a duplicate submission
    original_job_id: str = ""    # If duplicate, the original job_id
```

### Part 7: End-to-End Flow

```
┌─────────────────────────────────────────────────────────────────────────┐
│                     END-TO-END IDEMPOTENT SUBMISSION                     │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│  ┌──────────┐     ┌──────────┐     ┌──────────┐     ┌──────────┐        │
│  │  Client  │     │   Gate   │     │  Manager │     │  Worker  │        │
│  └────┬─────┘     └────┬─────┘     └────┬─────┘     └────┬─────┘        │
│       │                │                │                │               │
│       │ JobSubmission  │                │                │               │
│       │ idem_key=xyz   │                │                │               │
│       │ job_id=abc     │                │                │               │
│       │───────────────▶│                │                │               │
│       │                │                │                │               │
│       │                │ check cache    │                │               │
│       │                │ idem_key=xyz   │                │               │
│       │                │ NOT FOUND      │                │               │
│       │                │                │                │               │
│       │                │ insert PENDING │                │               │
│       │                │ idem_key=xyz   │                │               │
│       │                │                │                │               │
│       │                │ JobSubmission  │                │               │
│       │                │ idem_key=xyz   │                │               │
│       │                │───────────────▶│                │               │
│       │                │                │                │               │
│       │                │                │ check ledger   │               │
│       │                │                │ idem_key=xyz   │               │
│       │                │                │ NOT FOUND      │               │
│       │                │                │                │               │
│       │                │                │ reserve PENDING│               │
│       │                │                │ persist to WAL │               │
│       │                │                │                │               │
│       │                │                │ process job    │               │
│       │                │                │───────────────▶│               │
│       │                │                │                │ execute       │
│       │                │                │                │               │
│       │                │                │◀───────────────│               │
│       │                │                │                │               │
│       │                │                │ commit ledger  │               │
│       │                │                │ idem_key=xyz   │               │
│       │                │                │ persist to WAL │               │
│       │                │                │                │               │
│       │                │◀───────────────│                │               │
│       │                │ JobAck         │                │               │
│       │                │ job_id=abc     │                │               │
│       │                │                │                │               │
│       │                │ commit cache   │                │               │
│       │                │ idem_key=xyz   │                │               │
│       │                │                │                │               │
│       │◀───────────────│                │                │               │
│       │ JobAck         │                │                │               │
│       │ job_id=abc     │                │                │               │
│       │                │                │                │               │
│       │                │                │                │               │
│  ════════════════════════════════════════════════════════════════════   │
│  CLIENT RETRIES (response was lost):                                     │
│  ════════════════════════════════════════════════════════════════════   │
│       │                │                │                │               │
│       │ JobSubmission  │                │                │               │
│       │ idem_key=xyz   │ ← SAME KEY     │                │               │
│       │ job_id=def     │ ← NEW JOB ID   │                │               │
│       │───────────────▶│                │                │               │
│       │                │                │                │               │
│       │                │ check cache    │                │               │
│       │                │ idem_key=xyz   │                │               │
│       │                │ FOUND:COMMITTED│                │               │
│       │                │                │                │               │
│       │◀───────────────│                │                │               │
│       │ JobAck         │ ← Returns      │                │               │
│       │ job_id=abc     │   cached       │                │               │
│       │ was_dup=true   │   result       │                │               │
│       │                │                │                │               │
│  JOB def IS NEVER CREATED - DUPLICATE DETECTED                          │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘
```

### Part 8: Cross-DC Consistency

#### Integration with Per-Job VSR (AD-38)

Idempotency entries are replicated as part of the job's VSR log:

```python
from dataclasses import dataclass
from enum import Enum, auto


class JobEventType(Enum):
    """Types of job events in the VSR log."""
    JOB_CREATED = auto()
    JOB_CANCELLED = auto()
    JOB_COMPLETED = auto()
    IDEMPOTENCY_RESERVED = auto()   # AD-40: Idempotency reservation
    IDEMPOTENCY_COMMITTED = auto()  # AD-40: Idempotency commit


@dataclass(slots=True)
class IdempotencyReservedEvent:
    """
    Event logged when idempotency key is reserved.

    This event is replicated via VSR to all replicas in the job's
    replica set, ensuring cross-DC consistency.
    """
    idempotency_key: str
    job_id: str
    reserved_at: float
    source_dc: str


@dataclass(slots=True)
class IdempotencyCommittedEvent:
    """
    Event logged when idempotency key is committed.

    Includes serialized result so replicas can respond to
    duplicate requests without contacting the primary.
    """
    idempotency_key: str
    job_id: str
    committed_at: float
    result_serialized: bytes


class JobVSRCoordinatorWithIdempotency(Generic[T]):
    """
    Extended VSR coordinator with idempotency support.

    Idempotency events are logged in the same VSR stream as job
    events, ensuring atomic commitment and consistent ordering.
    """

    async def reserve_idempotency(
        self,
        job_id: str,
        idempotency_key: IdempotencyKey,
        source_dc: str,
    ) -> bool:
        """
        Reserve idempotency key via VSR.

        Returns True if reservation succeeded, False if duplicate.
        """
        # Create reservation event
        event = IdempotencyReservedEvent(
            idempotency_key=str(idempotency_key),
            job_id=job_id,
            reserved_at=time.time(),
            source_dc=source_dc,
        )

        # Write via VSR (prepare + commit)
        # This replicates to all job replicas
        try:
            await self.write(job_id, event)
            return True
        except DuplicateIdempotencyKeyError:
            return False

    async def commit_idempotency(
        self,
        job_id: str,
        idempotency_key: IdempotencyKey,
        result_serialized: bytes,
    ) -> None:
        """
        Commit idempotency key with result via VSR.
        """
        event = IdempotencyCommittedEvent(
            idempotency_key=str(idempotency_key),
            job_id=job_id,
            committed_at=time.time(),
            result_serialized=result_serialized,
        )

        await self.write(job_id, event)
```

#### Cross-DC Deduplication Diagram

```
┌─────────────────────────────────────────────────────────────────────────┐
│                     CROSS-DC IDEMPOTENCY VIA VSR                         │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│  Client submits to DC1, network partition, client retries to DC2         │
│                                                                          │
│  ┌─────────────────────────────┐   ┌─────────────────────────────┐      │
│  │           DC1               │   │           DC2               │      │
│  │  ┌───────┐    ┌─────────┐  │   │  ┌───────┐    ┌─────────┐  │      │
│  │  │ Gate1 │    │ Manager1│  │   │  │ Gate2 │    │ Manager2│  │      │
│  │  │       │    │ (Leader)│  │   │  │       │    │(Replica)│  │      │
│  │  └───┬───┘    └────┬────┘  │   │  └───┬───┘    └────┬────┘  │      │
│  │      │             │       │   │      │             │       │      │
│  └──────┼─────────────┼───────┘   └──────┼─────────────┼───────┘      │
│         │             │                  │             │               │
│         │             │                  │             │               │
│  1. JobSubmission     │                  │             │               │
│     idem_key=xyz      │                  │             │               │
│     ─────────────────▶│                  │             │               │
│                       │                  │             │               │
│  2. Reserve via VSR   │                  │             │               │
│     (Prepare)         │══════════════════╪════════════▶│               │
│                       │                  │             │ 3. Prepare    │
│                       │                  │             │    received   │
│                       │◀═════════════════╪═════════════│    ack sent   │
│  4. Quorum ack        │                  │             │               │
│     → Commit          │══════════════════╪════════════▶│               │
│                       │                  │             │ 5. Commit     │
│                       │                  │             │    applied    │
│                       │                  │             │               │
│  ════════════════════════════════════════════════════════════════════  │
│  NETWORK PARTITION - Client retries to DC2                              │
│  ════════════════════════════════════════════════════════════════════  │
│                       │                  │             │               │
│                       │    6. JobSubmission            │               │
│                       │       idem_key=xyz (SAME)      │               │
│                       │       ──────────────────────▶  │               │
│                       │                  │             │               │
│                       │                  │  7. Check   │               │
│                       │                  │     ledger  │               │
│                       │                  │     FOUND!  │               │
│                       │                  │             │               │
│                       │    8. Return cached result     │               │
│                       │       job_id=abc               │               │
│                       │       was_duplicate=true       │               │
│                       │       ◀────────────────────────│               │
│                       │                  │             │               │
│  DUPLICATE DETECTED AT DC2 VIA REPLICATED LEDGER                        │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘
```

### Part 9: Failure Scenarios

```
┌─────────────────────────────────────────────────────────────────────────┐
│                         FAILURE SCENARIOS                                │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│  SCENARIO 1: Gate crashes after receiving request, before forwarding     │
│  ────────────────────────────────────────────────────────────────────── │
│                                                                          │
│  Client        Gate (crashes)      Manager                               │
│     │              │                  │                                  │
│     │──JobSub─────▶│                  │                                  │
│     │  idem=xyz    │ ╳ CRASH          │                                  │
│     │              │                  │                                  │
│     │──(timeout)───│                  │                                  │
│     │              │                  │                                  │
│     │──JobSub─────▶│ (new gate)       │                                  │
│     │  idem=xyz    │──JobSub─────────▶│  → NEW REQUEST                  │
│     │              │                  │    (gate cache lost)             │
│     │              │◀──JobAck────────│                                  │
│     │◀──JobAck────│                  │                                  │
│                                                                          │
│  OUTCOME: Job created once (manager is authoritative)                    │
│                                                                          │
│  ──────────────────────────────────────────────────────────────────────  │
│                                                                          │
│  SCENARIO 2: Manager crashes after WAL persist, before response          │
│  ────────────────────────────────────────────────────────────────────── │
│                                                                          │
│  Client        Gate            Manager (crashes)                         │
│     │            │                  │                                    │
│     │──JobSub───▶│──JobSub─────────▶│                                   │
│     │  idem=xyz  │                  │──reserve PENDING                  │
│     │            │                  │──persist to WAL                   │
│     │            │                  │  ╳ CRASH                          │
│     │            │                  │                                    │
│     │──(timeout)─│                  │ (manager restarts)                │
│     │            │                  │──replay WAL                       │
│     │            │                  │  xyz=PENDING                      │
│     │──JobSub───▶│──JobSub─────────▶│                                   │
│     │  idem=xyz  │                  │──check ledger                     │
│     │            │                  │  xyz=PENDING                      │
│     │            │                  │──resume processing                │
│     │            │◀──JobAck────────│                                   │
│     │◀──JobAck──│                  │                                    │
│                                                                          │
│  OUTCOME: Job created once (WAL recovery)                                │
│                                                                          │
│  ──────────────────────────────────────────────────────────────────────  │
│                                                                          │
│  SCENARIO 3: Client retries before original completes                    │
│  ────────────────────────────────────────────────────────────────────── │
│                                                                          │
│  Client        Gate                Manager                               │
│     │            │                     │                                 │
│     │──JobSub───▶│──JobSub────────────▶│ t=0                            │
│     │  idem=xyz  │  insert PENDING     │──reserve PENDING               │
│     │            │                     │──start processing              │
│     │            │                     │  (slow...)                     │
│     │            │                     │                                 │
│     │──(timeout, │                     │ t=5s                           │
│     │  retry)────▶│                     │                                │
│     │  idem=xyz  │  check cache        │                                 │
│     │            │  xyz=PENDING        │                                 │
│     │            │  wait...            │                                 │
│     │            │                     │                                 │
│     │            │                     │──complete processing           │
│     │            │◀──JobAck───────────│ t=10s                          │
│     │            │  commit cache       │                                 │
│     │            │  xyz=COMMITTED      │                                 │
│     │            │  notify waiters     │                                 │
│     │◀──JobAck──│                     │                                 │
│                                                                          │
│  OUTCOME: Single response to both requests (waiter pattern)              │
│                                                                          │
│  ──────────────────────────────────────────────────────────────────────  │
│                                                                          │
│  SCENARIO 4: Idempotency key expires, client retries                     │
│  ────────────────────────────────────────────────────────────────────── │
│                                                                          │
│  Client        Gate            Manager                                   │
│     │            │                │                                      │
│     │──JobSub───▶│──JobSub───────▶│ t=0                                 │
│     │  idem=xyz  │                │──create job abc                     │
│     │◀──JobAck──│◀──JobAck──────│                                      │
│     │  job=abc   │                │                                      │
│     │            │                │                                      │
│     │            │  (TTL passes)  │  (TTL passes)                        │
│     │            │  xyz evicted   │  xyz evicted                         │
│     │            │                │                                      │
│     │──JobSub───▶│──JobSub───────▶│ t=TTL+1                             │
│     │  idem=xyz  │  NOT FOUND     │  NOT FOUND                          │
│     │            │                │──create job def (!)                 │
│     │◀──JobAck──│◀──JobAck──────│                                      │
│     │  job=def   │                │                                      │
│                                                                          │
│  OUTCOME: DUPLICATE JOB CREATED (TTL violation)                          │
│                                                                          │
│  MITIGATION: TTL must be > client's maximum retry window                 │
│              Recommend: TTL = 5min, max retry window = 2min              │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘
```

### Part 10: Integration Guide

#### Client-Side Integration

```python
import secrets
from dataclasses import dataclass
from hyperscale.distributed_rewrite.nodes.client import DistributedClient


class IdempotentJobClient:
    """
    Client wrapper that provides idempotent job submissions.

    Usage:
        client = IdempotentJobClient(distributed_client, client_id="myapp-host1")

        # First attempt
        result = await client.submit_job(workflows, ...)

        # If timeout/failure, safe to retry with same params
        # (internally uses same idempotency key for retries)
        result = await client.submit_job_with_retry(workflows, ..., max_retries=3)
    """

    def __init__(self, inner_client: DistributedClient, client_id: str):
        self._client = inner_client
        self._key_generator = IdempotencyKeyGenerator(client_id)

        # Track pending submissions for retry
        self._pending: dict[int, IdempotencyKey] = {}  # seq -> key

    async def submit_job(
        self,
        workflows: list,
        vus: int,
        timeout_seconds: float,
        target_dcs: list[str],
        idempotency_key: IdempotencyKey | None = None,
    ) -> JobAck:
        """
        Submit job with idempotency.

        If idempotency_key is None, generates a new one (new logical request).
        Pass the same key to retry a failed submission.
        """
        if idempotency_key is None:
            idempotency_key = self._key_generator.generate()

        # Submit with idempotency key
        return await self._client.submit_job(
            workflows=workflows,
            vus=vus,
            timeout_seconds=timeout_seconds,
            target_dcs=target_dcs,
            idempotency_key=str(idempotency_key),
        )

    async def submit_job_with_retry(
        self,
        workflows: list,
        vus: int,
        timeout_seconds: float,
        target_dcs: list[str],
        max_retries: int = 3,
        retry_delay_seconds: float = 1.0,
    ) -> JobAck:
        """
        Submit job with automatic retry on failure.

        Uses same idempotency key across retries to ensure at-most-once.
        """
        idempotency_key = self._key_generator.generate()

        last_error: Exception | None = None

        for attempt in range(max_retries + 1):
            try:
                result = await self.submit_job(
                    workflows=workflows,
                    vus=vus,
                    timeout_seconds=timeout_seconds,
                    target_dcs=target_dcs,
                    idempotency_key=idempotency_key,
                )

                if result.was_duplicate:
                    # Our previous attempt succeeded, use that result
                    pass

                return result

            except Exception as e:
                last_error = e
                if attempt < max_retries:
                    await asyncio.sleep(retry_delay_seconds * (2 ** attempt))

        raise last_error
```

#### Gate-Side Integration

```python
class GateJobHandler:
    """
    Gate handler for job submissions with idempotency.
    """

    def __init__(
        self,
        idempotency_cache: GateIdempotencyCache[JobAck],
        manager_client: ManagerClient,
        gate_id: str,
    ):
        self._cache = idempotency_cache
        self._manager = manager_client
        self._gate_id = gate_id

    async def handle_job_submission(
        self,
        submission: JobSubmission,
        client_addr: tuple[str, int],
    ) -> JobAck:
        """
        Handle job submission with idempotency check.
        """
        # Parse idempotency key (empty = legacy client, no idempotency)
        if not submission.idempotency_key:
            # Legacy path - no idempotency
            return await self._forward_to_manager(submission)

        try:
            idem_key = IdempotencyKey.parse(submission.idempotency_key)
        except ValueError:
            # Invalid key format - reject
            return JobAck(
                job_id=submission.job_id,
                accepted=False,
                error="Invalid idempotency key format",
            )

        # Check cache
        is_duplicate, entry = await self._cache.check_or_insert(
            key=idem_key,
            job_id=submission.job_id,
            source_gate_id=self._gate_id,
        )

        if is_duplicate and entry is not None:
            # Return cached result
            if entry.result is not None:
                result = entry.result
                # Mark as duplicate for client awareness
                return JobAck(
                    job_id=result.job_id,
                    accepted=result.accepted,
                    error=result.error,
                    queued_position=result.queued_position,
                    idempotency_key=submission.idempotency_key,
                    was_duplicate=True,
                    original_job_id=entry.job_id or "",
                )
            else:
                # PENDING with no result - shouldn't happen if wait_for_pending=True
                return JobAck(
                    job_id=submission.job_id,
                    accepted=False,
                    error="Request pending, please retry",
                )

        # New request - forward to manager
        try:
            result = await self._forward_to_manager(submission)

            # Commit to cache
            if result.accepted:
                await self._cache.commit(idem_key, result)
            else:
                await self._cache.reject(idem_key, result)

            return result

        except Exception as e:
            # Manager error - don't commit, allow retry
            # Remove PENDING entry so retry can try again
            # (This is safe because manager hasn't committed)
            raise

    async def _forward_to_manager(self, submission: JobSubmission) -> JobAck:
        """Forward submission to manager."""
        return await self._manager.submit_job(submission)
```

#### Manager-Side Integration

```python
class ManagerJobHandler:
    """
    Manager handler for job submissions with idempotency.
    """

    def __init__(
        self,
        idempotency_ledger: ManagerIdempotencyLedger[JobAck],
        job_store: JobStore,
        vsr_coordinator: JobVSRCoordinatorWithIdempotency,
    ):
        self._ledger = idempotency_ledger
        self._jobs = job_store
        self._vsr = vsr_coordinator

    async def handle_job_submission(
        self,
        submission: JobSubmission,
    ) -> JobAck:
        """
        Handle job submission with idempotency check.
        """
        # Parse idempotency key
        if not submission.idempotency_key:
            # Legacy path
            return await self._process_submission(submission)

        try:
            idem_key = IdempotencyKey.parse(submission.idempotency_key)
        except ValueError:
            return JobAck(
                job_id=submission.job_id,
                accepted=False,
                error="Invalid idempotency key format",
            )

        # Check ledger
        is_duplicate, entry = await self._ledger.check_or_reserve(
            key=idem_key,
            job_id=submission.job_id,
        )

        if is_duplicate and entry is not None:
            # Return cached result
            if entry.result_serialized:
                # Deserialize and return
                result = self._deserialize_result(entry.result_serialized)
                return JobAck(
                    job_id=result.job_id,
                    accepted=result.accepted,
                    error=result.error,
                    idempotency_key=submission.idempotency_key,
                    was_duplicate=True,
                    original_job_id=entry.job_id,
                )
            else:
                # Still PENDING - race condition, return pending response
                return JobAck(
                    job_id=submission.job_id,
                    accepted=False,
                    error="Request pending",
                )

        # Process submission
        result = await self._process_submission(submission)

        # Commit to ledger
        result_bytes = self._serialize_result(result)
        if result.accepted:
            await self._ledger.commit(idem_key, result_bytes)
        else:
            await self._ledger.reject(idem_key, result_bytes)

        return result

    async def _process_submission(self, submission: JobSubmission) -> JobAck:
        """Process job submission (create job, dispatch, etc.)."""
        # ... existing job processing logic ...
        pass

    def _serialize_result(self, result: JobAck) -> bytes:
        """Serialize JobAck for storage."""
        import cloudpickle
        return cloudpickle.dumps(result)

    def _deserialize_result(self, data: bytes) -> JobAck:
        """Deserialize JobAck from storage."""
        import cloudpickle
        return cloudpickle.loads(data)
```

### Part 11: Configuration Recommendations

```
┌─────────────────────────────────────────────────────────────────────────┐
│                    CONFIGURATION RECOMMENDATIONS                         │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│  DEPLOYMENT PROFILE         GATE CACHE         MANAGER LEDGER            │
│  ────────────────────────────────────────────────────────────────────── │
│                                                                          │
│  Development/Testing                                                     │
│    pending_ttl:              30s                60s                      │
│    committed_ttl:            60s                120s                     │
│    max_entries:              1,000              10,000                   │
│    cleanup_interval:         5s                 10s                      │
│                                                                          │
│  Production (Single DC)                                                  │
│    pending_ttl:              60s                120s                     │
│    committed_ttl:            300s (5min)        600s (10min)             │
│    max_entries:              100,000            500,000                  │
│    cleanup_interval:         10s                30s                      │
│                                                                          │
│  Production (Multi-DC)                                                   │
│    pending_ttl:              120s               300s                     │
│    committed_ttl:            600s (10min)       1800s (30min)            │
│    max_entries:              100,000            1,000,000                │
│    cleanup_interval:         30s                60s                      │
│                                                                          │
│  RATIONALE:                                                              │
│                                                                          │
│  - pending_ttl: Must exceed slowest expected processing time             │
│  - committed_ttl: Must exceed client's maximum retry window              │
│  - Multi-DC needs longer TTLs due to cross-DC latency                   │
│  - Manager TTLs > Gate TTLs for authoritative dedup                     │
│                                                                          │
│  MEMORY ESTIMATION:                                                      │
│                                                                          │
│  Entry size ≈ 200 bytes (key + metadata + small result)                 │
│                                                                          │
│  100,000 entries × 200 bytes = 20 MB per gate                           │
│  500,000 entries × 200 bytes = 100 MB per manager                       │
│                                                                          │
│  TUNING GUIDELINES:                                                      │
│                                                                          │
│  1. Monitor cache hit rates:                                             │
│     - High hit rate (>5%) suggests aggressive client retries            │
│     - Increase committed_ttl if clients retry after TTL                 │
│                                                                          │
│  2. Monitor eviction rates:                                              │
│     - High eviction suggests max_entries too low                        │
│     - Increase or add more gate/manager capacity                        │
│                                                                          │
│  3. Monitor pending timeouts:                                            │
│     - Frequent timeouts suggest pending_ttl too short                   │
│     - Or indicates manager processing delays                            │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘
```

### Part 12: Correctness Argument

#### At-Most-Once Guarantee

The system provides at-most-once semantics through layered deduplication:

**Layer 1: Gate Cache (Fast Path)**
- Catches retries to the same gate within TTL
- Not authoritative (can lose state on restart)
- Provides latency optimization, not correctness guarantee

**Layer 2: Manager Ledger (Authoritative)**
- WAL-persisted, survives restarts
- Checked on every new request
- Provides the correctness guarantee

**Layer 3: VSR Replication (Cross-DC)**
- Idempotency entries replicated with job events
- Ensures any replica can detect duplicates
- Survives DC-level failures

#### Proof Sketch

**Claim**: A job submission with idempotency key K executes at most once.

**Proof**:

1. **First arrival at any manager**:
   - Manager checks ledger, K not found
   - Manager reserves K (PENDING) in WAL
   - WAL flush ensures reservation survives crash
   - Job processing begins

2. **Duplicate arrival before commit**:
   - If same manager: ledger check finds K=PENDING, waits
   - If different manager (via different gate): VSR replication ensures K seen
   - No duplicate processing starts

3. **Duplicate arrival after commit**:
   - Manager commits K with result in WAL
   - VSR replicates commit to all replicas
   - Any subsequent lookup finds K=COMMITTED, returns cached result

4. **Manager crash during processing**:
   - K=PENDING persisted in WAL
   - On recovery, replay reconstructs PENDING state
   - Client retry finds K=PENDING, waits for completion
   - Processing resumes (not restarted)

5. **TTL expiration**:
   - If K evicted before client retry: duplicate may occur
   - **Mitigation**: TTL must exceed maximum client retry window
   - This is a deployment configuration requirement, not a protocol flaw

**QED**: Under correct configuration (TTL > retry window), at-most-once holds.

#### Failure Mode Analysis

| Failure | Idempotency Preserved? | Notes |
|---------|------------------------|-------|
| Gate crash before forward | Yes | Manager never saw request |
| Gate crash after forward | Yes | Manager has authoritative state |
| Manager crash before WAL | Yes | No state = retry allowed |
| Manager crash after WAL | Yes | WAL recovery restores state |
| Network partition (same DC) | Yes | Manager is single authority |
| Network partition (cross-DC) | Yes | VSR ensures consistency |
| TTL expiration + late retry | **No** | Config issue, not protocol |
| Clock skew affecting TTL | Degraded | Use HLC for TTL if critical |

### Summary: AD-40 Design Decisions

```
┌─────────────────────────────────────────────────────────────────────────┐
│                    AD-40 DESIGN DECISION SUMMARY                         │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│  DECISION                  CHOICE                 RATIONALE              │
│  ────────────────────────────────────────────────────────────────────── │
│                                                                          │
│  Key structure             client:seq:nonce       Collision-resistant,   │
│                                                   restart-safe           │
│                                                                          │
│  Gate cache                LRU + TTL              Fast path, bounded     │
│                                                   memory                 │
│                                                                          │
│  Manager persistence       WAL                    Crash recovery,        │
│                                                   integrates with VSR    │
│                                                                          │
│  Cross-DC consistency      Per-job VSR            Same log as job        │
│                            replication            events = atomic        │
│                                                                          │
│  Pending request handling  Wait + notify          Coalesce duplicates,   │
│                                                   single response        │
│                                                                          │
│  Result caching            Full result            Enables response       │
│                            serialized             without re-processing  │
│                                                                          │
│  TTL strategy              Status-dependent       PENDING short,         │
│                                                   COMMITTED longer       │
│                                                                          │
│  Legacy compatibility      Empty key = no         Gradual migration      │
│                            idempotency            supported              │
│                                                                          │
│  WHY THIS IS MAXIMALLY CORRECT:                                          │
│                                                                          │
│  1. Two-tier dedup (gate + manager) provides defense in depth           │
│  2. WAL persistence survives crashes without re-execution               │
│  3. VSR integration ensures cross-DC consistency atomically             │
│  4. Waiter pattern handles concurrent duplicates elegantly              │
│  5. Bounded memory through LRU + TTL (no unbounded growth)              │
│  6. Explicit failure modes with clear configuration requirements        │
│                                                                          │
└─────────────────────────────────────────────────────────────────────────┘
```
